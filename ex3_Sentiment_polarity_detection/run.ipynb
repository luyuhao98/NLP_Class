{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "WARNING:tensorflow:From C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\train.py:54: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\preprocessing\\text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n",
      "WARNING:tensorflow:From C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\text_cnn.py:78: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\n",
      "\n",
      "2018-10-26T15:49:27.695992: step 1, loss 2.03485, acc 0.53125\n",
      "2018-10-26T15:49:28.084953: step 2, loss 2.01888, acc 0.46875\n",
      "2018-10-26T15:49:28.442001: step 3, loss 2.41012, acc 0.484375\n",
      "2018-10-26T15:49:28.821044: step 4, loss 2.31307, acc 0.515625\n",
      "2018-10-26T15:49:29.246853: step 5, loss 1.76789, acc 0.5\n",
      "2018-10-26T15:49:29.691716: step 6, loss 1.38206, acc 0.578125\n",
      "2018-10-26T15:49:30.015918: step 7, loss 2.13585, acc 0.46875\n",
      "2018-10-26T15:49:30.341922: step 8, loss 2.16953, acc 0.515625\n",
      "2018-10-26T15:49:30.679127: step 9, loss 1.62723, acc 0.53125\n",
      "2018-10-26T15:49:30.979219: step 10, loss 2.5792, acc 0.40625\n",
      "2018-10-26T15:49:31.303353: step 11, loss 1.48477, acc 0.625\n",
      "2018-10-26T15:49:31.696304: step 12, loss 1.56257, acc 0.5625\n",
      "2018-10-26T15:49:32.289717: step 13, loss 1.66722, acc 0.5\n",
      "2018-10-26T15:49:32.594904: step 14, loss 1.94739, acc 0.484375\n",
      "2018-10-26T15:49:33.040714: step 15, loss 1.54287, acc 0.5625\n",
      "2018-10-26T15:49:33.413939: step 16, loss 2.11317, acc 0.4375\n",
      "2018-10-26T15:49:33.744832: step 17, loss 2.46444, acc 0.4375\n",
      "2018-10-26T15:49:34.066970: step 18, loss 2.23025, acc 0.484375\n",
      "2018-10-26T15:49:34.402075: step 19, loss 1.54941, acc 0.609375\n",
      "2018-10-26T15:49:34.694319: step 20, loss 1.91581, acc 0.46875\n",
      "2018-10-26T15:49:35.002472: step 21, loss 2.14989, acc 0.359375\n",
      "2018-10-26T15:49:35.306658: step 22, loss 2.09037, acc 0.421875\n",
      "2018-10-26T15:49:35.594017: step 23, loss 1.18126, acc 0.671875\n",
      "2018-10-26T15:49:35.910046: step 24, loss 1.53379, acc 0.5\n",
      "2018-10-26T15:49:36.253130: step 25, loss 1.66949, acc 0.53125\n",
      "2018-10-26T15:49:36.606186: step 26, loss 1.84405, acc 0.328125\n",
      "2018-10-26T15:49:36.895461: step 27, loss 1.58972, acc 0.53125\n",
      "2018-10-26T15:49:37.318283: step 28, loss 1.27848, acc 0.609375\n",
      "2018-10-26T15:49:37.685305: step 29, loss 1.49568, acc 0.546875\n",
      "2018-10-26T15:49:38.005447: step 30, loss 1.67718, acc 0.484375\n",
      "2018-10-26T15:49:38.328582: step 31, loss 1.67866, acc 0.546875\n",
      "2018-10-26T15:49:38.652716: step 32, loss 1.64511, acc 0.484375\n",
      "2018-10-26T15:49:38.963885: step 33, loss 1.65206, acc 0.453125\n",
      "2018-10-26T15:49:39.289022: step 34, loss 1.93152, acc 0.515625\n",
      "2018-10-26T15:49:39.636094: step 35, loss 1.66214, acc 0.5\n",
      "2018-10-26T15:49:39.961270: step 36, loss 1.50201, acc 0.421875\n",
      "2018-10-26T15:49:40.288357: step 37, loss 1.32409, acc 0.53125\n",
      "2018-10-26T15:49:40.648386: step 38, loss 1.72789, acc 0.5\n",
      "2018-10-26T15:49:40.991510: step 39, loss 1.35277, acc 0.546875\n",
      "2018-10-26T15:49:41.317636: step 40, loss 1.85124, acc 0.5625\n",
      "2018-10-26T15:49:41.642727: step 41, loss 1.89661, acc 0.546875\n",
      "2018-10-26T15:49:41.974841: step 42, loss 1.14697, acc 0.640625\n",
      "2018-10-26T15:49:42.291994: step 43, loss 1.52952, acc 0.421875\n",
      "2018-10-26T15:49:42.617125: step 44, loss 1.68349, acc 0.5625\n",
      "2018-10-26T15:49:43.013068: step 45, loss 1.59411, acc 0.484375\n",
      "2018-10-26T15:49:43.509740: step 46, loss 1.91602, acc 0.46875\n",
      "2018-10-26T15:49:43.985468: step 47, loss 1.62041, acc 0.515625\n",
      "2018-10-26T15:49:44.417316: step 48, loss 1.57829, acc 0.5625\n",
      "2018-10-26T15:49:44.912991: step 49, loss 1.30273, acc 0.5625\n",
      "2018-10-26T15:49:45.404676: step 50, loss 1.63855, acc 0.46875\n",
      "2018-10-26T15:49:45.913318: step 51, loss 1.45418, acc 0.5\n",
      "2018-10-26T15:49:46.395031: step 52, loss 1.56372, acc 0.4375\n",
      "2018-10-26T15:49:46.926612: step 53, loss 2.23858, acc 0.421875\n",
      "2018-10-26T15:49:47.378404: step 54, loss 1.38702, acc 0.484375\n",
      "2018-10-26T15:49:47.800278: step 55, loss 1.48735, acc 0.578125\n",
      "2018-10-26T15:49:48.205193: step 56, loss 1.97966, acc 0.5\n",
      "2018-10-26T15:49:48.538377: step 57, loss 1.33479, acc 0.59375\n",
      "2018-10-26T15:49:49.021047: step 58, loss 1.33844, acc 0.5\n",
      "2018-10-26T15:49:49.331237: step 59, loss 1.66276, acc 0.5\n",
      "2018-10-26T15:49:49.651329: step 60, loss 1.71901, acc 0.5625\n",
      "2018-10-26T15:49:49.977460: step 61, loss 1.79369, acc 0.5\n",
      "2018-10-26T15:49:50.333539: step 62, loss 1.74172, acc 0.484375\n",
      "2018-10-26T15:49:50.655712: step 63, loss 1.40565, acc 0.546875\n",
      "2018-10-26T15:49:50.972801: step 64, loss 2.15295, acc 0.46875\n",
      "2018-10-26T15:49:51.300960: step 65, loss 1.63656, acc 0.453125\n",
      "2018-10-26T15:49:51.635034: step 66, loss 1.327, acc 0.546875\n",
      "2018-10-26T15:49:52.141677: step 67, loss 1.30208, acc 0.46875\n",
      "2018-10-26T15:49:52.654308: step 68, loss 2.42612, acc 0.40625\n",
      "2018-10-26T15:49:53.013404: step 69, loss 1.30321, acc 0.546875\n",
      "2018-10-26T15:49:53.427241: step 70, loss 1.58661, acc 0.515625\n",
      "2018-10-26T15:49:53.760353: step 71, loss 1.39872, acc 0.484375\n",
      "2018-10-26T15:49:54.237085: step 72, loss 1.84201, acc 0.484375\n",
      "2018-10-26T15:49:54.649976: step 73, loss 1.5306, acc 0.515625\n",
      "2018-10-26T15:49:55.041007: step 74, loss 1.60235, acc 0.515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:49:55.407953: step 75, loss 1.26175, acc 0.515625\n",
      "2018-10-26T15:49:55.763998: step 76, loss 1.68342, acc 0.515625\n",
      "2018-10-26T15:49:56.146975: step 77, loss 1.80242, acc 0.4375\n",
      "2018-10-26T15:49:56.663600: step 78, loss 1.37858, acc 0.5\n",
      "2018-10-26T15:49:57.163311: step 79, loss 1.51642, acc 0.5625\n",
      "2018-10-26T15:49:57.619073: step 80, loss 1.52181, acc 0.578125\n",
      "2018-10-26T15:49:58.055875: step 81, loss 1.30056, acc 0.609375\n",
      "2018-10-26T15:49:58.414916: step 82, loss 1.50127, acc 0.484375\n",
      "2018-10-26T15:49:58.789039: step 83, loss 1.66039, acc 0.46875\n",
      "2018-10-26T15:49:59.175918: step 84, loss 0.9456, acc 0.671875\n",
      "2018-10-26T15:49:59.577811: step 85, loss 1.19832, acc 0.53125\n",
      "2018-10-26T15:49:59.973754: step 86, loss 1.20181, acc 0.640625\n",
      "2018-10-26T15:50:00.398640: step 87, loss 1.21694, acc 0.578125\n",
      "2018-10-26T15:50:00.776608: step 88, loss 1.56431, acc 0.46875\n",
      "2018-10-26T15:50:01.171618: step 89, loss 1.25263, acc 0.5\n",
      "2018-10-26T15:50:01.555524: step 90, loss 1.89527, acc 0.390625\n",
      "2018-10-26T15:50:01.870683: step 91, loss 1.7163, acc 0.390625\n",
      "2018-10-26T15:50:02.223743: step 92, loss 1.4295, acc 0.59375\n",
      "2018-10-26T15:50:02.531041: step 93, loss 1.32212, acc 0.53125\n",
      "2018-10-26T15:50:02.886971: step 94, loss 1.84617, acc 0.453125\n",
      "2018-10-26T15:50:03.367683: step 95, loss 1.97221, acc 0.421875\n",
      "2018-10-26T15:50:03.720739: step 96, loss 1.09032, acc 0.578125\n",
      "2018-10-26T15:50:04.073796: step 97, loss 1.44955, acc 0.484375\n",
      "2018-10-26T15:50:04.391946: step 98, loss 1.7647, acc 0.390625\n",
      "2018-10-26T15:50:04.769988: step 99, loss 1.67436, acc 0.5\n",
      "2018-10-26T15:50:05.093076: step 100, loss 1.21051, acc 0.546875\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:50:06.065474: step 100, loss 0.996028, acc 0.533771\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-100\n",
      "\n",
      "2018-10-26T15:50:06.744751: step 101, loss 1.49993, acc 0.5625\n",
      "2018-10-26T15:50:07.105738: step 102, loss 1.44185, acc 0.515625\n",
      "2018-10-26T15:50:07.677172: step 103, loss 1.22562, acc 0.5625\n",
      "2018-10-26T15:50:08.024244: step 104, loss 1.20796, acc 0.640625\n",
      "2018-10-26T15:50:08.350372: step 105, loss 1.56979, acc 0.53125\n",
      "2018-10-26T15:50:08.717390: step 106, loss 1.50675, acc 0.515625\n",
      "2018-10-26T15:50:09.171222: step 107, loss 1.36377, acc 0.5\n",
      "2018-10-26T15:50:09.531247: step 108, loss 1.35367, acc 0.5\n",
      "2018-10-26T15:50:09.870309: step 109, loss 1.77521, acc 0.46875\n",
      "2018-10-26T15:50:10.217381: step 110, loss 1.15158, acc 0.5625\n",
      "2018-10-26T15:50:10.528549: step 111, loss 0.919841, acc 0.578125\n",
      "2018-10-26T15:50:10.873742: step 112, loss 1.3413, acc 0.53125\n",
      "2018-10-26T15:50:11.237686: step 113, loss 0.944549, acc 0.640625\n",
      "2018-10-26T15:50:11.567777: step 114, loss 1.07001, acc 0.578125\n",
      "2018-10-26T15:50:11.946939: step 115, loss 1.27821, acc 0.53125\n",
      "2018-10-26T15:50:12.333728: step 116, loss 1.52737, acc 0.46875\n",
      "2018-10-26T15:50:12.662934: step 117, loss 1.56563, acc 0.53125\n",
      "2018-10-26T15:50:13.090707: step 118, loss 1.27492, acc 0.578125\n",
      "2018-10-26T15:50:13.525556: step 119, loss 1.17893, acc 0.578125\n",
      "2018-10-26T15:50:13.911571: step 120, loss 1.11754, acc 0.578125\n",
      "2018-10-26T15:50:14.230660: step 121, loss 1.62323, acc 0.484375\n",
      "2018-10-26T15:50:14.635814: step 122, loss 1.26671, acc 0.53125\n",
      "2018-10-26T15:50:15.018553: step 123, loss 1.23961, acc 0.546875\n",
      "2018-10-26T15:50:15.406517: step 124, loss 1.24476, acc 0.578125\n",
      "2018-10-26T15:50:15.802460: step 125, loss 1.47527, acc 0.453125\n",
      "2018-10-26T15:50:16.174464: step 126, loss 1.12687, acc 0.59375\n",
      "2018-10-26T15:50:16.552455: step 127, loss 1.16082, acc 0.609375\n",
      "2018-10-26T15:50:16.889583: step 128, loss 1.44671, acc 0.5\n",
      "2018-10-26T15:50:17.242610: step 129, loss 1.24067, acc 0.5625\n",
      "2018-10-26T15:50:17.583805: step 130, loss 1.25001, acc 0.5625\n",
      "2018-10-26T15:50:18.094663: step 131, loss 1.20465, acc 0.59375\n",
      "2018-10-26T15:50:18.603977: step 132, loss 1.23729, acc 0.5625\n",
      "2018-10-26T15:50:19.030837: step 133, loss 1.36127, acc 0.515625\n",
      "2018-10-26T15:50:19.401842: step 134, loss 1.34678, acc 0.453125\n",
      "2018-10-26T15:50:19.813740: step 135, loss 1.37127, acc 0.453125\n",
      "2018-10-26T15:50:20.303432: step 136, loss 1.2461, acc 0.53125\n",
      "2018-10-26T15:50:20.646515: step 137, loss 1.19593, acc 0.5\n",
      "2018-10-26T15:50:20.991595: step 138, loss 0.976123, acc 0.578125\n",
      "2018-10-26T15:50:21.319722: step 139, loss 1.22732, acc 0.5\n",
      "2018-10-26T15:50:21.657848: step 140, loss 1.25593, acc 0.515625\n",
      "2018-10-26T15:50:22.028823: step 141, loss 1.28798, acc 0.53125\n",
      "2018-10-26T15:50:22.351002: step 142, loss 1.12618, acc 0.546875\n",
      "2018-10-26T15:50:22.670109: step 143, loss 0.937461, acc 0.609375\n",
      "2018-10-26T15:50:23.029151: step 144, loss 1.39039, acc 0.59375\n",
      "2018-10-26T15:50:23.349294: step 145, loss 0.980796, acc 0.609375\n",
      "2018-10-26T15:50:23.668443: step 146, loss 1.12499, acc 0.53125\n",
      "2018-10-26T15:50:24.010778: step 147, loss 1.42522, acc 0.46875\n",
      "2018-10-26T15:50:24.341695: step 148, loss 1.08846, acc 0.640625\n",
      "2018-10-26T15:50:24.682733: step 149, loss 1.07506, acc 0.546875\n",
      "2018-10-26T15:50:25.019837: step 150, loss 1.28837, acc 0.566667\n",
      "2018-10-26T15:50:25.347762: step 151, loss 1.01014, acc 0.671875\n",
      "2018-10-26T15:50:25.704003: step 152, loss 1.07958, acc 0.59375\n",
      "2018-10-26T15:50:26.129865: step 153, loss 0.887305, acc 0.625\n",
      "2018-10-26T15:50:26.670423: step 154, loss 0.900988, acc 0.671875\n",
      "2018-10-26T15:50:27.016500: step 155, loss 1.06188, acc 0.5625\n",
      "2018-10-26T15:50:27.330703: step 156, loss 0.920709, acc 0.625\n",
      "2018-10-26T15:50:27.690751: step 157, loss 0.793048, acc 0.65625\n",
      "2018-10-26T15:50:28.028791: step 158, loss 0.844991, acc 0.6875\n",
      "2018-10-26T15:50:28.325997: step 159, loss 1.20573, acc 0.59375\n",
      "2018-10-26T15:50:28.612232: step 160, loss 1.17541, acc 0.515625\n",
      "2018-10-26T15:50:28.933374: step 161, loss 0.953178, acc 0.59375\n",
      "2018-10-26T15:50:29.253520: step 162, loss 1.12583, acc 0.546875\n",
      "2018-10-26T15:50:29.575674: step 163, loss 1.10626, acc 0.578125\n",
      "2018-10-26T15:50:29.906774: step 164, loss 0.764963, acc 0.640625\n",
      "2018-10-26T15:50:30.248861: step 165, loss 1.13412, acc 0.515625\n",
      "2018-10-26T15:50:30.573995: step 166, loss 0.821338, acc 0.65625\n",
      "2018-10-26T15:50:30.944063: step 167, loss 1.10399, acc 0.59375\n",
      "2018-10-26T15:50:31.266141: step 168, loss 0.861054, acc 0.640625\n",
      "2018-10-26T15:50:31.599255: step 169, loss 1.03052, acc 0.578125\n",
      "2018-10-26T15:50:31.935514: step 170, loss 0.872243, acc 0.671875\n",
      "2018-10-26T15:50:32.341366: step 171, loss 0.898402, acc 0.59375\n",
      "2018-10-26T15:50:32.768131: step 172, loss 1.01674, acc 0.609375\n",
      "2018-10-26T15:50:33.138139: step 173, loss 1.08984, acc 0.5625\n",
      "2018-10-26T15:50:33.528097: step 174, loss 0.936471, acc 0.59375\n",
      "2018-10-26T15:50:33.857246: step 175, loss 1.11311, acc 0.625\n",
      "2018-10-26T15:50:34.159410: step 176, loss 0.878084, acc 0.640625\n",
      "2018-10-26T15:50:34.590266: step 177, loss 1.38095, acc 0.546875\n",
      "2018-10-26T15:50:35.063029: step 178, loss 1.07354, acc 0.5625\n",
      "2018-10-26T15:50:35.373196: step 179, loss 0.994427, acc 0.5625\n",
      "2018-10-26T15:50:35.768147: step 180, loss 0.91378, acc 0.625\n",
      "2018-10-26T15:50:36.134139: step 181, loss 0.897274, acc 0.640625\n",
      "2018-10-26T15:50:36.535065: step 182, loss 1.18631, acc 0.515625\n",
      "2018-10-26T15:50:36.931317: step 183, loss 0.770514, acc 0.671875\n",
      "2018-10-26T15:50:37.307001: step 184, loss 0.81307, acc 0.640625\n",
      "2018-10-26T15:50:37.705936: step 185, loss 1.17177, acc 0.578125\n",
      "2018-10-26T15:50:38.072185: step 186, loss 0.947361, acc 0.484375\n",
      "2018-10-26T15:50:38.480919: step 187, loss 1.01372, acc 0.5625\n",
      "2018-10-26T15:50:38.875205: step 188, loss 0.986386, acc 0.59375\n",
      "2018-10-26T15:50:39.274744: step 189, loss 0.903269, acc 0.65625\n",
      "2018-10-26T15:50:39.626803: step 190, loss 0.975877, acc 0.5625\n",
      "2018-10-26T15:50:40.081589: step 191, loss 0.635549, acc 0.71875\n",
      "2018-10-26T15:50:40.544359: step 192, loss 0.9426, acc 0.59375\n",
      "2018-10-26T15:50:40.950383: step 193, loss 1.07174, acc 0.5625\n",
      "2018-10-26T15:50:41.396078: step 194, loss 1.16793, acc 0.484375\n",
      "2018-10-26T15:50:41.716221: step 195, loss 0.765321, acc 0.671875\n",
      "2018-10-26T15:50:42.038390: step 196, loss 0.842245, acc 0.671875\n",
      "2018-10-26T15:50:42.411367: step 197, loss 1.04011, acc 0.515625\n",
      "2018-10-26T15:50:42.756441: step 198, loss 1.18403, acc 0.5625\n",
      "2018-10-26T15:50:43.122465: step 199, loss 1.02721, acc 0.484375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:50:43.483537: step 200, loss 1.07758, acc 0.484375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:50:44.296327: step 200, loss 0.690392, acc 0.58349\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-200\n",
      "\n",
      "2018-10-26T15:50:44.842869: step 201, loss 0.827363, acc 0.625\n",
      "2018-10-26T15:50:45.171987: step 202, loss 0.88798, acc 0.578125\n",
      "2018-10-26T15:50:45.617990: step 203, loss 0.700315, acc 0.640625\n",
      "2018-10-26T15:50:45.964872: step 204, loss 0.940576, acc 0.59375\n",
      "2018-10-26T15:50:46.306956: step 205, loss 0.861991, acc 0.609375\n",
      "2018-10-26T15:50:46.631158: step 206, loss 0.721926, acc 0.703125\n",
      "2018-10-26T15:50:46.950237: step 207, loss 0.935707, acc 0.59375\n",
      "2018-10-26T15:50:47.300300: step 208, loss 0.887399, acc 0.65625\n",
      "2018-10-26T15:50:47.654355: step 209, loss 0.913234, acc 0.53125\n",
      "2018-10-26T15:50:48.045523: step 210, loss 0.907982, acc 0.53125\n",
      "2018-10-26T15:50:48.530015: step 211, loss 0.791943, acc 0.640625\n",
      "2018-10-26T15:50:48.967891: step 212, loss 0.783445, acc 0.6875\n",
      "2018-10-26T15:50:49.434598: step 213, loss 1.00047, acc 0.53125\n",
      "2018-10-26T15:50:49.833532: step 214, loss 1.24574, acc 0.46875\n",
      "2018-10-26T15:50:50.317240: step 215, loss 0.762357, acc 0.5625\n",
      "2018-10-26T15:50:50.788025: step 216, loss 0.735899, acc 0.625\n",
      "2018-10-26T15:50:51.308592: step 217, loss 1.07356, acc 0.515625\n",
      "2018-10-26T15:50:51.701541: step 218, loss 1.02205, acc 0.53125\n",
      "2018-10-26T15:50:52.116432: step 219, loss 0.911595, acc 0.53125\n",
      "2018-10-26T15:50:52.530330: step 220, loss 0.808676, acc 0.53125\n",
      "2018-10-26T15:50:52.955193: step 221, loss 0.633651, acc 0.734375\n",
      "2018-10-26T15:50:53.355123: step 222, loss 0.764419, acc 0.703125\n",
      "2018-10-26T15:50:53.764031: step 223, loss 0.757286, acc 0.578125\n",
      "2018-10-26T15:50:54.087168: step 224, loss 0.942348, acc 0.546875\n",
      "2018-10-26T15:50:54.433242: step 225, loss 0.947501, acc 0.609375\n",
      "2018-10-26T15:50:54.768349: step 226, loss 0.921554, acc 0.609375\n",
      "2018-10-26T15:50:55.129387: step 227, loss 0.85567, acc 0.640625\n",
      "2018-10-26T15:50:55.467626: step 228, loss 0.78563, acc 0.609375\n",
      "2018-10-26T15:50:55.798598: step 229, loss 0.86201, acc 0.640625\n",
      "2018-10-26T15:50:56.135723: step 230, loss 1.37113, acc 0.515625\n",
      "2018-10-26T15:50:56.452846: step 231, loss 0.743318, acc 0.609375\n",
      "2018-10-26T15:50:56.780969: step 232, loss 0.794979, acc 0.578125\n",
      "2018-10-26T15:50:57.122059: step 233, loss 0.816448, acc 0.578125\n",
      "2018-10-26T15:50:57.450185: step 234, loss 0.924639, acc 0.515625\n",
      "2018-10-26T15:50:57.803238: step 235, loss 0.675267, acc 0.75\n",
      "2018-10-26T15:50:58.136348: step 236, loss 0.966441, acc 0.59375\n",
      "2018-10-26T15:50:58.471454: step 237, loss 1.02562, acc 0.5625\n",
      "2018-10-26T15:50:58.814539: step 238, loss 0.693172, acc 0.609375\n",
      "2018-10-26T15:50:59.143657: step 239, loss 0.860206, acc 0.578125\n",
      "2018-10-26T15:50:59.482779: step 240, loss 0.884364, acc 0.609375\n",
      "2018-10-26T15:50:59.795915: step 241, loss 0.983481, acc 0.625\n",
      "2018-10-26T15:51:00.138002: step 242, loss 0.801858, acc 0.59375\n",
      "2018-10-26T15:51:00.476097: step 243, loss 0.97131, acc 0.5625\n",
      "2018-10-26T15:51:00.783275: step 244, loss 0.95434, acc 0.546875\n",
      "2018-10-26T15:51:01.109419: step 245, loss 0.823534, acc 0.609375\n",
      "2018-10-26T15:51:01.441520: step 246, loss 0.871671, acc 0.609375\n",
      "2018-10-26T15:51:01.751887: step 247, loss 0.777247, acc 0.71875\n",
      "2018-10-26T15:51:02.062859: step 248, loss 1.13446, acc 0.5\n",
      "2018-10-26T15:51:02.403003: step 249, loss 0.813095, acc 0.65625\n",
      "2018-10-26T15:51:02.769972: step 250, loss 0.822891, acc 0.640625\n",
      "2018-10-26T15:51:03.081211: step 251, loss 0.985891, acc 0.5\n",
      "2018-10-26T15:51:03.395297: step 252, loss 0.779657, acc 0.6875\n",
      "2018-10-26T15:51:03.774286: step 253, loss 0.905823, acc 0.546875\n",
      "2018-10-26T15:51:04.167238: step 254, loss 0.90984, acc 0.53125\n",
      "2018-10-26T15:51:04.503337: step 255, loss 0.789911, acc 0.65625\n",
      "2018-10-26T15:51:04.846423: step 256, loss 0.792247, acc 0.609375\n",
      "2018-10-26T15:51:05.228400: step 257, loss 0.773064, acc 0.59375\n",
      "2018-10-26T15:51:05.617364: step 258, loss 0.816218, acc 0.59375\n",
      "2018-10-26T15:51:05.999342: step 259, loss 0.692892, acc 0.671875\n",
      "2018-10-26T15:51:06.354391: step 260, loss 0.785351, acc 0.59375\n",
      "2018-10-26T15:51:06.769283: step 261, loss 0.642514, acc 0.671875\n",
      "2018-10-26T15:51:07.162232: step 262, loss 0.836689, acc 0.5625\n",
      "2018-10-26T15:51:07.499332: step 263, loss 0.645771, acc 0.71875\n",
      "2018-10-26T15:51:07.858377: step 264, loss 0.694825, acc 0.578125\n",
      "2018-10-26T15:51:08.263296: step 265, loss 0.776301, acc 0.609375\n",
      "2018-10-26T15:51:08.649259: step 266, loss 0.691947, acc 0.6875\n",
      "2018-10-26T15:51:09.002324: step 267, loss 0.820947, acc 0.59375\n",
      "2018-10-26T15:51:09.382350: step 268, loss 0.668571, acc 0.671875\n",
      "2018-10-26T15:51:09.836088: step 269, loss 0.789323, acc 0.640625\n",
      "2018-10-26T15:51:10.199191: step 270, loss 0.9501, acc 0.640625\n",
      "2018-10-26T15:51:10.491337: step 271, loss 0.706491, acc 0.59375\n",
      "2018-10-26T15:51:10.795525: step 272, loss 0.73767, acc 0.65625\n",
      "2018-10-26T15:51:11.103722: step 273, loss 0.723215, acc 0.671875\n",
      "2018-10-26T15:51:11.433819: step 274, loss 0.681443, acc 0.578125\n",
      "2018-10-26T15:51:11.899577: step 275, loss 0.669184, acc 0.671875\n",
      "2018-10-26T15:51:12.313469: step 276, loss 0.842181, acc 0.5625\n",
      "2018-10-26T15:51:12.628630: step 277, loss 0.981715, acc 0.578125\n",
      "2018-10-26T15:51:12.966765: step 278, loss 0.738059, acc 0.671875\n",
      "2018-10-26T15:51:13.295848: step 279, loss 0.570089, acc 0.75\n",
      "2018-10-26T15:51:13.665857: step 280, loss 0.683437, acc 0.640625\n",
      "2018-10-26T15:51:14.033873: step 281, loss 0.918286, acc 0.53125\n",
      "2018-10-26T15:51:14.347036: step 282, loss 0.828073, acc 0.625\n",
      "2018-10-26T15:51:14.792854: step 283, loss 1.02352, acc 0.484375\n",
      "2018-10-26T15:51:15.253613: step 284, loss 0.85157, acc 0.578125\n",
      "2018-10-26T15:51:15.601946: step 285, loss 0.856636, acc 0.609375\n",
      "2018-10-26T15:51:15.941776: step 286, loss 0.912148, acc 0.546875\n",
      "2018-10-26T15:51:16.278876: step 287, loss 0.914364, acc 0.625\n",
      "2018-10-26T15:51:16.608116: step 288, loss 0.678142, acc 0.71875\n",
      "2018-10-26T15:51:16.948087: step 289, loss 0.677244, acc 0.640625\n",
      "2018-10-26T15:51:17.285186: step 290, loss 0.831688, acc 0.625\n",
      "2018-10-26T15:51:17.612356: step 291, loss 0.834145, acc 0.59375\n",
      "2018-10-26T15:51:17.956394: step 292, loss 0.784815, acc 0.609375\n",
      "2018-10-26T15:51:18.299475: step 293, loss 0.761307, acc 0.640625\n",
      "2018-10-26T15:51:18.651537: step 294, loss 0.836233, acc 0.5625\n",
      "2018-10-26T15:51:19.014568: step 295, loss 0.822942, acc 0.578125\n",
      "2018-10-26T15:51:19.360690: step 296, loss 0.799784, acc 0.59375\n",
      "2018-10-26T15:51:19.875270: step 297, loss 0.758798, acc 0.640625\n",
      "2018-10-26T15:51:20.280186: step 298, loss 0.76523, acc 0.625\n",
      "2018-10-26T15:51:20.600328: step 299, loss 0.571345, acc 0.71875\n",
      "2018-10-26T15:51:20.912495: step 300, loss 0.926321, acc 0.583333\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:51:21.687423: step 300, loss 0.646022, acc 0.621013\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-300\n",
      "\n",
      "2018-10-26T15:51:22.621929: step 301, loss 0.660766, acc 0.703125\n",
      "2018-10-26T15:51:23.144531: step 302, loss 0.587757, acc 0.796875\n",
      "2018-10-26T15:51:23.512617: step 303, loss 0.699097, acc 0.6875\n",
      "2018-10-26T15:51:23.847683: step 304, loss 0.614004, acc 0.71875\n",
      "2018-10-26T15:51:24.167797: step 305, loss 0.862018, acc 0.546875\n",
      "2018-10-26T15:51:24.514869: step 306, loss 0.652134, acc 0.671875\n",
      "2018-10-26T15:51:24.851968: step 307, loss 0.728332, acc 0.65625\n",
      "2018-10-26T15:51:25.211012: step 308, loss 0.737346, acc 0.65625\n",
      "2018-10-26T15:51:25.550105: step 309, loss 0.588606, acc 0.765625\n",
      "2018-10-26T15:51:25.880267: step 310, loss 0.729618, acc 0.671875\n",
      "2018-10-26T15:51:26.214330: step 311, loss 0.576434, acc 0.734375\n",
      "2018-10-26T15:51:26.535474: step 312, loss 0.647945, acc 0.6875\n",
      "2018-10-26T15:51:26.870658: step 313, loss 0.49708, acc 0.703125\n",
      "2018-10-26T15:51:27.199697: step 314, loss 0.587168, acc 0.703125\n",
      "2018-10-26T15:51:27.522876: step 315, loss 0.64024, acc 0.71875\n",
      "2018-10-26T15:51:27.854949: step 316, loss 0.688592, acc 0.6875\n",
      "2018-10-26T15:51:28.180078: step 317, loss 0.673943, acc 0.609375\n",
      "2018-10-26T15:51:28.502218: step 318, loss 0.691666, acc 0.609375\n",
      "2018-10-26T15:51:29.037793: step 319, loss 0.61155, acc 0.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:51:29.437785: step 320, loss 0.654639, acc 0.65625\n",
      "2018-10-26T15:51:29.842634: step 321, loss 0.719405, acc 0.625\n",
      "2018-10-26T15:51:30.161835: step 322, loss 0.76062, acc 0.609375\n",
      "2018-10-26T15:51:30.497992: step 323, loss 0.681456, acc 0.6875\n",
      "2018-10-26T15:51:30.841997: step 324, loss 0.820778, acc 0.640625\n",
      "2018-10-26T15:51:31.180060: step 325, loss 0.765801, acc 0.609375\n",
      "2018-10-26T15:51:31.499218: step 326, loss 0.487255, acc 0.8125\n",
      "2018-10-26T15:51:31.839303: step 327, loss 0.609392, acc 0.671875\n",
      "2018-10-26T15:51:32.184380: step 328, loss 0.56987, acc 0.71875\n",
      "2018-10-26T15:51:32.519481: step 329, loss 0.704421, acc 0.640625\n",
      "2018-10-26T15:51:32.924438: step 330, loss 0.553361, acc 0.796875\n",
      "2018-10-26T15:51:33.216641: step 331, loss 0.785269, acc 0.609375\n",
      "2018-10-26T15:51:33.515890: step 332, loss 0.841242, acc 0.515625\n",
      "2018-10-26T15:51:33.809036: step 333, loss 0.748741, acc 0.625\n",
      "2018-10-26T15:51:34.099359: step 334, loss 0.762678, acc 0.65625\n",
      "2018-10-26T15:51:34.381507: step 335, loss 0.738612, acc 0.59375\n",
      "2018-10-26T15:51:34.679709: step 336, loss 0.64987, acc 0.6875\n",
      "2018-10-26T15:51:34.986908: step 337, loss 0.652423, acc 0.734375\n",
      "2018-10-26T15:51:35.299076: step 338, loss 0.604401, acc 0.765625\n",
      "2018-10-26T15:51:35.596299: step 339, loss 0.572651, acc 0.703125\n",
      "2018-10-26T15:51:35.900718: step 340, loss 0.605702, acc 0.65625\n",
      "2018-10-26T15:51:36.235552: step 341, loss 0.613436, acc 0.703125\n",
      "2018-10-26T15:51:36.541780: step 342, loss 0.551802, acc 0.6875\n",
      "2018-10-26T15:51:36.854897: step 343, loss 0.622288, acc 0.671875\n",
      "2018-10-26T15:51:37.160082: step 344, loss 0.576461, acc 0.71875\n",
      "2018-10-26T15:51:37.450340: step 345, loss 0.529582, acc 0.703125\n",
      "2018-10-26T15:51:37.726570: step 346, loss 0.569308, acc 0.65625\n",
      "2018-10-26T15:51:38.020783: step 347, loss 0.702771, acc 0.625\n",
      "2018-10-26T15:51:38.313030: step 348, loss 0.72237, acc 0.609375\n",
      "2018-10-26T15:51:38.607215: step 349, loss 0.493293, acc 0.8125\n",
      "2018-10-26T15:51:38.920382: step 350, loss 0.591434, acc 0.71875\n",
      "2018-10-26T15:51:39.220619: step 351, loss 0.451266, acc 0.796875\n",
      "2018-10-26T15:51:39.553687: step 352, loss 0.500931, acc 0.703125\n",
      "2018-10-26T15:51:39.864856: step 353, loss 0.60331, acc 0.703125\n",
      "2018-10-26T15:51:40.174064: step 354, loss 0.651082, acc 0.65625\n",
      "2018-10-26T15:51:40.489187: step 355, loss 0.655622, acc 0.65625\n",
      "2018-10-26T15:51:40.789387: step 356, loss 0.496696, acc 0.78125\n",
      "2018-10-26T15:51:41.127481: step 357, loss 0.562355, acc 0.703125\n",
      "2018-10-26T15:51:41.485528: step 358, loss 0.602875, acc 0.6875\n",
      "2018-10-26T15:51:41.806718: step 359, loss 0.74805, acc 0.609375\n",
      "2018-10-26T15:51:42.145763: step 360, loss 0.606908, acc 0.6875\n",
      "2018-10-26T15:51:42.478871: step 361, loss 0.558715, acc 0.6875\n",
      "2018-10-26T15:51:42.782122: step 362, loss 0.746924, acc 0.625\n",
      "2018-10-26T15:51:43.114175: step 363, loss 0.820428, acc 0.46875\n",
      "2018-10-26T15:51:43.551007: step 364, loss 0.652952, acc 0.625\n",
      "2018-10-26T15:51:43.877136: step 365, loss 0.629967, acc 0.65625\n",
      "2018-10-26T15:51:44.150405: step 366, loss 0.668476, acc 0.703125\n",
      "2018-10-26T15:51:44.477533: step 367, loss 0.578857, acc 0.6875\n",
      "2018-10-26T15:51:44.853528: step 368, loss 0.623772, acc 0.6875\n",
      "2018-10-26T15:51:45.191624: step 369, loss 0.513452, acc 0.78125\n",
      "2018-10-26T15:51:45.489826: step 370, loss 0.507721, acc 0.75\n",
      "2018-10-26T15:51:45.846872: step 371, loss 0.68916, acc 0.65625\n",
      "2018-10-26T15:51:46.190955: step 372, loss 0.7703, acc 0.578125\n",
      "2018-10-26T15:51:46.574928: step 373, loss 0.641562, acc 0.671875\n",
      "2018-10-26T15:51:46.914091: step 374, loss 0.501473, acc 0.71875\n",
      "2018-10-26T15:51:47.200257: step 375, loss 0.489387, acc 0.734375\n",
      "2018-10-26T15:51:47.522397: step 376, loss 0.622983, acc 0.71875\n",
      "2018-10-26T15:51:47.835561: step 377, loss 0.658416, acc 0.65625\n",
      "2018-10-26T15:51:48.146765: step 378, loss 0.670186, acc 0.6875\n",
      "2018-10-26T15:51:48.459891: step 379, loss 0.554332, acc 0.734375\n",
      "2018-10-26T15:51:48.805043: step 380, loss 0.502545, acc 0.734375\n",
      "2018-10-26T15:51:49.116176: step 381, loss 0.689944, acc 0.671875\n",
      "2018-10-26T15:51:49.483157: step 382, loss 0.621354, acc 0.6875\n",
      "2018-10-26T15:51:49.827237: step 383, loss 0.510738, acc 0.71875\n",
      "2018-10-26T15:51:50.133419: step 384, loss 0.578993, acc 0.71875\n",
      "2018-10-26T15:51:50.455559: step 385, loss 0.567052, acc 0.703125\n",
      "2018-10-26T15:51:50.756755: step 386, loss 0.699794, acc 0.625\n",
      "2018-10-26T15:51:51.080916: step 387, loss 0.665441, acc 0.65625\n",
      "2018-10-26T15:51:51.388069: step 388, loss 0.684488, acc 0.5625\n",
      "2018-10-26T15:51:51.694250: step 389, loss 0.620174, acc 0.71875\n",
      "2018-10-26T15:51:52.047305: step 390, loss 0.482717, acc 0.75\n",
      "2018-10-26T15:51:52.363460: step 391, loss 0.548543, acc 0.75\n",
      "2018-10-26T15:51:52.664701: step 392, loss 0.704499, acc 0.609375\n",
      "2018-10-26T15:51:52.938923: step 393, loss 0.581527, acc 0.703125\n",
      "2018-10-26T15:51:53.274085: step 394, loss 0.722504, acc 0.671875\n",
      "2018-10-26T15:51:53.652020: step 395, loss 0.647974, acc 0.65625\n",
      "2018-10-26T15:51:54.088918: step 396, loss 0.653276, acc 0.71875\n",
      "2018-10-26T15:51:54.437065: step 397, loss 0.523124, acc 0.75\n",
      "2018-10-26T15:51:54.797957: step 398, loss 0.536417, acc 0.796875\n",
      "2018-10-26T15:51:55.188964: step 399, loss 0.605911, acc 0.703125\n",
      "2018-10-26T15:51:55.622989: step 400, loss 0.773336, acc 0.609375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:51:56.660979: step 400, loss 0.675442, acc 0.586304\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-400\n",
      "\n",
      "2018-10-26T15:51:57.339195: step 401, loss 0.508999, acc 0.78125\n",
      "2018-10-26T15:51:57.791957: step 402, loss 0.662993, acc 0.640625\n",
      "2018-10-26T15:51:58.274668: step 403, loss 0.606378, acc 0.71875\n",
      "2018-10-26T15:51:58.642684: step 404, loss 0.716796, acc 0.6875\n",
      "2018-10-26T15:51:59.003720: step 405, loss 0.490633, acc 0.78125\n",
      "2018-10-26T15:51:59.337848: step 406, loss 0.638403, acc 0.734375\n",
      "2018-10-26T15:51:59.641017: step 407, loss 0.591681, acc 0.6875\n",
      "2018-10-26T15:52:00.031973: step 408, loss 0.633596, acc 0.71875\n",
      "2018-10-26T15:52:00.350166: step 409, loss 0.651288, acc 0.65625\n",
      "2018-10-26T15:52:00.688221: step 410, loss 0.518407, acc 0.703125\n",
      "2018-10-26T15:52:01.014347: step 411, loss 0.51153, acc 0.75\n",
      "2018-10-26T15:52:01.307596: step 412, loss 0.521103, acc 0.75\n",
      "2018-10-26T15:52:01.626740: step 413, loss 0.537922, acc 0.703125\n",
      "2018-10-26T15:52:01.949890: step 414, loss 0.783412, acc 0.5625\n",
      "2018-10-26T15:52:02.330062: step 415, loss 0.752867, acc 0.65625\n",
      "2018-10-26T15:52:02.654771: step 416, loss 0.622048, acc 0.71875\n",
      "2018-10-26T15:52:03.002396: step 417, loss 0.55112, acc 0.765625\n",
      "2018-10-26T15:52:03.357566: step 418, loss 0.589639, acc 0.671875\n",
      "2018-10-26T15:52:03.681437: step 419, loss 0.719819, acc 0.671875\n",
      "2018-10-26T15:52:04.041450: step 420, loss 0.573658, acc 0.71875\n",
      "2018-10-26T15:52:04.360189: step 421, loss 0.553391, acc 0.671875\n",
      "2018-10-26T15:52:04.681748: step 422, loss 0.646918, acc 0.59375\n",
      "2018-10-26T15:52:05.006906: step 423, loss 0.46278, acc 0.75\n",
      "2018-10-26T15:52:05.318079: step 424, loss 0.732873, acc 0.609375\n",
      "2018-10-26T15:52:05.635201: step 425, loss 0.643719, acc 0.671875\n",
      "2018-10-26T15:52:06.047100: step 426, loss 0.486153, acc 0.8125\n",
      "2018-10-26T15:52:06.437059: step 427, loss 0.746287, acc 0.59375\n",
      "2018-10-26T15:52:06.744241: step 428, loss 0.663492, acc 0.59375\n",
      "2018-10-26T15:52:07.089315: step 429, loss 0.635352, acc 0.703125\n",
      "2018-10-26T15:52:07.387518: step 430, loss 0.548869, acc 0.71875\n",
      "2018-10-26T15:52:07.695714: step 431, loss 0.599303, acc 0.671875\n",
      "2018-10-26T15:52:08.031833: step 432, loss 0.617984, acc 0.6875\n",
      "2018-10-26T15:52:08.325012: step 433, loss 0.637439, acc 0.671875\n",
      "2018-10-26T15:52:08.659161: step 434, loss 0.626242, acc 0.625\n",
      "2018-10-26T15:52:09.055064: step 435, loss 0.603982, acc 0.671875\n",
      "2018-10-26T15:52:09.349278: step 436, loss 0.523471, acc 0.71875\n",
      "2018-10-26T15:52:09.744222: step 437, loss 0.654296, acc 0.65625\n",
      "2018-10-26T15:52:10.088391: step 438, loss 0.704479, acc 0.609375\n",
      "2018-10-26T15:52:10.396567: step 439, loss 0.604334, acc 0.640625\n",
      "2018-10-26T15:52:10.710640: step 440, loss 0.652941, acc 0.734375\n",
      "2018-10-26T15:52:11.057712: step 441, loss 0.559027, acc 0.703125\n",
      "2018-10-26T15:52:11.363893: step 442, loss 0.506357, acc 0.765625\n",
      "2018-10-26T15:52:11.752854: step 443, loss 0.676782, acc 0.609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:52:12.055046: step 444, loss 0.578635, acc 0.75\n",
      "2018-10-26T15:52:12.393146: step 445, loss 0.591244, acc 0.71875\n",
      "2018-10-26T15:52:12.721317: step 446, loss 0.65654, acc 0.65625\n",
      "2018-10-26T15:52:13.040432: step 447, loss 0.680375, acc 0.65625\n",
      "2018-10-26T15:52:13.335672: step 448, loss 0.540868, acc 0.703125\n",
      "2018-10-26T15:52:13.675721: step 449, loss 0.637829, acc 0.671875\n",
      "2018-10-26T15:52:14.009866: step 450, loss 0.642412, acc 0.633333\n",
      "2018-10-26T15:52:14.336042: step 451, loss 0.553017, acc 0.703125\n",
      "2018-10-26T15:52:14.651160: step 452, loss 0.608923, acc 0.703125\n",
      "2018-10-26T15:52:15.010185: step 453, loss 0.611117, acc 0.640625\n",
      "2018-10-26T15:52:15.344260: step 454, loss 0.519329, acc 0.703125\n",
      "2018-10-26T15:52:15.653432: step 455, loss 0.582141, acc 0.671875\n",
      "2018-10-26T15:52:15.957619: step 456, loss 0.596573, acc 0.75\n",
      "2018-10-26T15:52:16.262804: step 457, loss 0.650998, acc 0.6875\n",
      "2018-10-26T15:52:16.562049: step 458, loss 0.501947, acc 0.8125\n",
      "2018-10-26T15:52:16.859237: step 459, loss 0.540636, acc 0.75\n",
      "2018-10-26T15:52:17.220343: step 460, loss 0.580034, acc 0.71875\n",
      "2018-10-26T15:52:17.573305: step 461, loss 0.602758, acc 0.6875\n",
      "2018-10-26T15:52:17.900433: step 462, loss 0.545317, acc 0.703125\n",
      "2018-10-26T15:52:18.227591: step 463, loss 0.530999, acc 0.734375\n",
      "2018-10-26T15:52:18.525796: step 464, loss 0.53625, acc 0.703125\n",
      "2018-10-26T15:52:18.806036: step 465, loss 0.525379, acc 0.703125\n",
      "2018-10-26T15:52:19.098229: step 466, loss 0.456748, acc 0.75\n",
      "2018-10-26T15:52:19.402415: step 467, loss 0.5619, acc 0.71875\n",
      "2018-10-26T15:52:19.685661: step 468, loss 0.560182, acc 0.703125\n",
      "2018-10-26T15:52:19.988884: step 469, loss 0.464464, acc 0.8125\n",
      "2018-10-26T15:52:20.303010: step 470, loss 0.562014, acc 0.671875\n",
      "2018-10-26T15:52:20.615176: step 471, loss 0.651225, acc 0.6875\n",
      "2018-10-26T15:52:20.954296: step 472, loss 0.624773, acc 0.6875\n",
      "2018-10-26T15:52:21.246487: step 473, loss 0.60943, acc 0.671875\n",
      "2018-10-26T15:52:21.526752: step 474, loss 0.628168, acc 0.71875\n",
      "2018-10-26T15:52:21.812038: step 475, loss 0.599983, acc 0.703125\n",
      "2018-10-26T15:52:22.139103: step 476, loss 0.685719, acc 0.703125\n",
      "2018-10-26T15:52:22.486179: step 477, loss 0.580199, acc 0.734375\n",
      "2018-10-26T15:52:22.771414: step 478, loss 0.446087, acc 0.8125\n",
      "2018-10-26T15:52:23.081656: step 479, loss 0.535955, acc 0.765625\n",
      "2018-10-26T15:52:23.381828: step 480, loss 0.621571, acc 0.71875\n",
      "2018-10-26T15:52:23.707912: step 481, loss 0.531344, acc 0.71875\n",
      "2018-10-26T15:52:24.009214: step 482, loss 0.515723, acc 0.6875\n",
      "2018-10-26T15:52:24.355215: step 483, loss 0.474955, acc 0.75\n",
      "2018-10-26T15:52:24.663406: step 484, loss 0.578795, acc 0.703125\n",
      "2018-10-26T15:52:24.969581: step 485, loss 0.594389, acc 0.734375\n",
      "2018-10-26T15:52:25.253782: step 486, loss 0.617725, acc 0.671875\n",
      "2018-10-26T15:52:25.550988: step 487, loss 0.445676, acc 0.859375\n",
      "2018-10-26T15:52:25.810295: step 488, loss 0.556733, acc 0.71875\n",
      "2018-10-26T15:52:26.139418: step 489, loss 0.503281, acc 0.75\n",
      "2018-10-26T15:52:26.459599: step 490, loss 0.521302, acc 0.765625\n",
      "2018-10-26T15:52:26.780744: step 491, loss 0.467706, acc 0.796875\n",
      "2018-10-26T15:52:27.106830: step 492, loss 0.461856, acc 0.765625\n",
      "2018-10-26T15:52:27.404073: step 493, loss 0.666693, acc 0.625\n",
      "2018-10-26T15:52:27.708225: step 494, loss 0.557595, acc 0.734375\n",
      "2018-10-26T15:52:28.075244: step 495, loss 0.549888, acc 0.6875\n",
      "2018-10-26T15:52:28.380428: step 496, loss 0.616684, acc 0.609375\n",
      "2018-10-26T15:52:28.711544: step 497, loss 0.617928, acc 0.71875\n",
      "2018-10-26T15:52:29.035678: step 498, loss 0.537318, acc 0.75\n",
      "2018-10-26T15:52:29.401701: step 499, loss 0.50631, acc 0.765625\n",
      "2018-10-26T15:52:29.771711: step 500, loss 0.597021, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:52:30.502757: step 500, loss 0.628938, acc 0.649156\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-500\n",
      "\n",
      "2018-10-26T15:52:31.091186: step 501, loss 0.759127, acc 0.671875\n",
      "2018-10-26T15:52:31.445240: step 502, loss 0.653009, acc 0.671875\n",
      "2018-10-26T15:52:31.861127: step 503, loss 0.541421, acc 0.734375\n",
      "2018-10-26T15:52:32.222163: step 504, loss 0.584017, acc 0.609375\n",
      "2018-10-26T15:52:32.547343: step 505, loss 0.398009, acc 0.828125\n",
      "2018-10-26T15:52:32.903379: step 506, loss 0.615656, acc 0.703125\n",
      "2018-10-26T15:52:33.209525: step 507, loss 0.478157, acc 0.78125\n",
      "2018-10-26T15:52:33.531909: step 508, loss 0.515276, acc 0.71875\n",
      "2018-10-26T15:52:33.853806: step 509, loss 0.493515, acc 0.703125\n",
      "2018-10-26T15:52:34.171954: step 510, loss 0.433871, acc 0.828125\n",
      "2018-10-26T15:52:34.486115: step 511, loss 0.442581, acc 0.796875\n",
      "2018-10-26T15:52:34.801369: step 512, loss 0.695193, acc 0.609375\n",
      "2018-10-26T15:52:35.145355: step 513, loss 0.60889, acc 0.703125\n",
      "2018-10-26T15:52:35.440566: step 514, loss 0.539449, acc 0.6875\n",
      "2018-10-26T15:52:35.764727: step 515, loss 0.513763, acc 0.71875\n",
      "2018-10-26T15:52:36.067925: step 516, loss 0.639481, acc 0.671875\n",
      "2018-10-26T15:52:36.393020: step 517, loss 0.663114, acc 0.703125\n",
      "2018-10-26T15:52:36.721184: step 518, loss 0.561011, acc 0.703125\n",
      "2018-10-26T15:52:37.054253: step 519, loss 0.596783, acc 0.734375\n",
      "2018-10-26T15:52:37.395343: step 520, loss 0.625006, acc 0.609375\n",
      "2018-10-26T15:52:37.716487: step 521, loss 0.542396, acc 0.71875\n",
      "2018-10-26T15:52:38.069539: step 522, loss 0.558501, acc 0.765625\n",
      "2018-10-26T15:52:38.374724: step 523, loss 0.593563, acc 0.703125\n",
      "2018-10-26T15:52:38.679908: step 524, loss 0.636582, acc 0.671875\n",
      "2018-10-26T15:52:39.012021: step 525, loss 0.506535, acc 0.765625\n",
      "2018-10-26T15:52:39.304274: step 526, loss 0.442025, acc 0.75\n",
      "2018-10-26T15:52:39.582498: step 527, loss 0.620684, acc 0.671875\n",
      "2018-10-26T15:52:39.910621: step 528, loss 0.594025, acc 0.703125\n",
      "2018-10-26T15:52:40.216803: step 529, loss 0.635319, acc 0.734375\n",
      "2018-10-26T15:52:40.531963: step 530, loss 0.521402, acc 0.765625\n",
      "2018-10-26T15:52:40.871055: step 531, loss 0.578903, acc 0.71875\n",
      "2018-10-26T15:52:41.191260: step 532, loss 0.574219, acc 0.765625\n",
      "2018-10-26T15:52:41.528330: step 533, loss 0.470962, acc 0.765625\n",
      "2018-10-26T15:52:41.854429: step 534, loss 0.576796, acc 0.703125\n",
      "2018-10-26T15:52:42.175620: step 535, loss 0.544261, acc 0.734375\n",
      "2018-10-26T15:52:42.492721: step 536, loss 0.550343, acc 0.71875\n",
      "2018-10-26T15:52:42.870823: step 537, loss 0.597734, acc 0.671875\n",
      "2018-10-26T15:52:43.172905: step 538, loss 0.551134, acc 0.71875\n",
      "2018-10-26T15:52:43.504020: step 539, loss 0.610408, acc 0.75\n",
      "2018-10-26T15:52:43.837132: step 540, loss 0.577466, acc 0.734375\n",
      "2018-10-26T15:52:44.213126: step 541, loss 0.564727, acc 0.75\n",
      "2018-10-26T15:52:44.526375: step 542, loss 0.508651, acc 0.703125\n",
      "2018-10-26T15:52:44.853509: step 543, loss 0.522517, acc 0.703125\n",
      "2018-10-26T15:52:45.161592: step 544, loss 0.595703, acc 0.703125\n",
      "2018-10-26T15:52:45.550560: step 545, loss 0.723552, acc 0.546875\n",
      "2018-10-26T15:52:45.879702: step 546, loss 0.699691, acc 0.640625\n",
      "2018-10-26T15:52:46.198842: step 547, loss 0.525434, acc 0.75\n",
      "2018-10-26T15:52:46.513978: step 548, loss 0.502248, acc 0.734375\n",
      "2018-10-26T15:52:46.851077: step 549, loss 0.56985, acc 0.6875\n",
      "2018-10-26T15:52:47.166235: step 550, loss 0.517218, acc 0.796875\n",
      "2018-10-26T15:52:47.461695: step 551, loss 0.500829, acc 0.765625\n",
      "2018-10-26T15:52:47.796554: step 552, loss 0.465526, acc 0.765625\n",
      "2018-10-26T15:52:48.089805: step 553, loss 0.560745, acc 0.734375\n",
      "2018-10-26T15:52:48.374007: step 554, loss 0.563897, acc 0.671875\n",
      "2018-10-26T15:52:48.706122: step 555, loss 0.490442, acc 0.765625\n",
      "2018-10-26T15:52:49.074138: step 556, loss 0.576866, acc 0.640625\n",
      "2018-10-26T15:52:49.432180: step 557, loss 0.548074, acc 0.6875\n",
      "2018-10-26T15:52:49.739360: step 558, loss 0.472846, acc 0.75\n",
      "2018-10-26T15:52:50.081487: step 559, loss 0.549775, acc 0.640625\n",
      "2018-10-26T15:52:50.373701: step 560, loss 0.585889, acc 0.734375\n",
      "2018-10-26T15:52:50.675615: step 561, loss 0.430656, acc 0.796875\n",
      "2018-10-26T15:52:51.036893: step 562, loss 0.495079, acc 0.75\n",
      "2018-10-26T15:52:51.338088: step 563, loss 0.451379, acc 0.765625\n",
      "2018-10-26T15:52:51.698127: step 564, loss 0.57128, acc 0.734375\n",
      "2018-10-26T15:52:51.997358: step 565, loss 0.477943, acc 0.8125\n",
      "2018-10-26T15:52:52.329469: step 566, loss 0.5707, acc 0.6875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:52:52.712420: step 567, loss 0.560368, acc 0.703125\n",
      "2018-10-26T15:52:53.053506: step 568, loss 0.392634, acc 0.8125\n",
      "2018-10-26T15:52:53.376739: step 569, loss 0.552871, acc 0.734375\n",
      "2018-10-26T15:52:53.645922: step 570, loss 0.759433, acc 0.625\n",
      "2018-10-26T15:52:53.910216: step 571, loss 0.566718, acc 0.734375\n",
      "2018-10-26T15:52:54.213406: step 572, loss 0.710714, acc 0.640625\n",
      "2018-10-26T15:52:54.505625: step 573, loss 0.645036, acc 0.734375\n",
      "2018-10-26T15:52:54.787897: step 574, loss 0.571966, acc 0.671875\n",
      "2018-10-26T15:52:55.068122: step 575, loss 0.474718, acc 0.75\n",
      "2018-10-26T15:52:55.365364: step 576, loss 0.493922, acc 0.765625\n",
      "2018-10-26T15:52:55.709410: step 577, loss 0.458265, acc 0.78125\n",
      "2018-10-26T15:52:56.008631: step 578, loss 0.546299, acc 0.75\n",
      "2018-10-26T15:52:56.306814: step 579, loss 0.576353, acc 0.65625\n",
      "2018-10-26T15:52:56.634194: step 580, loss 0.503396, acc 0.71875\n",
      "2018-10-26T15:52:56.929150: step 581, loss 0.516352, acc 0.75\n",
      "2018-10-26T15:52:57.233352: step 582, loss 0.637682, acc 0.65625\n",
      "2018-10-26T15:52:57.587426: step 583, loss 0.495458, acc 0.828125\n",
      "2018-10-26T15:52:57.913283: step 584, loss 0.59428, acc 0.6875\n",
      "2018-10-26T15:52:58.231852: step 585, loss 0.513946, acc 0.78125\n",
      "2018-10-26T15:52:58.557384: step 586, loss 0.619187, acc 0.625\n",
      "2018-10-26T15:52:58.886981: step 587, loss 0.477553, acc 0.796875\n",
      "2018-10-26T15:52:59.218035: step 588, loss 0.603976, acc 0.65625\n",
      "2018-10-26T15:52:59.543165: step 589, loss 0.536475, acc 0.765625\n",
      "2018-10-26T15:52:59.911182: step 590, loss 0.47854, acc 0.703125\n",
      "2018-10-26T15:53:00.308122: step 591, loss 0.546137, acc 0.71875\n",
      "2018-10-26T15:53:00.739995: step 592, loss 0.404963, acc 0.8125\n",
      "2018-10-26T15:53:01.140896: step 593, loss 0.567665, acc 0.75\n",
      "2018-10-26T15:53:01.564764: step 594, loss 0.484022, acc 0.8125\n",
      "2018-10-26T15:53:01.960707: step 595, loss 0.589152, acc 0.65625\n",
      "2018-10-26T15:53:02.371608: step 596, loss 0.642932, acc 0.703125\n",
      "2018-10-26T15:53:02.747642: step 597, loss 0.64703, acc 0.65625\n",
      "2018-10-26T15:53:03.117615: step 598, loss 0.504857, acc 0.78125\n",
      "2018-10-26T15:53:03.481643: step 599, loss 0.4933, acc 0.78125\n",
      "2018-10-26T15:53:03.864620: step 600, loss 0.576522, acc 0.733333\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:53:04.744269: step 600, loss 0.637323, acc 0.639775\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-600\n",
      "\n",
      "2018-10-26T15:53:05.321790: step 601, loss 0.403216, acc 0.859375\n",
      "2018-10-26T15:53:05.665807: step 602, loss 0.574821, acc 0.6875\n",
      "2018-10-26T15:53:06.007893: step 603, loss 0.460291, acc 0.8125\n",
      "2018-10-26T15:53:06.360951: step 604, loss 0.540489, acc 0.734375\n",
      "2018-10-26T15:53:06.650226: step 605, loss 0.548184, acc 0.75\n",
      "2018-10-26T15:53:06.954364: step 606, loss 0.447086, acc 0.78125\n",
      "2018-10-26T15:53:07.265533: step 607, loss 0.516607, acc 0.75\n",
      "2018-10-26T15:53:07.593755: step 608, loss 0.63051, acc 0.65625\n",
      "2018-10-26T15:53:07.967664: step 609, loss 0.511239, acc 0.75\n",
      "2018-10-26T15:53:08.335771: step 610, loss 0.308517, acc 0.890625\n",
      "2018-10-26T15:53:08.631942: step 611, loss 0.505809, acc 0.734375\n",
      "2018-10-26T15:53:08.974966: step 612, loss 0.437772, acc 0.84375\n",
      "2018-10-26T15:53:09.275163: step 613, loss 0.520277, acc 0.796875\n",
      "2018-10-26T15:53:09.570374: step 614, loss 0.538677, acc 0.65625\n",
      "2018-10-26T15:53:09.891519: step 615, loss 0.455078, acc 0.8125\n",
      "2018-10-26T15:53:10.212659: step 616, loss 0.502541, acc 0.765625\n",
      "2018-10-26T15:53:10.541852: step 617, loss 0.594536, acc 0.765625\n",
      "2018-10-26T15:53:10.855953: step 618, loss 0.39219, acc 0.796875\n",
      "2018-10-26T15:53:11.191045: step 619, loss 0.453476, acc 0.78125\n",
      "2018-10-26T15:53:11.537120: step 620, loss 0.543118, acc 0.765625\n",
      "2018-10-26T15:53:11.927117: step 621, loss 0.483484, acc 0.8125\n",
      "2018-10-26T15:53:12.242337: step 622, loss 0.393254, acc 0.796875\n",
      "2018-10-26T15:53:12.619232: step 623, loss 0.461239, acc 0.8125\n",
      "2018-10-26T15:53:12.952339: step 624, loss 0.462644, acc 0.765625\n",
      "2018-10-26T15:53:13.232589: step 625, loss 0.428167, acc 0.78125\n",
      "2018-10-26T15:53:13.527822: step 626, loss 0.507349, acc 0.75\n",
      "2018-10-26T15:53:13.862942: step 627, loss 0.431158, acc 0.8125\n",
      "2018-10-26T15:53:14.198011: step 628, loss 0.499372, acc 0.703125\n",
      "2018-10-26T15:53:14.516441: step 629, loss 0.539951, acc 0.78125\n",
      "2018-10-26T15:53:14.853300: step 630, loss 0.480229, acc 0.703125\n",
      "2018-10-26T15:53:15.258206: step 631, loss 0.565513, acc 0.6875\n",
      "2018-10-26T15:53:15.643168: step 632, loss 0.57106, acc 0.75\n",
      "2018-10-26T15:53:15.954319: step 633, loss 0.438986, acc 0.828125\n",
      "2018-10-26T15:53:16.268479: step 634, loss 0.434209, acc 0.8125\n",
      "2018-10-26T15:53:16.575666: step 635, loss 0.671907, acc 0.65625\n",
      "2018-10-26T15:53:16.907774: step 636, loss 0.506395, acc 0.6875\n",
      "2018-10-26T15:53:17.207969: step 637, loss 0.626787, acc 0.703125\n",
      "2018-10-26T15:53:17.511295: step 638, loss 0.529739, acc 0.6875\n",
      "2018-10-26T15:53:17.832300: step 639, loss 0.658096, acc 0.671875\n",
      "2018-10-26T15:53:18.306038: step 640, loss 0.517118, acc 0.6875\n",
      "2018-10-26T15:53:18.687199: step 641, loss 0.445627, acc 0.796875\n",
      "2018-10-26T15:53:19.032416: step 642, loss 0.414502, acc 0.859375\n",
      "2018-10-26T15:53:19.451974: step 643, loss 0.482581, acc 0.734375\n",
      "2018-10-26T15:53:19.844931: step 644, loss 0.521372, acc 0.703125\n",
      "2018-10-26T15:53:20.214934: step 645, loss 0.500471, acc 0.78125\n",
      "2018-10-26T15:53:20.529095: step 646, loss 0.383694, acc 0.859375\n",
      "2018-10-26T15:53:21.094547: step 647, loss 0.503019, acc 0.734375\n",
      "2018-10-26T15:53:21.496407: step 648, loss 0.575896, acc 0.703125\n",
      "2018-10-26T15:53:21.900328: step 649, loss 0.425767, acc 0.765625\n",
      "2018-10-26T15:53:22.306242: step 650, loss 0.453165, acc 0.8125\n",
      "2018-10-26T15:53:22.652316: step 651, loss 0.452295, acc 0.8125\n",
      "2018-10-26T15:53:23.020331: step 652, loss 0.599081, acc 0.671875\n",
      "2018-10-26T15:53:23.387417: step 653, loss 0.467337, acc 0.75\n",
      "2018-10-26T15:53:23.701511: step 654, loss 0.448338, acc 0.796875\n",
      "2018-10-26T15:53:23.987746: step 655, loss 0.363684, acc 0.84375\n",
      "2018-10-26T15:53:24.291933: step 656, loss 0.467978, acc 0.78125\n",
      "2018-10-26T15:53:24.586146: step 657, loss 0.472163, acc 0.796875\n",
      "2018-10-26T15:53:24.916266: step 658, loss 0.523598, acc 0.78125\n",
      "2018-10-26T15:53:25.295309: step 659, loss 0.330983, acc 0.875\n",
      "2018-10-26T15:53:25.598494: step 660, loss 0.381045, acc 0.828125\n",
      "2018-10-26T15:53:25.900683: step 661, loss 0.70532, acc 0.65625\n",
      "2018-10-26T15:53:26.203825: step 662, loss 0.482591, acc 0.765625\n",
      "2018-10-26T15:53:26.533947: step 663, loss 0.575856, acc 0.6875\n",
      "2018-10-26T15:53:26.834141: step 664, loss 0.50103, acc 0.75\n",
      "2018-10-26T15:53:27.254024: step 665, loss 0.538721, acc 0.78125\n",
      "2018-10-26T15:53:27.542249: step 666, loss 0.482586, acc 0.765625\n",
      "2018-10-26T15:53:27.850428: step 667, loss 0.590338, acc 0.6875\n",
      "2018-10-26T15:53:28.204481: step 668, loss 0.394894, acc 0.796875\n",
      "2018-10-26T15:53:28.565515: step 669, loss 0.445896, acc 0.828125\n",
      "2018-10-26T15:53:28.869702: step 670, loss 0.475697, acc 0.765625\n",
      "2018-10-26T15:53:29.158962: step 671, loss 0.483669, acc 0.765625\n",
      "2018-10-26T15:53:29.424221: step 672, loss 0.419955, acc 0.78125\n",
      "2018-10-26T15:53:29.721476: step 673, loss 0.506485, acc 0.71875\n",
      "2018-10-26T15:53:30.187182: step 674, loss 0.460653, acc 0.828125\n",
      "2018-10-26T15:53:30.502676: step 675, loss 0.533463, acc 0.703125\n",
      "2018-10-26T15:53:30.881364: step 676, loss 0.622315, acc 0.71875\n",
      "2018-10-26T15:53:31.288240: step 677, loss 0.475713, acc 0.765625\n",
      "2018-10-26T15:53:31.606391: step 678, loss 0.420836, acc 0.828125\n",
      "2018-10-26T15:53:31.977399: step 679, loss 0.608732, acc 0.640625\n",
      "2018-10-26T15:53:32.238703: step 680, loss 0.560799, acc 0.78125\n",
      "2018-10-26T15:53:32.609710: step 681, loss 0.490686, acc 0.71875\n",
      "2018-10-26T15:53:32.910905: step 682, loss 0.55122, acc 0.765625\n",
      "2018-10-26T15:53:33.203212: step 683, loss 0.580223, acc 0.71875\n",
      "2018-10-26T15:53:33.488365: step 684, loss 0.552298, acc 0.65625\n",
      "2018-10-26T15:53:33.885301: step 685, loss 0.513259, acc 0.6875\n",
      "2018-10-26T15:53:34.168571: step 686, loss 0.378258, acc 0.84375\n",
      "2018-10-26T15:53:34.452954: step 687, loss 0.485565, acc 0.765625\n",
      "2018-10-26T15:53:34.736029: step 688, loss 0.510496, acc 0.75\n",
      "2018-10-26T15:53:35.036228: step 689, loss 0.381822, acc 0.890625\n",
      "2018-10-26T15:53:35.452116: step 690, loss 0.474389, acc 0.78125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:53:35.786223: step 691, loss 0.405847, acc 0.828125\n",
      "2018-10-26T15:53:36.174187: step 692, loss 0.502987, acc 0.734375\n",
      "2018-10-26T15:53:36.506300: step 693, loss 0.520189, acc 0.78125\n",
      "2018-10-26T15:53:36.827445: step 694, loss 0.361796, acc 0.859375\n",
      "2018-10-26T15:53:37.089741: step 695, loss 0.461866, acc 0.78125\n",
      "2018-10-26T15:53:37.347142: step 696, loss 0.45383, acc 0.78125\n",
      "2018-10-26T15:53:37.635311: step 697, loss 0.440483, acc 0.828125\n",
      "2018-10-26T15:53:37.948480: step 698, loss 0.510573, acc 0.75\n",
      "2018-10-26T15:53:38.305491: step 699, loss 0.453187, acc 0.765625\n",
      "2018-10-26T15:53:38.716393: step 700, loss 0.523377, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:53:39.556150: step 700, loss 0.614152, acc 0.661351\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-700\n",
      "\n",
      "2018-10-26T15:53:40.101822: step 701, loss 0.482256, acc 0.796875\n",
      "2018-10-26T15:53:40.367012: step 702, loss 0.562335, acc 0.640625\n",
      "2018-10-26T15:53:40.816834: step 703, loss 0.407446, acc 0.796875\n",
      "2018-10-26T15:53:41.125989: step 704, loss 0.464418, acc 0.796875\n",
      "2018-10-26T15:53:41.448096: step 705, loss 0.526576, acc 0.734375\n",
      "2018-10-26T15:53:41.743306: step 706, loss 0.422523, acc 0.84375\n",
      "2018-10-26T15:53:42.050528: step 707, loss 0.601828, acc 0.734375\n",
      "2018-10-26T15:53:42.343740: step 708, loss 0.446101, acc 0.828125\n",
      "2018-10-26T15:53:42.667835: step 709, loss 0.632325, acc 0.671875\n",
      "2018-10-26T15:53:42.992016: step 710, loss 0.472181, acc 0.765625\n",
      "2018-10-26T15:53:43.286184: step 711, loss 0.425088, acc 0.8125\n",
      "2018-10-26T15:53:43.555128: step 712, loss 0.503587, acc 0.78125\n",
      "2018-10-26T15:53:43.838452: step 713, loss 0.410571, acc 0.828125\n",
      "2018-10-26T15:53:44.123212: step 714, loss 0.469633, acc 0.765625\n",
      "2018-10-26T15:53:44.453036: step 715, loss 0.480339, acc 0.765625\n",
      "2018-10-26T15:53:44.837970: step 716, loss 0.502525, acc 0.71875\n",
      "2018-10-26T15:53:45.096047: step 717, loss 0.425217, acc 0.8125\n",
      "2018-10-26T15:53:45.403187: step 718, loss 0.54785, acc 0.6875\n",
      "2018-10-26T15:53:45.700165: step 719, loss 0.474079, acc 0.78125\n",
      "2018-10-26T15:53:45.997966: step 720, loss 0.454572, acc 0.78125\n",
      "2018-10-26T15:53:46.271300: step 721, loss 0.526845, acc 0.765625\n",
      "2018-10-26T15:53:46.563826: step 722, loss 0.405726, acc 0.78125\n",
      "2018-10-26T15:53:46.867070: step 723, loss 0.536373, acc 0.6875\n",
      "2018-10-26T15:53:47.177156: step 724, loss 0.443215, acc 0.796875\n",
      "2018-10-26T15:53:47.518268: step 725, loss 0.689096, acc 0.65625\n",
      "2018-10-26T15:53:47.788519: step 726, loss 0.704197, acc 0.65625\n",
      "2018-10-26T15:53:48.070795: step 727, loss 0.425535, acc 0.78125\n",
      "2018-10-26T15:53:48.321097: step 728, loss 0.422309, acc 0.8125\n",
      "2018-10-26T15:53:48.619302: step 729, loss 0.582238, acc 0.703125\n",
      "2018-10-26T15:53:48.946427: step 730, loss 0.653101, acc 0.671875\n",
      "2018-10-26T15:53:49.312500: step 731, loss 0.455462, acc 0.78125\n",
      "2018-10-26T15:53:49.618875: step 732, loss 0.485474, acc 0.78125\n",
      "2018-10-26T15:53:49.917121: step 733, loss 0.582439, acc 0.734375\n",
      "2018-10-26T15:53:50.228004: step 734, loss 0.472673, acc 0.765625\n",
      "2018-10-26T15:53:50.516282: step 735, loss 0.449178, acc 0.828125\n",
      "2018-10-26T15:53:50.808476: step 736, loss 0.609305, acc 0.734375\n",
      "2018-10-26T15:53:51.083737: step 737, loss 0.503751, acc 0.75\n",
      "2018-10-26T15:53:51.386109: step 738, loss 0.402936, acc 0.859375\n",
      "2018-10-26T15:53:51.674140: step 739, loss 0.388047, acc 0.8125\n",
      "2018-10-26T15:53:51.959380: step 740, loss 0.456041, acc 0.71875\n",
      "2018-10-26T15:53:52.276531: step 741, loss 0.526926, acc 0.75\n",
      "2018-10-26T15:53:52.567778: step 742, loss 0.559536, acc 0.71875\n",
      "2018-10-26T15:53:52.861966: step 743, loss 0.556253, acc 0.71875\n",
      "2018-10-26T15:53:53.166228: step 744, loss 0.500105, acc 0.734375\n",
      "2018-10-26T15:53:53.511262: step 745, loss 0.476094, acc 0.75\n",
      "2018-10-26T15:53:53.818455: step 746, loss 0.442681, acc 0.796875\n",
      "2018-10-26T15:53:54.132570: step 747, loss 0.551364, acc 0.734375\n",
      "2018-10-26T15:53:54.450720: step 748, loss 0.485638, acc 0.796875\n",
      "2018-10-26T15:53:54.756014: step 749, loss 0.464298, acc 0.75\n",
      "2018-10-26T15:53:55.033165: step 750, loss 0.438502, acc 0.766667\n",
      "2018-10-26T15:53:55.317406: step 751, loss 0.403317, acc 0.8125\n",
      "2018-10-26T15:53:55.619632: step 752, loss 0.368554, acc 0.84375\n",
      "2018-10-26T15:53:55.936750: step 753, loss 0.541905, acc 0.78125\n",
      "2018-10-26T15:53:56.244928: step 754, loss 0.421764, acc 0.828125\n",
      "2018-10-26T15:53:56.536148: step 755, loss 0.382118, acc 0.828125\n",
      "2018-10-26T15:53:56.831527: step 756, loss 0.472775, acc 0.734375\n",
      "2018-10-26T15:53:57.238432: step 757, loss 0.323983, acc 0.9375\n",
      "2018-10-26T15:53:57.549490: step 758, loss 0.343514, acc 0.859375\n",
      "2018-10-26T15:53:57.854050: step 759, loss 0.468134, acc 0.796875\n",
      "2018-10-26T15:53:58.266530: step 760, loss 0.408684, acc 0.8125\n",
      "2018-10-26T15:53:58.629620: step 761, loss 0.493035, acc 0.796875\n",
      "2018-10-26T15:53:59.005555: step 762, loss 0.316442, acc 0.890625\n",
      "2018-10-26T15:53:59.371572: step 763, loss 0.385634, acc 0.78125\n",
      "2018-10-26T15:53:59.870826: step 764, loss 0.508445, acc 0.765625\n",
      "2018-10-26T15:54:00.229867: step 765, loss 0.371389, acc 0.84375\n",
      "2018-10-26T15:54:00.604117: step 766, loss 0.35116, acc 0.859375\n",
      "2018-10-26T15:54:00.951938: step 767, loss 0.352397, acc 0.828125\n",
      "2018-10-26T15:54:01.323945: step 768, loss 0.427732, acc 0.796875\n",
      "2018-10-26T15:54:01.775803: step 769, loss 0.291743, acc 0.859375\n",
      "2018-10-26T15:54:02.109871: step 770, loss 0.438597, acc 0.8125\n",
      "2018-10-26T15:54:02.432981: step 771, loss 0.300494, acc 0.890625\n",
      "2018-10-26T15:54:02.896742: step 772, loss 0.431627, acc 0.765625\n",
      "2018-10-26T15:54:03.339557: step 773, loss 0.393143, acc 0.796875\n",
      "2018-10-26T15:54:03.630826: step 774, loss 0.358364, acc 0.828125\n",
      "2018-10-26T15:54:03.947963: step 775, loss 0.465429, acc 0.78125\n",
      "2018-10-26T15:54:04.261095: step 776, loss 0.387262, acc 0.8125\n",
      "2018-10-26T15:54:04.588222: step 777, loss 0.368232, acc 0.828125\n",
      "2018-10-26T15:54:04.930308: step 778, loss 0.402278, acc 0.78125\n",
      "2018-10-26T15:54:05.236489: step 779, loss 0.456124, acc 0.765625\n",
      "2018-10-26T15:54:05.606568: step 780, loss 0.512273, acc 0.75\n",
      "2018-10-26T15:54:05.983649: step 781, loss 0.379349, acc 0.84375\n",
      "2018-10-26T15:54:06.372454: step 782, loss 0.368864, acc 0.84375\n",
      "2018-10-26T15:54:06.777398: step 783, loss 0.332884, acc 0.875\n",
      "2018-10-26T15:54:07.263074: step 784, loss 0.487138, acc 0.734375\n",
      "2018-10-26T15:54:07.803630: step 785, loss 0.328402, acc 0.890625\n",
      "2018-10-26T15:54:08.235520: step 786, loss 0.451632, acc 0.78125\n",
      "2018-10-26T15:54:08.657350: step 787, loss 0.325317, acc 0.890625\n",
      "2018-10-26T15:54:09.066256: step 788, loss 0.415132, acc 0.796875\n",
      "2018-10-26T15:54:09.468183: step 789, loss 0.404904, acc 0.796875\n",
      "2018-10-26T15:54:09.899031: step 790, loss 0.515767, acc 0.765625\n",
      "2018-10-26T15:54:10.429614: step 791, loss 0.345127, acc 0.828125\n",
      "2018-10-26T15:54:11.036993: step 792, loss 0.398897, acc 0.828125\n",
      "2018-10-26T15:54:11.467975: step 793, loss 0.410393, acc 0.84375\n",
      "2018-10-26T15:54:11.850865: step 794, loss 0.424287, acc 0.796875\n",
      "2018-10-26T15:54:12.229810: step 795, loss 0.42126, acc 0.75\n",
      "2018-10-26T15:54:12.525017: step 796, loss 0.462363, acc 0.8125\n",
      "2018-10-26T15:54:12.817262: step 797, loss 0.599904, acc 0.65625\n",
      "2018-10-26T15:54:13.128505: step 798, loss 0.391409, acc 0.859375\n",
      "2018-10-26T15:54:13.520359: step 799, loss 0.557533, acc 0.65625\n",
      "2018-10-26T15:54:13.849475: step 800, loss 0.487988, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:54:14.612438: step 800, loss 0.610042, acc 0.677298\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-800\n",
      "\n",
      "2018-10-26T15:54:15.224802: step 801, loss 0.304265, acc 0.890625\n",
      "2018-10-26T15:54:15.566892: step 802, loss 0.389915, acc 0.84375\n",
      "2018-10-26T15:54:16.070544: step 803, loss 0.503972, acc 0.75\n",
      "2018-10-26T15:54:16.392681: step 804, loss 0.32288, acc 0.875\n",
      "2018-10-26T15:54:16.760700: step 805, loss 0.392754, acc 0.8125\n",
      "2018-10-26T15:54:17.106773: step 806, loss 0.356521, acc 0.859375\n",
      "2018-10-26T15:54:17.423926: step 807, loss 0.448332, acc 0.734375\n",
      "2018-10-26T15:54:17.739084: step 808, loss 0.480051, acc 0.828125\n",
      "2018-10-26T15:54:18.057236: step 809, loss 0.456701, acc 0.765625\n",
      "2018-10-26T15:54:18.372392: step 810, loss 0.388453, acc 0.84375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:54:18.694533: step 811, loss 0.429521, acc 0.8125\n",
      "2018-10-26T15:54:19.031687: step 812, loss 0.492958, acc 0.765625\n",
      "2018-10-26T15:54:19.374715: step 813, loss 0.421, acc 0.828125\n",
      "2018-10-26T15:54:19.747803: step 814, loss 0.330723, acc 0.875\n",
      "2018-10-26T15:54:20.179565: step 815, loss 0.381369, acc 0.796875\n",
      "2018-10-26T15:54:20.722116: step 816, loss 0.620564, acc 0.75\n",
      "2018-10-26T15:54:21.073179: step 817, loss 0.460867, acc 0.8125\n",
      "2018-10-26T15:54:21.396313: step 818, loss 0.362735, acc 0.859375\n",
      "2018-10-26T15:54:21.724656: step 819, loss 0.363901, acc 0.875\n",
      "2018-10-26T15:54:22.286939: step 820, loss 0.466409, acc 0.78125\n",
      "2018-10-26T15:54:22.647968: step 821, loss 0.454289, acc 0.8125\n",
      "2018-10-26T15:54:22.981079: step 822, loss 0.266278, acc 0.90625\n",
      "2018-10-26T15:54:23.352087: step 823, loss 0.385608, acc 0.796875\n",
      "2018-10-26T15:54:23.679248: step 824, loss 0.352454, acc 0.84375\n",
      "2018-10-26T15:54:24.027319: step 825, loss 0.426844, acc 0.796875\n",
      "2018-10-26T15:54:24.418237: step 826, loss 0.430078, acc 0.75\n",
      "2018-10-26T15:54:24.703494: step 827, loss 0.431011, acc 0.796875\n",
      "2018-10-26T15:54:25.054539: step 828, loss 0.458457, acc 0.796875\n",
      "2018-10-26T15:54:25.442501: step 829, loss 0.40745, acc 0.796875\n",
      "2018-10-26T15:54:25.726742: step 830, loss 0.438267, acc 0.78125\n",
      "2018-10-26T15:54:26.012977: step 831, loss 0.415034, acc 0.796875\n",
      "2018-10-26T15:54:26.315170: step 832, loss 0.553242, acc 0.8125\n",
      "2018-10-26T15:54:26.648338: step 833, loss 0.592289, acc 0.734375\n",
      "2018-10-26T15:54:27.016297: step 834, loss 0.380836, acc 0.84375\n",
      "2018-10-26T15:54:27.390349: step 835, loss 0.565511, acc 0.71875\n",
      "2018-10-26T15:54:27.755322: step 836, loss 0.492993, acc 0.765625\n",
      "2018-10-26T15:54:28.045549: step 837, loss 0.455896, acc 0.8125\n",
      "2018-10-26T15:54:28.375664: step 838, loss 0.380107, acc 0.78125\n",
      "2018-10-26T15:54:28.694813: step 839, loss 0.331493, acc 0.84375\n",
      "2018-10-26T15:54:29.056912: step 840, loss 0.477464, acc 0.75\n",
      "2018-10-26T15:54:29.419874: step 841, loss 0.48091, acc 0.765625\n",
      "2018-10-26T15:54:29.875656: step 842, loss 0.5049, acc 0.78125\n",
      "2018-10-26T15:54:30.279644: step 843, loss 0.506578, acc 0.78125\n",
      "2018-10-26T15:54:30.788220: step 844, loss 0.399198, acc 0.8125\n",
      "2018-10-26T15:54:31.268934: step 845, loss 0.428984, acc 0.796875\n",
      "2018-10-26T15:54:31.637948: step 846, loss 0.398266, acc 0.84375\n",
      "2018-10-26T15:54:32.131634: step 847, loss 0.441845, acc 0.734375\n",
      "2018-10-26T15:54:32.617666: step 848, loss 0.380942, acc 0.828125\n",
      "2018-10-26T15:54:33.070122: step 849, loss 0.378257, acc 0.8125\n",
      "2018-10-26T15:54:33.474042: step 850, loss 0.528264, acc 0.71875\n",
      "2018-10-26T15:54:33.834085: step 851, loss 0.43024, acc 0.8125\n",
      "2018-10-26T15:54:34.217058: step 852, loss 0.423002, acc 0.796875\n",
      "2018-10-26T15:54:34.640007: step 853, loss 0.514199, acc 0.765625\n",
      "2018-10-26T15:54:35.026058: step 854, loss 0.512135, acc 0.671875\n",
      "2018-10-26T15:54:35.362076: step 855, loss 0.428074, acc 0.859375\n",
      "2018-10-26T15:54:35.694111: step 856, loss 0.537469, acc 0.703125\n",
      "2018-10-26T15:54:36.023272: step 857, loss 0.366333, acc 0.796875\n",
      "2018-10-26T15:54:36.350417: step 858, loss 0.374211, acc 0.859375\n",
      "2018-10-26T15:54:36.830179: step 859, loss 0.424983, acc 0.765625\n",
      "2018-10-26T15:54:37.208068: step 860, loss 0.354143, acc 0.796875\n",
      "2018-10-26T15:54:37.601163: step 861, loss 0.584432, acc 0.71875\n",
      "2018-10-26T15:54:38.026878: step 862, loss 0.466958, acc 0.765625\n",
      "2018-10-26T15:54:38.402876: step 863, loss 0.421702, acc 0.8125\n",
      "2018-10-26T15:54:38.793882: step 864, loss 0.614442, acc 0.6875\n",
      "2018-10-26T15:54:39.205843: step 865, loss 0.363097, acc 0.828125\n",
      "2018-10-26T15:54:39.676473: step 866, loss 0.344572, acc 0.84375\n",
      "2018-10-26T15:54:40.099383: step 867, loss 0.457472, acc 0.8125\n",
      "2018-10-26T15:54:40.584046: step 868, loss 0.391768, acc 0.84375\n",
      "2018-10-26T15:54:41.041908: step 869, loss 0.349051, acc 0.859375\n",
      "2018-10-26T15:54:41.533509: step 870, loss 0.392732, acc 0.796875\n",
      "2018-10-26T15:54:42.006245: step 871, loss 0.414245, acc 0.828125\n",
      "2018-10-26T15:54:42.469011: step 872, loss 0.33086, acc 0.875\n",
      "2018-10-26T15:54:42.886913: step 873, loss 0.437854, acc 0.828125\n",
      "2018-10-26T15:54:43.258899: step 874, loss 0.436977, acc 0.84375\n",
      "2018-10-26T15:54:43.579068: step 875, loss 0.464343, acc 0.796875\n",
      "2018-10-26T15:54:43.939102: step 876, loss 0.431183, acc 0.8125\n",
      "2018-10-26T15:54:44.281168: step 877, loss 0.48289, acc 0.796875\n",
      "2018-10-26T15:54:44.624250: step 878, loss 0.357713, acc 0.8125\n",
      "2018-10-26T15:54:45.019196: step 879, loss 0.515758, acc 0.734375\n",
      "2018-10-26T15:54:45.354300: step 880, loss 0.456167, acc 0.796875\n",
      "2018-10-26T15:54:45.736281: step 881, loss 0.511364, acc 0.75\n",
      "2018-10-26T15:54:46.179184: step 882, loss 0.4763, acc 0.84375\n",
      "2018-10-26T15:54:46.627899: step 883, loss 0.423404, acc 0.8125\n",
      "2018-10-26T15:54:47.108612: step 884, loss 0.427292, acc 0.765625\n",
      "2018-10-26T15:54:47.491596: step 885, loss 0.414704, acc 0.796875\n",
      "2018-10-26T15:54:47.852626: step 886, loss 0.536124, acc 0.78125\n",
      "2018-10-26T15:54:48.265522: step 887, loss 0.532024, acc 0.75\n",
      "2018-10-26T15:54:48.623596: step 888, loss 0.463103, acc 0.75\n",
      "2018-10-26T15:54:49.055412: step 889, loss 0.306175, acc 0.890625\n",
      "2018-10-26T15:54:49.420454: step 890, loss 0.361102, acc 0.84375\n",
      "2018-10-26T15:54:49.778479: step 891, loss 0.574682, acc 0.75\n",
      "2018-10-26T15:54:50.131539: step 892, loss 0.436865, acc 0.78125\n",
      "2018-10-26T15:54:50.477640: step 893, loss 0.408174, acc 0.796875\n",
      "2018-10-26T15:54:50.913473: step 894, loss 0.564188, acc 0.765625\n",
      "2018-10-26T15:54:51.316370: step 895, loss 0.392539, acc 0.8125\n",
      "2018-10-26T15:54:51.720295: step 896, loss 0.378522, acc 0.78125\n",
      "2018-10-26T15:54:52.132191: step 897, loss 0.493147, acc 0.734375\n",
      "2018-10-26T15:54:52.648812: step 898, loss 0.465314, acc 0.765625\n",
      "2018-10-26T15:54:53.193387: step 899, loss 0.285701, acc 0.90625\n",
      "2018-10-26T15:54:53.570353: step 900, loss 0.468017, acc 0.833333\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:54:54.478921: step 900, loss 0.594866, acc 0.690432\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-900\n",
      "\n",
      "2018-10-26T15:54:55.200024: step 901, loss 0.286445, acc 0.90625\n",
      "2018-10-26T15:54:55.556045: step 902, loss 0.385852, acc 0.8125\n",
      "2018-10-26T15:54:55.977074: step 903, loss 0.430487, acc 0.8125\n",
      "2018-10-26T15:54:56.413752: step 904, loss 0.375277, acc 0.796875\n",
      "2018-10-26T15:54:56.820707: step 905, loss 0.317518, acc 0.828125\n",
      "2018-10-26T15:54:57.240542: step 906, loss 0.344814, acc 0.84375\n",
      "2018-10-26T15:54:57.609557: step 907, loss 0.409716, acc 0.890625\n",
      "2018-10-26T15:54:57.988543: step 908, loss 0.334502, acc 0.859375\n",
      "2018-10-26T15:54:58.374514: step 909, loss 0.393211, acc 0.84375\n",
      "2018-10-26T15:54:58.813340: step 910, loss 0.416374, acc 0.765625\n",
      "2018-10-26T15:54:59.206290: step 911, loss 0.505788, acc 0.8125\n",
      "2018-10-26T15:54:59.611252: step 912, loss 0.402868, acc 0.84375\n",
      "2018-10-26T15:54:59.965268: step 913, loss 0.330453, acc 0.875\n",
      "2018-10-26T15:55:00.378189: step 914, loss 0.381689, acc 0.828125\n",
      "2018-10-26T15:55:00.762134: step 915, loss 0.252075, acc 0.90625\n",
      "2018-10-26T15:55:01.164099: step 916, loss 0.291649, acc 0.875\n",
      "2018-10-26T15:55:01.557009: step 917, loss 0.311435, acc 0.875\n",
      "2018-10-26T15:55:01.997003: step 918, loss 0.315658, acc 0.875\n",
      "2018-10-26T15:55:02.525424: step 919, loss 0.323456, acc 0.875\n",
      "2018-10-26T15:55:03.090973: step 920, loss 0.304482, acc 0.875\n",
      "2018-10-26T15:55:03.525769: step 921, loss 0.328271, acc 0.84375\n",
      "2018-10-26T15:55:03.911717: step 922, loss 0.383369, acc 0.875\n",
      "2018-10-26T15:55:04.236851: step 923, loss 0.322032, acc 0.890625\n",
      "2018-10-26T15:55:04.697618: step 924, loss 0.368048, acc 0.78125\n",
      "2018-10-26T15:55:05.023748: step 925, loss 0.435062, acc 0.875\n",
      "2018-10-26T15:55:05.329928: step 926, loss 0.296545, acc 0.875\n",
      "2018-10-26T15:55:05.649075: step 927, loss 0.417784, acc 0.8125\n",
      "2018-10-26T15:55:05.949275: step 928, loss 0.286663, acc 0.875\n",
      "2018-10-26T15:55:06.221546: step 929, loss 0.348736, acc 0.8125\n",
      "2018-10-26T15:55:06.500837: step 930, loss 0.298584, acc 0.828125\n",
      "2018-10-26T15:55:06.804036: step 931, loss 0.343277, acc 0.859375\n",
      "2018-10-26T15:55:07.119149: step 932, loss 0.368476, acc 0.796875\n",
      "2018-10-26T15:55:07.413362: step 933, loss 0.252213, acc 0.890625\n",
      "2018-10-26T15:55:07.768416: step 934, loss 0.432525, acc 0.796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:55:08.090765: step 935, loss 0.341931, acc 0.84375\n",
      "2018-10-26T15:55:08.430645: step 936, loss 0.363016, acc 0.84375\n",
      "2018-10-26T15:55:08.726883: step 937, loss 0.298221, acc 0.9375\n",
      "2018-10-26T15:55:09.048992: step 938, loss 0.386112, acc 0.796875\n",
      "2018-10-26T15:55:09.418008: step 939, loss 0.270659, acc 0.859375\n",
      "2018-10-26T15:55:09.730173: step 940, loss 0.322683, acc 0.921875\n",
      "2018-10-26T15:55:10.046327: step 941, loss 0.378425, acc 0.875\n",
      "2018-10-26T15:55:10.333561: step 942, loss 0.439618, acc 0.75\n",
      "2018-10-26T15:55:10.674648: step 943, loss 0.39944, acc 0.828125\n",
      "2018-10-26T15:55:10.978863: step 944, loss 0.272379, acc 0.90625\n",
      "2018-10-26T15:55:11.330895: step 945, loss 0.366295, acc 0.859375\n",
      "2018-10-26T15:55:11.647051: step 946, loss 0.455499, acc 0.78125\n",
      "2018-10-26T15:55:12.018058: step 947, loss 0.305456, acc 0.890625\n",
      "2018-10-26T15:55:12.384107: step 948, loss 0.360558, acc 0.859375\n",
      "2018-10-26T15:55:12.725169: step 949, loss 0.247942, acc 0.921875\n",
      "2018-10-26T15:55:13.069250: step 950, loss 0.363261, acc 0.890625\n",
      "2018-10-26T15:55:13.459207: step 951, loss 0.248795, acc 0.921875\n",
      "2018-10-26T15:55:13.850204: step 952, loss 0.395113, acc 0.8125\n",
      "2018-10-26T15:55:14.231184: step 953, loss 0.37032, acc 0.84375\n",
      "2018-10-26T15:55:14.635067: step 954, loss 0.39627, acc 0.859375\n",
      "2018-10-26T15:55:14.984134: step 955, loss 0.353236, acc 0.890625\n",
      "2018-10-26T15:55:15.332777: step 956, loss 0.478319, acc 0.84375\n",
      "2018-10-26T15:55:15.768067: step 957, loss 0.338466, acc 0.84375\n",
      "2018-10-26T15:55:16.121095: step 958, loss 0.401419, acc 0.78125\n",
      "2018-10-26T15:55:16.535011: step 959, loss 0.62223, acc 0.71875\n",
      "2018-10-26T15:55:16.888080: step 960, loss 0.400948, acc 0.796875\n",
      "2018-10-26T15:55:17.227141: step 961, loss 0.351447, acc 0.828125\n",
      "2018-10-26T15:55:17.546287: step 962, loss 0.247148, acc 0.921875\n",
      "2018-10-26T15:55:17.829756: step 963, loss 0.276369, acc 0.875\n",
      "2018-10-26T15:55:18.106275: step 964, loss 0.315871, acc 0.90625\n",
      "2018-10-26T15:55:18.394024: step 965, loss 0.421509, acc 0.796875\n",
      "2018-10-26T15:55:18.700207: step 966, loss 0.33871, acc 0.84375\n",
      "2018-10-26T15:55:19.036403: step 967, loss 0.444218, acc 0.8125\n",
      "2018-10-26T15:55:19.396347: step 968, loss 0.417839, acc 0.84375\n",
      "2018-10-26T15:55:19.741508: step 969, loss 0.509168, acc 0.75\n",
      "2018-10-26T15:55:20.068601: step 970, loss 0.373649, acc 0.828125\n",
      "2018-10-26T15:55:20.392683: step 971, loss 0.412798, acc 0.796875\n",
      "2018-10-26T15:55:20.658972: step 972, loss 0.312143, acc 0.875\n",
      "2018-10-26T15:55:20.975145: step 973, loss 0.241692, acc 0.921875\n",
      "2018-10-26T15:55:21.296270: step 974, loss 0.356285, acc 0.890625\n",
      "2018-10-26T15:55:21.621400: step 975, loss 0.336798, acc 0.875\n",
      "2018-10-26T15:55:21.926584: step 976, loss 0.313142, acc 0.890625\n",
      "2018-10-26T15:55:22.288616: step 977, loss 0.329446, acc 0.859375\n",
      "2018-10-26T15:55:22.624742: step 978, loss 0.274671, acc 0.875\n",
      "2018-10-26T15:55:22.932896: step 979, loss 0.309936, acc 0.859375\n",
      "2018-10-26T15:55:23.207614: step 980, loss 0.294023, acc 0.890625\n",
      "2018-10-26T15:55:23.538281: step 981, loss 0.306162, acc 0.875\n",
      "2018-10-26T15:55:23.913319: step 982, loss 0.383477, acc 0.84375\n",
      "2018-10-26T15:55:24.283289: step 983, loss 0.309866, acc 0.84375\n",
      "2018-10-26T15:55:24.606456: step 984, loss 0.310676, acc 0.890625\n",
      "2018-10-26T15:55:24.904630: step 985, loss 0.289027, acc 0.859375\n",
      "2018-10-26T15:55:25.230756: step 986, loss 0.264717, acc 0.90625\n",
      "2018-10-26T15:55:25.527962: step 987, loss 0.294903, acc 0.859375\n",
      "2018-10-26T15:55:25.826226: step 988, loss 0.367191, acc 0.84375\n",
      "2018-10-26T15:55:26.181247: step 989, loss 0.253418, acc 0.890625\n",
      "2018-10-26T15:55:26.565197: step 990, loss 0.341358, acc 0.796875\n",
      "2018-10-26T15:55:26.907296: step 991, loss 0.360952, acc 0.859375\n",
      "2018-10-26T15:55:27.195506: step 992, loss 0.36186, acc 0.84375\n",
      "2018-10-26T15:55:27.465857: step 993, loss 0.313732, acc 0.859375\n",
      "2018-10-26T15:55:27.794572: step 994, loss 0.446316, acc 0.796875\n",
      "2018-10-26T15:55:28.121726: step 995, loss 0.349045, acc 0.859375\n",
      "2018-10-26T15:55:28.453057: step 996, loss 0.377081, acc 0.8125\n",
      "2018-10-26T15:55:28.798029: step 997, loss 0.357388, acc 0.828125\n",
      "2018-10-26T15:55:29.153960: step 998, loss 0.416973, acc 0.78125\n",
      "2018-10-26T15:55:29.501685: step 999, loss 0.375852, acc 0.84375\n",
      "2018-10-26T15:55:29.826738: step 1000, loss 0.49594, acc 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:55:30.626385: step 1000, loss 0.601883, acc 0.693246\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1000\n",
      "\n",
      "2018-10-26T15:55:31.130039: step 1001, loss 0.525037, acc 0.78125\n",
      "2018-10-26T15:55:31.459161: step 1002, loss 0.314884, acc 0.890625\n",
      "2018-10-26T15:55:31.800249: step 1003, loss 0.379451, acc 0.859375\n",
      "2018-10-26T15:55:32.153413: step 1004, loss 0.410365, acc 0.84375\n",
      "2018-10-26T15:55:32.447520: step 1005, loss 0.420797, acc 0.859375\n",
      "2018-10-26T15:55:32.747746: step 1006, loss 0.410826, acc 0.8125\n",
      "2018-10-26T15:55:33.028972: step 1007, loss 0.358141, acc 0.84375\n",
      "2018-10-26T15:55:33.347116: step 1008, loss 0.490746, acc 0.765625\n",
      "2018-10-26T15:55:33.653298: step 1009, loss 0.439341, acc 0.78125\n",
      "2018-10-26T15:55:33.974440: step 1010, loss 0.232693, acc 0.921875\n",
      "2018-10-26T15:55:34.279667: step 1011, loss 0.413615, acc 0.8125\n",
      "2018-10-26T15:55:34.569850: step 1012, loss 0.436081, acc 0.8125\n",
      "2018-10-26T15:55:34.888030: step 1013, loss 0.409858, acc 0.765625\n",
      "2018-10-26T15:55:35.200166: step 1014, loss 0.29148, acc 0.828125\n",
      "2018-10-26T15:55:35.505352: step 1015, loss 0.439538, acc 0.78125\n",
      "2018-10-26T15:55:35.858439: step 1016, loss 0.359908, acc 0.875\n",
      "2018-10-26T15:55:36.212538: step 1017, loss 0.335128, acc 0.828125\n",
      "2018-10-26T15:55:36.513658: step 1018, loss 0.408124, acc 0.84375\n",
      "2018-10-26T15:55:36.850754: step 1019, loss 0.368038, acc 0.84375\n",
      "2018-10-26T15:55:37.143972: step 1020, loss 0.396201, acc 0.875\n",
      "2018-10-26T15:55:37.465114: step 1021, loss 0.405738, acc 0.8125\n",
      "2018-10-26T15:55:37.749404: step 1022, loss 0.387263, acc 0.90625\n",
      "2018-10-26T15:55:38.071522: step 1023, loss 0.312241, acc 0.890625\n",
      "2018-10-26T15:55:38.390641: step 1024, loss 0.325607, acc 0.875\n",
      "2018-10-26T15:55:38.744694: step 1025, loss 0.398535, acc 0.859375\n",
      "2018-10-26T15:55:39.063865: step 1026, loss 0.427686, acc 0.8125\n",
      "2018-10-26T15:55:39.445820: step 1027, loss 0.429283, acc 0.828125\n",
      "2018-10-26T15:55:39.746020: step 1028, loss 0.357279, acc 0.796875\n",
      "2018-10-26T15:55:40.065172: step 1029, loss 0.423698, acc 0.8125\n",
      "2018-10-26T15:55:40.400270: step 1030, loss 0.406121, acc 0.84375\n",
      "2018-10-26T15:55:40.702465: step 1031, loss 0.399331, acc 0.84375\n",
      "2018-10-26T15:55:41.008645: step 1032, loss 0.422961, acc 0.8125\n",
      "2018-10-26T15:55:41.315824: step 1033, loss 0.210256, acc 0.921875\n",
      "2018-10-26T15:55:41.645942: step 1034, loss 0.308871, acc 0.859375\n",
      "2018-10-26T15:55:41.989026: step 1035, loss 0.347136, acc 0.859375\n",
      "2018-10-26T15:55:42.320205: step 1036, loss 0.346118, acc 0.84375\n",
      "2018-10-26T15:55:42.705115: step 1037, loss 0.411368, acc 0.859375\n",
      "2018-10-26T15:55:43.029291: step 1038, loss 0.28542, acc 0.90625\n",
      "2018-10-26T15:55:43.322464: step 1039, loss 0.420295, acc 0.75\n",
      "2018-10-26T15:55:43.626651: step 1040, loss 0.47867, acc 0.734375\n",
      "2018-10-26T15:55:43.966742: step 1041, loss 0.327912, acc 0.828125\n",
      "2018-10-26T15:55:44.313814: step 1042, loss 0.289664, acc 0.828125\n",
      "2018-10-26T15:55:44.593069: step 1043, loss 0.297882, acc 0.90625\n",
      "2018-10-26T15:55:44.917237: step 1044, loss 0.502933, acc 0.765625\n",
      "2018-10-26T15:55:45.219445: step 1045, loss 0.338942, acc 0.859375\n",
      "2018-10-26T15:55:45.576442: step 1046, loss 0.289301, acc 0.921875\n",
      "2018-10-26T15:55:45.948446: step 1047, loss 0.446899, acc 0.875\n",
      "2018-10-26T15:55:46.229737: step 1048, loss 0.338484, acc 0.84375\n",
      "2018-10-26T15:55:46.563803: step 1049, loss 0.364003, acc 0.84375\n",
      "2018-10-26T15:55:46.956756: step 1050, loss 0.30454, acc 0.866667\n",
      "2018-10-26T15:55:47.392589: step 1051, loss 0.263802, acc 0.90625\n",
      "2018-10-26T15:55:47.752658: step 1052, loss 0.310237, acc 0.875\n",
      "2018-10-26T15:55:48.081000: step 1053, loss 0.251639, acc 0.890625\n",
      "2018-10-26T15:55:48.413983: step 1054, loss 0.357383, acc 0.84375\n",
      "2018-10-26T15:55:48.874629: step 1055, loss 0.393187, acc 0.796875\n",
      "2018-10-26T15:55:49.176821: step 1056, loss 0.341831, acc 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:55:49.485996: step 1057, loss 0.231615, acc 0.921875\n",
      "2018-10-26T15:55:49.802201: step 1058, loss 0.333851, acc 0.84375\n",
      "2018-10-26T15:55:50.134262: step 1059, loss 0.282837, acc 0.875\n",
      "2018-10-26T15:55:50.445431: step 1060, loss 0.306427, acc 0.90625\n",
      "2018-10-26T15:55:51.051814: step 1061, loss 0.48092, acc 0.796875\n",
      "2018-10-26T15:55:51.423817: step 1062, loss 0.222912, acc 0.890625\n",
      "2018-10-26T15:55:51.758922: step 1063, loss 0.303188, acc 0.84375\n",
      "2018-10-26T15:55:52.060189: step 1064, loss 0.255508, acc 0.890625\n",
      "2018-10-26T15:55:52.627603: step 1065, loss 0.254117, acc 0.90625\n",
      "2018-10-26T15:55:52.953797: step 1066, loss 0.208848, acc 0.9375\n",
      "2018-10-26T15:55:53.239965: step 1067, loss 0.350107, acc 0.828125\n",
      "2018-10-26T15:55:53.605988: step 1068, loss 0.245019, acc 0.890625\n",
      "2018-10-26T15:55:54.145568: step 1069, loss 0.235171, acc 0.921875\n",
      "2018-10-26T15:55:54.462728: step 1070, loss 0.392178, acc 0.859375\n",
      "2018-10-26T15:55:54.774889: step 1071, loss 0.298815, acc 0.875\n",
      "2018-10-26T15:55:55.073086: step 1072, loss 0.348692, acc 0.828125\n",
      "2018-10-26T15:55:55.363780: step 1073, loss 0.379594, acc 0.796875\n",
      "2018-10-26T15:55:55.652519: step 1074, loss 0.464318, acc 0.8125\n",
      "2018-10-26T15:55:55.940748: step 1075, loss 0.206547, acc 0.875\n",
      "2018-10-26T15:55:56.261891: step 1076, loss 0.326843, acc 0.890625\n",
      "2018-10-26T15:55:56.592079: step 1077, loss 0.211499, acc 0.953125\n",
      "2018-10-26T15:55:56.979152: step 1078, loss 0.278618, acc 0.875\n",
      "2018-10-26T15:55:57.364222: step 1079, loss 0.274632, acc 0.890625\n",
      "2018-10-26T15:55:57.741936: step 1080, loss 0.272439, acc 0.890625\n",
      "2018-10-26T15:55:58.090007: step 1081, loss 0.229757, acc 0.921875\n",
      "2018-10-26T15:55:58.402175: step 1082, loss 0.305543, acc 0.859375\n",
      "2018-10-26T15:55:58.704364: step 1083, loss 0.174955, acc 0.921875\n",
      "2018-10-26T15:55:59.035484: step 1084, loss 0.260007, acc 0.875\n",
      "2018-10-26T15:55:59.397515: step 1085, loss 0.293878, acc 0.8125\n",
      "2018-10-26T15:55:59.735610: step 1086, loss 0.244688, acc 0.90625\n",
      "2018-10-26T15:56:00.075704: step 1087, loss 0.299518, acc 0.859375\n",
      "2018-10-26T15:56:00.409808: step 1088, loss 0.231649, acc 0.90625\n",
      "2018-10-26T15:56:00.750901: step 1089, loss 0.251897, acc 0.90625\n",
      "2018-10-26T15:56:01.093984: step 1090, loss 0.234429, acc 0.859375\n",
      "2018-10-26T15:56:01.429085: step 1091, loss 0.301942, acc 0.859375\n",
      "2018-10-26T15:56:01.794109: step 1092, loss 0.311566, acc 0.859375\n",
      "2018-10-26T15:56:02.149161: step 1093, loss 0.235815, acc 0.921875\n",
      "2018-10-26T15:56:02.544108: step 1094, loss 0.295832, acc 0.84375\n",
      "2018-10-26T15:56:02.878308: step 1095, loss 0.308813, acc 0.875\n",
      "2018-10-26T15:56:03.205342: step 1096, loss 0.318409, acc 0.875\n",
      "2018-10-26T15:56:03.534464: step 1097, loss 0.272035, acc 0.90625\n",
      "2018-10-26T15:56:03.863579: step 1098, loss 0.315511, acc 0.859375\n",
      "2018-10-26T15:56:04.189713: step 1099, loss 0.200161, acc 0.953125\n",
      "2018-10-26T15:56:04.524816: step 1100, loss 0.195088, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:56:05.408453: step 1100, loss 0.610439, acc 0.698874\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1100\n",
      "\n",
      "2018-10-26T15:56:06.101600: step 1101, loss 0.295626, acc 0.859375\n",
      "2018-10-26T15:56:06.514498: step 1102, loss 0.311367, acc 0.9375\n",
      "2018-10-26T15:56:07.000202: step 1103, loss 0.322105, acc 0.875\n",
      "2018-10-26T15:56:07.339293: step 1104, loss 0.304222, acc 0.890625\n",
      "2018-10-26T15:56:07.676397: step 1105, loss 0.236976, acc 0.921875\n",
      "2018-10-26T15:56:07.999569: step 1106, loss 0.233772, acc 0.9375\n",
      "2018-10-26T15:56:08.396614: step 1107, loss 0.258381, acc 0.90625\n",
      "2018-10-26T15:56:08.841283: step 1108, loss 0.39016, acc 0.84375\n",
      "2018-10-26T15:56:09.317009: step 1109, loss 0.324694, acc 0.84375\n",
      "2018-10-26T15:56:09.641148: step 1110, loss 0.273049, acc 0.890625\n",
      "2018-10-26T15:56:09.990210: step 1111, loss 0.368559, acc 0.828125\n",
      "2018-10-26T15:56:10.353305: step 1112, loss 0.296918, acc 0.875\n",
      "2018-10-26T15:56:10.686477: step 1113, loss 0.270606, acc 0.828125\n",
      "2018-10-26T15:56:11.179036: step 1114, loss 0.289198, acc 0.859375\n",
      "2018-10-26T15:56:11.633822: step 1115, loss 0.288518, acc 0.890625\n",
      "2018-10-26T15:56:11.935015: step 1116, loss 0.184576, acc 0.9375\n",
      "2018-10-26T15:56:12.249175: step 1117, loss 0.259219, acc 0.890625\n",
      "2018-10-26T15:56:12.582287: step 1118, loss 0.330339, acc 0.875\n",
      "2018-10-26T15:56:12.933347: step 1119, loss 0.240337, acc 0.890625\n",
      "2018-10-26T15:56:13.229558: step 1120, loss 0.307832, acc 0.875\n",
      "2018-10-26T15:56:13.661503: step 1121, loss 0.231679, acc 0.890625\n",
      "2018-10-26T15:56:14.062332: step 1122, loss 0.205757, acc 0.90625\n",
      "2018-10-26T15:56:14.450294: step 1123, loss 0.283076, acc 0.921875\n",
      "2018-10-26T15:56:14.783405: step 1124, loss 0.196352, acc 0.921875\n",
      "2018-10-26T15:56:15.132471: step 1125, loss 0.312356, acc 0.90625\n",
      "2018-10-26T15:56:15.596321: step 1126, loss 0.224167, acc 0.921875\n",
      "2018-10-26T15:56:16.013161: step 1127, loss 0.216797, acc 0.921875\n",
      "2018-10-26T15:56:16.352216: step 1128, loss 0.318628, acc 0.859375\n",
      "2018-10-26T15:56:16.747158: step 1129, loss 0.308644, acc 0.828125\n",
      "2018-10-26T15:56:17.244827: step 1130, loss 0.17075, acc 0.96875\n",
      "2018-10-26T15:56:17.590951: step 1131, loss 0.231634, acc 0.890625\n",
      "2018-10-26T15:56:18.058653: step 1132, loss 0.260235, acc 0.859375\n",
      "2018-10-26T15:56:18.442649: step 1133, loss 0.192117, acc 0.9375\n",
      "2018-10-26T15:56:18.827634: step 1134, loss 0.207978, acc 0.890625\n",
      "2018-10-26T15:56:19.162704: step 1135, loss 0.329794, acc 0.796875\n",
      "2018-10-26T15:56:19.512767: step 1136, loss 0.290481, acc 0.875\n",
      "2018-10-26T15:56:19.851875: step 1137, loss 0.185156, acc 0.953125\n",
      "2018-10-26T15:56:20.214891: step 1138, loss 0.309637, acc 0.859375\n",
      "2018-10-26T15:56:20.560966: step 1139, loss 0.394406, acc 0.859375\n",
      "2018-10-26T15:56:20.929982: step 1140, loss 0.194221, acc 0.921875\n",
      "2018-10-26T15:56:21.295005: step 1141, loss 0.255878, acc 0.859375\n",
      "2018-10-26T15:56:21.648064: step 1142, loss 0.293898, acc 0.90625\n",
      "2018-10-26T15:56:22.002117: step 1143, loss 0.20364, acc 0.921875\n",
      "2018-10-26T15:56:22.359277: step 1144, loss 0.281129, acc 0.875\n",
      "2018-10-26T15:56:22.660358: step 1145, loss 0.201337, acc 0.9375\n",
      "2018-10-26T15:56:22.982498: step 1146, loss 0.285503, acc 0.890625\n",
      "2018-10-26T15:56:23.273768: step 1147, loss 0.187452, acc 0.9375\n",
      "2018-10-26T15:56:23.565939: step 1148, loss 0.242616, acc 0.90625\n",
      "2018-10-26T15:56:23.845194: step 1149, loss 0.305896, acc 0.875\n",
      "2018-10-26T15:56:24.143396: step 1150, loss 0.330056, acc 0.828125\n",
      "2018-10-26T15:56:24.455564: step 1151, loss 0.302803, acc 0.828125\n",
      "2018-10-26T15:56:24.807622: step 1152, loss 0.242629, acc 0.890625\n",
      "2018-10-26T15:56:25.151735: step 1153, loss 0.273301, acc 0.921875\n",
      "2018-10-26T15:56:25.471845: step 1154, loss 0.24765, acc 0.890625\n",
      "2018-10-26T15:56:25.733148: step 1155, loss 0.438701, acc 0.828125\n",
      "2018-10-26T15:56:26.054290: step 1156, loss 0.326161, acc 0.84375\n",
      "2018-10-26T15:56:26.406397: step 1157, loss 0.290746, acc 0.859375\n",
      "2018-10-26T15:56:26.735505: step 1158, loss 0.299436, acc 0.859375\n",
      "2018-10-26T15:56:27.035668: step 1159, loss 0.243237, acc 0.90625\n",
      "2018-10-26T15:56:27.335892: step 1160, loss 0.307026, acc 0.78125\n",
      "2018-10-26T15:56:27.637062: step 1161, loss 0.263319, acc 0.9375\n",
      "2018-10-26T15:56:27.932272: step 1162, loss 0.25045, acc 0.875\n",
      "2018-10-26T15:56:28.220558: step 1163, loss 0.294149, acc 0.859375\n",
      "2018-10-26T15:56:28.526684: step 1164, loss 0.400586, acc 0.8125\n",
      "2018-10-26T15:56:28.854841: step 1165, loss 0.245389, acc 0.921875\n",
      "2018-10-26T15:56:29.171960: step 1166, loss 0.275315, acc 0.890625\n",
      "2018-10-26T15:56:29.570992: step 1167, loss 0.443578, acc 0.875\n",
      "2018-10-26T15:56:29.946889: step 1168, loss 0.198109, acc 0.90625\n",
      "2018-10-26T15:56:30.243098: step 1169, loss 0.298863, acc 0.890625\n",
      "2018-10-26T15:56:30.567231: step 1170, loss 0.2782, acc 0.890625\n",
      "2018-10-26T15:56:30.869462: step 1171, loss 0.352019, acc 0.828125\n",
      "2018-10-26T15:56:31.218535: step 1172, loss 0.31164, acc 0.9375\n",
      "2018-10-26T15:56:31.490819: step 1173, loss 0.461117, acc 0.765625\n",
      "2018-10-26T15:56:31.834873: step 1174, loss 0.2711, acc 0.859375\n",
      "2018-10-26T15:56:32.151056: step 1175, loss 0.20809, acc 0.9375\n",
      "2018-10-26T15:56:32.553925: step 1176, loss 0.21729, acc 0.921875\n",
      "2018-10-26T15:56:32.930954: step 1177, loss 0.273152, acc 0.875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:56:33.261070: step 1178, loss 0.300343, acc 0.859375\n",
      "2018-10-26T15:56:33.636058: step 1179, loss 0.279327, acc 0.84375\n",
      "2018-10-26T15:56:33.984102: step 1180, loss 0.40444, acc 0.796875\n",
      "2018-10-26T15:56:34.287292: step 1181, loss 0.258646, acc 0.875\n",
      "2018-10-26T15:56:34.584499: step 1182, loss 0.252514, acc 0.921875\n",
      "2018-10-26T15:56:34.883749: step 1183, loss 0.201164, acc 0.90625\n",
      "2018-10-26T15:56:35.206836: step 1184, loss 0.304271, acc 0.890625\n",
      "2018-10-26T15:56:35.521993: step 1185, loss 0.385167, acc 0.796875\n",
      "2018-10-26T15:56:35.842138: step 1186, loss 0.378038, acc 0.859375\n",
      "2018-10-26T15:56:36.138348: step 1187, loss 0.173591, acc 0.953125\n",
      "2018-10-26T15:56:36.491404: step 1188, loss 0.301038, acc 0.875\n",
      "2018-10-26T15:56:36.890338: step 1189, loss 0.237644, acc 0.921875\n",
      "2018-10-26T15:56:37.211525: step 1190, loss 0.29203, acc 0.890625\n",
      "2018-10-26T15:56:37.590536: step 1191, loss 0.262023, acc 0.890625\n",
      "2018-10-26T15:56:37.931555: step 1192, loss 0.335892, acc 0.890625\n",
      "2018-10-26T15:56:38.239732: step 1193, loss 0.213799, acc 0.90625\n",
      "2018-10-26T15:56:38.521042: step 1194, loss 0.217077, acc 0.921875\n",
      "2018-10-26T15:56:38.839131: step 1195, loss 0.210722, acc 0.859375\n",
      "2018-10-26T15:56:39.127366: step 1196, loss 0.300808, acc 0.859375\n",
      "2018-10-26T15:56:39.402626: step 1197, loss 0.302504, acc 0.859375\n",
      "2018-10-26T15:56:39.712799: step 1198, loss 0.277677, acc 0.875\n",
      "2018-10-26T15:56:39.978121: step 1199, loss 0.425, acc 0.796875\n",
      "2018-10-26T15:56:40.236397: step 1200, loss 0.236753, acc 0.916667\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:56:40.955511: step 1200, loss 0.620431, acc 0.695122\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1200\n",
      "\n",
      "2018-10-26T15:56:41.588784: step 1201, loss 0.211391, acc 0.921875\n",
      "2018-10-26T15:56:41.991709: step 1202, loss 0.233565, acc 0.890625\n",
      "2018-10-26T15:56:42.419564: step 1203, loss 0.203963, acc 0.9375\n",
      "2018-10-26T15:56:42.832462: step 1204, loss 0.13089, acc 0.953125\n",
      "2018-10-26T15:56:43.215437: step 1205, loss 0.222204, acc 0.890625\n",
      "2018-10-26T15:56:43.524612: step 1206, loss 0.148698, acc 0.953125\n",
      "2018-10-26T15:56:43.919845: step 1207, loss 0.177532, acc 0.921875\n",
      "2018-10-26T15:56:44.282587: step 1208, loss 0.185159, acc 0.953125\n",
      "2018-10-26T15:56:44.687539: step 1209, loss 0.184037, acc 0.9375\n",
      "2018-10-26T15:56:44.996679: step 1210, loss 0.167576, acc 0.921875\n",
      "2018-10-26T15:56:45.315826: step 1211, loss 0.213822, acc 0.90625\n",
      "2018-10-26T15:56:45.717885: step 1212, loss 0.291612, acc 0.875\n",
      "2018-10-26T15:56:46.121674: step 1213, loss 0.171608, acc 0.921875\n",
      "2018-10-26T15:56:46.456825: step 1214, loss 0.185736, acc 0.890625\n",
      "2018-10-26T15:56:46.887719: step 1215, loss 0.184093, acc 0.90625\n",
      "2018-10-26T15:56:47.185909: step 1216, loss 0.245664, acc 0.859375\n",
      "2018-10-26T15:56:47.496003: step 1217, loss 0.22617, acc 0.921875\n",
      "2018-10-26T15:56:47.818170: step 1218, loss 0.208163, acc 0.9375\n",
      "2018-10-26T15:56:48.118734: step 1219, loss 0.282497, acc 0.90625\n",
      "2018-10-26T15:56:48.398888: step 1220, loss 0.255956, acc 0.859375\n",
      "2018-10-26T15:56:48.708761: step 1221, loss 0.3522, acc 0.8125\n",
      "2018-10-26T15:56:48.988014: step 1222, loss 0.264305, acc 0.921875\n",
      "2018-10-26T15:56:49.297703: step 1223, loss 0.259575, acc 0.890625\n",
      "2018-10-26T15:56:49.605366: step 1224, loss 0.26557, acc 0.90625\n",
      "2018-10-26T15:56:49.956455: step 1225, loss 0.217788, acc 0.90625\n",
      "2018-10-26T15:56:50.313474: step 1226, loss 0.166005, acc 0.953125\n",
      "2018-10-26T15:56:50.660545: step 1227, loss 0.119289, acc 0.953125\n",
      "2018-10-26T15:56:51.075438: step 1228, loss 0.178197, acc 0.9375\n",
      "2018-10-26T15:56:51.472377: step 1229, loss 0.17803, acc 0.9375\n",
      "2018-10-26T15:56:51.840393: step 1230, loss 0.226643, acc 0.90625\n",
      "2018-10-26T15:56:52.306152: step 1231, loss 0.20406, acc 0.9375\n",
      "2018-10-26T15:56:52.817815: step 1232, loss 0.348725, acc 0.859375\n",
      "2018-10-26T15:56:53.181924: step 1233, loss 0.159326, acc 0.9375\n",
      "2018-10-26T15:56:53.623630: step 1234, loss 0.185147, acc 0.9375\n",
      "2018-10-26T15:56:53.968707: step 1235, loss 0.18164, acc 0.9375\n",
      "2018-10-26T15:56:54.454409: step 1236, loss 0.189791, acc 0.90625\n",
      "2018-10-26T15:56:54.775589: step 1237, loss 0.195435, acc 0.90625\n",
      "2018-10-26T15:56:55.114688: step 1238, loss 0.274889, acc 0.890625\n",
      "2018-10-26T15:56:55.479116: step 1239, loss 0.352721, acc 0.859375\n",
      "2018-10-26T15:56:55.808791: step 1240, loss 0.164096, acc 0.9375\n",
      "2018-10-26T15:56:56.186784: step 1241, loss 0.273403, acc 0.90625\n",
      "2018-10-26T15:56:56.507921: step 1242, loss 0.214735, acc 0.921875\n",
      "2018-10-26T15:56:56.852003: step 1243, loss 0.19853, acc 0.90625\n",
      "2018-10-26T15:56:57.191221: step 1244, loss 0.296349, acc 0.84375\n",
      "2018-10-26T15:56:57.564104: step 1245, loss 0.178191, acc 0.953125\n",
      "2018-10-26T15:56:57.958048: step 1246, loss 0.334944, acc 0.796875\n",
      "2018-10-26T15:56:58.289166: step 1247, loss 0.263339, acc 0.875\n",
      "2018-10-26T15:56:58.633244: step 1248, loss 0.20338, acc 0.90625\n",
      "2018-10-26T15:56:59.040160: step 1249, loss 0.192606, acc 0.9375\n",
      "2018-10-26T15:56:59.376259: step 1250, loss 0.272091, acc 0.84375\n",
      "2018-10-26T15:56:59.714356: step 1251, loss 0.148177, acc 0.96875\n",
      "2018-10-26T15:57:00.056442: step 1252, loss 0.159009, acc 0.953125\n",
      "2018-10-26T15:57:00.392546: step 1253, loss 0.137278, acc 0.96875\n",
      "2018-10-26T15:57:00.769537: step 1254, loss 0.203953, acc 0.890625\n",
      "2018-10-26T15:57:01.249254: step 1255, loss 0.378883, acc 0.78125\n",
      "2018-10-26T15:57:01.595330: step 1256, loss 0.216962, acc 0.890625\n",
      "2018-10-26T15:57:02.005233: step 1257, loss 0.292433, acc 0.859375\n",
      "2018-10-26T15:57:02.351494: step 1258, loss 0.301871, acc 0.9375\n",
      "2018-10-26T15:57:02.717332: step 1259, loss 0.189428, acc 0.953125\n",
      "2018-10-26T15:57:03.068395: step 1260, loss 0.264428, acc 0.90625\n",
      "2018-10-26T15:57:03.418567: step 1261, loss 0.190597, acc 0.90625\n",
      "2018-10-26T15:57:03.802434: step 1262, loss 0.202612, acc 0.921875\n",
      "2018-10-26T15:57:04.155492: step 1263, loss 0.303059, acc 0.84375\n",
      "2018-10-26T15:57:04.513533: step 1264, loss 0.24073, acc 0.90625\n",
      "2018-10-26T15:57:04.833680: step 1265, loss 0.307562, acc 0.828125\n",
      "2018-10-26T15:57:05.176764: step 1266, loss 0.144929, acc 0.9375\n",
      "2018-10-26T15:57:05.510868: step 1267, loss 0.307043, acc 0.84375\n",
      "2018-10-26T15:57:05.855950: step 1268, loss 0.244426, acc 0.90625\n",
      "2018-10-26T15:57:06.226962: step 1269, loss 0.25, acc 0.921875\n",
      "2018-10-26T15:57:06.684734: step 1270, loss 0.14323, acc 0.96875\n",
      "2018-10-26T15:57:07.117576: step 1271, loss 0.241187, acc 0.921875\n",
      "2018-10-26T15:57:07.463653: step 1272, loss 0.142397, acc 0.953125\n",
      "2018-10-26T15:57:07.802744: step 1273, loss 0.463906, acc 0.828125\n",
      "2018-10-26T15:57:08.155844: step 1274, loss 0.211674, acc 0.90625\n",
      "2018-10-26T15:57:08.609590: step 1275, loss 0.497214, acc 0.78125\n",
      "2018-10-26T15:57:09.072353: step 1276, loss 0.404591, acc 0.796875\n",
      "2018-10-26T15:57:09.469292: step 1277, loss 0.2102, acc 0.890625\n",
      "2018-10-26T15:57:09.809387: step 1278, loss 0.259825, acc 0.890625\n",
      "2018-10-26T15:57:10.235325: step 1279, loss 0.294643, acc 0.875\n",
      "2018-10-26T15:57:10.808757: step 1280, loss 0.217705, acc 0.90625\n",
      "2018-10-26T15:57:11.215626: step 1281, loss 0.281762, acc 0.84375\n",
      "2018-10-26T15:57:11.618687: step 1282, loss 0.238541, acc 0.890625\n",
      "2018-10-26T15:57:12.051392: step 1283, loss 0.234051, acc 0.921875\n",
      "2018-10-26T15:57:12.397468: step 1284, loss 0.282872, acc 0.90625\n",
      "2018-10-26T15:57:12.745542: step 1285, loss 0.285731, acc 0.875\n",
      "2018-10-26T15:57:13.203316: step 1286, loss 0.26349, acc 0.859375\n",
      "2018-10-26T15:57:13.550387: step 1287, loss 0.228871, acc 0.921875\n",
      "2018-10-26T15:57:13.965280: step 1288, loss 0.151767, acc 0.921875\n",
      "2018-10-26T15:57:14.410090: step 1289, loss 0.201882, acc 0.875\n",
      "2018-10-26T15:57:14.769134: step 1290, loss 0.173469, acc 0.953125\n",
      "2018-10-26T15:57:15.149117: step 1291, loss 0.233593, acc 0.859375\n",
      "2018-10-26T15:57:15.496193: step 1292, loss 0.210367, acc 0.921875\n",
      "2018-10-26T15:57:15.829343: step 1293, loss 0.132997, acc 0.9375\n",
      "2018-10-26T15:57:16.247183: step 1294, loss 0.19605, acc 0.953125\n",
      "2018-10-26T15:57:16.607313: step 1295, loss 0.205541, acc 0.921875\n",
      "2018-10-26T15:57:16.997181: step 1296, loss 0.290401, acc 0.890625\n",
      "2018-10-26T15:57:17.413021: step 1297, loss 0.22938, acc 0.90625\n",
      "2018-10-26T15:57:17.806018: step 1298, loss 0.312135, acc 0.890625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:57:18.135144: step 1299, loss 0.302732, acc 0.84375\n",
      "2018-10-26T15:57:18.540055: step 1300, loss 0.213297, acc 0.9375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:57:19.339970: step 1300, loss 0.623157, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1300\n",
      "\n",
      "2018-10-26T15:57:20.145767: step 1301, loss 0.181768, acc 0.9375\n",
      "2018-10-26T15:57:20.531823: step 1302, loss 0.148667, acc 0.9375\n",
      "2018-10-26T15:57:20.947657: step 1303, loss 0.295552, acc 0.890625\n",
      "2018-10-26T15:57:21.327608: step 1304, loss 0.306063, acc 0.890625\n",
      "2018-10-26T15:57:21.707608: step 1305, loss 0.274537, acc 0.90625\n",
      "2018-10-26T15:57:22.054666: step 1306, loss 0.287262, acc 0.859375\n",
      "2018-10-26T15:57:22.417697: step 1307, loss 0.216984, acc 0.90625\n",
      "2018-10-26T15:57:22.835579: step 1308, loss 0.365014, acc 0.84375\n",
      "2018-10-26T15:57:23.190630: step 1309, loss 0.387648, acc 0.828125\n",
      "2018-10-26T15:57:23.687304: step 1310, loss 0.211199, acc 0.90625\n",
      "2018-10-26T15:57:24.142089: step 1311, loss 0.125803, acc 0.96875\n",
      "2018-10-26T15:57:24.627790: step 1312, loss 0.234183, acc 0.859375\n",
      "2018-10-26T15:57:25.057642: step 1313, loss 0.242565, acc 0.875\n",
      "2018-10-26T15:57:25.471602: step 1314, loss 0.222817, acc 0.90625\n",
      "2018-10-26T15:57:25.934300: step 1315, loss 0.21653, acc 0.953125\n",
      "2018-10-26T15:57:26.433359: step 1316, loss 0.222917, acc 0.921875\n",
      "2018-10-26T15:57:26.917672: step 1317, loss 0.344943, acc 0.8125\n",
      "2018-10-26T15:57:27.400429: step 1318, loss 0.311133, acc 0.859375\n",
      "2018-10-26T15:57:27.845194: step 1319, loss 0.184547, acc 0.9375\n",
      "2018-10-26T15:57:28.271095: step 1320, loss 0.264147, acc 0.9375\n",
      "2018-10-26T15:57:28.649045: step 1321, loss 0.161706, acc 0.9375\n",
      "2018-10-26T15:57:29.104828: step 1322, loss 0.208281, acc 0.90625\n",
      "2018-10-26T15:57:29.493187: step 1323, loss 0.247655, acc 0.90625\n",
      "2018-10-26T15:57:29.951567: step 1324, loss 0.316883, acc 0.875\n",
      "2018-10-26T15:57:30.336606: step 1325, loss 0.238598, acc 0.859375\n",
      "2018-10-26T15:57:30.663663: step 1326, loss 0.154961, acc 0.9375\n",
      "2018-10-26T15:57:31.009739: step 1327, loss 0.217619, acc 0.90625\n",
      "2018-10-26T15:57:31.403688: step 1328, loss 0.179074, acc 0.921875\n",
      "2018-10-26T15:57:31.737940: step 1329, loss 0.287362, acc 0.90625\n",
      "2018-10-26T15:57:32.076436: step 1330, loss 0.225786, acc 0.890625\n",
      "2018-10-26T15:57:32.378086: step 1331, loss 0.293634, acc 0.890625\n",
      "2018-10-26T15:57:32.770037: step 1332, loss 0.175854, acc 0.953125\n",
      "2018-10-26T15:57:33.323556: step 1333, loss 0.175945, acc 0.90625\n",
      "2018-10-26T15:57:33.722491: step 1334, loss 0.276315, acc 0.9375\n",
      "2018-10-26T15:57:34.048819: step 1335, loss 0.231638, acc 0.90625\n",
      "2018-10-26T15:57:34.361791: step 1336, loss 0.269089, acc 0.90625\n",
      "2018-10-26T15:57:34.729814: step 1337, loss 0.226674, acc 0.921875\n",
      "2018-10-26T15:57:35.088193: step 1338, loss 0.208757, acc 0.890625\n",
      "2018-10-26T15:57:35.465046: step 1339, loss 0.221995, acc 0.90625\n",
      "2018-10-26T15:57:35.797470: step 1340, loss 0.271539, acc 0.859375\n",
      "2018-10-26T15:57:36.152211: step 1341, loss 0.219513, acc 0.890625\n",
      "2018-10-26T15:57:36.507263: step 1342, loss 0.289344, acc 0.859375\n",
      "2018-10-26T15:57:37.011915: step 1343, loss 0.303764, acc 0.890625\n",
      "2018-10-26T15:57:37.430825: step 1344, loss 0.0909035, acc 1\n",
      "2018-10-26T15:57:37.834715: step 1345, loss 0.397328, acc 0.8125\n",
      "2018-10-26T15:57:38.214775: step 1346, loss 0.163806, acc 0.953125\n",
      "2018-10-26T15:57:38.763318: step 1347, loss 0.254301, acc 0.890625\n",
      "2018-10-26T15:57:39.283842: step 1348, loss 0.257488, acc 0.859375\n",
      "2018-10-26T15:57:39.730662: step 1349, loss 0.20237, acc 0.921875\n",
      "2018-10-26T15:57:40.057773: step 1350, loss 0.194604, acc 0.9\n",
      "2018-10-26T15:57:40.589417: step 1351, loss 0.145192, acc 0.953125\n",
      "2018-10-26T15:57:41.017212: step 1352, loss 0.191957, acc 0.921875\n",
      "2018-10-26T15:57:41.325671: step 1353, loss 0.193834, acc 0.9375\n",
      "2018-10-26T15:57:41.817075: step 1354, loss 0.192336, acc 0.90625\n",
      "2018-10-26T15:57:42.255900: step 1355, loss 0.199947, acc 0.90625\n",
      "2018-10-26T15:57:42.621959: step 1356, loss 0.140682, acc 0.90625\n",
      "2018-10-26T15:57:43.051774: step 1357, loss 0.164948, acc 0.9375\n",
      "2018-10-26T15:57:43.455694: step 1358, loss 0.260449, acc 0.875\n",
      "2018-10-26T15:57:43.882554: step 1359, loss 0.167878, acc 0.921875\n",
      "2018-10-26T15:57:44.223644: step 1360, loss 0.128761, acc 0.921875\n",
      "2018-10-26T15:57:44.567724: step 1361, loss 0.146801, acc 0.953125\n",
      "2018-10-26T15:57:44.995581: step 1362, loss 0.222523, acc 0.921875\n",
      "2018-10-26T15:57:45.314728: step 1363, loss 0.108731, acc 0.96875\n",
      "2018-10-26T15:57:45.663795: step 1364, loss 0.202078, acc 0.921875\n",
      "2018-10-26T15:57:46.054752: step 1365, loss 0.184282, acc 0.921875\n",
      "2018-10-26T15:57:46.438724: step 1366, loss 0.160085, acc 0.96875\n",
      "2018-10-26T15:57:46.882544: step 1367, loss 0.137753, acc 0.9375\n",
      "2018-10-26T15:57:47.222632: step 1368, loss 0.246628, acc 0.890625\n",
      "2018-10-26T15:57:47.554747: step 1369, loss 0.130427, acc 0.953125\n",
      "2018-10-26T15:57:47.883863: step 1370, loss 0.146952, acc 0.921875\n",
      "2018-10-26T15:57:48.230936: step 1371, loss 0.112922, acc 1\n",
      "2018-10-26T15:57:48.578010: step 1372, loss 0.104338, acc 0.953125\n",
      "2018-10-26T15:57:48.901214: step 1373, loss 0.187448, acc 0.9375\n",
      "2018-10-26T15:57:49.201436: step 1374, loss 0.112009, acc 0.984375\n",
      "2018-10-26T15:57:49.534452: step 1375, loss 0.20076, acc 0.9375\n",
      "2018-10-26T15:57:49.850608: step 1376, loss 0.254046, acc 0.875\n",
      "2018-10-26T15:57:50.193024: step 1377, loss 0.176245, acc 0.953125\n",
      "2018-10-26T15:57:50.521963: step 1378, loss 0.207743, acc 0.9375\n",
      "2018-10-26T15:57:50.855046: step 1379, loss 0.191532, acc 0.953125\n",
      "2018-10-26T15:57:51.183048: step 1380, loss 0.109283, acc 0.96875\n",
      "2018-10-26T15:57:51.514165: step 1381, loss 0.161504, acc 0.921875\n",
      "2018-10-26T15:57:51.859245: step 1382, loss 0.181998, acc 0.9375\n",
      "2018-10-26T15:57:52.197341: step 1383, loss 0.184212, acc 0.921875\n",
      "2018-10-26T15:57:52.526656: step 1384, loss 0.211168, acc 0.921875\n",
      "2018-10-26T15:57:52.859593: step 1385, loss 0.133025, acc 0.953125\n",
      "2018-10-26T15:57:53.179714: step 1386, loss 0.175458, acc 0.921875\n",
      "2018-10-26T15:57:53.512824: step 1387, loss 0.212788, acc 0.9375\n",
      "2018-10-26T15:57:53.846071: step 1388, loss 0.143639, acc 0.953125\n",
      "2018-10-26T15:57:54.166078: step 1389, loss 0.156143, acc 0.953125\n",
      "2018-10-26T15:57:54.502179: step 1390, loss 0.260306, acc 0.859375\n",
      "2018-10-26T15:57:54.842356: step 1391, loss 0.205182, acc 0.90625\n",
      "2018-10-26T15:57:55.166512: step 1392, loss 0.225164, acc 0.921875\n",
      "2018-10-26T15:57:55.499515: step 1393, loss 0.144295, acc 0.921875\n",
      "2018-10-26T15:57:55.850577: step 1394, loss 0.134738, acc 0.9375\n",
      "2018-10-26T15:57:56.179744: step 1395, loss 0.315015, acc 0.890625\n",
      "2018-10-26T15:57:56.490866: step 1396, loss 0.226829, acc 0.890625\n",
      "2018-10-26T15:57:56.827967: step 1397, loss 0.242827, acc 0.875\n",
      "2018-10-26T15:57:57.162073: step 1398, loss 0.200804, acc 0.9375\n",
      "2018-10-26T15:57:57.504159: step 1399, loss 0.212288, acc 0.921875\n",
      "2018-10-26T15:57:57.836275: step 1400, loss 0.2177, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:57:58.629171: step 1400, loss 0.65177, acc 0.704503\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1400\n",
      "\n",
      "2018-10-26T15:57:59.228552: step 1401, loss 0.204687, acc 0.921875\n",
      "2018-10-26T15:57:59.558671: step 1402, loss 0.190388, acc 0.9375\n",
      "2018-10-26T15:58:00.010463: step 1403, loss 0.321391, acc 0.859375\n",
      "2018-10-26T15:58:00.368506: step 1404, loss 0.170728, acc 0.9375\n",
      "2018-10-26T15:58:00.692643: step 1405, loss 0.190366, acc 0.921875\n",
      "2018-10-26T15:58:01.026751: step 1406, loss 0.326362, acc 0.890625\n",
      "2018-10-26T15:58:01.402745: step 1407, loss 0.254464, acc 0.90625\n",
      "2018-10-26T15:58:01.746825: step 1408, loss 0.120118, acc 0.96875\n",
      "2018-10-26T15:58:02.085380: step 1409, loss 0.167683, acc 0.921875\n",
      "2018-10-26T15:58:02.404067: step 1410, loss 0.21707, acc 0.90625\n",
      "2018-10-26T15:58:02.756130: step 1411, loss 0.0812282, acc 0.96875\n",
      "2018-10-26T15:58:03.094227: step 1412, loss 0.166022, acc 0.9375\n",
      "2018-10-26T15:58:03.419940: step 1413, loss 0.153488, acc 0.921875\n",
      "2018-10-26T15:58:03.742492: step 1414, loss 0.102203, acc 0.984375\n",
      "2018-10-26T15:58:04.078597: step 1415, loss 0.177647, acc 0.9375\n",
      "2018-10-26T15:58:04.419770: step 1416, loss 0.19105, acc 0.921875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:58:04.729852: step 1417, loss 0.171905, acc 0.90625\n",
      "2018-10-26T15:58:05.106846: step 1418, loss 0.17888, acc 0.953125\n",
      "2018-10-26T15:58:05.431031: step 1419, loss 0.167486, acc 0.953125\n",
      "2018-10-26T15:58:05.753119: step 1420, loss 0.190081, acc 0.921875\n",
      "2018-10-26T15:58:06.104181: step 1421, loss 0.164262, acc 0.96875\n",
      "2018-10-26T15:58:06.446303: step 1422, loss 0.248268, acc 0.890625\n",
      "2018-10-26T15:58:06.794337: step 1423, loss 0.12456, acc 0.96875\n",
      "2018-10-26T15:58:07.132434: step 1424, loss 0.141551, acc 0.953125\n",
      "2018-10-26T15:58:07.547327: step 1425, loss 0.187448, acc 0.921875\n",
      "2018-10-26T15:58:07.877443: step 1426, loss 0.0958617, acc 0.96875\n",
      "2018-10-26T15:58:08.205575: step 1427, loss 0.18202, acc 0.921875\n",
      "2018-10-26T15:58:08.544665: step 1428, loss 0.0692698, acc 0.96875\n",
      "2018-10-26T15:58:08.865804: step 1429, loss 0.336231, acc 0.890625\n",
      "2018-10-26T15:58:09.200077: step 1430, loss 0.209366, acc 0.90625\n",
      "2018-10-26T15:58:09.532022: step 1431, loss 0.144396, acc 0.9375\n",
      "2018-10-26T15:58:09.846330: step 1432, loss 0.20352, acc 0.9375\n",
      "2018-10-26T15:58:10.169319: step 1433, loss 0.189787, acc 0.921875\n",
      "2018-10-26T15:58:10.499486: step 1434, loss 0.127421, acc 0.953125\n",
      "2018-10-26T15:58:10.841523: step 1435, loss 0.0561998, acc 1\n",
      "2018-10-26T15:58:11.175632: step 1436, loss 0.161462, acc 0.9375\n",
      "2018-10-26T15:58:11.510736: step 1437, loss 0.174906, acc 0.921875\n",
      "2018-10-26T15:58:11.841893: step 1438, loss 0.216821, acc 0.921875\n",
      "2018-10-26T15:58:12.191916: step 1439, loss 0.15016, acc 0.953125\n",
      "2018-10-26T15:58:12.535045: step 1440, loss 0.34108, acc 0.875\n",
      "2018-10-26T15:58:12.845172: step 1441, loss 0.177554, acc 0.953125\n",
      "2018-10-26T15:58:13.180279: step 1442, loss 0.183336, acc 0.921875\n",
      "2018-10-26T15:58:13.561330: step 1443, loss 0.237872, acc 0.90625\n",
      "2018-10-26T15:58:13.892374: step 1444, loss 0.138587, acc 0.96875\n",
      "2018-10-26T15:58:14.214513: step 1445, loss 0.13312, acc 0.9375\n",
      "2018-10-26T15:58:14.539643: step 1446, loss 0.0769783, acc 0.984375\n",
      "2018-10-26T15:58:14.898685: step 1447, loss 0.21228, acc 0.859375\n",
      "2018-10-26T15:58:15.276677: step 1448, loss 0.137618, acc 0.953125\n",
      "2018-10-26T15:58:15.602803: step 1449, loss 0.295176, acc 0.828125\n",
      "2018-10-26T15:58:16.390762: step 1450, loss 0.109494, acc 0.953125\n",
      "2018-10-26T15:58:16.816619: step 1451, loss 0.235596, acc 0.875\n",
      "2018-10-26T15:58:17.182581: step 1452, loss 0.140106, acc 0.9375\n",
      "2018-10-26T15:58:17.555593: step 1453, loss 0.122109, acc 0.96875\n",
      "2018-10-26T15:58:17.873736: step 1454, loss 0.139564, acc 0.9375\n",
      "2018-10-26T15:58:18.200862: step 1455, loss 0.18473, acc 0.921875\n",
      "2018-10-26T15:58:18.540993: step 1456, loss 0.358832, acc 0.890625\n",
      "2018-10-26T15:58:18.884038: step 1457, loss 0.12657, acc 0.953125\n",
      "2018-10-26T15:58:19.233103: step 1458, loss 0.242246, acc 0.890625\n",
      "2018-10-26T15:58:19.585165: step 1459, loss 0.116644, acc 0.96875\n",
      "2018-10-26T15:58:20.062887: step 1460, loss 0.173759, acc 0.9375\n",
      "2018-10-26T15:58:20.477811: step 1461, loss 0.110467, acc 0.953125\n",
      "2018-10-26T15:58:20.898654: step 1462, loss 0.176918, acc 0.953125\n",
      "2018-10-26T15:58:21.216804: step 1463, loss 0.235556, acc 0.875\n",
      "2018-10-26T15:58:21.561881: step 1464, loss 0.229814, acc 0.90625\n",
      "2018-10-26T15:58:22.109418: step 1465, loss 0.101165, acc 0.96875\n",
      "2018-10-26T15:58:22.553233: step 1466, loss 0.170012, acc 0.890625\n",
      "2018-10-26T15:58:22.937207: step 1467, loss 0.195013, acc 0.921875\n",
      "2018-10-26T15:58:23.449840: step 1468, loss 0.153759, acc 0.9375\n",
      "2018-10-26T15:58:23.789020: step 1469, loss 0.108148, acc 0.984375\n",
      "2018-10-26T15:58:24.141987: step 1470, loss 0.158753, acc 0.921875\n",
      "2018-10-26T15:58:24.491055: step 1471, loss 0.180096, acc 0.921875\n",
      "2018-10-26T15:58:24.846145: step 1472, loss 0.260072, acc 0.90625\n",
      "2018-10-26T15:58:25.186262: step 1473, loss 0.17354, acc 0.90625\n",
      "2018-10-26T15:58:25.534268: step 1474, loss 0.158147, acc 0.953125\n",
      "2018-10-26T15:58:25.879344: step 1475, loss 0.184713, acc 0.9375\n",
      "2018-10-26T15:58:26.233399: step 1476, loss 0.139854, acc 0.953125\n",
      "2018-10-26T15:58:26.637321: step 1477, loss 0.249035, acc 0.859375\n",
      "2018-10-26T15:58:27.188849: step 1478, loss 0.21612, acc 0.890625\n",
      "2018-10-26T15:58:27.692636: step 1479, loss 0.132367, acc 0.953125\n",
      "2018-10-26T15:58:28.137312: step 1480, loss 0.341246, acc 0.875\n",
      "2018-10-26T15:58:28.630021: step 1481, loss 0.125617, acc 0.96875\n",
      "2018-10-26T15:58:29.125672: step 1482, loss 0.143824, acc 0.96875\n",
      "2018-10-26T15:58:29.703130: step 1483, loss 0.134666, acc 0.9375\n",
      "2018-10-26T15:58:30.254751: step 1484, loss 0.207786, acc 0.921875\n",
      "2018-10-26T15:58:30.689494: step 1485, loss 0.151616, acc 0.9375\n",
      "2018-10-26T15:58:31.201327: step 1486, loss 0.170078, acc 0.9375\n",
      "2018-10-26T15:58:31.655911: step 1487, loss 0.260973, acc 0.875\n",
      "2018-10-26T15:58:32.233368: step 1488, loss 0.283164, acc 0.90625\n",
      "2018-10-26T15:58:32.973522: step 1489, loss 0.247017, acc 0.875\n",
      "2018-10-26T15:58:33.444133: step 1490, loss 0.135377, acc 0.953125\n",
      "2018-10-26T15:58:33.844064: step 1491, loss 0.104967, acc 0.984375\n",
      "2018-10-26T15:58:34.199115: step 1492, loss 0.236742, acc 0.90625\n",
      "2018-10-26T15:58:34.569127: step 1493, loss 0.212231, acc 0.921875\n",
      "2018-10-26T15:58:34.933157: step 1494, loss 0.167428, acc 0.9375\n",
      "2018-10-26T15:58:35.291249: step 1495, loss 0.273685, acc 0.875\n",
      "2018-10-26T15:58:35.610345: step 1496, loss 0.124589, acc 0.96875\n",
      "2018-10-26T15:58:35.970412: step 1497, loss 0.366066, acc 0.84375\n",
      "2018-10-26T15:58:36.421226: step 1498, loss 0.145159, acc 0.96875\n",
      "2018-10-26T15:58:36.732347: step 1499, loss 0.21316, acc 0.90625\n",
      "2018-10-26T15:58:37.057478: step 1500, loss 0.222924, acc 0.883333\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:58:37.943138: step 1500, loss 0.701145, acc 0.696998\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1500\n",
      "\n",
      "2018-10-26T15:58:38.475958: step 1501, loss 0.0820239, acc 1\n",
      "2018-10-26T15:58:38.794836: step 1502, loss 0.188734, acc 0.9375\n",
      "2018-10-26T15:58:39.228677: step 1503, loss 0.0651782, acc 0.984375\n",
      "2018-10-26T15:58:39.595697: step 1504, loss 0.0811009, acc 0.984375\n",
      "2018-10-26T15:58:39.992636: step 1505, loss 0.087713, acc 0.984375\n",
      "2018-10-26T15:58:40.299857: step 1506, loss 0.187281, acc 0.96875\n",
      "2018-10-26T15:58:40.609988: step 1507, loss 0.256307, acc 0.890625\n",
      "2018-10-26T15:58:40.969027: step 1508, loss 0.127801, acc 0.953125\n",
      "2018-10-26T15:58:41.310119: step 1509, loss 0.149199, acc 0.9375\n",
      "2018-10-26T15:58:41.686140: step 1510, loss 0.0739282, acc 1\n",
      "2018-10-26T15:58:41.991297: step 1511, loss 0.0736691, acc 0.96875\n",
      "2018-10-26T15:58:42.290497: step 1512, loss 0.162182, acc 0.953125\n",
      "2018-10-26T15:58:42.577729: step 1513, loss 0.0893332, acc 0.984375\n",
      "2018-10-26T15:58:42.865960: step 1514, loss 0.146339, acc 0.921875\n",
      "2018-10-26T15:58:43.188217: step 1515, loss 0.11302, acc 0.953125\n",
      "2018-10-26T15:58:43.517218: step 1516, loss 0.147517, acc 0.953125\n",
      "2018-10-26T15:58:43.812431: step 1517, loss 0.165991, acc 0.9375\n",
      "2018-10-26T15:58:44.131581: step 1518, loss 0.130055, acc 0.953125\n",
      "2018-10-26T15:58:44.434772: step 1519, loss 0.130788, acc 0.921875\n",
      "2018-10-26T15:58:44.752918: step 1520, loss 0.13856, acc 0.953125\n",
      "2018-10-26T15:58:45.084032: step 1521, loss 0.0945615, acc 0.96875\n",
      "2018-10-26T15:58:45.410161: step 1522, loss 0.141577, acc 0.90625\n",
      "2018-10-26T15:58:45.696397: step 1523, loss 0.181675, acc 0.921875\n",
      "2018-10-26T15:58:45.995917: step 1524, loss 0.257282, acc 0.90625\n",
      "2018-10-26T15:58:46.305802: step 1525, loss 0.113663, acc 0.96875\n",
      "2018-10-26T15:58:46.612984: step 1526, loss 0.0536433, acc 1\n",
      "2018-10-26T15:58:46.941072: step 1527, loss 0.0760633, acc 0.96875\n",
      "2018-10-26T15:58:47.363171: step 1528, loss 0.208234, acc 0.921875\n",
      "2018-10-26T15:58:47.670125: step 1529, loss 0.0822584, acc 0.984375\n",
      "2018-10-26T15:58:47.953367: step 1530, loss 0.104706, acc 0.96875\n",
      "2018-10-26T15:58:48.230628: step 1531, loss 0.122182, acc 0.953125\n",
      "2018-10-26T15:58:48.581690: step 1532, loss 0.126216, acc 0.96875\n",
      "2018-10-26T15:58:48.875902: step 1533, loss 0.118332, acc 0.96875\n",
      "2018-10-26T15:58:49.334675: step 1534, loss 0.204443, acc 0.9375\n",
      "2018-10-26T15:58:49.701699: step 1535, loss 0.0984894, acc 0.984375\n",
      "2018-10-26T15:58:50.064725: step 1536, loss 0.14342, acc 0.96875\n",
      "2018-10-26T15:58:50.386892: step 1537, loss 0.0955475, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:58:50.739921: step 1538, loss 0.169147, acc 0.9375\n",
      "2018-10-26T15:58:51.129879: step 1539, loss 0.102204, acc 0.96875\n",
      "2018-10-26T15:58:51.421193: step 1540, loss 0.121788, acc 0.96875\n",
      "2018-10-26T15:58:51.752230: step 1541, loss 0.0875507, acc 0.984375\n",
      "2018-10-26T15:58:52.096342: step 1542, loss 0.232051, acc 0.90625\n",
      "2018-10-26T15:58:52.442372: step 1543, loss 0.095623, acc 0.96875\n",
      "2018-10-26T15:58:52.793587: step 1544, loss 0.101002, acc 0.953125\n",
      "2018-10-26T15:58:53.244230: step 1545, loss 0.220802, acc 0.875\n",
      "2018-10-26T15:58:53.737911: step 1546, loss 0.158434, acc 0.9375\n",
      "2018-10-26T15:58:54.171752: step 1547, loss 0.184189, acc 0.9375\n",
      "2018-10-26T15:58:54.583652: step 1548, loss 0.155817, acc 0.921875\n",
      "2018-10-26T15:58:55.003530: step 1549, loss 0.106643, acc 0.96875\n",
      "2018-10-26T15:58:55.400469: step 1550, loss 0.0626397, acc 0.984375\n",
      "2018-10-26T15:58:55.737567: step 1551, loss 0.0882408, acc 0.984375\n",
      "2018-10-26T15:58:56.121542: step 1552, loss 0.150057, acc 0.9375\n",
      "2018-10-26T15:58:56.499532: step 1553, loss 0.155803, acc 0.921875\n",
      "2018-10-26T15:58:56.833639: step 1554, loss 0.132809, acc 0.9375\n",
      "2018-10-26T15:58:57.199662: step 1555, loss 0.147641, acc 0.953125\n",
      "2018-10-26T15:58:57.556707: step 1556, loss 0.11411, acc 0.9375\n",
      "2018-10-26T15:58:57.883234: step 1557, loss 0.120875, acc 0.9375\n",
      "2018-10-26T15:58:58.237888: step 1558, loss 0.215135, acc 0.921875\n",
      "2018-10-26T15:58:58.593097: step 1559, loss 0.197129, acc 0.9375\n",
      "2018-10-26T15:58:59.011820: step 1560, loss 0.0769408, acc 0.984375\n",
      "2018-10-26T15:58:59.368866: step 1561, loss 0.070495, acc 0.96875\n",
      "2018-10-26T15:58:59.838611: step 1562, loss 0.122674, acc 0.96875\n",
      "2018-10-26T15:59:00.331295: step 1563, loss 0.143858, acc 0.90625\n",
      "2018-10-26T15:59:00.739207: step 1564, loss 0.0964956, acc 0.96875\n",
      "2018-10-26T15:59:01.141131: step 1565, loss 0.118994, acc 0.953125\n",
      "2018-10-26T15:59:01.571040: step 1566, loss 0.116971, acc 0.953125\n",
      "2018-10-26T15:59:01.972908: step 1567, loss 0.0863591, acc 0.96875\n",
      "2018-10-26T15:59:02.354888: step 1568, loss 0.260979, acc 0.890625\n",
      "2018-10-26T15:59:02.664062: step 1569, loss 0.0928164, acc 0.984375\n",
      "2018-10-26T15:59:02.995178: step 1570, loss 0.16739, acc 0.9375\n",
      "2018-10-26T15:59:03.314323: step 1571, loss 0.185232, acc 0.9375\n",
      "2018-10-26T15:59:03.644442: step 1572, loss 0.144699, acc 0.9375\n",
      "2018-10-26T15:59:03.964587: step 1573, loss 0.100495, acc 0.96875\n",
      "2018-10-26T15:59:04.299691: step 1574, loss 0.150419, acc 0.921875\n",
      "2018-10-26T15:59:04.710593: step 1575, loss 0.191999, acc 0.921875\n",
      "2018-10-26T15:59:04.994833: step 1576, loss 0.10541, acc 0.953125\n",
      "2018-10-26T15:59:05.318993: step 1577, loss 0.089842, acc 0.953125\n",
      "2018-10-26T15:59:05.727876: step 1578, loss 0.145231, acc 0.921875\n",
      "2018-10-26T15:59:06.190639: step 1579, loss 0.139778, acc 0.9375\n",
      "2018-10-26T15:59:06.537717: step 1580, loss 0.162163, acc 0.9375\n",
      "2018-10-26T15:59:06.921988: step 1581, loss 0.12628, acc 0.9375\n",
      "2018-10-26T15:59:07.359515: step 1582, loss 0.119054, acc 0.953125\n",
      "2018-10-26T15:59:07.720623: step 1583, loss 0.167901, acc 0.96875\n",
      "2018-10-26T15:59:08.045684: step 1584, loss 0.151385, acc 0.953125\n",
      "2018-10-26T15:59:08.410708: step 1585, loss 0.143754, acc 0.9375\n",
      "2018-10-26T15:59:08.777727: step 1586, loss 0.135501, acc 0.96875\n",
      "2018-10-26T15:59:09.171674: step 1587, loss 0.0863197, acc 0.96875\n",
      "2018-10-26T15:59:09.517849: step 1588, loss 0.174647, acc 0.921875\n",
      "2018-10-26T15:59:09.836899: step 1589, loss 0.158799, acc 0.921875\n",
      "2018-10-26T15:59:10.173046: step 1590, loss 0.0981383, acc 0.9375\n",
      "2018-10-26T15:59:10.565950: step 1591, loss 0.128239, acc 0.953125\n",
      "2018-10-26T15:59:10.905042: step 1592, loss 0.236203, acc 0.921875\n",
      "2018-10-26T15:59:11.274058: step 1593, loss 0.280385, acc 0.828125\n",
      "2018-10-26T15:59:11.650052: step 1594, loss 0.162932, acc 0.921875\n",
      "2018-10-26T15:59:12.029042: step 1595, loss 0.221891, acc 0.890625\n",
      "2018-10-26T15:59:12.405646: step 1596, loss 0.170914, acc 0.921875\n",
      "2018-10-26T15:59:12.858823: step 1597, loss 0.0683278, acc 0.984375\n",
      "2018-10-26T15:59:13.263744: step 1598, loss 0.0932147, acc 0.96875\n",
      "2018-10-26T15:59:13.644724: step 1599, loss 0.121243, acc 0.96875\n",
      "2018-10-26T15:59:14.022716: step 1600, loss 0.11134, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:59:14.886721: step 1600, loss 0.70523, acc 0.703565\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1600\n",
      "\n",
      "2018-10-26T15:59:15.637399: step 1601, loss 0.27027, acc 0.90625\n",
      "2018-10-26T15:59:16.033341: step 1602, loss 0.104609, acc 0.96875\n",
      "2018-10-26T15:59:16.526024: step 1603, loss 0.122968, acc 0.96875\n",
      "2018-10-26T15:59:16.906009: step 1604, loss 0.3115, acc 0.90625\n",
      "2018-10-26T15:59:17.252086: step 1605, loss 0.123198, acc 0.96875\n",
      "2018-10-26T15:59:17.583201: step 1606, loss 0.124815, acc 0.953125\n",
      "2018-10-26T15:59:17.922296: step 1607, loss 0.137695, acc 0.9375\n",
      "2018-10-26T15:59:18.253409: step 1608, loss 0.0888609, acc 0.96875\n",
      "2018-10-26T15:59:18.581659: step 1609, loss 0.101172, acc 0.96875\n",
      "2018-10-26T15:59:18.957529: step 1610, loss 0.276728, acc 0.90625\n",
      "2018-10-26T15:59:19.368431: step 1611, loss 0.279393, acc 0.921875\n",
      "2018-10-26T15:59:19.698548: step 1612, loss 0.163848, acc 0.9375\n",
      "2018-10-26T15:59:20.089503: step 1613, loss 0.146313, acc 0.9375\n",
      "2018-10-26T15:59:20.453530: step 1614, loss 0.204485, acc 0.90625\n",
      "2018-10-26T15:59:20.878439: step 1615, loss 0.144682, acc 0.953125\n",
      "2018-10-26T15:59:21.312348: step 1616, loss 0.0920665, acc 0.96875\n",
      "2018-10-26T15:59:21.723141: step 1617, loss 0.0992881, acc 0.984375\n",
      "2018-10-26T15:59:22.090158: step 1618, loss 0.146129, acc 0.953125\n",
      "2018-10-26T15:59:22.424266: step 1619, loss 0.116211, acc 0.984375\n",
      "2018-10-26T15:59:22.734438: step 1620, loss 0.149935, acc 0.9375\n",
      "2018-10-26T15:59:23.057612: step 1621, loss 0.0868091, acc 0.984375\n",
      "2018-10-26T15:59:23.370739: step 1622, loss 0.142763, acc 0.9375\n",
      "2018-10-26T15:59:23.702850: step 1623, loss 0.14725, acc 0.953125\n",
      "2018-10-26T15:59:24.025986: step 1624, loss 0.111405, acc 0.96875\n",
      "2018-10-26T15:59:24.347127: step 1625, loss 0.0805561, acc 0.9375\n",
      "2018-10-26T15:59:24.668308: step 1626, loss 0.135759, acc 0.953125\n",
      "2018-10-26T15:59:25.008404: step 1627, loss 0.169314, acc 0.9375\n",
      "2018-10-26T15:59:25.345622: step 1628, loss 0.152457, acc 0.921875\n",
      "2018-10-26T15:59:25.805247: step 1629, loss 0.106824, acc 0.953125\n",
      "2018-10-26T15:59:26.128415: step 1630, loss 0.110416, acc 0.953125\n",
      "2018-10-26T15:59:26.464477: step 1631, loss 0.138139, acc 0.921875\n",
      "2018-10-26T15:59:26.826504: step 1632, loss 0.172085, acc 0.9375\n",
      "2018-10-26T15:59:27.235410: step 1633, loss 0.17196, acc 0.921875\n",
      "2018-10-26T15:59:27.580489: step 1634, loss 0.0994991, acc 0.96875\n",
      "2018-10-26T15:59:27.993388: step 1635, loss 0.109116, acc 0.953125\n",
      "2018-10-26T15:59:28.438198: step 1636, loss 0.159279, acc 0.921875\n",
      "2018-10-26T15:59:28.748367: step 1637, loss 0.219344, acc 0.875\n",
      "2018-10-26T15:59:29.187195: step 1638, loss 0.0939911, acc 0.953125\n",
      "2018-10-26T15:59:29.486395: step 1639, loss 0.0996146, acc 0.953125\n",
      "2018-10-26T15:59:29.832472: step 1640, loss 0.132223, acc 0.96875\n",
      "2018-10-26T15:59:30.170615: step 1641, loss 0.100268, acc 0.953125\n",
      "2018-10-26T15:59:30.504677: step 1642, loss 0.10005, acc 0.96875\n",
      "2018-10-26T15:59:30.841775: step 1643, loss 0.100173, acc 0.96875\n",
      "2018-10-26T15:59:31.185859: step 1644, loss 0.126252, acc 0.9375\n",
      "2018-10-26T15:59:31.555867: step 1645, loss 0.137703, acc 0.953125\n",
      "2018-10-26T15:59:31.883991: step 1646, loss 0.155496, acc 0.890625\n",
      "2018-10-26T15:59:32.304942: step 1647, loss 0.135782, acc 0.9375\n",
      "2018-10-26T15:59:32.678867: step 1648, loss 0.177879, acc 0.890625\n",
      "2018-10-26T15:59:33.059850: step 1649, loss 0.143779, acc 0.953125\n",
      "2018-10-26T15:59:33.371018: step 1650, loss 0.121071, acc 0.966667\n",
      "2018-10-26T15:59:33.728065: step 1651, loss 0.0775769, acc 0.96875\n",
      "2018-10-26T15:59:34.253659: step 1652, loss 0.0950454, acc 0.96875\n",
      "2018-10-26T15:59:34.638629: step 1653, loss 0.0570598, acc 0.984375\n",
      "2018-10-26T15:59:35.026594: step 1654, loss 0.119532, acc 0.9375\n",
      "2018-10-26T15:59:35.538251: step 1655, loss 0.0985644, acc 0.953125\n",
      "2018-10-26T15:59:36.061922: step 1656, loss 0.139302, acc 0.96875\n",
      "2018-10-26T15:59:36.530576: step 1657, loss 0.168824, acc 0.90625\n",
      "2018-10-26T15:59:36.996331: step 1658, loss 0.125291, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T15:59:37.468182: step 1659, loss 0.135056, acc 0.953125\n",
      "2018-10-26T15:59:37.947821: step 1660, loss 0.0775215, acc 0.984375\n",
      "2018-10-26T15:59:38.409554: step 1661, loss 0.0835767, acc 0.984375\n",
      "2018-10-26T15:59:38.845559: step 1662, loss 0.131999, acc 0.953125\n",
      "2018-10-26T15:59:39.247317: step 1663, loss 0.108124, acc 0.96875\n",
      "2018-10-26T15:59:39.703100: step 1664, loss 0.102659, acc 0.953125\n",
      "2018-10-26T15:59:40.112006: step 1665, loss 0.0814418, acc 0.984375\n",
      "2018-10-26T15:59:40.494982: step 1666, loss 0.150572, acc 0.9375\n",
      "2018-10-26T15:59:40.833116: step 1667, loss 0.104767, acc 0.96875\n",
      "2018-10-26T15:59:41.177160: step 1668, loss 0.11682, acc 0.9375\n",
      "2018-10-26T15:59:41.536200: step 1669, loss 0.0803008, acc 0.96875\n",
      "2018-10-26T15:59:41.876300: step 1670, loss 0.0825581, acc 0.96875\n",
      "2018-10-26T15:59:42.202420: step 1671, loss 0.0837951, acc 0.953125\n",
      "2018-10-26T15:59:42.564453: step 1672, loss 0.136106, acc 0.953125\n",
      "2018-10-26T15:59:42.926565: step 1673, loss 0.214747, acc 0.9375\n",
      "2018-10-26T15:59:43.313454: step 1674, loss 0.151547, acc 0.90625\n",
      "2018-10-26T15:59:43.698424: step 1675, loss 0.141773, acc 0.96875\n",
      "2018-10-26T15:59:44.058462: step 1676, loss 0.0766034, acc 0.96875\n",
      "2018-10-26T15:59:44.397559: step 1677, loss 0.0764251, acc 0.984375\n",
      "2018-10-26T15:59:44.735656: step 1678, loss 0.136378, acc 0.9375\n",
      "2018-10-26T15:59:45.085718: step 1679, loss 0.0454052, acc 1\n",
      "2018-10-26T15:59:45.460762: step 1680, loss 0.188082, acc 0.921875\n",
      "2018-10-26T15:59:45.872616: step 1681, loss 0.128844, acc 0.953125\n",
      "2018-10-26T15:59:46.247613: step 1682, loss 0.14685, acc 0.9375\n",
      "2018-10-26T15:59:46.597678: step 1683, loss 0.157205, acc 0.9375\n",
      "2018-10-26T15:59:46.970680: step 1684, loss 0.13306, acc 0.9375\n",
      "2018-10-26T15:59:47.318752: step 1685, loss 0.0772671, acc 0.984375\n",
      "2018-10-26T15:59:47.630958: step 1686, loss 0.10545, acc 0.953125\n",
      "2018-10-26T15:59:48.047841: step 1687, loss 0.0553628, acc 0.984375\n",
      "2018-10-26T15:59:48.408840: step 1688, loss 0.125565, acc 0.96875\n",
      "2018-10-26T15:59:48.814753: step 1689, loss 0.191018, acc 0.921875\n",
      "2018-10-26T15:59:49.166814: step 1690, loss 0.169133, acc 0.90625\n",
      "2018-10-26T15:59:49.509939: step 1691, loss 0.0672557, acc 0.984375\n",
      "2018-10-26T15:59:49.855975: step 1692, loss 0.0481396, acc 0.984375\n",
      "2018-10-26T15:59:50.239948: step 1693, loss 0.115089, acc 0.96875\n",
      "2018-10-26T15:59:50.583029: step 1694, loss 0.209585, acc 0.9375\n",
      "2018-10-26T15:59:50.981091: step 1695, loss 0.117004, acc 0.96875\n",
      "2018-10-26T15:59:51.323053: step 1696, loss 0.161565, acc 0.90625\n",
      "2018-10-26T15:59:51.716003: step 1697, loss 0.146734, acc 0.953125\n",
      "2018-10-26T15:59:52.049113: step 1698, loss 0.0722895, acc 0.984375\n",
      "2018-10-26T15:59:52.407155: step 1699, loss 0.118605, acc 0.9375\n",
      "2018-10-26T15:59:52.818106: step 1700, loss 0.175675, acc 0.953125\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T15:59:53.600055: step 1700, loss 0.739015, acc 0.699812\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1700\n",
      "\n",
      "2018-10-26T15:59:54.257216: step 1701, loss 0.0617641, acc 1\n",
      "2018-10-26T15:59:54.596310: step 1702, loss 0.122224, acc 0.921875\n",
      "2018-10-26T15:59:55.054083: step 1703, loss 0.0586779, acc 1\n",
      "2018-10-26T15:59:55.390185: step 1704, loss 0.121872, acc 0.9375\n",
      "2018-10-26T15:59:55.784207: step 1705, loss 0.128566, acc 0.9375\n",
      "2018-10-26T15:59:56.143174: step 1706, loss 0.0914107, acc 0.984375\n",
      "2018-10-26T15:59:56.501216: step 1707, loss 0.100093, acc 0.96875\n",
      "2018-10-26T15:59:56.840361: step 1708, loss 0.122539, acc 0.921875\n",
      "2018-10-26T15:59:57.195362: step 1709, loss 0.119469, acc 0.9375\n",
      "2018-10-26T15:59:57.529469: step 1710, loss 0.0693643, acc 0.984375\n",
      "2018-10-26T15:59:57.913495: step 1711, loss 0.0790397, acc 0.96875\n",
      "2018-10-26T15:59:58.246633: step 1712, loss 0.0923558, acc 0.96875\n",
      "2018-10-26T15:59:58.607590: step 1713, loss 0.075029, acc 0.984375\n",
      "2018-10-26T15:59:58.956657: step 1714, loss 0.113145, acc 0.953125\n",
      "2018-10-26T15:59:59.327667: step 1715, loss 0.0970134, acc 0.96875\n",
      "2018-10-26T15:59:59.669750: step 1716, loss 0.107594, acc 0.953125\n",
      "2018-10-26T16:00:00.017820: step 1717, loss 0.149608, acc 0.9375\n",
      "2018-10-26T16:00:00.376863: step 1718, loss 0.100359, acc 0.953125\n",
      "2018-10-26T16:00:00.737953: step 1719, loss 0.195727, acc 0.9375\n",
      "2018-10-26T16:00:01.107911: step 1720, loss 0.206257, acc 0.921875\n",
      "2018-10-26T16:00:01.457973: step 1721, loss 0.0627118, acc 0.96875\n",
      "2018-10-26T16:00:01.781110: step 1722, loss 0.0934704, acc 0.96875\n",
      "2018-10-26T16:00:02.128183: step 1723, loss 0.0618649, acc 0.984375\n",
      "2018-10-26T16:00:02.445336: step 1724, loss 0.0399061, acc 1\n",
      "2018-10-26T16:00:02.766477: step 1725, loss 0.096145, acc 0.96875\n",
      "2018-10-26T16:00:03.160425: step 1726, loss 0.205563, acc 0.90625\n",
      "2018-10-26T16:00:03.525450: step 1727, loss 0.134899, acc 0.953125\n",
      "2018-10-26T16:00:03.861552: step 1728, loss 0.0603961, acc 0.984375\n",
      "2018-10-26T16:00:04.253507: step 1729, loss 0.111375, acc 0.96875\n",
      "2018-10-26T16:00:04.599580: step 1730, loss 0.175025, acc 0.953125\n",
      "2018-10-26T16:00:04.963606: step 1731, loss 0.118136, acc 0.953125\n",
      "2018-10-26T16:00:05.311739: step 1732, loss 0.216417, acc 0.921875\n",
      "2018-10-26T16:00:05.655757: step 1733, loss 0.0950496, acc 0.984375\n",
      "2018-10-26T16:00:05.998841: step 1734, loss 0.11716, acc 0.953125\n",
      "2018-10-26T16:00:06.344915: step 1735, loss 0.210382, acc 0.890625\n",
      "2018-10-26T16:00:06.688000: step 1736, loss 0.0791966, acc 0.984375\n",
      "2018-10-26T16:00:07.080349: step 1737, loss 0.10021, acc 0.953125\n",
      "2018-10-26T16:00:07.467916: step 1738, loss 0.183598, acc 0.9375\n",
      "2018-10-26T16:00:07.783073: step 1739, loss 0.120275, acc 0.921875\n",
      "2018-10-26T16:00:08.154082: step 1740, loss 0.0453176, acc 1\n",
      "2018-10-26T16:00:08.531182: step 1741, loss 0.107332, acc 0.953125\n",
      "2018-10-26T16:00:08.885128: step 1742, loss 0.0813855, acc 0.984375\n",
      "2018-10-26T16:00:09.219237: step 1743, loss 0.132133, acc 0.921875\n",
      "2018-10-26T16:00:09.559332: step 1744, loss 0.153452, acc 0.9375\n",
      "2018-10-26T16:00:09.927346: step 1745, loss 0.115442, acc 0.96875\n",
      "2018-10-26T16:00:10.282476: step 1746, loss 0.103769, acc 0.953125\n",
      "2018-10-26T16:00:10.649415: step 1747, loss 0.0888655, acc 0.953125\n",
      "2018-10-26T16:00:11.006462: step 1748, loss 0.194557, acc 0.90625\n",
      "2018-10-26T16:00:11.392429: step 1749, loss 0.0787831, acc 0.96875\n",
      "2018-10-26T16:00:11.722549: step 1750, loss 0.0498871, acc 1\n",
      "2018-10-26T16:00:12.078597: step 1751, loss 0.0653279, acc 0.96875\n",
      "2018-10-26T16:00:12.457587: step 1752, loss 0.072346, acc 0.96875\n",
      "2018-10-26T16:00:12.824754: step 1753, loss 0.0587022, acc 0.984375\n",
      "2018-10-26T16:00:13.190630: step 1754, loss 0.12194, acc 0.984375\n",
      "2018-10-26T16:00:13.541775: step 1755, loss 0.184995, acc 0.921875\n",
      "2018-10-26T16:00:13.929650: step 1756, loss 0.147405, acc 0.9375\n",
      "2018-10-26T16:00:14.257776: step 1757, loss 0.0541136, acc 1\n",
      "2018-10-26T16:00:14.600858: step 1758, loss 0.0603042, acc 0.984375\n",
      "2018-10-26T16:00:14.975867: step 1759, loss 0.0939799, acc 0.96875\n",
      "2018-10-26T16:00:15.375787: step 1760, loss 0.123065, acc 0.9375\n",
      "2018-10-26T16:00:15.810625: step 1761, loss 0.051427, acc 0.96875\n",
      "2018-10-26T16:00:16.188617: step 1762, loss 0.0774183, acc 0.96875\n",
      "2018-10-26T16:00:16.507761: step 1763, loss 0.20111, acc 0.9375\n",
      "2018-10-26T16:00:16.834888: step 1764, loss 0.109525, acc 0.953125\n",
      "2018-10-26T16:00:17.172989: step 1765, loss 0.0787053, acc 0.984375\n",
      "2018-10-26T16:00:17.503297: step 1766, loss 0.125682, acc 0.984375\n",
      "2018-10-26T16:00:17.857157: step 1767, loss 0.0598436, acc 0.984375\n",
      "2018-10-26T16:00:18.230161: step 1768, loss 0.104573, acc 0.96875\n",
      "2018-10-26T16:00:18.641113: step 1769, loss 0.17787, acc 0.921875\n",
      "2018-10-26T16:00:19.118788: step 1770, loss 0.0686518, acc 0.984375\n",
      "2018-10-26T16:00:19.480837: step 1771, loss 0.131205, acc 0.953125\n",
      "2018-10-26T16:00:19.798968: step 1772, loss 0.0525667, acc 0.984375\n",
      "2018-10-26T16:00:20.113128: step 1773, loss 0.113719, acc 0.96875\n",
      "2018-10-26T16:00:20.432276: step 1774, loss 0.155297, acc 0.96875\n",
      "2018-10-26T16:00:20.810499: step 1775, loss 0.192783, acc 0.9375\n",
      "2018-10-26T16:00:21.162325: step 1776, loss 0.136869, acc 0.96875\n",
      "2018-10-26T16:00:21.551293: step 1777, loss 0.0852847, acc 0.96875\n",
      "2018-10-26T16:00:21.924300: step 1778, loss 0.0453566, acc 0.984375\n",
      "2018-10-26T16:00:22.249461: step 1779, loss 0.0879557, acc 0.953125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:00:22.585527: step 1780, loss 0.164629, acc 0.9375\n",
      "2018-10-26T16:00:22.961555: step 1781, loss 0.0371042, acc 1\n",
      "2018-10-26T16:00:23.317568: step 1782, loss 0.145091, acc 0.96875\n",
      "2018-10-26T16:00:23.706613: step 1783, loss 0.0368297, acc 1\n",
      "2018-10-26T16:00:24.046622: step 1784, loss 0.0714975, acc 0.984375\n",
      "2018-10-26T16:00:24.375744: step 1785, loss 0.153699, acc 0.96875\n",
      "2018-10-26T16:00:24.702866: step 1786, loss 0.0798357, acc 0.96875\n",
      "2018-10-26T16:00:25.038968: step 1787, loss 0.125324, acc 0.953125\n",
      "2018-10-26T16:00:25.363102: step 1788, loss 0.0982159, acc 0.953125\n",
      "2018-10-26T16:00:25.689230: step 1789, loss 0.0942755, acc 0.96875\n",
      "2018-10-26T16:00:26.010459: step 1790, loss 0.062838, acc 0.984375\n",
      "2018-10-26T16:00:26.325531: step 1791, loss 0.0753873, acc 0.984375\n",
      "2018-10-26T16:00:26.648668: step 1792, loss 0.0948879, acc 0.953125\n",
      "2018-10-26T16:00:26.937944: step 1793, loss 0.157029, acc 0.921875\n",
      "2018-10-26T16:00:27.234416: step 1794, loss 0.100779, acc 0.953125\n",
      "2018-10-26T16:00:27.551296: step 1795, loss 0.0660415, acc 0.984375\n",
      "2018-10-26T16:00:27.876388: step 1796, loss 0.16017, acc 0.9375\n",
      "2018-10-26T16:00:28.204536: step 1797, loss 0.105018, acc 0.953125\n",
      "2018-10-26T16:00:28.521664: step 1798, loss 0.123777, acc 0.953125\n",
      "2018-10-26T16:00:28.821958: step 1799, loss 0.172233, acc 0.90625\n",
      "2018-10-26T16:00:29.157974: step 1800, loss 0.0724566, acc 0.983333\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:00:29.967798: step 1800, loss 0.762618, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1800\n",
      "\n",
      "2018-10-26T16:00:30.749709: step 1801, loss 0.115489, acc 0.953125\n",
      "2018-10-26T16:00:31.147682: step 1802, loss 0.0628014, acc 0.984375\n",
      "2018-10-26T16:00:31.575504: step 1803, loss 0.0564442, acc 0.984375\n",
      "2018-10-26T16:00:31.909611: step 1804, loss 0.0433747, acc 1\n",
      "2018-10-26T16:00:32.259771: step 1805, loss 0.0277171, acc 1\n",
      "2018-10-26T16:00:32.652788: step 1806, loss 0.0497454, acc 0.984375\n",
      "2018-10-26T16:00:33.049278: step 1807, loss 0.0570701, acc 0.984375\n",
      "2018-10-26T16:00:33.417633: step 1808, loss 0.0598851, acc 0.984375\n",
      "2018-10-26T16:00:33.824496: step 1809, loss 0.105521, acc 0.96875\n",
      "2018-10-26T16:00:34.261484: step 1810, loss 0.0824084, acc 0.953125\n",
      "2018-10-26T16:00:34.597430: step 1811, loss 0.0547608, acc 0.984375\n",
      "2018-10-26T16:00:35.000420: step 1812, loss 0.0988315, acc 0.953125\n",
      "2018-10-26T16:00:35.335461: step 1813, loss 0.0941607, acc 0.953125\n",
      "2018-10-26T16:00:35.672558: step 1814, loss 0.0594981, acc 0.984375\n",
      "2018-10-26T16:00:36.026610: step 1815, loss 0.0526896, acc 0.984375\n",
      "2018-10-26T16:00:36.409599: step 1816, loss 0.0555267, acc 1\n",
      "2018-10-26T16:00:36.787651: step 1817, loss 0.0587803, acc 0.984375\n",
      "2018-10-26T16:00:37.102735: step 1818, loss 0.0837108, acc 0.984375\n",
      "2018-10-26T16:00:37.476741: step 1819, loss 0.0402927, acc 0.984375\n",
      "2018-10-26T16:00:37.817825: step 1820, loss 0.0632759, acc 0.984375\n",
      "2018-10-26T16:00:38.200843: step 1821, loss 0.0941255, acc 0.96875\n",
      "2018-10-26T16:00:38.528115: step 1822, loss 0.0893242, acc 0.96875\n",
      "2018-10-26T16:00:38.864029: step 1823, loss 0.130551, acc 0.953125\n",
      "2018-10-26T16:00:39.241103: step 1824, loss 0.14798, acc 0.9375\n",
      "2018-10-26T16:00:39.563256: step 1825, loss 0.113568, acc 0.984375\n",
      "2018-10-26T16:00:39.950131: step 1826, loss 0.0770581, acc 0.953125\n",
      "2018-10-26T16:00:40.324128: step 1827, loss 0.0646135, acc 0.96875\n",
      "2018-10-26T16:00:40.671244: step 1828, loss 0.0431188, acc 1\n",
      "2018-10-26T16:00:41.077188: step 1829, loss 0.0833696, acc 0.984375\n",
      "2018-10-26T16:00:41.501984: step 1830, loss 0.0435759, acc 1\n",
      "2018-10-26T16:00:41.947967: step 1831, loss 0.115006, acc 0.9375\n",
      "2018-10-26T16:00:42.365675: step 1832, loss 0.0513613, acc 0.984375\n",
      "2018-10-26T16:00:42.824448: step 1833, loss 0.103167, acc 0.9375\n",
      "2018-10-26T16:00:43.249312: step 1834, loss 0.107014, acc 0.9375\n",
      "2018-10-26T16:00:43.656226: step 1835, loss 0.0350976, acc 0.984375\n",
      "2018-10-26T16:00:44.141928: step 1836, loss 0.0531416, acc 1\n",
      "2018-10-26T16:00:44.557815: step 1837, loss 0.0812768, acc 0.9375\n",
      "2018-10-26T16:00:44.886937: step 1838, loss 0.107041, acc 0.953125\n",
      "2018-10-26T16:00:45.270910: step 1839, loss 0.0258315, acc 1\n",
      "2018-10-26T16:00:45.663865: step 1840, loss 0.0617414, acc 0.96875\n",
      "2018-10-26T16:00:46.042849: step 1841, loss 0.0421673, acc 0.984375\n",
      "2018-10-26T16:00:46.433803: step 1842, loss 0.121258, acc 0.96875\n",
      "2018-10-26T16:00:46.837728: step 1843, loss 0.0400314, acc 0.984375\n",
      "2018-10-26T16:00:47.193943: step 1844, loss 0.063851, acc 0.984375\n",
      "2018-10-26T16:00:47.536891: step 1845, loss 0.111632, acc 0.96875\n",
      "2018-10-26T16:00:47.866975: step 1846, loss 0.082891, acc 0.96875\n",
      "2018-10-26T16:00:48.208064: step 1847, loss 0.0769465, acc 0.96875\n",
      "2018-10-26T16:00:48.551204: step 1848, loss 0.100989, acc 0.984375\n",
      "2018-10-26T16:00:48.905201: step 1849, loss 0.0712925, acc 0.96875\n",
      "2018-10-26T16:00:49.251315: step 1850, loss 0.0861016, acc 0.984375\n",
      "2018-10-26T16:00:49.582392: step 1851, loss 0.0603164, acc 1\n",
      "2018-10-26T16:00:49.936446: step 1852, loss 0.0740495, acc 0.984375\n",
      "2018-10-26T16:00:50.371286: step 1853, loss 0.159483, acc 0.921875\n",
      "2018-10-26T16:00:50.735312: step 1854, loss 0.103079, acc 0.953125\n",
      "2018-10-26T16:00:51.099339: step 1855, loss 0.0530712, acc 0.96875\n",
      "2018-10-26T16:00:51.474448: step 1856, loss 0.162599, acc 0.921875\n",
      "2018-10-26T16:00:51.937100: step 1857, loss 0.112952, acc 0.984375\n",
      "2018-10-26T16:00:52.348005: step 1858, loss 0.0632726, acc 0.96875\n",
      "2018-10-26T16:00:52.682110: step 1859, loss 0.0669994, acc 0.984375\n",
      "2018-10-26T16:00:53.004252: step 1860, loss 0.0886713, acc 0.984375\n",
      "2018-10-26T16:00:53.337359: step 1861, loss 0.101457, acc 0.953125\n",
      "2018-10-26T16:00:53.627584: step 1862, loss 0.113332, acc 0.96875\n",
      "2018-10-26T16:00:53.965681: step 1863, loss 0.0949553, acc 0.953125\n",
      "2018-10-26T16:00:54.297793: step 1864, loss 0.0909101, acc 0.9375\n",
      "2018-10-26T16:00:54.634895: step 1865, loss 0.0423832, acc 0.984375\n",
      "2018-10-26T16:00:54.957032: step 1866, loss 0.0469322, acc 1\n",
      "2018-10-26T16:00:55.287152: step 1867, loss 0.0183948, acc 1\n",
      "2018-10-26T16:00:55.635219: step 1868, loss 0.0342913, acc 1\n",
      "2018-10-26T16:00:55.960350: step 1869, loss 0.0295495, acc 1\n",
      "2018-10-26T16:00:56.400216: step 1870, loss 0.0461344, acc 0.984375\n",
      "2018-10-26T16:00:56.715334: step 1871, loss 0.0591561, acc 0.96875\n",
      "2018-10-26T16:00:57.031491: step 1872, loss 0.139954, acc 0.984375\n",
      "2018-10-26T16:00:57.395517: step 1873, loss 0.0529704, acc 0.984375\n",
      "2018-10-26T16:00:57.714711: step 1874, loss 0.105027, acc 0.96875\n",
      "2018-10-26T16:00:58.065726: step 1875, loss 0.0538115, acc 0.984375\n",
      "2018-10-26T16:00:58.407844: step 1876, loss 0.0568219, acc 0.96875\n",
      "2018-10-26T16:00:58.811734: step 1877, loss 0.0848665, acc 0.953125\n",
      "2018-10-26T16:00:59.196704: step 1878, loss 0.0587384, acc 0.96875\n",
      "2018-10-26T16:00:59.586665: step 1879, loss 0.0835196, acc 0.96875\n",
      "2018-10-26T16:00:59.924760: step 1880, loss 0.0249313, acc 1\n",
      "2018-10-26T16:01:00.275821: step 1881, loss 0.0374317, acc 0.984375\n",
      "2018-10-26T16:01:00.601948: step 1882, loss 0.0680597, acc 0.96875\n",
      "2018-10-26T16:01:00.977950: step 1883, loss 0.0533092, acc 0.984375\n",
      "2018-10-26T16:01:01.356124: step 1884, loss 0.0558271, acc 0.984375\n",
      "2018-10-26T16:01:01.724948: step 1885, loss 0.165407, acc 0.9375\n",
      "2018-10-26T16:01:02.076013: step 1886, loss 0.0758634, acc 0.984375\n",
      "2018-10-26T16:01:02.544815: step 1887, loss 0.0942312, acc 0.953125\n",
      "2018-10-26T16:01:02.887842: step 1888, loss 0.0602393, acc 0.96875\n",
      "2018-10-26T16:01:03.221007: step 1889, loss 0.0411075, acc 1\n",
      "2018-10-26T16:01:03.572016: step 1890, loss 0.0903902, acc 0.984375\n",
      "2018-10-26T16:01:03.943022: step 1891, loss 0.0788046, acc 0.96875\n",
      "2018-10-26T16:01:04.308051: step 1892, loss 0.0786729, acc 0.96875\n",
      "2018-10-26T16:01:04.776798: step 1893, loss 0.0567056, acc 1\n",
      "2018-10-26T16:01:05.159773: step 1894, loss 0.0811004, acc 0.96875\n",
      "2018-10-26T16:01:05.499651: step 1895, loss 0.0742062, acc 0.984375\n",
      "2018-10-26T16:01:05.861897: step 1896, loss 0.0764327, acc 0.984375\n",
      "2018-10-26T16:01:06.286764: step 1897, loss 0.0525004, acc 0.96875\n",
      "2018-10-26T16:01:06.641812: step 1898, loss 0.0594253, acc 0.984375\n",
      "2018-10-26T16:01:07.026862: step 1899, loss 0.177136, acc 0.96875\n",
      "2018-10-26T16:01:07.412795: step 1900, loss 0.0384688, acc 1\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:01:08.276526: step 1900, loss 0.804574, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-1900\n",
      "\n",
      "2018-10-26T16:01:08.938674: step 1901, loss 0.0413969, acc 0.984375\n",
      "2018-10-26T16:01:09.307193: step 1902, loss 0.14309, acc 0.921875\n",
      "2018-10-26T16:01:09.809348: step 1903, loss 0.161257, acc 0.9375\n",
      "2018-10-26T16:01:10.178365: step 1904, loss 0.0912645, acc 0.984375\n",
      "2018-10-26T16:01:10.547376: step 1905, loss 0.129002, acc 0.96875\n",
      "2018-10-26T16:01:10.891457: step 1906, loss 0.0896573, acc 0.96875\n",
      "2018-10-26T16:01:11.272440: step 1907, loss 0.060875, acc 0.984375\n",
      "2018-10-26T16:01:11.593658: step 1908, loss 0.114716, acc 0.953125\n",
      "2018-10-26T16:01:11.958605: step 1909, loss 0.144463, acc 0.953125\n",
      "2018-10-26T16:01:12.398430: step 1910, loss 0.0464906, acc 1\n",
      "2018-10-26T16:01:12.876154: step 1911, loss 0.0577665, acc 0.984375\n",
      "2018-10-26T16:01:13.172363: step 1912, loss 0.0364098, acc 1\n",
      "2018-10-26T16:01:13.552350: step 1913, loss 0.0844514, acc 0.96875\n",
      "2018-10-26T16:01:14.012121: step 1914, loss 0.072203, acc 0.984375\n",
      "2018-10-26T16:01:14.428008: step 1915, loss 0.0684219, acc 0.96875\n",
      "2018-10-26T16:01:14.753139: step 1916, loss 0.0643426, acc 0.96875\n",
      "2018-10-26T16:01:15.064309: step 1917, loss 0.0402657, acc 0.984375\n",
      "2018-10-26T16:01:15.376593: step 1918, loss 0.0615874, acc 0.984375\n",
      "2018-10-26T16:01:15.794508: step 1919, loss 0.0977023, acc 0.96875\n",
      "2018-10-26T16:01:16.108519: step 1920, loss 0.0356549, acc 1\n",
      "2018-10-26T16:01:16.430656: step 1921, loss 0.100652, acc 0.984375\n",
      "2018-10-26T16:01:16.746840: step 1922, loss 0.0727415, acc 0.96875\n",
      "2018-10-26T16:01:17.126838: step 1923, loss 0.0436299, acc 0.984375\n",
      "2018-10-26T16:01:17.466888: step 1924, loss 0.084153, acc 0.96875\n",
      "2018-10-26T16:01:17.803988: step 1925, loss 0.0678499, acc 0.984375\n",
      "2018-10-26T16:01:18.135106: step 1926, loss 0.107103, acc 0.9375\n",
      "2018-10-26T16:01:18.598865: step 1927, loss 0.0439024, acc 0.984375\n",
      "2018-10-26T16:01:18.974863: step 1928, loss 0.132966, acc 0.953125\n",
      "2018-10-26T16:01:19.423661: step 1929, loss 0.0431499, acc 0.984375\n",
      "2018-10-26T16:01:19.873462: step 1930, loss 0.0764972, acc 0.984375\n",
      "2018-10-26T16:01:20.302359: step 1931, loss 0.0610755, acc 0.984375\n",
      "2018-10-26T16:01:20.662376: step 1932, loss 0.0792359, acc 0.984375\n",
      "2018-10-26T16:01:21.073252: step 1933, loss 0.125097, acc 0.953125\n",
      "2018-10-26T16:01:21.512158: step 1934, loss 0.0521377, acc 0.984375\n",
      "2018-10-26T16:01:21.834223: step 1935, loss 0.0739237, acc 1\n",
      "2018-10-26T16:01:22.169326: step 1936, loss 0.188374, acc 0.9375\n",
      "2018-10-26T16:01:22.548314: step 1937, loss 0.151155, acc 0.921875\n",
      "2018-10-26T16:01:22.997112: step 1938, loss 0.0712633, acc 0.96875\n",
      "2018-10-26T16:01:23.317259: step 1939, loss 0.0801068, acc 0.953125\n",
      "2018-10-26T16:01:23.740127: step 1940, loss 0.0929733, acc 0.96875\n",
      "2018-10-26T16:01:24.044314: step 1941, loss 0.123006, acc 0.921875\n",
      "2018-10-26T16:01:24.352490: step 1942, loss 0.168287, acc 0.921875\n",
      "2018-10-26T16:01:24.627755: step 1943, loss 0.0709616, acc 0.984375\n",
      "2018-10-26T16:01:25.112461: step 1944, loss 0.127155, acc 0.953125\n",
      "2018-10-26T16:01:25.467542: step 1945, loss 0.0616857, acc 0.96875\n",
      "2018-10-26T16:01:25.769705: step 1946, loss 0.113096, acc 0.9375\n",
      "2018-10-26T16:01:26.066910: step 1947, loss 0.0874847, acc 0.984375\n",
      "2018-10-26T16:01:26.398026: step 1948, loss 0.102014, acc 0.953125\n",
      "2018-10-26T16:01:26.711189: step 1949, loss 0.0578944, acc 0.96875\n",
      "2018-10-26T16:01:27.027343: step 1950, loss 0.0737826, acc 0.983333\n",
      "2018-10-26T16:01:27.322556: step 1951, loss 0.0433938, acc 0.984375\n",
      "2018-10-26T16:01:27.780332: step 1952, loss 0.0262669, acc 1\n",
      "2018-10-26T16:01:28.215170: step 1953, loss 0.0966286, acc 0.96875\n",
      "2018-10-26T16:01:28.590170: step 1954, loss 0.0922528, acc 0.96875\n",
      "2018-10-26T16:01:28.865432: step 1955, loss 0.041829, acc 0.984375\n",
      "2018-10-26T16:01:29.158650: step 1956, loss 0.0576054, acc 0.96875\n",
      "2018-10-26T16:01:29.470890: step 1957, loss 0.124512, acc 0.953125\n",
      "2018-10-26T16:01:29.812006: step 1958, loss 0.0590189, acc 0.984375\n",
      "2018-10-26T16:01:30.143052: step 1959, loss 0.0737443, acc 0.984375\n",
      "2018-10-26T16:01:30.476130: step 1960, loss 0.0356402, acc 0.984375\n",
      "2018-10-26T16:01:30.804298: step 1961, loss 0.0321875, acc 1\n",
      "2018-10-26T16:01:31.150467: step 1962, loss 0.032974, acc 0.984375\n",
      "2018-10-26T16:01:31.552256: step 1963, loss 0.0638934, acc 0.953125\n",
      "2018-10-26T16:01:31.961211: step 1964, loss 0.0726074, acc 0.96875\n",
      "2018-10-26T16:01:32.285296: step 1965, loss 0.0377993, acc 0.984375\n",
      "2018-10-26T16:01:32.655455: step 1966, loss 0.064291, acc 0.96875\n",
      "2018-10-26T16:01:33.147044: step 1967, loss 0.0400413, acc 1\n",
      "2018-10-26T16:01:33.493119: step 1968, loss 0.0707171, acc 0.953125\n",
      "2018-10-26T16:01:33.826179: step 1969, loss 0.023947, acc 1\n",
      "2018-10-26T16:01:34.172253: step 1970, loss 0.127224, acc 0.9375\n",
      "2018-10-26T16:01:34.650016: step 1971, loss 0.0325403, acc 1\n",
      "2018-10-26T16:01:34.986081: step 1972, loss 0.0476808, acc 1\n",
      "2018-10-26T16:01:35.299242: step 1973, loss 0.0299028, acc 1\n",
      "2018-10-26T16:01:35.654295: step 1974, loss 0.0227705, acc 1\n",
      "2018-10-26T16:01:36.098110: step 1975, loss 0.0758061, acc 0.96875\n",
      "2018-10-26T16:01:36.455186: step 1976, loss 0.0216778, acc 1\n",
      "2018-10-26T16:01:36.867100: step 1977, loss 0.10258, acc 0.953125\n",
      "2018-10-26T16:01:37.258011: step 1978, loss 0.0288366, acc 1\n",
      "2018-10-26T16:01:37.671905: step 1979, loss 0.0220227, acc 1\n",
      "2018-10-26T16:01:38.016983: step 1980, loss 0.0543415, acc 0.984375\n",
      "2018-10-26T16:01:38.335131: step 1981, loss 0.0606439, acc 0.984375\n",
      "2018-10-26T16:01:38.688219: step 1982, loss 0.0659252, acc 0.96875\n",
      "2018-10-26T16:01:39.021299: step 1983, loss 0.0330879, acc 1\n",
      "2018-10-26T16:01:39.374411: step 1984, loss 0.0420499, acc 0.984375\n",
      "2018-10-26T16:01:39.747362: step 1985, loss 0.0676224, acc 0.96875\n",
      "2018-10-26T16:01:40.146292: step 1986, loss 0.0378252, acc 1\n",
      "2018-10-26T16:01:40.499349: step 1987, loss 0.121177, acc 0.9375\n",
      "2018-10-26T16:01:40.904270: step 1988, loss 0.0413786, acc 0.984375\n",
      "2018-10-26T16:01:41.247355: step 1989, loss 0.0474327, acc 0.984375\n",
      "2018-10-26T16:01:41.617361: step 1990, loss 0.0644084, acc 0.953125\n",
      "2018-10-26T16:01:42.111044: step 1991, loss 0.0621551, acc 0.984375\n",
      "2018-10-26T16:01:42.771279: step 1992, loss 0.0567597, acc 0.96875\n",
      "2018-10-26T16:01:43.283909: step 1993, loss 0.0845777, acc 0.953125\n",
      "2018-10-26T16:01:43.708774: step 1994, loss 0.0403845, acc 0.984375\n",
      "2018-10-26T16:01:44.163559: step 1995, loss 0.0220022, acc 1\n",
      "2018-10-26T16:01:44.547574: step 1996, loss 0.0855782, acc 0.96875\n",
      "2018-10-26T16:01:44.903582: step 1997, loss 0.0152472, acc 1\n",
      "2018-10-26T16:01:45.250654: step 1998, loss 0.0306202, acc 1\n",
      "2018-10-26T16:01:45.681503: step 1999, loss 0.0483853, acc 0.984375\n",
      "2018-10-26T16:01:46.042539: step 2000, loss 0.0359865, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:01:47.047853: step 2000, loss 0.824942, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2000\n",
      "\n",
      "2018-10-26T16:01:47.942465: step 2001, loss 0.0287387, acc 0.984375\n",
      "2018-10-26T16:01:48.464070: step 2002, loss 0.0761215, acc 0.984375\n",
      "2018-10-26T16:01:49.000635: step 2003, loss 0.015871, acc 1\n",
      "2018-10-26T16:01:49.481405: step 2004, loss 0.0556115, acc 0.984375\n",
      "2018-10-26T16:01:49.942120: step 2005, loss 0.0467767, acc 0.984375\n",
      "2018-10-26T16:01:50.390919: step 2006, loss 0.0257594, acc 1\n",
      "2018-10-26T16:01:50.819776: step 2007, loss 0.0569619, acc 0.96875\n",
      "2018-10-26T16:01:51.223695: step 2008, loss 0.0335203, acc 1\n",
      "2018-10-26T16:01:51.627616: step 2009, loss 0.0702568, acc 0.96875\n",
      "2018-10-26T16:01:51.982667: step 2010, loss 0.0153148, acc 1\n",
      "2018-10-26T16:01:52.390611: step 2011, loss 0.0355254, acc 0.984375\n",
      "2018-10-26T16:01:52.810485: step 2012, loss 0.0381487, acc 1\n",
      "2018-10-26T16:01:53.165509: step 2013, loss 0.0601652, acc 0.984375\n",
      "2018-10-26T16:01:53.526543: step 2014, loss 0.0808911, acc 0.953125\n",
      "2018-10-26T16:01:53.868630: step 2015, loss 0.0401214, acc 0.984375\n",
      "2018-10-26T16:01:54.238643: step 2016, loss 0.0820687, acc 0.96875\n",
      "2018-10-26T16:01:54.588707: step 2017, loss 0.0306552, acc 1\n",
      "2018-10-26T16:01:54.985644: step 2018, loss 0.0311449, acc 0.984375\n",
      "2018-10-26T16:01:55.350670: step 2019, loss 0.0358618, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:01:55.724673: step 2020, loss 0.14847, acc 0.96875\n",
      "2018-10-26T16:01:56.074736: step 2021, loss 0.0219749, acc 1\n",
      "2018-10-26T16:01:56.408842: step 2022, loss 0.0622515, acc 0.984375\n",
      "2018-10-26T16:01:56.747936: step 2023, loss 0.0352634, acc 1\n",
      "2018-10-26T16:01:57.128917: step 2024, loss 0.125484, acc 0.96875\n",
      "2018-10-26T16:01:57.517878: step 2025, loss 0.0968959, acc 0.96875\n",
      "2018-10-26T16:01:57.885921: step 2026, loss 0.0263531, acc 0.984375\n",
      "2018-10-26T16:01:58.249923: step 2027, loss 0.0250949, acc 1\n",
      "2018-10-26T16:01:58.584029: step 2028, loss 0.039751, acc 0.984375\n",
      "2018-10-26T16:01:58.924121: step 2029, loss 0.058327, acc 0.984375\n",
      "2018-10-26T16:01:59.273188: step 2030, loss 0.0412531, acc 1\n",
      "2018-10-26T16:01:59.637216: step 2031, loss 0.071171, acc 0.9375\n",
      "2018-10-26T16:01:59.980300: step 2032, loss 0.0401816, acc 1\n",
      "2018-10-26T16:02:00.365271: step 2033, loss 0.11856, acc 0.984375\n",
      "2018-10-26T16:02:00.703374: step 2034, loss 0.145256, acc 0.9375\n",
      "2018-10-26T16:02:01.028498: step 2035, loss 0.105172, acc 0.953125\n",
      "2018-10-26T16:02:01.396514: step 2036, loss 0.0543587, acc 0.984375\n",
      "2018-10-26T16:02:01.740600: step 2037, loss 0.0628083, acc 0.984375\n",
      "2018-10-26T16:02:02.087668: step 2038, loss 0.0280867, acc 1\n",
      "2018-10-26T16:02:02.437736: step 2039, loss 0.03315, acc 0.984375\n",
      "2018-10-26T16:02:02.783808: step 2040, loss 0.0381824, acc 0.984375\n",
      "2018-10-26T16:02:03.155816: step 2041, loss 0.0192653, acc 1\n",
      "2018-10-26T16:02:03.555839: step 2042, loss 0.0673259, acc 0.96875\n",
      "2018-10-26T16:02:03.935730: step 2043, loss 0.0443366, acc 0.984375\n",
      "2018-10-26T16:02:04.287791: step 2044, loss 0.0552928, acc 0.984375\n",
      "2018-10-26T16:02:04.632867: step 2045, loss 0.144894, acc 0.9375\n",
      "2018-10-26T16:02:05.008864: step 2046, loss 0.0718526, acc 0.953125\n",
      "2018-10-26T16:02:05.390847: step 2047, loss 0.071582, acc 0.96875\n",
      "2018-10-26T16:02:05.740953: step 2048, loss 0.0365703, acc 1\n",
      "2018-10-26T16:02:06.063046: step 2049, loss 0.0594193, acc 0.984375\n",
      "2018-10-26T16:02:06.390173: step 2050, loss 0.0322458, acc 0.984375\n",
      "2018-10-26T16:02:06.745269: step 2051, loss 0.0226578, acc 1\n",
      "2018-10-26T16:02:07.095292: step 2052, loss 0.086341, acc 0.96875\n",
      "2018-10-26T16:02:07.470338: step 2053, loss 0.0660613, acc 1\n",
      "2018-10-26T16:02:07.809382: step 2054, loss 0.0728751, acc 0.953125\n",
      "2018-10-26T16:02:08.151467: step 2055, loss 0.0236492, acc 1\n",
      "2018-10-26T16:02:08.497587: step 2056, loss 0.0415194, acc 0.984375\n",
      "2018-10-26T16:02:08.856633: step 2057, loss 0.11435, acc 0.984375\n",
      "2018-10-26T16:02:09.210637: step 2058, loss 0.0574291, acc 0.984375\n",
      "2018-10-26T16:02:09.545741: step 2059, loss 0.0945714, acc 0.96875\n",
      "2018-10-26T16:02:09.887861: step 2060, loss 0.1044, acc 0.96875\n",
      "2018-10-26T16:02:10.224928: step 2061, loss 0.0668908, acc 0.984375\n",
      "2018-10-26T16:02:10.565040: step 2062, loss 0.0569047, acc 0.96875\n",
      "2018-10-26T16:02:10.897133: step 2063, loss 0.0762091, acc 0.96875\n",
      "2018-10-26T16:02:11.248192: step 2064, loss 0.0188647, acc 1\n",
      "2018-10-26T16:02:11.634324: step 2065, loss 0.0606754, acc 1\n",
      "2018-10-26T16:02:11.971333: step 2066, loss 0.0421957, acc 0.984375\n",
      "2018-10-26T16:02:12.304374: step 2067, loss 0.0643959, acc 0.984375\n",
      "2018-10-26T16:02:12.647453: step 2068, loss 0.0393605, acc 1\n",
      "2018-10-26T16:02:13.002505: step 2069, loss 0.0717163, acc 0.96875\n",
      "2018-10-26T16:02:13.359552: step 2070, loss 0.0431033, acc 0.984375\n",
      "2018-10-26T16:02:13.701637: step 2071, loss 0.103168, acc 0.953125\n",
      "2018-10-26T16:02:14.063673: step 2072, loss 0.0582623, acc 0.984375\n",
      "2018-10-26T16:02:14.452631: step 2073, loss 0.0867682, acc 0.96875\n",
      "2018-10-26T16:02:14.800748: step 2074, loss 0.0578901, acc 0.984375\n",
      "2018-10-26T16:02:15.160811: step 2075, loss 0.0245491, acc 1\n",
      "2018-10-26T16:02:15.520817: step 2076, loss 0.0372094, acc 0.984375\n",
      "2018-10-26T16:02:15.895776: step 2077, loss 0.0854241, acc 0.984375\n",
      "2018-10-26T16:02:16.277809: step 2078, loss 0.103907, acc 0.953125\n",
      "2018-10-26T16:02:16.626824: step 2079, loss 0.0859535, acc 0.953125\n",
      "2018-10-26T16:02:16.981873: step 2080, loss 0.077241, acc 0.96875\n",
      "2018-10-26T16:02:17.338919: step 2081, loss 0.128464, acc 0.96875\n",
      "2018-10-26T16:02:17.700018: step 2082, loss 0.084374, acc 0.953125\n",
      "2018-10-26T16:02:18.062988: step 2083, loss 0.0583792, acc 0.953125\n",
      "2018-10-26T16:02:18.430005: step 2084, loss 0.10741, acc 0.953125\n",
      "2018-10-26T16:02:18.774119: step 2085, loss 0.0565249, acc 0.984375\n",
      "2018-10-26T16:02:19.119384: step 2086, loss 0.0966606, acc 0.96875\n",
      "2018-10-26T16:02:19.496158: step 2087, loss 0.0763208, acc 0.96875\n",
      "2018-10-26T16:02:19.860184: step 2088, loss 0.127789, acc 0.96875\n",
      "2018-10-26T16:02:20.206262: step 2089, loss 0.052213, acc 0.984375\n",
      "2018-10-26T16:02:20.585250: step 2090, loss 0.0621393, acc 0.984375\n",
      "2018-10-26T16:02:20.943336: step 2091, loss 0.0501562, acc 0.96875\n",
      "2018-10-26T16:02:21.285375: step 2092, loss 0.047175, acc 0.984375\n",
      "2018-10-26T16:02:21.635442: step 2093, loss 0.0520395, acc 0.984375\n",
      "2018-10-26T16:02:21.979522: step 2094, loss 0.110907, acc 0.96875\n",
      "2018-10-26T16:02:22.329589: step 2095, loss 0.0480066, acc 0.984375\n",
      "2018-10-26T16:02:22.691617: step 2096, loss 0.0452118, acc 0.984375\n",
      "2018-10-26T16:02:23.102519: step 2097, loss 0.10129, acc 0.984375\n",
      "2018-10-26T16:02:23.430643: step 2098, loss 0.151197, acc 0.9375\n",
      "2018-10-26T16:02:23.769737: step 2099, loss 0.0270091, acc 1\n",
      "2018-10-26T16:02:24.116810: step 2100, loss 0.0446515, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:02:24.932630: step 2100, loss 0.866297, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2100\n",
      "\n",
      "2018-10-26T16:02:25.605831: step 2101, loss 0.0495031, acc 0.96875\n",
      "2018-10-26T16:02:25.957891: step 2102, loss 0.0849558, acc 0.96875\n",
      "2018-10-26T16:02:26.429632: step 2103, loss 0.02542, acc 0.984375\n",
      "2018-10-26T16:02:26.779695: step 2104, loss 0.0133487, acc 1\n",
      "2018-10-26T16:02:27.136741: step 2105, loss 0.0111963, acc 1\n",
      "2018-10-26T16:02:27.491793: step 2106, loss 0.0620032, acc 0.953125\n",
      "2018-10-26T16:02:27.823905: step 2107, loss 0.0436268, acc 0.984375\n",
      "2018-10-26T16:02:28.173273: step 2108, loss 0.0347785, acc 1\n",
      "2018-10-26T16:02:28.523038: step 2109, loss 0.0180633, acc 1\n",
      "2018-10-26T16:02:28.884174: step 2110, loss 0.0241716, acc 1\n",
      "2018-10-26T16:02:29.265219: step 2111, loss 0.0264459, acc 1\n",
      "2018-10-26T16:02:29.747767: step 2112, loss 0.0143961, acc 1\n",
      "2018-10-26T16:02:30.160704: step 2113, loss 0.0361361, acc 0.984375\n",
      "2018-10-26T16:02:30.587524: step 2114, loss 0.0374638, acc 0.984375\n",
      "2018-10-26T16:02:30.946562: step 2115, loss 0.0954383, acc 0.953125\n",
      "2018-10-26T16:02:31.319578: step 2116, loss 0.0380787, acc 0.984375\n",
      "2018-10-26T16:02:31.665640: step 2117, loss 0.0561658, acc 0.984375\n",
      "2018-10-26T16:02:31.995759: step 2118, loss 0.128949, acc 0.984375\n",
      "2018-10-26T16:02:32.417631: step 2119, loss 0.0183338, acc 1\n",
      "2018-10-26T16:02:32.831528: step 2120, loss 0.0856779, acc 0.96875\n",
      "2018-10-26T16:02:33.209518: step 2121, loss 0.0517679, acc 0.96875\n",
      "2018-10-26T16:02:33.577564: step 2122, loss 0.0227624, acc 1\n",
      "2018-10-26T16:02:33.939565: step 2123, loss 0.0243692, acc 1\n",
      "2018-10-26T16:02:34.280688: step 2124, loss 0.0389023, acc 0.984375\n",
      "2018-10-26T16:02:34.670611: step 2125, loss 0.0336363, acc 0.984375\n",
      "2018-10-26T16:02:35.106448: step 2126, loss 0.028524, acc 0.984375\n",
      "2018-10-26T16:02:35.445542: step 2127, loss 0.0114418, acc 1\n",
      "2018-10-26T16:02:35.863428: step 2128, loss 0.056521, acc 0.96875\n",
      "2018-10-26T16:02:36.216480: step 2129, loss 0.116767, acc 0.953125\n",
      "2018-10-26T16:02:36.587490: step 2130, loss 0.146167, acc 0.96875\n",
      "2018-10-26T16:02:36.938553: step 2131, loss 0.0584705, acc 0.984375\n",
      "2018-10-26T16:02:37.310559: step 2132, loss 0.0750128, acc 0.96875\n",
      "2018-10-26T16:02:37.648659: step 2133, loss 0.0158082, acc 1\n",
      "2018-10-26T16:02:38.020691: step 2134, loss 0.0798486, acc 0.96875\n",
      "2018-10-26T16:02:38.398665: step 2135, loss 0.0439217, acc 1\n",
      "2018-10-26T16:02:38.755697: step 2136, loss 0.0572591, acc 0.96875\n",
      "2018-10-26T16:02:39.101855: step 2137, loss 0.0894933, acc 0.96875\n",
      "2018-10-26T16:02:39.509683: step 2138, loss 0.0428607, acc 1\n",
      "2018-10-26T16:02:39.906621: step 2139, loss 0.052856, acc 0.984375\n",
      "2018-10-26T16:02:40.272644: step 2140, loss 0.0321115, acc 1\n",
      "2018-10-26T16:02:40.651632: step 2141, loss 0.0205208, acc 1\n",
      "2018-10-26T16:02:41.077494: step 2142, loss 0.132616, acc 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:02:41.434646: step 2143, loss 0.0769332, acc 0.96875\n",
      "2018-10-26T16:02:41.800565: step 2144, loss 0.0468018, acc 0.96875\n",
      "2018-10-26T16:02:42.173568: step 2145, loss 0.0617512, acc 0.96875\n",
      "2018-10-26T16:02:42.547604: step 2146, loss 0.0325141, acc 0.984375\n",
      "2018-10-26T16:02:42.903616: step 2147, loss 0.043989, acc 0.984375\n",
      "2018-10-26T16:02:43.291577: step 2148, loss 0.0492311, acc 0.984375\n",
      "2018-10-26T16:02:43.668572: step 2149, loss 0.0353574, acc 1\n",
      "2018-10-26T16:02:44.065510: step 2150, loss 0.0219267, acc 1\n",
      "2018-10-26T16:02:44.429537: step 2151, loss 0.099268, acc 0.96875\n",
      "2018-10-26T16:02:44.777658: step 2152, loss 0.0621109, acc 0.984375\n",
      "2018-10-26T16:02:45.131735: step 2153, loss 0.0353643, acc 1\n",
      "2018-10-26T16:02:45.499678: step 2154, loss 0.0698769, acc 0.96875\n",
      "2018-10-26T16:02:45.869690: step 2155, loss 0.0230252, acc 1\n",
      "2018-10-26T16:02:46.239702: step 2156, loss 0.0277011, acc 1\n",
      "2018-10-26T16:02:46.594752: step 2157, loss 0.0772664, acc 0.96875\n",
      "2018-10-26T16:02:46.945818: step 2158, loss 0.0260861, acc 1\n",
      "2018-10-26T16:02:47.322806: step 2159, loss 0.046319, acc 0.984375\n",
      "2018-10-26T16:02:47.687881: step 2160, loss 0.0370184, acc 0.984375\n",
      "2018-10-26T16:02:48.092750: step 2161, loss 0.0411371, acc 1\n",
      "2018-10-26T16:02:48.498666: step 2162, loss 0.0645458, acc 0.96875\n",
      "2018-10-26T16:02:48.918543: step 2163, loss 0.0366065, acc 0.984375\n",
      "2018-10-26T16:02:49.277583: step 2164, loss 0.0313292, acc 1\n",
      "2018-10-26T16:02:49.701502: step 2165, loss 0.0628022, acc 0.984375\n",
      "2018-10-26T16:02:50.107372: step 2166, loss 0.0373677, acc 1\n",
      "2018-10-26T16:02:50.561315: step 2167, loss 0.0217632, acc 1\n",
      "2018-10-26T16:02:51.007961: step 2168, loss 0.0407728, acc 0.984375\n",
      "2018-10-26T16:02:51.439866: step 2169, loss 0.035936, acc 0.984375\n",
      "2018-10-26T16:02:51.943464: step 2170, loss 0.047783, acc 0.984375\n",
      "2018-10-26T16:02:52.408221: step 2171, loss 0.0165013, acc 1\n",
      "2018-10-26T16:02:52.887939: step 2172, loss 0.113327, acc 0.96875\n",
      "2018-10-26T16:02:53.396610: step 2173, loss 0.102808, acc 0.953125\n",
      "2018-10-26T16:02:53.913214: step 2174, loss 0.0293858, acc 1\n",
      "2018-10-26T16:02:54.369978: step 2175, loss 0.0206781, acc 1\n",
      "2018-10-26T16:02:54.848699: step 2176, loss 0.0979268, acc 0.96875\n",
      "2018-10-26T16:02:55.296502: step 2177, loss 0.0350182, acc 1\n",
      "2018-10-26T16:02:55.714385: step 2178, loss 0.0704034, acc 0.953125\n",
      "2018-10-26T16:02:56.131353: step 2179, loss 0.16112, acc 0.953125\n",
      "2018-10-26T16:02:56.548201: step 2180, loss 0.0291176, acc 1\n",
      "2018-10-26T16:02:56.966042: step 2181, loss 0.0628784, acc 0.96875\n",
      "2018-10-26T16:02:57.425814: step 2182, loss 0.072694, acc 0.953125\n",
      "2018-10-26T16:02:57.850678: step 2183, loss 0.109148, acc 0.96875\n",
      "2018-10-26T16:02:58.223680: step 2184, loss 0.0663988, acc 0.984375\n",
      "2018-10-26T16:02:58.526872: step 2185, loss 0.0343264, acc 0.984375\n",
      "2018-10-26T16:02:58.842032: step 2186, loss 0.0503894, acc 0.96875\n",
      "2018-10-26T16:02:59.155241: step 2187, loss 0.0166447, acc 1\n",
      "2018-10-26T16:02:59.468355: step 2188, loss 0.0320428, acc 0.984375\n",
      "2018-10-26T16:02:59.796482: step 2189, loss 0.0480988, acc 0.984375\n",
      "2018-10-26T16:03:00.173474: step 2190, loss 0.052685, acc 0.984375\n",
      "2018-10-26T16:03:00.496608: step 2191, loss 0.0907682, acc 0.96875\n",
      "2018-10-26T16:03:00.831767: step 2192, loss 0.0262854, acc 0.984375\n",
      "2018-10-26T16:03:01.148865: step 2193, loss 0.0532969, acc 0.96875\n",
      "2018-10-26T16:03:01.477987: step 2194, loss 0.0683411, acc 0.96875\n",
      "2018-10-26T16:03:01.801194: step 2195, loss 0.164057, acc 0.953125\n",
      "2018-10-26T16:03:02.130245: step 2196, loss 0.0887014, acc 0.984375\n",
      "2018-10-26T16:03:02.453381: step 2197, loss 0.0711565, acc 0.984375\n",
      "2018-10-26T16:03:02.784495: step 2198, loss 0.0229563, acc 1\n",
      "2018-10-26T16:03:03.117641: step 2199, loss 0.0752588, acc 0.96875\n",
      "2018-10-26T16:03:03.461825: step 2200, loss 0.0495955, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:03:04.210746: step 2200, loss 0.919208, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2200\n",
      "\n",
      "2018-10-26T16:03:04.835020: step 2201, loss 0.00559813, acc 1\n",
      "2018-10-26T16:03:05.145188: step 2202, loss 0.0423792, acc 1\n",
      "2018-10-26T16:03:05.606954: step 2203, loss 0.0595454, acc 0.96875\n",
      "2018-10-26T16:03:05.942123: step 2204, loss 0.0228862, acc 1\n",
      "2018-10-26T16:03:06.268189: step 2205, loss 0.02762, acc 1\n",
      "2018-10-26T16:03:06.599304: step 2206, loss 0.0415676, acc 0.984375\n",
      "2018-10-26T16:03:06.916458: step 2207, loss 0.0432709, acc 0.984375\n",
      "2018-10-26T16:03:07.237600: step 2208, loss 0.0427559, acc 0.96875\n",
      "2018-10-26T16:03:07.607611: step 2209, loss 0.0192804, acc 1\n",
      "2018-10-26T16:03:07.964658: step 2210, loss 0.0735902, acc 0.984375\n",
      "2018-10-26T16:03:08.330676: step 2211, loss 0.00977903, acc 1\n",
      "2018-10-26T16:03:08.640889: step 2212, loss 0.0342545, acc 1\n",
      "2018-10-26T16:03:08.959995: step 2213, loss 0.0393517, acc 0.984375\n",
      "2018-10-26T16:03:09.280142: step 2214, loss 0.109941, acc 0.96875\n",
      "2018-10-26T16:03:09.638211: step 2215, loss 0.0560253, acc 0.96875\n",
      "2018-10-26T16:03:09.956332: step 2216, loss 0.031237, acc 0.984375\n",
      "2018-10-26T16:03:10.287485: step 2217, loss 0.0475498, acc 0.984375\n",
      "2018-10-26T16:03:10.625546: step 2218, loss 0.076441, acc 0.96875\n",
      "2018-10-26T16:03:10.952671: step 2219, loss 0.0288981, acc 1\n",
      "2018-10-26T16:03:11.281831: step 2220, loss 0.0368672, acc 0.984375\n",
      "2018-10-26T16:03:11.594956: step 2221, loss 0.0214178, acc 1\n",
      "2018-10-26T16:03:11.912108: step 2222, loss 0.054682, acc 0.96875\n",
      "2018-10-26T16:03:12.243224: step 2223, loss 0.119319, acc 0.96875\n",
      "2018-10-26T16:03:12.609246: step 2224, loss 0.0273927, acc 1\n",
      "2018-10-26T16:03:12.924406: step 2225, loss 0.0846149, acc 0.96875\n",
      "2018-10-26T16:03:13.243550: step 2226, loss 0.0201511, acc 1\n",
      "2018-10-26T16:03:13.566688: step 2227, loss 0.0399051, acc 0.984375\n",
      "2018-10-26T16:03:13.887187: step 2228, loss 0.054357, acc 0.96875\n",
      "2018-10-26T16:03:14.219948: step 2229, loss 0.0851809, acc 0.96875\n",
      "2018-10-26T16:03:14.581979: step 2230, loss 0.0233238, acc 1\n",
      "2018-10-26T16:03:14.907108: step 2231, loss 0.0467137, acc 0.96875\n",
      "2018-10-26T16:03:15.235229: step 2232, loss 0.0231408, acc 1\n",
      "2018-10-26T16:03:15.619241: step 2233, loss 0.0357308, acc 1\n",
      "2018-10-26T16:03:15.949321: step 2234, loss 0.046378, acc 0.96875\n",
      "2018-10-26T16:03:16.268563: step 2235, loss 0.0279586, acc 1\n",
      "2018-10-26T16:03:16.583628: step 2236, loss 0.0316976, acc 1\n",
      "2018-10-26T16:03:16.920754: step 2237, loss 0.0312822, acc 1\n",
      "2018-10-26T16:03:17.238876: step 2238, loss 0.0500002, acc 0.984375\n",
      "2018-10-26T16:03:17.555032: step 2239, loss 0.0239469, acc 1\n",
      "2018-10-26T16:03:17.868201: step 2240, loss 0.089896, acc 0.984375\n",
      "2018-10-26T16:03:18.195323: step 2241, loss 0.119126, acc 0.953125\n",
      "2018-10-26T16:03:18.530424: step 2242, loss 0.0411581, acc 0.984375\n",
      "2018-10-26T16:03:18.899440: step 2243, loss 0.0415011, acc 0.984375\n",
      "2018-10-26T16:03:19.215594: step 2244, loss 0.0333829, acc 0.984375\n",
      "2018-10-26T16:03:19.541762: step 2245, loss 0.0411223, acc 0.984375\n",
      "2018-10-26T16:03:19.851944: step 2246, loss 0.0373291, acc 0.984375\n",
      "2018-10-26T16:03:20.172040: step 2247, loss 0.0459587, acc 0.96875\n",
      "2018-10-26T16:03:20.482210: step 2248, loss 0.0300395, acc 0.984375\n",
      "2018-10-26T16:03:20.829319: step 2249, loss 0.0963322, acc 0.953125\n",
      "2018-10-26T16:03:21.144441: step 2250, loss 0.0553716, acc 0.966667\n",
      "2018-10-26T16:03:21.467579: step 2251, loss 0.0381403, acc 1\n",
      "2018-10-26T16:03:21.792708: step 2252, loss 0.0263332, acc 1\n",
      "2018-10-26T16:03:22.125818: step 2253, loss 0.027837, acc 0.984375\n",
      "2018-10-26T16:03:22.467905: step 2254, loss 0.0200164, acc 1\n",
      "2018-10-26T16:03:22.828941: step 2255, loss 0.0161472, acc 1\n",
      "2018-10-26T16:03:23.164118: step 2256, loss 0.0685445, acc 0.984375\n",
      "2018-10-26T16:03:23.494165: step 2257, loss 0.024786, acc 1\n",
      "2018-10-26T16:03:23.831353: step 2258, loss 0.0930655, acc 0.984375\n",
      "2018-10-26T16:03:24.145425: step 2259, loss 0.0732837, acc 0.984375\n",
      "2018-10-26T16:03:24.455593: step 2260, loss 0.0310564, acc 0.96875\n",
      "2018-10-26T16:03:24.806655: step 2261, loss 0.112336, acc 0.96875\n",
      "2018-10-26T16:03:25.105857: step 2262, loss 0.0451122, acc 1\n",
      "2018-10-26T16:03:25.421013: step 2263, loss 0.106704, acc 0.96875\n",
      "2018-10-26T16:03:25.748242: step 2264, loss 0.0173118, acc 1\n",
      "2018-10-26T16:03:26.068329: step 2265, loss 0.0362974, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:03:26.390476: step 2266, loss 0.0455941, acc 0.984375\n",
      "2018-10-26T16:03:26.718030: step 2267, loss 0.0728749, acc 0.96875\n",
      "2018-10-26T16:03:27.043680: step 2268, loss 0.0577863, acc 0.96875\n",
      "2018-10-26T16:03:27.363823: step 2269, loss 0.0208797, acc 1\n",
      "2018-10-26T16:03:27.680978: step 2270, loss 0.0855357, acc 0.96875\n",
      "2018-10-26T16:03:28.034036: step 2271, loss 0.016257, acc 1\n",
      "2018-10-26T16:03:28.343245: step 2272, loss 0.017258, acc 1\n",
      "2018-10-26T16:03:28.674322: step 2273, loss 0.0326018, acc 0.984375\n",
      "2018-10-26T16:03:29.007432: step 2274, loss 0.00882622, acc 1\n",
      "2018-10-26T16:03:29.357496: step 2275, loss 0.0511635, acc 0.984375\n",
      "2018-10-26T16:03:29.658694: step 2276, loss 0.0599454, acc 0.984375\n",
      "2018-10-26T16:03:29.976841: step 2277, loss 0.0254468, acc 0.984375\n",
      "2018-10-26T16:03:30.281076: step 2278, loss 0.0464759, acc 0.984375\n",
      "2018-10-26T16:03:30.601273: step 2279, loss 0.024712, acc 1\n",
      "2018-10-26T16:03:30.937279: step 2280, loss 0.056614, acc 0.96875\n",
      "2018-10-26T16:03:31.259414: step 2281, loss 0.0406449, acc 0.984375\n",
      "2018-10-26T16:03:31.600506: step 2282, loss 0.0456129, acc 0.984375\n",
      "2018-10-26T16:03:31.968521: step 2283, loss 0.0132151, acc 1\n",
      "2018-10-26T16:03:32.347512: step 2284, loss 0.0163576, acc 1\n",
      "2018-10-26T16:03:32.691645: step 2285, loss 0.0263571, acc 0.984375\n",
      "2018-10-26T16:03:33.060603: step 2286, loss 0.0531923, acc 0.984375\n",
      "2018-10-26T16:03:33.442784: step 2287, loss 0.0401914, acc 0.96875\n",
      "2018-10-26T16:03:33.774694: step 2288, loss 0.0451773, acc 0.96875\n",
      "2018-10-26T16:03:34.115786: step 2289, loss 0.0483698, acc 0.96875\n",
      "2018-10-26T16:03:34.457869: step 2290, loss 0.0356203, acc 0.984375\n",
      "2018-10-26T16:03:34.833865: step 2291, loss 0.0144513, acc 1\n",
      "2018-10-26T16:03:35.158037: step 2292, loss 0.0429612, acc 0.96875\n",
      "2018-10-26T16:03:35.483130: step 2293, loss 0.0429811, acc 0.96875\n",
      "2018-10-26T16:03:35.829206: step 2294, loss 0.0436437, acc 1\n",
      "2018-10-26T16:03:36.154338: step 2295, loss 0.0415577, acc 0.984375\n",
      "2018-10-26T16:03:36.514376: step 2296, loss 0.035376, acc 1\n",
      "2018-10-26T16:03:36.951214: step 2297, loss 0.0473229, acc 0.984375\n",
      "2018-10-26T16:03:37.277440: step 2298, loss 0.0398928, acc 0.984375\n",
      "2018-10-26T16:03:37.629394: step 2299, loss 0.0373759, acc 1\n",
      "2018-10-26T16:03:37.972479: step 2300, loss 0.0141768, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:03:38.774347: step 2300, loss 0.94193, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2300\n",
      "\n",
      "2018-10-26T16:03:39.617084: step 2301, loss 0.0481235, acc 0.984375\n",
      "2018-10-26T16:03:40.095809: step 2302, loss 0.00754364, acc 1\n",
      "2018-10-26T16:03:40.482773: step 2303, loss 0.0306233, acc 0.984375\n",
      "2018-10-26T16:03:40.884698: step 2304, loss 0.0567104, acc 0.96875\n",
      "2018-10-26T16:03:41.261067: step 2305, loss 0.0544721, acc 0.984375\n",
      "2018-10-26T16:03:41.632698: step 2306, loss 0.036121, acc 0.984375\n",
      "2018-10-26T16:03:42.044599: step 2307, loss 0.0335667, acc 1\n",
      "2018-10-26T16:03:42.450555: step 2308, loss 0.0930038, acc 0.953125\n",
      "2018-10-26T16:03:42.869544: step 2309, loss 0.0240032, acc 1\n",
      "2018-10-26T16:03:43.279300: step 2310, loss 0.040401, acc 0.984375\n",
      "2018-10-26T16:03:43.711245: step 2311, loss 0.018721, acc 1\n",
      "2018-10-26T16:03:44.180943: step 2312, loss 0.0510029, acc 0.984375\n",
      "2018-10-26T16:03:44.611800: step 2313, loss 0.0595259, acc 0.984375\n",
      "2018-10-26T16:03:44.998705: step 2314, loss 0.0177092, acc 1\n",
      "2018-10-26T16:03:45.378691: step 2315, loss 0.0151946, acc 1\n",
      "2018-10-26T16:03:45.707857: step 2316, loss 0.0126017, acc 1\n",
      "2018-10-26T16:03:46.093781: step 2317, loss 0.0152487, acc 1\n",
      "2018-10-26T16:03:46.465827: step 2318, loss 0.0927401, acc 0.984375\n",
      "2018-10-26T16:03:46.869741: step 2319, loss 0.0155383, acc 1\n",
      "2018-10-26T16:03:47.277654: step 2320, loss 0.0194629, acc 1\n",
      "2018-10-26T16:03:47.649623: step 2321, loss 0.0339538, acc 0.984375\n",
      "2018-10-26T16:03:48.003676: step 2322, loss 0.0419631, acc 0.984375\n",
      "2018-10-26T16:03:48.428541: step 2323, loss 0.0620635, acc 0.984375\n",
      "2018-10-26T16:03:48.754673: step 2324, loss 0.0279515, acc 0.984375\n",
      "2018-10-26T16:03:49.113748: step 2325, loss 0.0566419, acc 0.984375\n",
      "2018-10-26T16:03:49.468766: step 2326, loss 0.0527893, acc 0.984375\n",
      "2018-10-26T16:03:49.864707: step 2327, loss 0.0273275, acc 1\n",
      "2018-10-26T16:03:50.238708: step 2328, loss 0.0313024, acc 1\n",
      "2018-10-26T16:03:50.658588: step 2329, loss 0.0325842, acc 1\n",
      "2018-10-26T16:03:51.057516: step 2330, loss 0.0549845, acc 0.984375\n",
      "2018-10-26T16:03:51.435637: step 2331, loss 0.0585603, acc 0.96875\n",
      "2018-10-26T16:03:51.800535: step 2332, loss 0.0361406, acc 0.984375\n",
      "2018-10-26T16:03:52.167551: step 2333, loss 0.013667, acc 1\n",
      "2018-10-26T16:03:52.609372: step 2334, loss 0.015785, acc 1\n",
      "2018-10-26T16:03:52.944475: step 2335, loss 0.03102, acc 0.984375\n",
      "2018-10-26T16:03:53.287562: step 2336, loss 0.0293609, acc 1\n",
      "2018-10-26T16:03:53.624658: step 2337, loss 0.0501263, acc 0.984375\n",
      "2018-10-26T16:03:53.993673: step 2338, loss 0.0534316, acc 0.984375\n",
      "2018-10-26T16:03:54.411680: step 2339, loss 0.0413361, acc 0.984375\n",
      "2018-10-26T16:03:54.759625: step 2340, loss 0.0220543, acc 0.984375\n",
      "2018-10-26T16:03:55.116670: step 2341, loss 0.0305695, acc 1\n",
      "2018-10-26T16:03:55.469730: step 2342, loss 0.0221131, acc 1\n",
      "2018-10-26T16:03:55.815806: step 2343, loss 0.12983, acc 0.953125\n",
      "2018-10-26T16:03:56.165868: step 2344, loss 0.00473489, acc 1\n",
      "2018-10-26T16:03:56.532887: step 2345, loss 0.0349882, acc 0.984375\n",
      "2018-10-26T16:03:56.980692: step 2346, loss 0.0350168, acc 0.984375\n",
      "2018-10-26T16:03:57.352735: step 2347, loss 0.0459748, acc 0.953125\n",
      "2018-10-26T16:03:57.818453: step 2348, loss 0.0343256, acc 0.984375\n",
      "2018-10-26T16:03:58.229356: step 2349, loss 0.0387061, acc 0.984375\n",
      "2018-10-26T16:03:58.707080: step 2350, loss 0.0358835, acc 1\n",
      "2018-10-26T16:03:59.200760: step 2351, loss 0.0444838, acc 0.984375\n",
      "2018-10-26T16:03:59.689991: step 2352, loss 0.0283355, acc 1\n",
      "2018-10-26T16:04:00.208098: step 2353, loss 0.0449669, acc 0.984375\n",
      "2018-10-26T16:04:00.709727: step 2354, loss 0.0143364, acc 1\n",
      "2018-10-26T16:04:01.172491: step 2355, loss 0.0218562, acc 1\n",
      "2018-10-26T16:04:01.624284: step 2356, loss 0.0251389, acc 0.984375\n",
      "2018-10-26T16:04:02.008300: step 2357, loss 0.0304031, acc 0.984375\n",
      "2018-10-26T16:04:02.429135: step 2358, loss 0.0788982, acc 0.9375\n",
      "2018-10-26T16:04:02.862975: step 2359, loss 0.0166979, acc 1\n",
      "2018-10-26T16:04:03.247946: step 2360, loss 0.0317531, acc 0.96875\n",
      "2018-10-26T16:04:03.644888: step 2361, loss 0.029761, acc 0.984375\n",
      "2018-10-26T16:04:04.018888: step 2362, loss 0.00747773, acc 1\n",
      "2018-10-26T16:04:04.357984: step 2363, loss 0.046224, acc 0.984375\n",
      "2018-10-26T16:04:04.735970: step 2364, loss 0.034144, acc 1\n",
      "2018-10-26T16:04:05.145875: step 2365, loss 0.0767932, acc 0.953125\n",
      "2018-10-26T16:04:05.502924: step 2366, loss 0.0139739, acc 1\n",
      "2018-10-26T16:04:05.870989: step 2367, loss 0.0581466, acc 0.96875\n",
      "2018-10-26T16:04:06.223994: step 2368, loss 0.0955176, acc 0.96875\n",
      "2018-10-26T16:04:06.585269: step 2369, loss 0.0484951, acc 0.984375\n",
      "2018-10-26T16:04:06.964089: step 2370, loss 0.0515016, acc 0.96875\n",
      "2018-10-26T16:04:07.305107: step 2371, loss 0.0381081, acc 0.984375\n",
      "2018-10-26T16:04:07.657165: step 2372, loss 0.0136357, acc 1\n",
      "2018-10-26T16:04:08.053110: step 2373, loss 0.0512362, acc 0.984375\n",
      "2018-10-26T16:04:08.429106: step 2374, loss 0.0263978, acc 1\n",
      "2018-10-26T16:04:08.806099: step 2375, loss 0.0426004, acc 0.984375\n",
      "2018-10-26T16:04:09.169129: step 2376, loss 0.0242069, acc 1\n",
      "2018-10-26T16:04:09.525175: step 2377, loss 0.0222201, acc 0.984375\n",
      "2018-10-26T16:04:09.882220: step 2378, loss 0.0422831, acc 0.984375\n",
      "2018-10-26T16:04:10.223308: step 2379, loss 0.0196012, acc 1\n",
      "2018-10-26T16:04:10.609280: step 2380, loss 0.0430113, acc 0.984375\n",
      "2018-10-26T16:04:10.979290: step 2381, loss 0.0510166, acc 0.96875\n",
      "2018-10-26T16:04:11.376228: step 2382, loss 0.0534842, acc 0.984375\n",
      "2018-10-26T16:04:11.733276: step 2383, loss 0.0590501, acc 0.96875\n",
      "2018-10-26T16:04:12.130216: step 2384, loss 0.0244683, acc 1\n",
      "2018-10-26T16:04:12.514187: step 2385, loss 0.0940684, acc 0.96875\n",
      "2018-10-26T16:04:12.867246: step 2386, loss 0.046011, acc 0.984375\n",
      "2018-10-26T16:04:13.249228: step 2387, loss 0.0358549, acc 0.984375\n",
      "2018-10-26T16:04:13.601283: step 2388, loss 0.0433162, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:04:13.949354: step 2389, loss 0.0635803, acc 0.96875\n",
      "2018-10-26T16:04:14.293435: step 2390, loss 0.0982283, acc 0.953125\n",
      "2018-10-26T16:04:14.648485: step 2391, loss 0.0100554, acc 1\n",
      "2018-10-26T16:04:14.979604: step 2392, loss 0.0703998, acc 0.96875\n",
      "2018-10-26T16:04:15.309720: step 2393, loss 0.03179, acc 1\n",
      "2018-10-26T16:04:15.660781: step 2394, loss 0.0213166, acc 1\n",
      "2018-10-26T16:04:16.074679: step 2395, loss 0.0352571, acc 0.984375\n",
      "2018-10-26T16:04:16.437750: step 2396, loss 0.0414023, acc 0.96875\n",
      "2018-10-26T16:04:16.803728: step 2397, loss 0.0407007, acc 1\n",
      "2018-10-26T16:04:17.161775: step 2398, loss 0.0152995, acc 1\n",
      "2018-10-26T16:04:17.523802: step 2399, loss 0.0154487, acc 1\n",
      "2018-10-26T16:04:17.901797: step 2400, loss 0.0338952, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:04:18.770473: step 2400, loss 0.971932, acc 0.707317\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2400\n",
      "\n",
      "2018-10-26T16:04:19.396801: step 2401, loss 0.0090816, acc 1\n",
      "2018-10-26T16:04:19.766814: step 2402, loss 0.0360904, acc 0.984375\n",
      "2018-10-26T16:04:20.319539: step 2403, loss 0.0289711, acc 1\n",
      "2018-10-26T16:04:20.652444: step 2404, loss 0.022273, acc 1\n",
      "2018-10-26T16:04:20.998519: step 2405, loss 0.0220856, acc 1\n",
      "2018-10-26T16:04:21.365538: step 2406, loss 0.0470114, acc 0.984375\n",
      "2018-10-26T16:04:21.722586: step 2407, loss 0.034387, acc 0.984375\n",
      "2018-10-26T16:04:22.072648: step 2408, loss 0.0493452, acc 0.984375\n",
      "2018-10-26T16:04:22.404805: step 2409, loss 0.0341287, acc 0.984375\n",
      "2018-10-26T16:04:22.784749: step 2410, loss 0.0397474, acc 0.984375\n",
      "2018-10-26T16:04:23.134812: step 2411, loss 0.0604173, acc 0.984375\n",
      "2018-10-26T16:04:23.463022: step 2412, loss 0.0265184, acc 0.984375\n",
      "2018-10-26T16:04:23.808053: step 2413, loss 0.064263, acc 0.96875\n",
      "2018-10-26T16:04:24.152272: step 2414, loss 0.0172256, acc 1\n",
      "2018-10-26T16:04:24.524100: step 2415, loss 0.0562502, acc 0.984375\n",
      "2018-10-26T16:04:24.883142: step 2416, loss 0.0154675, acc 1\n",
      "2018-10-26T16:04:25.230212: step 2417, loss 0.044905, acc 0.984375\n",
      "2018-10-26T16:04:25.680027: step 2418, loss 0.0159557, acc 1\n",
      "2018-10-26T16:04:26.011125: step 2419, loss 0.0199979, acc 1\n",
      "2018-10-26T16:04:26.317309: step 2420, loss 0.052659, acc 0.984375\n",
      "2018-10-26T16:04:26.654408: step 2421, loss 0.0187965, acc 1\n",
      "2018-10-26T16:04:26.966634: step 2422, loss 0.0207925, acc 1\n",
      "2018-10-26T16:04:27.334591: step 2423, loss 0.0155036, acc 1\n",
      "2018-10-26T16:04:27.667701: step 2424, loss 0.00824675, acc 1\n",
      "2018-10-26T16:04:28.055663: step 2425, loss 0.0873435, acc 0.96875\n",
      "2018-10-26T16:04:28.398747: step 2426, loss 0.0192115, acc 1\n",
      "2018-10-26T16:04:28.751803: step 2427, loss 0.0310372, acc 1\n",
      "2018-10-26T16:04:29.115834: step 2428, loss 0.0370835, acc 0.984375\n",
      "2018-10-26T16:04:29.444952: step 2429, loss 0.0094811, acc 1\n",
      "2018-10-26T16:04:29.785042: step 2430, loss 0.016521, acc 1\n",
      "2018-10-26T16:04:30.114219: step 2431, loss 0.0796677, acc 0.96875\n",
      "2018-10-26T16:04:30.420349: step 2432, loss 0.0265433, acc 1\n",
      "2018-10-26T16:04:30.755499: step 2433, loss 0.0233223, acc 1\n",
      "2018-10-26T16:04:31.106560: step 2434, loss 0.0408321, acc 0.984375\n",
      "2018-10-26T16:04:31.430647: step 2435, loss 0.039581, acc 0.984375\n",
      "2018-10-26T16:04:31.757775: step 2436, loss 0.025188, acc 1\n",
      "2018-10-26T16:04:32.112825: step 2437, loss 0.0176738, acc 1\n",
      "2018-10-26T16:04:32.456117: step 2438, loss 0.010129, acc 1\n",
      "2018-10-26T16:04:32.786024: step 2439, loss 0.0614045, acc 0.96875\n",
      "2018-10-26T16:04:33.109164: step 2440, loss 0.0162202, acc 1\n",
      "2018-10-26T16:04:33.430307: step 2441, loss 0.0690989, acc 0.984375\n",
      "2018-10-26T16:04:33.750486: step 2442, loss 0.0455132, acc 0.984375\n",
      "2018-10-26T16:04:34.086550: step 2443, loss 0.00545687, acc 1\n",
      "2018-10-26T16:04:34.419660: step 2444, loss 0.0202327, acc 0.984375\n",
      "2018-10-26T16:04:34.789750: step 2445, loss 0.0164645, acc 1\n",
      "2018-10-26T16:04:35.120789: step 2446, loss 0.0167456, acc 1\n",
      "2018-10-26T16:04:35.439974: step 2447, loss 0.0149063, acc 1\n",
      "2018-10-26T16:04:35.757167: step 2448, loss 0.039485, acc 0.984375\n",
      "2018-10-26T16:04:36.084212: step 2449, loss 0.017117, acc 1\n",
      "2018-10-26T16:04:36.415331: step 2450, loss 0.0084375, acc 1\n",
      "2018-10-26T16:04:36.753424: step 2451, loss 0.0565674, acc 0.984375\n",
      "2018-10-26T16:04:37.066586: step 2452, loss 0.0643128, acc 0.96875\n",
      "2018-10-26T16:04:37.399701: step 2453, loss 0.0286807, acc 0.984375\n",
      "2018-10-26T16:04:37.717849: step 2454, loss 0.0340452, acc 0.984375\n",
      "2018-10-26T16:04:38.057941: step 2455, loss 0.0430348, acc 0.984375\n",
      "2018-10-26T16:04:38.379082: step 2456, loss 0.070468, acc 0.984375\n",
      "2018-10-26T16:04:38.721250: step 2457, loss 0.0482887, acc 0.984375\n",
      "2018-10-26T16:04:39.106137: step 2458, loss 0.020424, acc 0.984375\n",
      "2018-10-26T16:04:39.433264: step 2459, loss 0.0218698, acc 1\n",
      "2018-10-26T16:04:39.765376: step 2460, loss 0.0487938, acc 1\n",
      "2018-10-26T16:04:40.104471: step 2461, loss 0.011075, acc 1\n",
      "2018-10-26T16:04:40.404736: step 2462, loss 0.018883, acc 1\n",
      "2018-10-26T16:04:40.736784: step 2463, loss 0.070632, acc 0.96875\n",
      "2018-10-26T16:04:41.066899: step 2464, loss 0.0387698, acc 0.984375\n",
      "2018-10-26T16:04:41.393031: step 2465, loss 0.0575876, acc 0.96875\n",
      "2018-10-26T16:04:41.719157: step 2466, loss 0.067028, acc 0.984375\n",
      "2018-10-26T16:04:42.078201: step 2467, loss 0.0263447, acc 1\n",
      "2018-10-26T16:04:42.418351: step 2468, loss 0.0172429, acc 1\n",
      "2018-10-26T16:04:42.760378: step 2469, loss 0.00796465, acc 1\n",
      "2018-10-26T16:04:43.095482: step 2470, loss 0.0127465, acc 1\n",
      "2018-10-26T16:04:43.432579: step 2471, loss 0.0160194, acc 1\n",
      "2018-10-26T16:04:43.777657: step 2472, loss 0.0218847, acc 1\n",
      "2018-10-26T16:04:44.109771: step 2473, loss 0.0203764, acc 1\n",
      "2018-10-26T16:04:44.439890: step 2474, loss 0.0237193, acc 0.984375\n",
      "2018-10-26T16:04:44.786016: step 2475, loss 0.0107038, acc 1\n",
      "2018-10-26T16:04:45.126054: step 2476, loss 0.0176585, acc 1\n",
      "2018-10-26T16:04:45.438223: step 2477, loss 0.0143668, acc 1\n",
      "2018-10-26T16:04:45.769438: step 2478, loss 0.112543, acc 0.96875\n",
      "2018-10-26T16:04:46.088483: step 2479, loss 0.0578122, acc 0.984375\n",
      "2018-10-26T16:04:46.399654: step 2480, loss 0.0154361, acc 1\n",
      "2018-10-26T16:04:46.719879: step 2481, loss 0.0671061, acc 0.953125\n",
      "2018-10-26T16:04:47.049913: step 2482, loss 0.0243891, acc 0.984375\n",
      "2018-10-26T16:04:47.371100: step 2483, loss 0.0874162, acc 0.984375\n",
      "2018-10-26T16:04:47.725110: step 2484, loss 0.0225098, acc 1\n",
      "2018-10-26T16:04:48.044257: step 2485, loss 0.0185427, acc 1\n",
      "2018-10-26T16:04:48.365400: step 2486, loss 0.0392514, acc 0.984375\n",
      "2018-10-26T16:04:48.675570: step 2487, loss 0.0630922, acc 0.96875\n",
      "2018-10-26T16:04:48.996713: step 2488, loss 0.0280991, acc 0.984375\n",
      "2018-10-26T16:04:49.319849: step 2489, loss 0.0537681, acc 0.984375\n",
      "2018-10-26T16:04:49.648037: step 2490, loss 0.0307954, acc 0.984375\n",
      "2018-10-26T16:04:49.961137: step 2491, loss 0.0200167, acc 1\n",
      "2018-10-26T16:04:50.281339: step 2492, loss 0.0509379, acc 0.96875\n",
      "2018-10-26T16:04:50.662265: step 2493, loss 0.040363, acc 0.984375\n",
      "2018-10-26T16:04:50.992386: step 2494, loss 0.0609134, acc 0.96875\n",
      "2018-10-26T16:04:51.353415: step 2495, loss 0.0367503, acc 1\n",
      "2018-10-26T16:04:51.737391: step 2496, loss 0.108941, acc 0.953125\n",
      "2018-10-26T16:04:52.070503: step 2497, loss 0.0466665, acc 0.984375\n",
      "2018-10-26T16:04:52.391721: step 2498, loss 0.0401379, acc 0.984375\n",
      "2018-10-26T16:04:52.742777: step 2499, loss 0.0141832, acc 1\n",
      "2018-10-26T16:04:53.095799: step 2500, loss 0.0377882, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:04:53.861765: step 2500, loss 0.999259, acc 0.706379\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2500\n",
      "\n",
      "2018-10-26T16:04:54.462109: step 2501, loss 0.0508307, acc 0.984375\n",
      "2018-10-26T16:04:54.768329: step 2502, loss 0.0261326, acc 1\n",
      "2018-10-26T16:04:55.205124: step 2503, loss 0.00729401, acc 1\n",
      "2018-10-26T16:04:55.542223: step 2504, loss 0.015993, acc 1\n",
      "2018-10-26T16:04:55.879323: step 2505, loss 0.00923165, acc 1\n",
      "2018-10-26T16:04:56.213431: step 2506, loss 0.0204717, acc 1\n",
      "2018-10-26T16:04:56.628388: step 2507, loss 0.00935759, acc 1\n",
      "2018-10-26T16:04:56.952456: step 2508, loss 0.0308936, acc 0.984375\n",
      "2018-10-26T16:04:57.290553: step 2509, loss 0.053755, acc 0.96875\n",
      "2018-10-26T16:04:57.620670: step 2510, loss 0.0187458, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:04:57.955778: step 2511, loss 0.0232098, acc 0.984375\n",
      "2018-10-26T16:04:58.281904: step 2512, loss 0.041929, acc 0.984375\n",
      "2018-10-26T16:04:58.613161: step 2513, loss 0.0358567, acc 0.984375\n",
      "2018-10-26T16:04:58.926223: step 2514, loss 0.0435087, acc 0.984375\n",
      "2018-10-26T16:04:59.240342: step 2515, loss 0.0249711, acc 1\n",
      "2018-10-26T16:04:59.557495: step 2516, loss 0.0112718, acc 1\n",
      "2018-10-26T16:04:59.899664: step 2517, loss 0.0524735, acc 0.984375\n",
      "2018-10-26T16:05:00.258625: step 2518, loss 0.0204666, acc 1\n",
      "2018-10-26T16:05:00.597718: step 2519, loss 0.0216576, acc 1\n",
      "2018-10-26T16:05:00.927834: step 2520, loss 0.0320229, acc 0.984375\n",
      "2018-10-26T16:05:01.258950: step 2521, loss 0.0535071, acc 0.984375\n",
      "2018-10-26T16:05:01.589070: step 2522, loss 0.0134862, acc 1\n",
      "2018-10-26T16:05:01.936143: step 2523, loss 0.0349911, acc 0.984375\n",
      "2018-10-26T16:05:02.275365: step 2524, loss 0.0385621, acc 0.984375\n",
      "2018-10-26T16:05:02.688130: step 2525, loss 0.0206195, acc 1\n",
      "2018-10-26T16:05:03.063219: step 2526, loss 0.100876, acc 0.984375\n",
      "2018-10-26T16:05:03.443113: step 2527, loss 0.0630628, acc 0.984375\n",
      "2018-10-26T16:05:03.802154: step 2528, loss 0.0120481, acc 1\n",
      "2018-10-26T16:05:04.228084: step 2529, loss 0.0255433, acc 1\n",
      "2018-10-26T16:05:04.653878: step 2530, loss 0.033968, acc 1\n",
      "2018-10-26T16:05:05.133596: step 2531, loss 0.0312354, acc 1\n",
      "2018-10-26T16:05:05.584392: step 2532, loss 0.0102461, acc 1\n",
      "2018-10-26T16:05:06.039177: step 2533, loss 0.00567687, acc 1\n",
      "2018-10-26T16:05:06.452074: step 2534, loss 0.0289652, acc 0.984375\n",
      "2018-10-26T16:05:06.886088: step 2535, loss 0.014576, acc 1\n",
      "2018-10-26T16:05:07.325739: step 2536, loss 0.0387276, acc 0.984375\n",
      "2018-10-26T16:05:07.732652: step 2537, loss 0.0217839, acc 1\n",
      "2018-10-26T16:05:08.116742: step 2538, loss 0.0105944, acc 1\n",
      "2018-10-26T16:05:08.515561: step 2539, loss 0.0689901, acc 0.96875\n",
      "2018-10-26T16:05:08.869614: step 2540, loss 0.0240952, acc 1\n",
      "2018-10-26T16:05:09.188762: step 2541, loss 0.0292216, acc 0.984375\n",
      "2018-10-26T16:05:09.503921: step 2542, loss 0.0351746, acc 0.984375\n",
      "2018-10-26T16:05:09.816086: step 2543, loss 0.0316561, acc 0.984375\n",
      "2018-10-26T16:05:10.137228: step 2544, loss 0.0145538, acc 1\n",
      "2018-10-26T16:05:10.550203: step 2545, loss 0.0139903, acc 1\n",
      "2018-10-26T16:05:10.956041: step 2546, loss 0.0502217, acc 0.984375\n",
      "2018-10-26T16:05:11.289150: step 2547, loss 0.0365336, acc 0.984375\n",
      "2018-10-26T16:05:11.629244: step 2548, loss 0.00851222, acc 1\n",
      "2018-10-26T16:05:11.975317: step 2549, loss 0.0437106, acc 0.984375\n",
      "2018-10-26T16:05:12.296460: step 2550, loss 0.0113121, acc 1\n",
      "2018-10-26T16:05:12.648519: step 2551, loss 0.0356113, acc 0.984375\n",
      "2018-10-26T16:05:12.989607: step 2552, loss 0.0287917, acc 1\n",
      "2018-10-26T16:05:13.304768: step 2553, loss 0.0270486, acc 1\n",
      "2018-10-26T16:05:13.635879: step 2554, loss 0.00947602, acc 1\n",
      "2018-10-26T16:05:13.956074: step 2555, loss 0.00276036, acc 1\n",
      "2018-10-26T16:05:14.280162: step 2556, loss 0.0406504, acc 0.984375\n",
      "2018-10-26T16:05:14.604295: step 2557, loss 0.00997915, acc 1\n",
      "2018-10-26T16:05:14.962366: step 2558, loss 0.0170157, acc 1\n",
      "2018-10-26T16:05:15.323371: step 2559, loss 0.0320377, acc 0.984375\n",
      "2018-10-26T16:05:15.685404: step 2560, loss 0.0329881, acc 0.984375\n",
      "2018-10-26T16:05:16.021610: step 2561, loss 0.052166, acc 0.96875\n",
      "2018-10-26T16:05:16.352621: step 2562, loss 0.0300001, acc 1\n",
      "2018-10-26T16:05:16.663860: step 2563, loss 0.0154674, acc 1\n",
      "2018-10-26T16:05:16.993970: step 2564, loss 0.0437904, acc 0.96875\n",
      "2018-10-26T16:05:17.315052: step 2565, loss 0.047857, acc 0.984375\n",
      "2018-10-26T16:05:17.673092: step 2566, loss 0.0195568, acc 0.984375\n",
      "2018-10-26T16:05:17.989530: step 2567, loss 0.014076, acc 1\n",
      "2018-10-26T16:05:18.324353: step 2568, loss 0.0491508, acc 0.984375\n",
      "2018-10-26T16:05:18.640509: step 2569, loss 0.0116502, acc 1\n",
      "2018-10-26T16:05:18.966637: step 2570, loss 0.0667586, acc 0.984375\n",
      "2018-10-26T16:05:19.286851: step 2571, loss 0.0304059, acc 0.984375\n",
      "2018-10-26T16:05:19.628936: step 2572, loss 0.0275739, acc 0.984375\n",
      "2018-10-26T16:05:19.943032: step 2573, loss 0.00601153, acc 1\n",
      "2018-10-26T16:05:20.282212: step 2574, loss 0.033973, acc 0.984375\n",
      "2018-10-26T16:05:20.596286: step 2575, loss 0.0294529, acc 0.984375\n",
      "2018-10-26T16:05:20.919447: step 2576, loss 0.0217944, acc 1\n",
      "2018-10-26T16:05:21.244550: step 2577, loss 0.0144331, acc 1\n",
      "2018-10-26T16:05:21.573673: step 2578, loss 0.0348069, acc 0.984375\n",
      "2018-10-26T16:05:21.899800: step 2579, loss 0.0534456, acc 0.96875\n",
      "2018-10-26T16:05:22.236044: step 2580, loss 0.0328395, acc 0.984375\n",
      "2018-10-26T16:05:22.571009: step 2581, loss 0.100803, acc 0.953125\n",
      "2018-10-26T16:05:22.894142: step 2582, loss 0.0159345, acc 1\n",
      "2018-10-26T16:05:23.238225: step 2583, loss 0.028096, acc 0.984375\n",
      "2018-10-26T16:05:23.555518: step 2584, loss 0.0188019, acc 1\n",
      "2018-10-26T16:05:23.864550: step 2585, loss 0.0226201, acc 1\n",
      "2018-10-26T16:05:24.185758: step 2586, loss 0.0117305, acc 1\n",
      "2018-10-26T16:05:24.510864: step 2587, loss 0.00818336, acc 1\n",
      "2018-10-26T16:05:24.827076: step 2588, loss 0.0391346, acc 0.984375\n",
      "2018-10-26T16:05:25.142964: step 2589, loss 0.0230111, acc 1\n",
      "2018-10-26T16:05:25.482229: step 2590, loss 0.0245005, acc 1\n",
      "2018-10-26T16:05:25.812345: step 2591, loss 0.0502532, acc 0.96875\n",
      "2018-10-26T16:05:26.172385: step 2592, loss 0.00527173, acc 1\n",
      "2018-10-26T16:05:26.553367: step 2593, loss 0.00819348, acc 1\n",
      "2018-10-26T16:05:26.925407: step 2594, loss 0.0448964, acc 0.984375\n",
      "2018-10-26T16:05:27.260519: step 2595, loss 0.0603647, acc 0.984375\n",
      "2018-10-26T16:05:27.591592: step 2596, loss 0.0124944, acc 1\n",
      "2018-10-26T16:05:27.927695: step 2597, loss 0.0166588, acc 1\n",
      "2018-10-26T16:05:28.264794: step 2598, loss 0.00231237, acc 1\n",
      "2018-10-26T16:05:28.598900: step 2599, loss 0.0160282, acc 1\n",
      "2018-10-26T16:05:28.935039: step 2600, loss 0.0153865, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:05:29.917378: step 2600, loss 1.04642, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2600\n",
      "\n",
      "2018-10-26T16:05:30.543705: step 2601, loss 0.00721731, acc 1\n",
      "2018-10-26T16:05:30.891773: step 2602, loss 0.01007, acc 1\n",
      "2018-10-26T16:05:31.314647: step 2603, loss 0.00965356, acc 1\n",
      "2018-10-26T16:05:31.659723: step 2604, loss 0.110027, acc 0.984375\n",
      "2018-10-26T16:05:31.989936: step 2605, loss 0.0416702, acc 0.984375\n",
      "2018-10-26T16:05:32.338908: step 2606, loss 0.0850144, acc 0.9375\n",
      "2018-10-26T16:05:32.661149: step 2607, loss 0.00856351, acc 1\n",
      "2018-10-26T16:05:32.998235: step 2608, loss 0.0145671, acc 1\n",
      "2018-10-26T16:05:33.327310: step 2609, loss 0.0118051, acc 1\n",
      "2018-10-26T16:05:33.657436: step 2610, loss 0.0139492, acc 1\n",
      "2018-10-26T16:05:34.003518: step 2611, loss 0.020579, acc 1\n",
      "2018-10-26T16:05:34.359551: step 2612, loss 0.0202747, acc 1\n",
      "2018-10-26T16:05:34.716558: step 2613, loss 0.0241163, acc 1\n",
      "2018-10-26T16:05:35.144412: step 2614, loss 0.0147958, acc 1\n",
      "2018-10-26T16:05:35.487495: step 2615, loss 0.0156394, acc 1\n",
      "2018-10-26T16:05:35.859804: step 2616, loss 0.0409514, acc 0.984375\n",
      "2018-10-26T16:05:36.258437: step 2617, loss 0.031266, acc 0.984375\n",
      "2018-10-26T16:05:36.687290: step 2618, loss 0.0116077, acc 1\n",
      "2018-10-26T16:05:37.027382: step 2619, loss 0.018526, acc 0.984375\n",
      "2018-10-26T16:05:37.443273: step 2620, loss 0.0137543, acc 1\n",
      "2018-10-26T16:05:37.888088: step 2621, loss 0.0318415, acc 0.984375\n",
      "2018-10-26T16:05:38.281114: step 2622, loss 0.0316071, acc 1\n",
      "2018-10-26T16:05:38.636082: step 2623, loss 0.0130642, acc 1\n",
      "2018-10-26T16:05:38.991136: step 2624, loss 0.015326, acc 1\n",
      "2018-10-26T16:05:39.377142: step 2625, loss 0.0308207, acc 0.984375\n",
      "2018-10-26T16:05:39.746117: step 2626, loss 0.0342774, acc 1\n",
      "2018-10-26T16:05:40.162080: step 2627, loss 0.0343398, acc 0.984375\n",
      "2018-10-26T16:05:40.526034: step 2628, loss 0.0644179, acc 0.984375\n",
      "2018-10-26T16:05:40.939929: step 2629, loss 0.0168529, acc 1\n",
      "2018-10-26T16:05:41.341859: step 2630, loss 0.040581, acc 1\n",
      "2018-10-26T16:05:41.733807: step 2631, loss 0.0354696, acc 0.984375\n",
      "2018-10-26T16:05:42.098833: step 2632, loss 0.00597018, acc 1\n",
      "2018-10-26T16:05:42.469843: step 2633, loss 0.0181798, acc 1\n",
      "2018-10-26T16:05:42.848826: step 2634, loss 0.00998423, acc 1\n",
      "2018-10-26T16:05:43.213951: step 2635, loss 0.00830782, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:05:43.580873: step 2636, loss 0.027151, acc 1\n",
      "2018-10-26T16:05:43.960900: step 2637, loss 0.0251906, acc 0.984375\n",
      "2018-10-26T16:05:44.357796: step 2638, loss 0.0377464, acc 0.984375\n",
      "2018-10-26T16:05:44.700878: step 2639, loss 0.00751761, acc 1\n",
      "2018-10-26T16:05:45.055933: step 2640, loss 0.0100236, acc 1\n",
      "2018-10-26T16:05:45.401008: step 2641, loss 0.0405098, acc 0.984375\n",
      "2018-10-26T16:05:45.758092: step 2642, loss 0.0668963, acc 0.984375\n",
      "2018-10-26T16:05:46.163969: step 2643, loss 0.0185715, acc 1\n",
      "2018-10-26T16:05:46.514120: step 2644, loss 0.0877067, acc 0.9375\n",
      "2018-10-26T16:05:46.861107: step 2645, loss 0.0179403, acc 1\n",
      "2018-10-26T16:05:47.279988: step 2646, loss 0.00903283, acc 1\n",
      "2018-10-26T16:05:47.700894: step 2647, loss 0.0161378, acc 1\n",
      "2018-10-26T16:05:48.075862: step 2648, loss 0.0158267, acc 1\n",
      "2018-10-26T16:05:48.410973: step 2649, loss 0.015766, acc 0.984375\n",
      "2018-10-26T16:05:48.836827: step 2650, loss 0.0284312, acc 0.984375\n",
      "2018-10-26T16:05:49.246734: step 2651, loss 0.0352009, acc 0.984375\n",
      "2018-10-26T16:05:49.655640: step 2652, loss 0.0531564, acc 0.96875\n",
      "2018-10-26T16:05:50.009706: step 2653, loss 0.0170775, acc 1\n",
      "2018-10-26T16:05:50.421595: step 2654, loss 0.00801043, acc 1\n",
      "2018-10-26T16:05:50.821631: step 2655, loss 0.0144553, acc 1\n",
      "2018-10-26T16:05:51.163612: step 2656, loss 0.0109527, acc 1\n",
      "2018-10-26T16:05:51.556561: step 2657, loss 0.0208127, acc 1\n",
      "2018-10-26T16:05:51.942606: step 2658, loss 0.0234873, acc 1\n",
      "2018-10-26T16:05:52.358421: step 2659, loss 0.0755134, acc 0.984375\n",
      "2018-10-26T16:05:52.763338: step 2660, loss 0.0116423, acc 1\n",
      "2018-10-26T16:05:53.107418: step 2661, loss 0.0438332, acc 0.96875\n",
      "2018-10-26T16:05:53.516327: step 2662, loss 0.0415292, acc 0.984375\n",
      "2018-10-26T16:05:53.850470: step 2663, loss 0.0187508, acc 1\n",
      "2018-10-26T16:05:54.218450: step 2664, loss 0.0159248, acc 1\n",
      "2018-10-26T16:05:54.652303: step 2665, loss 0.0134529, acc 1\n",
      "2018-10-26T16:05:54.994376: step 2666, loss 0.0201779, acc 0.984375\n",
      "2018-10-26T16:05:55.364389: step 2667, loss 0.0170488, acc 1\n",
      "2018-10-26T16:05:55.715488: step 2668, loss 0.018105, acc 1\n",
      "2018-10-26T16:05:56.109397: step 2669, loss 0.0175504, acc 1\n",
      "2018-10-26T16:05:56.451484: step 2670, loss 0.0275039, acc 0.984375\n",
      "2018-10-26T16:05:56.827478: step 2671, loss 0.0315054, acc 1\n",
      "2018-10-26T16:05:57.204471: step 2672, loss 0.0440915, acc 0.984375\n",
      "2018-10-26T16:05:57.552540: step 2673, loss 0.0277959, acc 0.984375\n",
      "2018-10-26T16:05:57.895625: step 2674, loss 0.0189842, acc 1\n",
      "2018-10-26T16:05:58.264639: step 2675, loss 0.00817669, acc 1\n",
      "2018-10-26T16:05:58.581792: step 2676, loss 0.0156956, acc 1\n",
      "2018-10-26T16:05:58.984718: step 2677, loss 0.0083734, acc 1\n",
      "2018-10-26T16:05:59.333787: step 2678, loss 0.0105611, acc 1\n",
      "2018-10-26T16:05:59.737705: step 2679, loss 0.0955903, acc 0.96875\n",
      "2018-10-26T16:06:00.133644: step 2680, loss 0.0260039, acc 0.984375\n",
      "2018-10-26T16:06:00.482713: step 2681, loss 0.00820587, acc 1\n",
      "2018-10-26T16:06:00.824797: step 2682, loss 0.0214878, acc 1\n",
      "2018-10-26T16:06:01.235754: step 2683, loss 0.010637, acc 1\n",
      "2018-10-26T16:06:01.603717: step 2684, loss 0.067571, acc 0.984375\n",
      "2018-10-26T16:06:01.967743: step 2685, loss 0.0211596, acc 1\n",
      "2018-10-26T16:06:02.341746: step 2686, loss 0.0713019, acc 0.96875\n",
      "2018-10-26T16:06:02.714750: step 2687, loss 0.0151496, acc 1\n",
      "2018-10-26T16:06:03.067879: step 2688, loss 0.0166669, acc 1\n",
      "2018-10-26T16:06:03.506639: step 2689, loss 0.0130779, acc 1\n",
      "2018-10-26T16:06:03.907632: step 2690, loss 0.0113793, acc 1\n",
      "2018-10-26T16:06:04.269597: step 2691, loss 0.0288115, acc 1\n",
      "2018-10-26T16:06:04.614675: step 2692, loss 0.0719733, acc 0.984375\n",
      "2018-10-26T16:06:04.977702: step 2693, loss 0.0111523, acc 1\n",
      "2018-10-26T16:06:05.344723: step 2694, loss 0.0601934, acc 0.96875\n",
      "2018-10-26T16:06:05.758616: step 2695, loss 0.0121013, acc 1\n",
      "2018-10-26T16:06:06.122644: step 2696, loss 0.00829846, acc 1\n",
      "2018-10-26T16:06:06.505621: step 2697, loss 0.0161782, acc 1\n",
      "2018-10-26T16:06:06.962403: step 2698, loss 0.020923, acc 1\n",
      "2018-10-26T16:06:07.330507: step 2699, loss 0.0192446, acc 1\n",
      "2018-10-26T16:06:07.668513: step 2700, loss 0.00974833, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:06:08.613088: step 2700, loss 1.07397, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2700\n",
      "\n",
      "2018-10-26T16:06:09.361059: step 2701, loss 0.00699055, acc 1\n",
      "2018-10-26T16:06:09.784858: step 2702, loss 0.00704968, acc 1\n",
      "2018-10-26T16:06:10.382262: step 2703, loss 0.011201, acc 1\n",
      "2018-10-26T16:06:10.872951: step 2704, loss 0.0672405, acc 0.96875\n",
      "2018-10-26T16:06:11.360694: step 2705, loss 0.00235323, acc 1\n",
      "2018-10-26T16:06:11.873277: step 2706, loss 0.0212894, acc 0.984375\n",
      "2018-10-26T16:06:12.337039: step 2707, loss 0.0336444, acc 0.984375\n",
      "2018-10-26T16:06:12.762901: step 2708, loss 0.0199943, acc 0.984375\n",
      "2018-10-26T16:06:13.209709: step 2709, loss 0.0054297, acc 1\n",
      "2018-10-26T16:06:13.588695: step 2710, loss 0.0074956, acc 1\n",
      "2018-10-26T16:06:13.979650: step 2711, loss 0.0226257, acc 0.984375\n",
      "2018-10-26T16:06:14.346670: step 2712, loss 0.0102331, acc 1\n",
      "2018-10-26T16:06:14.751758: step 2713, loss 0.0257283, acc 0.984375\n",
      "2018-10-26T16:06:15.089685: step 2714, loss 0.0130731, acc 1\n",
      "2018-10-26T16:06:15.412825: step 2715, loss 0.0162014, acc 1\n",
      "2018-10-26T16:06:15.728977: step 2716, loss 0.00140419, acc 1\n",
      "2018-10-26T16:06:16.063085: step 2717, loss 0.0259815, acc 0.984375\n",
      "2018-10-26T16:06:16.388215: step 2718, loss 0.0195438, acc 1\n",
      "2018-10-26T16:06:16.718335: step 2719, loss 0.00933999, acc 1\n",
      "2018-10-26T16:06:17.040473: step 2720, loss 0.0233724, acc 1\n",
      "2018-10-26T16:06:17.358626: step 2721, loss 0.0217141, acc 0.984375\n",
      "2018-10-26T16:06:17.684798: step 2722, loss 0.0578798, acc 0.96875\n",
      "2018-10-26T16:06:18.006890: step 2723, loss 0.0175365, acc 1\n",
      "2018-10-26T16:06:18.351971: step 2724, loss 0.00926615, acc 1\n",
      "2018-10-26T16:06:18.658152: step 2725, loss 0.0192633, acc 1\n",
      "2018-10-26T16:06:18.974306: step 2726, loss 0.00874696, acc 1\n",
      "2018-10-26T16:06:19.316392: step 2727, loss 0.0130376, acc 1\n",
      "2018-10-26T16:06:19.644601: step 2728, loss 0.0130089, acc 1\n",
      "2018-10-26T16:06:19.957678: step 2729, loss 0.0229848, acc 1\n",
      "2018-10-26T16:06:20.282809: step 2730, loss 0.0029949, acc 1\n",
      "2018-10-26T16:06:20.636862: step 2731, loss 0.00744941, acc 1\n",
      "2018-10-26T16:06:20.952054: step 2732, loss 0.00506658, acc 1\n",
      "2018-10-26T16:06:21.258202: step 2733, loss 0.051287, acc 0.984375\n",
      "2018-10-26T16:06:21.577353: step 2734, loss 0.0288739, acc 0.984375\n",
      "2018-10-26T16:06:21.899490: step 2735, loss 0.0315843, acc 0.984375\n",
      "2018-10-26T16:06:22.233596: step 2736, loss 0.0389059, acc 0.984375\n",
      "2018-10-26T16:06:22.558733: step 2737, loss 0.012154, acc 1\n",
      "2018-10-26T16:06:22.887848: step 2738, loss 0.0226339, acc 0.984375\n",
      "2018-10-26T16:06:23.223952: step 2739, loss 0.0289862, acc 1\n",
      "2018-10-26T16:06:23.531174: step 2740, loss 0.0252488, acc 0.984375\n",
      "2018-10-26T16:06:23.863243: step 2741, loss 0.00583144, acc 1\n",
      "2018-10-26T16:06:24.202514: step 2742, loss 0.0376941, acc 0.984375\n",
      "2018-10-26T16:06:24.517567: step 2743, loss 0.0222205, acc 1\n",
      "2018-10-26T16:06:24.838638: step 2744, loss 0.0284117, acc 1\n",
      "2018-10-26T16:06:25.175739: step 2745, loss 0.0148357, acc 1\n",
      "2018-10-26T16:06:25.501931: step 2746, loss 0.0178398, acc 1\n",
      "2018-10-26T16:06:25.821011: step 2747, loss 0.0183477, acc 1\n",
      "2018-10-26T16:06:26.128194: step 2748, loss 0.023335, acc 0.984375\n",
      "2018-10-26T16:06:26.453536: step 2749, loss 0.00771312, acc 1\n",
      "2018-10-26T16:06:26.783441: step 2750, loss 0.0114569, acc 1\n",
      "2018-10-26T16:06:27.113625: step 2751, loss 0.0362611, acc 0.984375\n",
      "2018-10-26T16:06:27.481577: step 2752, loss 0.0125416, acc 1\n",
      "2018-10-26T16:06:27.822665: step 2753, loss 0.033222, acc 0.984375\n",
      "2018-10-26T16:06:28.137822: step 2754, loss 0.00638213, acc 1\n",
      "2018-10-26T16:06:28.644472: step 2755, loss 0.010534, acc 1\n",
      "2018-10-26T16:06:28.983986: step 2756, loss 0.0125837, acc 1\n",
      "2018-10-26T16:06:29.327678: step 2757, loss 0.0122923, acc 1\n",
      "2018-10-26T16:06:29.713613: step 2758, loss 0.0128706, acc 1\n",
      "2018-10-26T16:06:30.097826: step 2759, loss 0.0122141, acc 1\n",
      "2018-10-26T16:06:30.446653: step 2760, loss 0.0228511, acc 1\n",
      "2018-10-26T16:06:30.824642: step 2761, loss 0.011023, acc 1\n",
      "2018-10-26T16:06:31.205625: step 2762, loss 0.00533634, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:06:31.522779: step 2763, loss 0.00878108, acc 1\n",
      "2018-10-26T16:06:31.891855: step 2764, loss 0.0163783, acc 1\n",
      "2018-10-26T16:06:32.224902: step 2765, loss 0.00972706, acc 1\n",
      "2018-10-26T16:06:32.585939: step 2766, loss 0.0196169, acc 1\n",
      "2018-10-26T16:06:33.009805: step 2767, loss 0.00742892, acc 1\n",
      "2018-10-26T16:06:33.425698: step 2768, loss 0.0320406, acc 0.984375\n",
      "2018-10-26T16:06:33.778796: step 2769, loss 0.0296959, acc 0.984375\n",
      "2018-10-26T16:06:34.199629: step 2770, loss 0.0255885, acc 0.984375\n",
      "2018-10-26T16:06:34.610527: step 2771, loss 0.0145376, acc 1\n",
      "2018-10-26T16:06:35.028452: step 2772, loss 0.112819, acc 0.96875\n",
      "2018-10-26T16:06:35.445333: step 2773, loss 0.0198422, acc 0.984375\n",
      "2018-10-26T16:06:35.841242: step 2774, loss 0.0208733, acc 1\n",
      "2018-10-26T16:06:36.204273: step 2775, loss 0.0495408, acc 0.96875\n",
      "2018-10-26T16:06:36.552339: step 2776, loss 0.0139356, acc 1\n",
      "2018-10-26T16:06:36.984185: step 2777, loss 0.011356, acc 1\n",
      "2018-10-26T16:06:37.381192: step 2778, loss 0.0335781, acc 0.984375\n",
      "2018-10-26T16:06:37.811973: step 2779, loss 0.01365, acc 1\n",
      "2018-10-26T16:06:38.149122: step 2780, loss 0.0161601, acc 1\n",
      "2018-10-26T16:06:38.512166: step 2781, loss 0.0398616, acc 0.984375\n",
      "2018-10-26T16:06:38.853191: step 2782, loss 0.0056771, acc 1\n",
      "2018-10-26T16:06:39.201262: step 2783, loss 0.00854775, acc 1\n",
      "2018-10-26T16:06:39.592217: step 2784, loss 0.00668327, acc 1\n",
      "2018-10-26T16:06:39.936298: step 2785, loss 0.0209311, acc 1\n",
      "2018-10-26T16:06:40.282373: step 2786, loss 0.0225366, acc 0.984375\n",
      "2018-10-26T16:06:40.630443: step 2787, loss 0.0225521, acc 0.984375\n",
      "2018-10-26T16:06:40.979512: step 2788, loss 0.00203598, acc 1\n",
      "2018-10-26T16:06:41.326586: step 2789, loss 0.0637016, acc 0.96875\n",
      "2018-10-26T16:06:41.705575: step 2790, loss 0.0129663, acc 1\n",
      "2018-10-26T16:06:42.059624: step 2791, loss 0.0344154, acc 0.96875\n",
      "2018-10-26T16:06:42.402710: step 2792, loss 0.00544183, acc 1\n",
      "2018-10-26T16:06:42.766735: step 2793, loss 0.0162226, acc 1\n",
      "2018-10-26T16:06:43.126773: step 2794, loss 0.00875982, acc 1\n",
      "2018-10-26T16:06:43.462876: step 2795, loss 0.0271207, acc 0.984375\n",
      "2018-10-26T16:06:43.849841: step 2796, loss 0.0429045, acc 0.984375\n",
      "2018-10-26T16:06:44.210877: step 2797, loss 0.0279943, acc 0.984375\n",
      "2018-10-26T16:06:44.566925: step 2798, loss 0.0101835, acc 1\n",
      "2018-10-26T16:06:44.943918: step 2799, loss 0.0582283, acc 0.984375\n",
      "2018-10-26T16:06:45.285010: step 2800, loss 0.0234673, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:06:46.067952: step 2800, loss 1.11373, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2800\n",
      "\n",
      "2018-10-26T16:06:46.675450: step 2801, loss 0.00760065, acc 1\n",
      "2018-10-26T16:06:47.021367: step 2802, loss 0.0138746, acc 1\n",
      "2018-10-26T16:06:47.471246: step 2803, loss 0.00700018, acc 1\n",
      "2018-10-26T16:06:47.834198: step 2804, loss 0.0399513, acc 1\n",
      "2018-10-26T16:06:48.196232: step 2805, loss 0.0698681, acc 0.96875\n",
      "2018-10-26T16:06:48.566241: step 2806, loss 0.00928533, acc 1\n",
      "2018-10-26T16:06:48.932412: step 2807, loss 0.0185527, acc 1\n",
      "2018-10-26T16:06:49.339177: step 2808, loss 0.0175357, acc 1\n",
      "2018-10-26T16:06:49.696220: step 2809, loss 0.00584364, acc 1\n",
      "2018-10-26T16:06:50.058254: step 2810, loss 0.00364877, acc 1\n",
      "2018-10-26T16:06:50.388374: step 2811, loss 0.0118825, acc 1\n",
      "2018-10-26T16:06:50.794288: step 2812, loss 0.00775116, acc 1\n",
      "2018-10-26T16:06:51.142357: step 2813, loss 0.0120714, acc 1\n",
      "2018-10-26T16:06:51.484444: step 2814, loss 0.0255454, acc 1\n",
      "2018-10-26T16:06:51.881446: step 2815, loss 0.0226743, acc 1\n",
      "2018-10-26T16:06:52.241420: step 2816, loss 0.0152165, acc 1\n",
      "2018-10-26T16:06:52.569545: step 2817, loss 0.0129626, acc 1\n",
      "2018-10-26T16:06:52.956563: step 2818, loss 0.0376423, acc 0.984375\n",
      "2018-10-26T16:06:53.328546: step 2819, loss 0.0399781, acc 0.984375\n",
      "2018-10-26T16:06:53.721549: step 2820, loss 0.0396241, acc 0.96875\n",
      "2018-10-26T16:06:54.130373: step 2821, loss 0.0296309, acc 0.984375\n",
      "2018-10-26T16:06:54.509361: step 2822, loss 0.0107057, acc 1\n",
      "2018-10-26T16:06:54.871395: step 2823, loss 0.0154002, acc 1\n",
      "2018-10-26T16:06:55.219522: step 2824, loss 0.016005, acc 1\n",
      "2018-10-26T16:06:55.569531: step 2825, loss 0.0878865, acc 0.984375\n",
      "2018-10-26T16:06:55.914605: step 2826, loss 0.0066847, acc 1\n",
      "2018-10-26T16:06:56.282623: step 2827, loss 0.0143438, acc 1\n",
      "2018-10-26T16:06:56.697559: step 2828, loss 0.0111049, acc 1\n",
      "2018-10-26T16:06:57.061640: step 2829, loss 0.0934473, acc 0.96875\n",
      "2018-10-26T16:06:57.418589: step 2830, loss 0.0165742, acc 1\n",
      "2018-10-26T16:06:57.754690: step 2831, loss 0.00439751, acc 1\n",
      "2018-10-26T16:06:58.120712: step 2832, loss 0.0161561, acc 1\n",
      "2018-10-26T16:06:58.464797: step 2833, loss 0.0102374, acc 1\n",
      "2018-10-26T16:06:58.817853: step 2834, loss 0.00621737, acc 1\n",
      "2018-10-26T16:06:59.172949: step 2835, loss 0.017392, acc 0.984375\n",
      "2018-10-26T16:06:59.518978: step 2836, loss 0.00443176, acc 1\n",
      "2018-10-26T16:06:59.870040: step 2837, loss 0.00860897, acc 1\n",
      "2018-10-26T16:07:00.257007: step 2838, loss 0.0673403, acc 0.984375\n",
      "2018-10-26T16:07:00.630007: step 2839, loss 0.0740569, acc 0.984375\n",
      "2018-10-26T16:07:01.031933: step 2840, loss 0.0208554, acc 1\n",
      "2018-10-26T16:07:01.375083: step 2841, loss 0.0572863, acc 0.96875\n",
      "2018-10-26T16:07:01.739044: step 2842, loss 0.0192873, acc 1\n",
      "2018-10-26T16:07:02.088112: step 2843, loss 0.00573288, acc 1\n",
      "2018-10-26T16:07:02.427209: step 2844, loss 0.0503357, acc 0.984375\n",
      "2018-10-26T16:07:02.781424: step 2845, loss 0.0361831, acc 0.96875\n",
      "2018-10-26T16:07:03.137312: step 2846, loss 0.0329983, acc 0.984375\n",
      "2018-10-26T16:07:03.519291: step 2847, loss 0.00682774, acc 1\n",
      "2018-10-26T16:07:03.884313: step 2848, loss 0.0328489, acc 0.984375\n",
      "2018-10-26T16:07:04.255325: step 2849, loss 0.00862952, acc 1\n",
      "2018-10-26T16:07:04.623442: step 2850, loss 0.0102774, acc 1\n",
      "2018-10-26T16:07:04.993353: step 2851, loss 0.00694368, acc 1\n",
      "2018-10-26T16:07:05.360369: step 2852, loss 0.00444964, acc 1\n",
      "2018-10-26T16:07:05.699462: step 2853, loss 0.00378029, acc 1\n",
      "2018-10-26T16:07:06.053521: step 2854, loss 0.00643405, acc 1\n",
      "2018-10-26T16:07:06.407619: step 2855, loss 0.0171144, acc 1\n",
      "2018-10-26T16:07:06.763620: step 2856, loss 0.0351884, acc 0.984375\n",
      "2018-10-26T16:07:07.172530: step 2857, loss 0.00515575, acc 1\n",
      "2018-10-26T16:07:07.522591: step 2858, loss 0.00472733, acc 1\n",
      "2018-10-26T16:07:07.897654: step 2859, loss 0.00332648, acc 1\n",
      "2018-10-26T16:07:08.253642: step 2860, loss 0.0111333, acc 1\n",
      "2018-10-26T16:07:08.599714: step 2861, loss 0.018479, acc 1\n",
      "2018-10-26T16:07:08.938839: step 2862, loss 0.0259793, acc 0.984375\n",
      "2018-10-26T16:07:09.286882: step 2863, loss 0.0151418, acc 1\n",
      "2018-10-26T16:07:09.638940: step 2864, loss 0.0162207, acc 1\n",
      "2018-10-26T16:07:09.986009: step 2865, loss 0.00528419, acc 1\n",
      "2018-10-26T16:07:10.337076: step 2866, loss 0.00900673, acc 1\n",
      "2018-10-26T16:07:10.690131: step 2867, loss 0.00778038, acc 1\n",
      "2018-10-26T16:07:11.065130: step 2868, loss 0.0624296, acc 0.984375\n",
      "2018-10-26T16:07:11.415195: step 2869, loss 0.00463143, acc 1\n",
      "2018-10-26T16:07:11.759274: step 2870, loss 0.0268857, acc 0.984375\n",
      "2018-10-26T16:07:12.119312: step 2871, loss 0.0398147, acc 0.984375\n",
      "2018-10-26T16:07:12.479348: step 2872, loss 0.0134286, acc 1\n",
      "2018-10-26T16:07:12.856340: step 2873, loss 0.0188954, acc 1\n",
      "2018-10-26T16:07:13.212433: step 2874, loss 0.00784841, acc 1\n",
      "2018-10-26T16:07:13.563452: step 2875, loss 0.014519, acc 1\n",
      "2018-10-26T16:07:13.989316: step 2876, loss 0.066786, acc 0.984375\n",
      "2018-10-26T16:07:14.368301: step 2877, loss 0.00569069, acc 1\n",
      "2018-10-26T16:07:14.765240: step 2878, loss 0.0162369, acc 0.984375\n",
      "2018-10-26T16:07:15.190104: step 2879, loss 0.0111071, acc 1\n",
      "2018-10-26T16:07:15.576073: step 2880, loss 0.00495715, acc 1\n",
      "2018-10-26T16:07:16.123611: step 2881, loss 0.0177245, acc 0.984375\n",
      "2018-10-26T16:07:16.607319: step 2882, loss 0.015556, acc 1\n",
      "2018-10-26T16:07:17.090029: step 2883, loss 0.00584854, acc 1\n",
      "2018-10-26T16:07:17.562765: step 2884, loss 0.0120785, acc 1\n",
      "2018-10-26T16:07:17.993614: step 2885, loss 0.00727345, acc 1\n",
      "2018-10-26T16:07:18.491368: step 2886, loss 0.00654274, acc 1\n",
      "2018-10-26T16:07:18.900415: step 2887, loss 0.0108427, acc 1\n",
      "2018-10-26T16:07:19.305111: step 2888, loss 0.0116884, acc 1\n",
      "2018-10-26T16:07:19.708035: step 2889, loss 0.0157557, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:07:20.132898: step 2890, loss 0.00908627, acc 1\n",
      "2018-10-26T16:07:20.545797: step 2891, loss 0.0195083, acc 1\n",
      "2018-10-26T16:07:20.934756: step 2892, loss 0.0422691, acc 0.96875\n",
      "2018-10-26T16:07:21.266870: step 2893, loss 0.0283866, acc 0.984375\n",
      "2018-10-26T16:07:21.609952: step 2894, loss 0.00833377, acc 1\n",
      "2018-10-26T16:07:21.956028: step 2895, loss 0.00945023, acc 1\n",
      "2018-10-26T16:07:22.356086: step 2896, loss 0.00522456, acc 1\n",
      "2018-10-26T16:07:22.705026: step 2897, loss 0.00434472, acc 1\n",
      "2018-10-26T16:07:23.055090: step 2898, loss 0.0124101, acc 1\n",
      "2018-10-26T16:07:23.418121: step 2899, loss 0.0108016, acc 1\n",
      "2018-10-26T16:07:23.769186: step 2900, loss 0.00843216, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:07:24.582134: step 2900, loss 1.13257, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-2900\n",
      "\n",
      "2018-10-26T16:07:25.206343: step 2901, loss 0.0111428, acc 1\n",
      "2018-10-26T16:07:25.559399: step 2902, loss 0.00894877, acc 1\n",
      "2018-10-26T16:07:26.028147: step 2903, loss 0.0509496, acc 0.984375\n",
      "2018-10-26T16:07:26.361314: step 2904, loss 0.00436022, acc 1\n",
      "2018-10-26T16:07:26.711322: step 2905, loss 0.0190078, acc 0.984375\n",
      "2018-10-26T16:07:27.054406: step 2906, loss 0.0117753, acc 1\n",
      "2018-10-26T16:07:27.416441: step 2907, loss 0.0108382, acc 1\n",
      "2018-10-26T16:07:27.762514: step 2908, loss 0.00446057, acc 1\n",
      "2018-10-26T16:07:28.129534: step 2909, loss 0.0337335, acc 0.984375\n",
      "2018-10-26T16:07:28.478600: step 2910, loss 0.00500274, acc 1\n",
      "2018-10-26T16:07:28.830663: step 2911, loss 0.0312524, acc 0.984375\n",
      "2018-10-26T16:07:29.161777: step 2912, loss 0.00864632, acc 1\n",
      "2018-10-26T16:07:29.521815: step 2913, loss 0.00322363, acc 1\n",
      "2018-10-26T16:07:29.876868: step 2914, loss 0.00456656, acc 1\n",
      "2018-10-26T16:07:30.222013: step 2915, loss 0.0117985, acc 1\n",
      "2018-10-26T16:07:30.609975: step 2916, loss 0.00772884, acc 1\n",
      "2018-10-26T16:07:30.967949: step 2917, loss 0.0260038, acc 0.984375\n",
      "2018-10-26T16:07:31.348934: step 2918, loss 0.0122471, acc 1\n",
      "2018-10-26T16:07:31.698995: step 2919, loss 0.0051685, acc 1\n",
      "2018-10-26T16:07:32.039090: step 2920, loss 0.0138531, acc 1\n",
      "2018-10-26T16:07:32.388193: step 2921, loss 0.00291626, acc 1\n",
      "2018-10-26T16:07:32.732276: step 2922, loss 0.00679134, acc 1\n",
      "2018-10-26T16:07:33.079309: step 2923, loss 0.00958506, acc 1\n",
      "2018-10-26T16:07:33.468375: step 2924, loss 0.0340869, acc 0.984375\n",
      "2018-10-26T16:07:33.823324: step 2925, loss 0.0144959, acc 1\n",
      "2018-10-26T16:07:34.215277: step 2926, loss 0.00542129, acc 1\n",
      "2018-10-26T16:07:34.574313: step 2927, loss 0.00432114, acc 1\n",
      "2018-10-26T16:07:34.918397: step 2928, loss 0.00931551, acc 1\n",
      "2018-10-26T16:07:35.275444: step 2929, loss 0.00558191, acc 1\n",
      "2018-10-26T16:07:35.631527: step 2930, loss 0.00643028, acc 1\n",
      "2018-10-26T16:07:35.985545: step 2931, loss 0.0113072, acc 1\n",
      "2018-10-26T16:07:36.333613: step 2932, loss 0.00595978, acc 1\n",
      "2018-10-26T16:07:36.671753: step 2933, loss 0.0283047, acc 0.984375\n",
      "2018-10-26T16:07:37.022776: step 2934, loss 0.0189589, acc 0.984375\n",
      "2018-10-26T16:07:37.499497: step 2935, loss 0.0059518, acc 1\n",
      "2018-10-26T16:07:37.839588: step 2936, loss 0.01346, acc 1\n",
      "2018-10-26T16:07:38.200628: step 2937, loss 0.0162158, acc 1\n",
      "2018-10-26T16:07:38.553820: step 2938, loss 0.013265, acc 1\n",
      "2018-10-26T16:07:38.902750: step 2939, loss 0.0182675, acc 1\n",
      "2018-10-26T16:07:39.247830: step 2940, loss 0.00649762, acc 1\n",
      "2018-10-26T16:07:39.607864: step 2941, loss 0.0384925, acc 0.984375\n",
      "2018-10-26T16:07:39.943966: step 2942, loss 0.047297, acc 0.984375\n",
      "2018-10-26T16:07:40.286057: step 2943, loss 0.00720689, acc 1\n",
      "2018-10-26T16:07:40.640106: step 2944, loss 0.00429133, acc 1\n",
      "2018-10-26T16:07:40.994161: step 2945, loss 0.00728078, acc 1\n",
      "2018-10-26T16:07:41.350208: step 2946, loss 0.0131518, acc 1\n",
      "2018-10-26T16:07:41.703268: step 2947, loss 0.00607651, acc 1\n",
      "2018-10-26T16:07:42.040403: step 2948, loss 0.0332351, acc 0.984375\n",
      "2018-10-26T16:07:42.383451: step 2949, loss 0.00884859, acc 1\n",
      "2018-10-26T16:07:42.737548: step 2950, loss 0.035749, acc 0.984375\n",
      "2018-10-26T16:07:43.117489: step 2951, loss 0.00858604, acc 1\n",
      "2018-10-26T16:07:43.467614: step 2952, loss 0.0379074, acc 0.984375\n",
      "2018-10-26T16:07:43.818613: step 2953, loss 0.00644175, acc 1\n",
      "2018-10-26T16:07:44.174668: step 2954, loss 0.0102116, acc 1\n",
      "2018-10-26T16:07:44.564621: step 2955, loss 0.0102821, acc 1\n",
      "2018-10-26T16:07:44.903735: step 2956, loss 0.0242168, acc 0.984375\n",
      "2018-10-26T16:07:45.279846: step 2957, loss 0.00899563, acc 1\n",
      "2018-10-26T16:07:45.622794: step 2958, loss 0.0151071, acc 1\n",
      "2018-10-26T16:07:45.967870: step 2959, loss 0.00479725, acc 1\n",
      "2018-10-26T16:07:46.303974: step 2960, loss 0.00233643, acc 1\n",
      "2018-10-26T16:07:46.651109: step 2961, loss 0.00616369, acc 1\n",
      "2018-10-26T16:07:47.009091: step 2962, loss 0.00829506, acc 1\n",
      "2018-10-26T16:07:47.364189: step 2963, loss 0.0398196, acc 0.984375\n",
      "2018-10-26T16:07:47.698252: step 2964, loss 0.056279, acc 0.984375\n",
      "2018-10-26T16:07:48.042328: step 2965, loss 0.0111072, acc 1\n",
      "2018-10-26T16:07:48.383419: step 2966, loss 0.00427933, acc 1\n",
      "2018-10-26T16:07:48.752432: step 2967, loss 0.0897921, acc 0.984375\n",
      "2018-10-26T16:07:49.132416: step 2968, loss 0.0139018, acc 1\n",
      "2018-10-26T16:07:49.491649: step 2969, loss 0.0121186, acc 1\n",
      "2018-10-26T16:07:49.824570: step 2970, loss 0.0262643, acc 0.984375\n",
      "2018-10-26T16:07:50.176669: step 2971, loss 0.020364, acc 1\n",
      "2018-10-26T16:07:50.520785: step 2972, loss 0.0172765, acc 1\n",
      "2018-10-26T16:07:50.865785: step 2973, loss 0.0052795, acc 1\n",
      "2018-10-26T16:07:51.216869: step 2974, loss 0.0182558, acc 1\n",
      "2018-10-26T16:07:51.567949: step 2975, loss 0.0117604, acc 1\n",
      "2018-10-26T16:07:51.921964: step 2976, loss 0.0107649, acc 1\n",
      "2018-10-26T16:07:52.284040: step 2977, loss 0.00867682, acc 1\n",
      "2018-10-26T16:07:52.659992: step 2978, loss 0.0432342, acc 0.96875\n",
      "2018-10-26T16:07:53.028008: step 2979, loss 0.0400968, acc 0.984375\n",
      "2018-10-26T16:07:53.379070: step 2980, loss 0.00779206, acc 1\n",
      "2018-10-26T16:07:53.710185: step 2981, loss 0.00983662, acc 1\n",
      "2018-10-26T16:07:54.047287: step 2982, loss 0.0215146, acc 1\n",
      "2018-10-26T16:07:54.396356: step 2983, loss 0.00919234, acc 1\n",
      "2018-10-26T16:07:54.780369: step 2984, loss 0.0192456, acc 0.984375\n",
      "2018-10-26T16:07:55.154327: step 2985, loss 0.0254188, acc 0.984375\n",
      "2018-10-26T16:07:55.522343: step 2986, loss 0.0934913, acc 0.96875\n",
      "2018-10-26T16:07:55.863431: step 2987, loss 0.0146686, acc 1\n",
      "2018-10-26T16:07:56.213498: step 2988, loss 0.0276383, acc 0.984375\n",
      "2018-10-26T16:07:56.570618: step 2989, loss 0.00689992, acc 1\n",
      "2018-10-26T16:07:56.912630: step 2990, loss 0.0249337, acc 0.984375\n",
      "2018-10-26T16:07:57.267712: step 2991, loss 0.0164165, acc 1\n",
      "2018-10-26T16:07:57.602784: step 2992, loss 0.0507757, acc 0.984375\n",
      "2018-10-26T16:07:57.952850: step 2993, loss 0.0146558, acc 1\n",
      "2018-10-26T16:07:58.287954: step 2994, loss 0.00480008, acc 1\n",
      "2018-10-26T16:07:58.662952: step 2995, loss 0.00548647, acc 1\n",
      "2018-10-26T16:07:59.005041: step 2996, loss 0.0300485, acc 0.984375\n",
      "2018-10-26T16:07:59.360093: step 2997, loss 0.00401644, acc 1\n",
      "2018-10-26T16:07:59.721125: step 2998, loss 0.0243678, acc 1\n",
      "2018-10-26T16:08:00.123250: step 2999, loss 0.00906683, acc 1\n",
      "2018-10-26T16:08:00.463142: step 3000, loss 0.0209, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:08:01.271051: step 3000, loss 1.16337, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3000\n",
      "\n",
      "2018-10-26T16:08:01.923242: step 3001, loss 0.00248197, acc 1\n",
      "2018-10-26T16:08:02.316191: step 3002, loss 0.00468159, acc 1\n",
      "2018-10-26T16:08:02.836800: step 3003, loss 0.0111562, acc 1\n",
      "2018-10-26T16:08:03.188860: step 3004, loss 0.00274265, acc 1\n",
      "2018-10-26T16:08:03.531943: step 3005, loss 0.054362, acc 0.984375\n",
      "2018-10-26T16:08:03.878022: step 3006, loss 0.010961, acc 1\n",
      "2018-10-26T16:08:04.227089: step 3007, loss 0.0139731, acc 0.984375\n",
      "2018-10-26T16:08:04.579147: step 3008, loss 0.0219015, acc 1\n",
      "2018-10-26T16:08:04.934198: step 3009, loss 0.0108964, acc 1\n",
      "2018-10-26T16:08:05.305204: step 3010, loss 0.0209815, acc 0.984375\n",
      "2018-10-26T16:08:05.655273: step 3011, loss 0.0183031, acc 1\n",
      "2018-10-26T16:08:06.001397: step 3012, loss 0.00224866, acc 1\n",
      "2018-10-26T16:08:06.336453: step 3013, loss 0.00295083, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:08:06.695491: step 3014, loss 0.00268374, acc 1\n",
      "2018-10-26T16:08:07.036579: step 3015, loss 0.0196435, acc 0.984375\n",
      "2018-10-26T16:08:07.404599: step 3016, loss 0.00350546, acc 1\n",
      "2018-10-26T16:08:07.752666: step 3017, loss 0.017594, acc 1\n",
      "2018-10-26T16:08:08.106724: step 3018, loss 0.00452953, acc 1\n",
      "2018-10-26T16:08:08.480721: step 3019, loss 0.00252701, acc 1\n",
      "2018-10-26T16:08:08.822808: step 3020, loss 0.00894162, acc 1\n",
      "2018-10-26T16:08:09.164892: step 3021, loss 0.0330127, acc 0.984375\n",
      "2018-10-26T16:08:09.539893: step 3022, loss 0.0135005, acc 1\n",
      "2018-10-26T16:08:09.881977: step 3023, loss 0.00767843, acc 1\n",
      "2018-10-26T16:08:10.226057: step 3024, loss 0.00783836, acc 1\n",
      "2018-10-26T16:08:10.613443: step 3025, loss 0.0404753, acc 0.984375\n",
      "2018-10-26T16:08:10.954113: step 3026, loss 0.00596777, acc 1\n",
      "2018-10-26T16:08:11.335095: step 3027, loss 0.0079873, acc 1\n",
      "2018-10-26T16:08:11.696129: step 3028, loss 0.00724103, acc 1\n",
      "2018-10-26T16:08:12.043306: step 3029, loss 0.0608473, acc 0.96875\n",
      "2018-10-26T16:08:12.405235: step 3030, loss 0.00923228, acc 1\n",
      "2018-10-26T16:08:12.825147: step 3031, loss 0.00593534, acc 1\n",
      "2018-10-26T16:08:13.243993: step 3032, loss 0.0140406, acc 1\n",
      "2018-10-26T16:08:13.657888: step 3033, loss 0.00630477, acc 1\n",
      "2018-10-26T16:08:14.020917: step 3034, loss 0.0186196, acc 0.984375\n",
      "2018-10-26T16:08:14.437894: step 3035, loss 0.00378105, acc 1\n",
      "2018-10-26T16:08:14.789866: step 3036, loss 0.0115227, acc 1\n",
      "2018-10-26T16:08:15.178824: step 3037, loss 0.0275009, acc 0.984375\n",
      "2018-10-26T16:08:15.555855: step 3038, loss 0.0112975, acc 1\n",
      "2018-10-26T16:08:15.905024: step 3039, loss 0.00386266, acc 1\n",
      "2018-10-26T16:08:16.318778: step 3040, loss 0.00462613, acc 1\n",
      "2018-10-26T16:08:16.700757: step 3041, loss 0.0594251, acc 0.96875\n",
      "2018-10-26T16:08:17.078856: step 3042, loss 0.00568574, acc 1\n",
      "2018-10-26T16:08:17.440781: step 3043, loss 0.00370544, acc 1\n",
      "2018-10-26T16:08:17.874624: step 3044, loss 0.00248461, acc 1\n",
      "2018-10-26T16:08:18.236653: step 3045, loss 0.012623, acc 1\n",
      "2018-10-26T16:08:18.586722: step 3046, loss 0.00604729, acc 1\n",
      "2018-10-26T16:08:18.923855: step 3047, loss 0.0224751, acc 1\n",
      "2018-10-26T16:08:19.285853: step 3048, loss 0.0108964, acc 1\n",
      "2018-10-26T16:08:19.727670: step 3049, loss 0.0231725, acc 0.984375\n",
      "2018-10-26T16:08:20.120622: step 3050, loss 0.0133601, acc 1\n",
      "2018-10-26T16:08:20.495617: step 3051, loss 0.00313694, acc 1\n",
      "2018-10-26T16:08:20.879594: step 3052, loss 0.00414226, acc 1\n",
      "2018-10-26T16:08:21.305456: step 3053, loss 0.0199869, acc 1\n",
      "2018-10-26T16:08:21.851995: step 3054, loss 0.00425989, acc 1\n",
      "2018-10-26T16:08:22.298800: step 3055, loss 0.0232433, acc 1\n",
      "2018-10-26T16:08:22.801458: step 3056, loss 0.00317793, acc 1\n",
      "2018-10-26T16:08:23.281219: step 3057, loss 0.0176899, acc 1\n",
      "2018-10-26T16:08:23.733088: step 3058, loss 0.00685358, acc 1\n",
      "2018-10-26T16:08:24.163816: step 3059, loss 0.0199189, acc 0.984375\n",
      "2018-10-26T16:08:24.623589: step 3060, loss 0.0282331, acc 0.984375\n",
      "2018-10-26T16:08:25.062417: step 3061, loss 0.00929631, acc 1\n",
      "2018-10-26T16:08:25.436417: step 3062, loss 0.0051361, acc 1\n",
      "2018-10-26T16:08:25.848357: step 3063, loss 0.0220819, acc 0.984375\n",
      "2018-10-26T16:08:26.266201: step 3064, loss 0.0316637, acc 0.984375\n",
      "2018-10-26T16:08:26.645188: step 3065, loss 0.00409874, acc 1\n",
      "2018-10-26T16:08:27.002234: step 3066, loss 0.00720406, acc 1\n",
      "2018-10-26T16:08:27.329359: step 3067, loss 0.00702379, acc 1\n",
      "2018-10-26T16:08:27.683417: step 3068, loss 0.0021685, acc 1\n",
      "2018-10-26T16:08:28.068386: step 3069, loss 0.0303943, acc 0.984375\n",
      "2018-10-26T16:08:28.435406: step 3070, loss 0.00358933, acc 1\n",
      "2018-10-26T16:08:28.815388: step 3071, loss 0.0068408, acc 1\n",
      "2018-10-26T16:08:29.189389: step 3072, loss 0.012884, acc 1\n",
      "2018-10-26T16:08:29.547514: step 3073, loss 0.00837489, acc 1\n",
      "2018-10-26T16:08:29.898495: step 3074, loss 0.00169326, acc 1\n",
      "2018-10-26T16:08:30.250591: step 3075, loss 0.0213079, acc 0.984375\n",
      "2018-10-26T16:08:30.650594: step 3076, loss 0.0266988, acc 1\n",
      "2018-10-26T16:08:31.044433: step 3077, loss 0.0217549, acc 0.984375\n",
      "2018-10-26T16:08:31.462319: step 3078, loss 0.0182427, acc 1\n",
      "2018-10-26T16:08:31.844295: step 3079, loss 0.0205048, acc 1\n",
      "2018-10-26T16:08:32.178434: step 3080, loss 0.00348316, acc 1\n",
      "2018-10-26T16:08:32.557390: step 3081, loss 0.0104968, acc 1\n",
      "2018-10-26T16:08:32.908459: step 3082, loss 0.0154145, acc 1\n",
      "2018-10-26T16:08:33.360460: step 3083, loss 0.0196758, acc 1\n",
      "2018-10-26T16:08:33.742224: step 3084, loss 0.00154923, acc 1\n",
      "2018-10-26T16:08:34.083314: step 3085, loss 0.00972018, acc 1\n",
      "2018-10-26T16:08:34.455320: step 3086, loss 0.00459517, acc 1\n",
      "2018-10-26T16:08:34.779699: step 3087, loss 0.00674105, acc 1\n",
      "2018-10-26T16:08:35.181381: step 3088, loss 0.0105178, acc 1\n",
      "2018-10-26T16:08:35.539453: step 3089, loss 0.00463387, acc 1\n",
      "2018-10-26T16:08:35.893899: step 3090, loss 0.0132744, acc 1\n",
      "2018-10-26T16:08:36.279446: step 3091, loss 0.00841784, acc 1\n",
      "2018-10-26T16:08:36.636493: step 3092, loss 0.013694, acc 1\n",
      "2018-10-26T16:08:37.026451: step 3093, loss 0.00278387, acc 1\n",
      "2018-10-26T16:08:37.413419: step 3094, loss 0.00285955, acc 1\n",
      "2018-10-26T16:08:37.813352: step 3095, loss 0.00297458, acc 1\n",
      "2018-10-26T16:08:38.169509: step 3096, loss 0.00373044, acc 1\n",
      "2018-10-26T16:08:38.517465: step 3097, loss 0.0139121, acc 1\n",
      "2018-10-26T16:08:38.920392: step 3098, loss 0.00961334, acc 1\n",
      "2018-10-26T16:08:39.268461: step 3099, loss 0.0115913, acc 1\n",
      "2018-10-26T16:08:39.641565: step 3100, loss 0.0165703, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:08:40.497177: step 3100, loss 1.19285, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3100\n",
      "\n",
      "2018-10-26T16:08:41.254157: step 3101, loss 0.00300918, acc 1\n",
      "2018-10-26T16:08:41.621176: step 3102, loss 0.00462211, acc 1\n",
      "2018-10-26T16:08:42.088927: step 3103, loss 0.00696694, acc 1\n",
      "2018-10-26T16:08:42.467026: step 3104, loss 0.00506586, acc 1\n",
      "2018-10-26T16:08:42.834931: step 3105, loss 0.0267147, acc 0.984375\n",
      "2018-10-26T16:08:43.219904: step 3106, loss 0.0204823, acc 0.984375\n",
      "2018-10-26T16:08:43.610910: step 3107, loss 0.00605036, acc 1\n",
      "2018-10-26T16:08:43.966906: step 3108, loss 0.00994178, acc 1\n",
      "2018-10-26T16:08:44.316974: step 3109, loss 0.00740192, acc 1\n",
      "2018-10-26T16:08:44.703937: step 3110, loss 0.0119309, acc 1\n",
      "2018-10-26T16:08:45.068961: step 3111, loss 0.0265643, acc 0.984375\n",
      "2018-10-26T16:08:45.429000: step 3112, loss 0.0285422, acc 0.984375\n",
      "2018-10-26T16:08:45.803004: step 3113, loss 0.0370486, acc 0.984375\n",
      "2018-10-26T16:08:46.171104: step 3114, loss 0.0117243, acc 1\n",
      "2018-10-26T16:08:46.533118: step 3115, loss 0.00288721, acc 1\n",
      "2018-10-26T16:08:46.914031: step 3116, loss 0.00578762, acc 1\n",
      "2018-10-26T16:08:47.285042: step 3117, loss 0.0419259, acc 0.984375\n",
      "2018-10-26T16:08:47.648070: step 3118, loss 0.00983561, acc 1\n",
      "2018-10-26T16:08:48.000133: step 3119, loss 0.0132954, acc 1\n",
      "2018-10-26T16:08:48.370141: step 3120, loss 0.0191361, acc 0.984375\n",
      "2018-10-26T16:08:48.764091: step 3121, loss 0.0193221, acc 1\n",
      "2018-10-26T16:08:49.155044: step 3122, loss 0.00515323, acc 1\n",
      "2018-10-26T16:08:49.550987: step 3123, loss 0.00317045, acc 1\n",
      "2018-10-26T16:08:49.915018: step 3124, loss 0.00244678, acc 1\n",
      "2018-10-26T16:08:50.286027: step 3125, loss 0.0127906, acc 1\n",
      "2018-10-26T16:08:50.769021: step 3126, loss 0.0422297, acc 0.984375\n",
      "2018-10-26T16:08:51.130769: step 3127, loss 0.00637575, acc 1\n",
      "2018-10-26T16:08:51.512749: step 3128, loss 0.0132915, acc 1\n",
      "2018-10-26T16:08:51.965537: step 3129, loss 0.0401208, acc 0.984375\n",
      "2018-10-26T16:08:52.303632: step 3130, loss 0.0225138, acc 0.984375\n",
      "2018-10-26T16:08:52.737472: step 3131, loss 0.00287481, acc 1\n",
      "2018-10-26T16:08:53.155492: step 3132, loss 0.0970562, acc 0.96875\n",
      "2018-10-26T16:08:53.555293: step 3133, loss 0.0181765, acc 1\n",
      "2018-10-26T16:08:53.923375: step 3134, loss 0.00621947, acc 1\n",
      "2018-10-26T16:08:54.319247: step 3135, loss 0.00553185, acc 1\n",
      "2018-10-26T16:08:54.683274: step 3136, loss 0.0152566, acc 1\n",
      "2018-10-26T16:08:55.034336: step 3137, loss 0.00955549, acc 1\n",
      "2018-10-26T16:08:55.475159: step 3138, loss 0.0124169, acc 1\n",
      "2018-10-26T16:08:55.836196: step 3139, loss 0.00806632, acc 1\n",
      "2018-10-26T16:08:56.270036: step 3140, loss 0.0516075, acc 0.984375\n",
      "2018-10-26T16:08:56.630071: step 3141, loss 0.0193145, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:08:57.016041: step 3142, loss 0.00662966, acc 1\n",
      "2018-10-26T16:08:57.368134: step 3143, loss 0.0156546, acc 1\n",
      "2018-10-26T16:08:57.742103: step 3144, loss 0.0177903, acc 1\n",
      "2018-10-26T16:08:58.152306: step 3145, loss 0.0165538, acc 1\n",
      "2018-10-26T16:08:58.500077: step 3146, loss 0.00577952, acc 1\n",
      "2018-10-26T16:08:58.881062: step 3147, loss 0.00594152, acc 1\n",
      "2018-10-26T16:08:59.227334: step 3148, loss 0.00779044, acc 1\n",
      "2018-10-26T16:08:59.588264: step 3149, loss 0.00663459, acc 1\n",
      "2018-10-26T16:08:59.949208: step 3150, loss 0.0277252, acc 0.983333\n",
      "2018-10-26T16:09:00.343153: step 3151, loss 0.0114603, acc 1\n",
      "2018-10-26T16:09:00.718153: step 3152, loss 0.0141493, acc 1\n",
      "2018-10-26T16:09:01.081223: step 3153, loss 0.00315801, acc 1\n",
      "2018-10-26T16:09:01.401323: step 3154, loss 0.00144868, acc 1\n",
      "2018-10-26T16:09:01.782339: step 3155, loss 0.00620807, acc 1\n",
      "2018-10-26T16:09:02.129379: step 3156, loss 0.0121181, acc 1\n",
      "2018-10-26T16:09:02.521450: step 3157, loss 0.0118723, acc 1\n",
      "2018-10-26T16:09:02.901318: step 3158, loss 0.0276163, acc 0.984375\n",
      "2018-10-26T16:09:03.260362: step 3159, loss 0.00574109, acc 1\n",
      "2018-10-26T16:09:03.666277: step 3160, loss 0.00994276, acc 1\n",
      "2018-10-26T16:09:04.076180: step 3161, loss 0.0268915, acc 1\n",
      "2018-10-26T16:09:04.473116: step 3162, loss 0.0265299, acc 0.984375\n",
      "2018-10-26T16:09:04.826173: step 3163, loss 0.0159749, acc 1\n",
      "2018-10-26T16:09:05.181302: step 3164, loss 0.00299724, acc 1\n",
      "2018-10-26T16:09:05.583151: step 3165, loss 0.0626242, acc 0.984375\n",
      "2018-10-26T16:09:05.956154: step 3166, loss 0.00775919, acc 1\n",
      "2018-10-26T16:09:06.300234: step 3167, loss 0.00310034, acc 1\n",
      "2018-10-26T16:09:06.656286: step 3168, loss 0.00695734, acc 1\n",
      "2018-10-26T16:09:07.052259: step 3169, loss 0.00453326, acc 1\n",
      "2018-10-26T16:09:07.405282: step 3170, loss 0.004362, acc 1\n",
      "2018-10-26T16:09:07.764366: step 3171, loss 0.00215689, acc 1\n",
      "2018-10-26T16:09:08.095848: step 3172, loss 0.00439353, acc 1\n",
      "2018-10-26T16:09:08.507338: step 3173, loss 0.00483358, acc 1\n",
      "2018-10-26T16:09:08.914302: step 3174, loss 0.00241064, acc 1\n",
      "2018-10-26T16:09:09.286256: step 3175, loss 0.00287849, acc 1\n",
      "2018-10-26T16:09:09.653278: step 3176, loss 0.00347039, acc 1\n",
      "2018-10-26T16:09:10.008329: step 3177, loss 0.00712289, acc 1\n",
      "2018-10-26T16:09:10.348437: step 3178, loss 0.00367155, acc 1\n",
      "2018-10-26T16:09:10.637289: step 3179, loss 0.021535, acc 1\n",
      "2018-10-26T16:09:10.960784: step 3180, loss 0.00432387, acc 1\n",
      "2018-10-26T16:09:11.300875: step 3181, loss 0.00484252, acc 1\n",
      "2018-10-26T16:09:11.645952: step 3182, loss 0.0123011, acc 1\n",
      "2018-10-26T16:09:11.982053: step 3183, loss 0.0104926, acc 1\n",
      "2018-10-26T16:09:12.323144: step 3184, loss 0.0037495, acc 1\n",
      "2018-10-26T16:09:12.655254: step 3185, loss 0.00809014, acc 1\n",
      "2018-10-26T16:09:13.005422: step 3186, loss 0.00974989, acc 1\n",
      "2018-10-26T16:09:13.341473: step 3187, loss 0.00776585, acc 1\n",
      "2018-10-26T16:09:13.693482: step 3188, loss 0.00481328, acc 1\n",
      "2018-10-26T16:09:14.032575: step 3189, loss 0.00895819, acc 1\n",
      "2018-10-26T16:09:14.401700: step 3190, loss 0.00945324, acc 1\n",
      "2018-10-26T16:09:14.737337: step 3191, loss 0.0132148, acc 1\n",
      "2018-10-26T16:09:15.096802: step 3192, loss 0.00992674, acc 1\n",
      "2018-10-26T16:09:15.466743: step 3193, loss 0.00615789, acc 1\n",
      "2018-10-26T16:09:15.841744: step 3194, loss 0.00864835, acc 1\n",
      "2018-10-26T16:09:16.204773: step 3195, loss 0.0098975, acc 1\n",
      "2018-10-26T16:09:16.560851: step 3196, loss 0.00371535, acc 1\n",
      "2018-10-26T16:09:16.916869: step 3197, loss 0.00968708, acc 1\n",
      "2018-10-26T16:09:17.277968: step 3198, loss 0.0143234, acc 1\n",
      "2018-10-26T16:09:17.639942: step 3199, loss 0.00244121, acc 1\n",
      "2018-10-26T16:09:18.001970: step 3200, loss 0.00290188, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:09:18.871716: step 3200, loss 1.23117, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3200\n",
      "\n",
      "2018-10-26T16:09:19.513935: step 3201, loss 0.00845101, acc 1\n",
      "2018-10-26T16:09:19.970748: step 3202, loss 0.00249637, acc 1\n",
      "2018-10-26T16:09:20.439458: step 3203, loss 0.00508707, acc 1\n",
      "2018-10-26T16:09:20.837393: step 3204, loss 0.00442129, acc 1\n",
      "2018-10-26T16:09:21.217378: step 3205, loss 0.00390769, acc 1\n",
      "2018-10-26T16:09:21.606342: step 3206, loss 0.00228417, acc 1\n",
      "2018-10-26T16:09:22.026224: step 3207, loss 0.0016103, acc 1\n",
      "2018-10-26T16:09:22.405208: step 3208, loss 0.0453983, acc 0.984375\n",
      "2018-10-26T16:09:22.760256: step 3209, loss 0.0368333, acc 0.984375\n",
      "2018-10-26T16:09:23.167284: step 3210, loss 0.00788258, acc 1\n",
      "2018-10-26T16:09:23.519229: step 3211, loss 0.00337274, acc 1\n",
      "2018-10-26T16:09:23.901293: step 3212, loss 0.0190431, acc 0.984375\n",
      "2018-10-26T16:09:24.266652: step 3213, loss 0.0136041, acc 1\n",
      "2018-10-26T16:09:24.638301: step 3214, loss 0.00633573, acc 1\n",
      "2018-10-26T16:09:25.107984: step 3215, loss 0.00770411, acc 1\n",
      "2018-10-26T16:09:25.478995: step 3216, loss 0.00537275, acc 1\n",
      "2018-10-26T16:09:25.891008: step 3217, loss 0.0246772, acc 0.984375\n",
      "2018-10-26T16:09:26.304785: step 3218, loss 0.0381512, acc 0.96875\n",
      "2018-10-26T16:09:26.644878: step 3219, loss 0.0180318, acc 0.984375\n",
      "2018-10-26T16:09:27.039822: step 3220, loss 0.00450599, acc 1\n",
      "2018-10-26T16:09:27.534512: step 3221, loss 0.0105527, acc 1\n",
      "2018-10-26T16:09:28.041189: step 3222, loss 0.0035623, acc 1\n",
      "2018-10-26T16:09:28.570402: step 3223, loss 0.0090189, acc 1\n",
      "2018-10-26T16:09:29.082365: step 3224, loss 0.00257921, acc 1\n",
      "2018-10-26T16:09:29.525181: step 3225, loss 0.00468394, acc 1\n",
      "2018-10-26T16:09:29.971076: step 3226, loss 0.00287163, acc 1\n",
      "2018-10-26T16:09:30.330367: step 3227, loss 0.0044761, acc 1\n",
      "2018-10-26T16:09:30.741930: step 3228, loss 0.0114534, acc 1\n",
      "2018-10-26T16:09:31.182753: step 3229, loss 0.00487992, acc 1\n",
      "2018-10-26T16:09:31.582684: step 3230, loss 0.00211933, acc 1\n",
      "2018-10-26T16:09:32.055475: step 3231, loss 0.0262156, acc 0.984375\n",
      "2018-10-26T16:09:32.428458: step 3232, loss 0.00300472, acc 1\n",
      "2018-10-26T16:09:32.880259: step 3233, loss 0.027893, acc 0.984375\n",
      "2018-10-26T16:09:33.264193: step 3234, loss 0.0262829, acc 1\n",
      "2018-10-26T16:09:33.617251: step 3235, loss 0.00164631, acc 1\n",
      "2018-10-26T16:09:33.975295: step 3236, loss 0.00781797, acc 1\n",
      "2018-10-26T16:09:34.349291: step 3237, loss 0.00282483, acc 1\n",
      "2018-10-26T16:09:34.746232: step 3238, loss 0.0329239, acc 0.984375\n",
      "2018-10-26T16:09:35.112254: step 3239, loss 0.00812908, acc 1\n",
      "2018-10-26T16:09:35.518171: step 3240, loss 0.00452003, acc 1\n",
      "2018-10-26T16:09:35.875259: step 3241, loss 0.00532752, acc 1\n",
      "2018-10-26T16:09:36.250307: step 3242, loss 0.04168, acc 0.96875\n",
      "2018-10-26T16:09:36.600277: step 3243, loss 0.00783086, acc 1\n",
      "2018-10-26T16:09:36.962348: step 3244, loss 0.003629, acc 1\n",
      "2018-10-26T16:09:37.315366: step 3245, loss 0.0112245, acc 1\n",
      "2018-10-26T16:09:37.692416: step 3246, loss 0.00694359, acc 1\n",
      "2018-10-26T16:09:38.050431: step 3247, loss 0.0141983, acc 1\n",
      "2018-10-26T16:09:38.402462: step 3248, loss 0.00201346, acc 1\n",
      "2018-10-26T16:09:38.752527: step 3249, loss 0.00125061, acc 1\n",
      "2018-10-26T16:09:39.117555: step 3250, loss 0.00434808, acc 1\n",
      "2018-10-26T16:09:39.450689: step 3251, loss 0.0174094, acc 1\n",
      "2018-10-26T16:09:39.846604: step 3252, loss 0.00125668, acc 1\n",
      "2018-10-26T16:09:40.212629: step 3253, loss 0.00235409, acc 1\n",
      "2018-10-26T16:09:40.588625: step 3254, loss 0.00304675, acc 1\n",
      "2018-10-26T16:09:40.941681: step 3255, loss 0.00186549, acc 1\n",
      "2018-10-26T16:09:41.324656: step 3256, loss 0.00408699, acc 1\n",
      "2018-10-26T16:09:41.671777: step 3257, loss 0.0128726, acc 1\n",
      "2018-10-26T16:09:42.017833: step 3258, loss 0.0528306, acc 0.96875\n",
      "2018-10-26T16:09:42.389854: step 3259, loss 0.00759786, acc 1\n",
      "2018-10-26T16:09:42.721923: step 3260, loss 0.0287687, acc 0.984375\n",
      "2018-10-26T16:09:43.042067: step 3261, loss 0.00593773, acc 1\n",
      "2018-10-26T16:09:43.425044: step 3262, loss 0.00873661, acc 1\n",
      "2018-10-26T16:09:43.758156: step 3263, loss 0.012808, acc 1\n",
      "2018-10-26T16:09:44.103235: step 3264, loss 0.00252305, acc 1\n",
      "2018-10-26T16:09:44.481266: step 3265, loss 0.0181453, acc 1\n",
      "2018-10-26T16:09:44.863201: step 3266, loss 0.00899478, acc 1\n",
      "2018-10-26T16:09:45.218253: step 3267, loss 0.0309517, acc 0.984375\n",
      "2018-10-26T16:09:45.598236: step 3268, loss 0.0098224, acc 1\n",
      "2018-10-26T16:09:45.960272: step 3269, loss 0.00515021, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:09:46.312494: step 3270, loss 0.0245569, acc 0.984375\n",
      "2018-10-26T16:09:46.669411: step 3271, loss 0.0231693, acc 0.984375\n",
      "2018-10-26T16:09:47.063323: step 3272, loss 0.0238128, acc 0.984375\n",
      "2018-10-26T16:09:47.395435: step 3273, loss 0.001315, acc 1\n",
      "2018-10-26T16:09:47.777415: step 3274, loss 0.0516238, acc 0.984375\n",
      "2018-10-26T16:09:48.125486: step 3275, loss 0.052713, acc 0.96875\n",
      "2018-10-26T16:09:48.457598: step 3276, loss 0.0199746, acc 0.984375\n",
      "2018-10-26T16:09:48.799738: step 3277, loss 0.00482122, acc 1\n",
      "2018-10-26T16:09:49.158771: step 3278, loss 0.00251492, acc 1\n",
      "2018-10-26T16:09:49.504799: step 3279, loss 0.0130614, acc 1\n",
      "2018-10-26T16:09:49.897749: step 3280, loss 0.0595737, acc 0.984375\n",
      "2018-10-26T16:09:50.268759: step 3281, loss 0.0281483, acc 0.96875\n",
      "2018-10-26T16:09:50.646751: step 3282, loss 0.00757332, acc 1\n",
      "2018-10-26T16:09:50.974983: step 3283, loss 0.0113805, acc 1\n",
      "2018-10-26T16:09:51.323938: step 3284, loss 0.00598569, acc 1\n",
      "2018-10-26T16:09:51.656090: step 3285, loss 0.0953456, acc 0.96875\n",
      "2018-10-26T16:09:52.026062: step 3286, loss 0.00292291, acc 1\n",
      "2018-10-26T16:09:52.388096: step 3287, loss 0.00574195, acc 1\n",
      "2018-10-26T16:09:52.724285: step 3288, loss 0.00555083, acc 1\n",
      "2018-10-26T16:09:53.076277: step 3289, loss 0.0224958, acc 0.984375\n",
      "2018-10-26T16:09:53.470204: step 3290, loss 0.0110229, acc 1\n",
      "2018-10-26T16:09:53.841214: step 3291, loss 0.0284944, acc 0.984375\n",
      "2018-10-26T16:09:54.211285: step 3292, loss 0.0095288, acc 1\n",
      "2018-10-26T16:09:54.553310: step 3293, loss 0.0070524, acc 1\n",
      "2018-10-26T16:09:54.915345: step 3294, loss 0.0035476, acc 1\n",
      "2018-10-26T16:09:55.289380: step 3295, loss 0.00639066, acc 1\n",
      "2018-10-26T16:09:55.682292: step 3296, loss 0.00373251, acc 1\n",
      "2018-10-26T16:09:56.050314: step 3297, loss 0.0143584, acc 1\n",
      "2018-10-26T16:09:56.444258: step 3298, loss 0.0012257, acc 1\n",
      "2018-10-26T16:09:56.784350: step 3299, loss 0.00929751, acc 1\n",
      "2018-10-26T16:09:57.124440: step 3300, loss 0.0105697, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:09:57.955221: step 3300, loss 1.269, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3300\n",
      "\n",
      "2018-10-26T16:09:58.736136: step 3301, loss 0.00487178, acc 1\n",
      "2018-10-26T16:09:59.145042: step 3302, loss 0.00397233, acc 1\n",
      "2018-10-26T16:09:59.578885: step 3303, loss 0.00324114, acc 1\n",
      "2018-10-26T16:09:59.953882: step 3304, loss 0.00850186, acc 1\n",
      "2018-10-26T16:10:00.324893: step 3305, loss 0.0177045, acc 1\n",
      "2018-10-26T16:10:00.660992: step 3306, loss 0.00673841, acc 1\n",
      "2018-10-26T16:10:01.013097: step 3307, loss 0.00731905, acc 1\n",
      "2018-10-26T16:10:01.375084: step 3308, loss 0.00499295, acc 1\n",
      "2018-10-26T16:10:01.716176: step 3309, loss 0.00323, acc 1\n",
      "2018-10-26T16:10:02.083195: step 3310, loss 0.00435859, acc 1\n",
      "2018-10-26T16:10:02.427273: step 3311, loss 0.0263041, acc 0.984375\n",
      "2018-10-26T16:10:02.784319: step 3312, loss 0.00826825, acc 1\n",
      "2018-10-26T16:10:03.174327: step 3313, loss 0.0071329, acc 1\n",
      "2018-10-26T16:10:03.516363: step 3314, loss 0.00399227, acc 1\n",
      "2018-10-26T16:10:03.847481: step 3315, loss 0.00981439, acc 1\n",
      "2018-10-26T16:10:04.209514: step 3316, loss 0.00173762, acc 1\n",
      "2018-10-26T16:10:04.559578: step 3317, loss 0.011337, acc 1\n",
      "2018-10-26T16:10:04.903655: step 3318, loss 0.00783934, acc 1\n",
      "2018-10-26T16:10:05.248733: step 3319, loss 0.00316441, acc 1\n",
      "2018-10-26T16:10:05.571940: step 3320, loss 0.0162256, acc 1\n",
      "2018-10-26T16:10:05.900994: step 3321, loss 0.0107034, acc 1\n",
      "2018-10-26T16:10:06.246068: step 3322, loss 0.00180095, acc 1\n",
      "2018-10-26T16:10:06.601120: step 3323, loss 0.000903709, acc 1\n",
      "2018-10-26T16:10:06.941213: step 3324, loss 0.00246675, acc 1\n",
      "2018-10-26T16:10:07.268341: step 3325, loss 0.00121945, acc 1\n",
      "2018-10-26T16:10:07.598496: step 3326, loss 0.00280682, acc 1\n",
      "2018-10-26T16:10:07.933561: step 3327, loss 0.00458725, acc 1\n",
      "2018-10-26T16:10:08.296589: step 3328, loss 0.00306414, acc 1\n",
      "2018-10-26T16:10:08.675578: step 3329, loss 0.00317971, acc 1\n",
      "2018-10-26T16:10:09.031629: step 3330, loss 0.00544081, acc 1\n",
      "2018-10-26T16:10:09.388674: step 3331, loss 0.00176132, acc 1\n",
      "2018-10-26T16:10:09.741777: step 3332, loss 0.00926582, acc 1\n",
      "2018-10-26T16:10:10.087803: step 3333, loss 0.00197881, acc 1\n",
      "2018-10-26T16:10:10.455822: step 3334, loss 0.00344897, acc 1\n",
      "2018-10-26T16:10:10.814862: step 3335, loss 0.0192224, acc 1\n",
      "2018-10-26T16:10:11.183876: step 3336, loss 0.00394654, acc 1\n",
      "2018-10-26T16:10:11.577823: step 3337, loss 0.0145808, acc 1\n",
      "2018-10-26T16:10:11.922904: step 3338, loss 0.0101607, acc 1\n",
      "2018-10-26T16:10:12.273077: step 3339, loss 0.00869837, acc 1\n",
      "2018-10-26T16:10:12.654948: step 3340, loss 0.0560147, acc 0.96875\n",
      "2018-10-26T16:10:13.018993: step 3341, loss 0.00238104, acc 1\n",
      "2018-10-26T16:10:13.379015: step 3342, loss 0.00182305, acc 1\n",
      "2018-10-26T16:10:13.745036: step 3343, loss 0.002358, acc 1\n",
      "2018-10-26T16:10:14.115044: step 3344, loss 0.0131762, acc 0.984375\n",
      "2018-10-26T16:10:14.461193: step 3345, loss 0.00593257, acc 1\n",
      "2018-10-26T16:10:14.826144: step 3346, loss 0.00244349, acc 1\n",
      "2018-10-26T16:10:15.185187: step 3347, loss 0.0235341, acc 0.984375\n",
      "2018-10-26T16:10:15.535347: step 3348, loss 0.0027208, acc 1\n",
      "2018-10-26T16:10:15.897478: step 3349, loss 0.0166417, acc 1\n",
      "2018-10-26T16:10:16.273280: step 3350, loss 0.0079821, acc 1\n",
      "2018-10-26T16:10:16.653389: step 3351, loss 0.00131035, acc 1\n",
      "2018-10-26T16:10:17.012303: step 3352, loss 0.0126051, acc 1\n",
      "2018-10-26T16:10:17.388353: step 3353, loss 0.00199503, acc 1\n",
      "2018-10-26T16:10:17.769280: step 3354, loss 0.00285636, acc 1\n",
      "2018-10-26T16:10:18.118347: step 3355, loss 0.00938685, acc 1\n",
      "2018-10-26T16:10:18.518279: step 3356, loss 0.0151237, acc 0.984375\n",
      "2018-10-26T16:10:18.861412: step 3357, loss 0.0502426, acc 0.984375\n",
      "2018-10-26T16:10:19.222399: step 3358, loss 0.0175053, acc 1\n",
      "2018-10-26T16:10:19.588459: step 3359, loss 0.00240914, acc 1\n",
      "2018-10-26T16:10:19.986356: step 3360, loss 0.00225436, acc 1\n",
      "2018-10-26T16:10:20.340414: step 3361, loss 0.00383553, acc 1\n",
      "2018-10-26T16:10:20.683495: step 3362, loss 0.00157766, acc 1\n",
      "2018-10-26T16:10:21.055501: step 3363, loss 0.00296245, acc 1\n",
      "2018-10-26T16:10:21.414540: step 3364, loss 0.00859219, acc 1\n",
      "2018-10-26T16:10:21.772679: step 3365, loss 0.00295564, acc 1\n",
      "2018-10-26T16:10:22.158554: step 3366, loss 0.00307362, acc 1\n",
      "2018-10-26T16:10:22.522580: step 3367, loss 0.00637655, acc 1\n",
      "2018-10-26T16:10:22.874639: step 3368, loss 0.00803796, acc 1\n",
      "2018-10-26T16:10:23.244651: step 3369, loss 0.0255944, acc 0.984375\n",
      "2018-10-26T16:10:23.603738: step 3370, loss 0.00300699, acc 1\n",
      "2018-10-26T16:10:23.946776: step 3371, loss 0.0541792, acc 0.984375\n",
      "2018-10-26T16:10:24.313798: step 3372, loss 0.00618285, acc 1\n",
      "2018-10-26T16:10:24.670874: step 3373, loss 0.0193946, acc 0.984375\n",
      "2018-10-26T16:10:25.093741: step 3374, loss 0.00218439, acc 1\n",
      "2018-10-26T16:10:25.492646: step 3375, loss 0.00116994, acc 1\n",
      "2018-10-26T16:10:25.839792: step 3376, loss 0.00814632, acc 1\n",
      "2018-10-26T16:10:26.195769: step 3377, loss 0.0323136, acc 0.984375\n",
      "2018-10-26T16:10:26.558799: step 3378, loss 0.00283885, acc 1\n",
      "2018-10-26T16:10:26.941819: step 3379, loss 0.00767297, acc 1\n",
      "2018-10-26T16:10:27.281906: step 3380, loss 0.0159861, acc 1\n",
      "2018-10-26T16:10:27.625946: step 3381, loss 0.0205582, acc 0.984375\n",
      "2018-10-26T16:10:28.030863: step 3382, loss 0.00272811, acc 1\n",
      "2018-10-26T16:10:28.373947: step 3383, loss 0.00200008, acc 1\n",
      "2018-10-26T16:10:28.725009: step 3384, loss 0.0231832, acc 0.984375\n",
      "2018-10-26T16:10:29.109008: step 3385, loss 0.00301419, acc 1\n",
      "2018-10-26T16:10:29.459050: step 3386, loss 0.00623506, acc 1\n",
      "2018-10-26T16:10:29.814103: step 3387, loss 0.0294521, acc 0.984375\n",
      "2018-10-26T16:10:30.196094: step 3388, loss 0.0029323, acc 1\n",
      "2018-10-26T16:10:30.646874: step 3389, loss 0.00110936, acc 1\n",
      "2018-10-26T16:10:31.037876: step 3390, loss 0.00503763, acc 1\n",
      "2018-10-26T16:10:31.427822: step 3391, loss 0.00672093, acc 1\n",
      "2018-10-26T16:10:31.880576: step 3392, loss 0.00374993, acc 1\n",
      "2018-10-26T16:10:32.257571: step 3393, loss 0.0198439, acc 0.984375\n",
      "2018-10-26T16:10:32.623592: step 3394, loss 0.0102369, acc 1\n",
      "2018-10-26T16:10:33.024614: step 3395, loss 0.0247306, acc 0.984375\n",
      "2018-10-26T16:10:33.540144: step 3396, loss 0.0377922, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:10:34.027853: step 3397, loss 0.0043603, acc 1\n",
      "2018-10-26T16:10:34.496587: step 3398, loss 0.00435597, acc 1\n",
      "2018-10-26T16:10:34.928434: step 3399, loss 0.00345011, acc 1\n",
      "2018-10-26T16:10:35.402169: step 3400, loss 0.0161026, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:10:36.349741: step 3400, loss 1.28419, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3400\n",
      "\n",
      "2018-10-26T16:10:37.164459: step 3401, loss 0.0420264, acc 0.984375\n",
      "2018-10-26T16:10:37.647171: step 3402, loss 0.0197082, acc 1\n",
      "2018-10-26T16:10:38.166849: step 3403, loss 0.0058475, acc 1\n",
      "2018-10-26T16:10:38.514853: step 3404, loss 0.00621683, acc 1\n",
      "2018-10-26T16:10:38.879875: step 3405, loss 0.00706149, acc 1\n",
      "2018-10-26T16:10:39.238918: step 3406, loss 0.00806265, acc 1\n",
      "2018-10-26T16:10:39.601016: step 3407, loss 0.00225006, acc 1\n",
      "2018-10-26T16:10:39.975947: step 3408, loss 0.00959339, acc 1\n",
      "2018-10-26T16:10:40.334998: step 3409, loss 0.00330499, acc 1\n",
      "2018-10-26T16:10:40.690102: step 3410, loss 0.00422344, acc 1\n",
      "2018-10-26T16:10:41.049082: step 3411, loss 0.016614, acc 1\n",
      "2018-10-26T16:10:41.390168: step 3412, loss 0.00200714, acc 1\n",
      "2018-10-26T16:10:41.739240: step 3413, loss 0.0138394, acc 1\n",
      "2018-10-26T16:10:42.100271: step 3414, loss 0.0678954, acc 0.984375\n",
      "2018-10-26T16:10:42.449340: step 3415, loss 0.00338648, acc 1\n",
      "2018-10-26T16:10:42.797411: step 3416, loss 0.0104909, acc 1\n",
      "2018-10-26T16:10:43.155452: step 3417, loss 0.00592192, acc 1\n",
      "2018-10-26T16:10:43.506513: step 3418, loss 0.0178119, acc 0.984375\n",
      "2018-10-26T16:10:43.873536: step 3419, loss 0.0125551, acc 1\n",
      "2018-10-26T16:10:44.233575: step 3420, loss 0.00466265, acc 1\n",
      "2018-10-26T16:10:44.574660: step 3421, loss 0.00208165, acc 1\n",
      "2018-10-26T16:10:44.945669: step 3422, loss 0.0122799, acc 1\n",
      "2018-10-26T16:10:45.299774: step 3423, loss 0.00601259, acc 1\n",
      "2018-10-26T16:10:45.654777: step 3424, loss 0.0142254, acc 1\n",
      "2018-10-26T16:10:46.025786: step 3425, loss 0.00399772, acc 1\n",
      "2018-10-26T16:10:46.411962: step 3426, loss 0.0029921, acc 1\n",
      "2018-10-26T16:10:46.755385: step 3427, loss 0.019367, acc 0.984375\n",
      "2018-10-26T16:10:47.105897: step 3428, loss 0.0280199, acc 0.984375\n",
      "2018-10-26T16:10:47.429064: step 3429, loss 0.00407071, acc 1\n",
      "2018-10-26T16:10:47.766137: step 3430, loss 0.0143031, acc 0.984375\n",
      "2018-10-26T16:10:48.128201: step 3431, loss 0.00375129, acc 1\n",
      "2018-10-26T16:10:48.528099: step 3432, loss 0.0100093, acc 1\n",
      "2018-10-26T16:10:48.874176: step 3433, loss 0.00147093, acc 1\n",
      "2018-10-26T16:10:49.212306: step 3434, loss 0.00608192, acc 1\n",
      "2018-10-26T16:10:49.563334: step 3435, loss 0.00850036, acc 1\n",
      "2018-10-26T16:10:49.905460: step 3436, loss 0.00852047, acc 1\n",
      "2018-10-26T16:10:50.257481: step 3437, loss 0.00351908, acc 1\n",
      "2018-10-26T16:10:50.647434: step 3438, loss 0.00345526, acc 1\n",
      "2018-10-26T16:10:51.021466: step 3439, loss 0.00189108, acc 1\n",
      "2018-10-26T16:10:51.375488: step 3440, loss 0.0553284, acc 0.96875\n",
      "2018-10-26T16:10:51.764449: step 3441, loss 0.022589, acc 0.984375\n",
      "2018-10-26T16:10:52.117510: step 3442, loss 0.0627607, acc 0.984375\n",
      "2018-10-26T16:10:52.473825: step 3443, loss 0.00324159, acc 1\n",
      "2018-10-26T16:10:52.862518: step 3444, loss 0.00138213, acc 1\n",
      "2018-10-26T16:10:53.344332: step 3445, loss 0.0103577, acc 1\n",
      "2018-10-26T16:10:53.716236: step 3446, loss 0.00826286, acc 1\n",
      "2018-10-26T16:10:54.173015: step 3447, loss 0.00502509, acc 1\n",
      "2018-10-26T16:10:54.649741: step 3448, loss 0.0148417, acc 1\n",
      "2018-10-26T16:10:54.975869: step 3449, loss 0.0244969, acc 0.984375\n",
      "2018-10-26T16:10:55.327930: step 3450, loss 0.00329161, acc 1\n",
      "2018-10-26T16:10:55.720883: step 3451, loss 0.0237066, acc 1\n",
      "2018-10-26T16:10:56.052993: step 3452, loss 0.00296633, acc 1\n",
      "2018-10-26T16:10:56.401062: step 3453, loss 0.00439707, acc 1\n",
      "2018-10-26T16:10:56.793014: step 3454, loss 0.00260897, acc 1\n",
      "2018-10-26T16:10:57.145073: step 3455, loss 0.0196195, acc 0.984375\n",
      "2018-10-26T16:10:57.523063: step 3456, loss 0.00184364, acc 1\n",
      "2018-10-26T16:10:58.012756: step 3457, loss 0.00757355, acc 1\n",
      "2018-10-26T16:10:58.429642: step 3458, loss 0.00102029, acc 1\n",
      "2018-10-26T16:10:58.795869: step 3459, loss 0.0060065, acc 1\n",
      "2018-10-26T16:10:59.298322: step 3460, loss 0.0064289, acc 1\n",
      "2018-10-26T16:10:59.695264: step 3461, loss 0.00403334, acc 1\n",
      "2018-10-26T16:11:00.067267: step 3462, loss 0.00696592, acc 1\n",
      "2018-10-26T16:11:00.553969: step 3463, loss 0.00255843, acc 1\n",
      "2018-10-26T16:11:00.935949: step 3464, loss 0.0101196, acc 1\n",
      "2018-10-26T16:11:01.276063: step 3465, loss 0.00175996, acc 1\n",
      "2018-10-26T16:11:01.665000: step 3466, loss 0.00112262, acc 1\n",
      "2018-10-26T16:11:02.025096: step 3467, loss 0.00299936, acc 1\n",
      "2018-10-26T16:11:02.361140: step 3468, loss 0.00799472, acc 1\n",
      "2018-10-26T16:11:02.708211: step 3469, loss 0.0240146, acc 0.984375\n",
      "2018-10-26T16:11:03.093204: step 3470, loss 0.00171583, acc 1\n",
      "2018-10-26T16:11:03.403353: step 3471, loss 0.0165114, acc 0.984375\n",
      "2018-10-26T16:11:03.714521: step 3472, loss 0.0123949, acc 1\n",
      "2018-10-26T16:11:04.037660: step 3473, loss 0.00285623, acc 1\n",
      "2018-10-26T16:11:04.369772: step 3474, loss 0.00381063, acc 1\n",
      "2018-10-26T16:11:04.716844: step 3475, loss 0.00281513, acc 1\n",
      "2018-10-26T16:11:05.042972: step 3476, loss 0.00174937, acc 1\n",
      "2018-10-26T16:11:05.351191: step 3477, loss 0.00894288, acc 1\n",
      "2018-10-26T16:11:05.666307: step 3478, loss 0.0269168, acc 0.984375\n",
      "2018-10-26T16:11:05.987452: step 3479, loss 0.00613773, acc 1\n",
      "2018-10-26T16:11:06.310689: step 3480, loss 0.00412198, acc 1\n",
      "2018-10-26T16:11:06.637929: step 3481, loss 0.0160039, acc 1\n",
      "2018-10-26T16:11:06.953869: step 3482, loss 0.0100283, acc 1\n",
      "2018-10-26T16:11:07.281036: step 3483, loss 0.00375958, acc 1\n",
      "2018-10-26T16:11:07.653002: step 3484, loss 0.0124659, acc 1\n",
      "2018-10-26T16:11:08.016029: step 3485, loss 0.00182745, acc 1\n",
      "2018-10-26T16:11:08.344152: step 3486, loss 0.00827565, acc 1\n",
      "2018-10-26T16:11:08.677322: step 3487, loss 0.00482033, acc 1\n",
      "2018-10-26T16:11:08.990517: step 3488, loss 0.00759939, acc 1\n",
      "2018-10-26T16:11:09.290624: step 3489, loss 0.0171871, acc 0.984375\n",
      "2018-10-26T16:11:09.616751: step 3490, loss 0.0110318, acc 1\n",
      "2018-10-26T16:11:09.943878: step 3491, loss 0.00335901, acc 1\n",
      "2018-10-26T16:11:10.263025: step 3492, loss 0.0928887, acc 0.984375\n",
      "2018-10-26T16:11:10.575192: step 3493, loss 0.00847554, acc 1\n",
      "2018-10-26T16:11:10.893424: step 3494, loss 0.00303577, acc 1\n",
      "2018-10-26T16:11:11.205510: step 3495, loss 0.0136029, acc 1\n",
      "2018-10-26T16:11:11.539689: step 3496, loss 0.0123546, acc 1\n",
      "2018-10-26T16:11:11.854772: step 3497, loss 0.003853, acc 1\n",
      "2018-10-26T16:11:12.159958: step 3498, loss 0.00334335, acc 1\n",
      "2018-10-26T16:11:12.481144: step 3499, loss 0.0016068, acc 1\n",
      "2018-10-26T16:11:12.801310: step 3500, loss 0.00331673, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:11:13.575176: step 3500, loss 1.32583, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3500\n",
      "\n",
      "2018-10-26T16:11:14.230604: step 3501, loss 0.000850488, acc 1\n",
      "2018-10-26T16:11:14.559545: step 3502, loss 0.0018098, acc 1\n",
      "2018-10-26T16:11:15.037302: step 3503, loss 0.0204212, acc 0.984375\n",
      "2018-10-26T16:11:15.400298: step 3504, loss 0.0111088, acc 1\n",
      "2018-10-26T16:11:15.718450: step 3505, loss 0.00180106, acc 1\n",
      "2018-10-26T16:11:16.040592: step 3506, loss 0.0288062, acc 0.984375\n",
      "2018-10-26T16:11:16.366716: step 3507, loss 0.0184937, acc 0.984375\n",
      "2018-10-26T16:11:16.666914: step 3508, loss 0.0058705, acc 1\n",
      "2018-10-26T16:11:17.044981: step 3509, loss 0.0103684, acc 1\n",
      "2018-10-26T16:11:17.354080: step 3510, loss 0.0135758, acc 1\n",
      "2018-10-26T16:11:17.724091: step 3511, loss 0.0063347, acc 1\n",
      "2018-10-26T16:11:18.047258: step 3512, loss 0.0281539, acc 0.96875\n",
      "2018-10-26T16:11:18.386322: step 3513, loss 0.00826275, acc 1\n",
      "2018-10-26T16:11:18.737412: step 3514, loss 0.0205523, acc 0.984375\n",
      "2018-10-26T16:11:19.119361: step 3515, loss 0.00192213, acc 1\n",
      "2018-10-26T16:11:19.434524: step 3516, loss 0.0439149, acc 0.96875\n",
      "2018-10-26T16:11:19.815519: step 3517, loss 0.00570365, acc 1\n",
      "2018-10-26T16:11:20.189503: step 3518, loss 0.00124943, acc 1\n",
      "2018-10-26T16:11:20.518624: step 3519, loss 0.137323, acc 0.96875\n",
      "2018-10-26T16:11:20.859713: step 3520, loss 0.00818648, acc 1\n",
      "2018-10-26T16:11:21.183845: step 3521, loss 0.0138226, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:11:21.507983: step 3522, loss 0.00427039, acc 1\n",
      "2018-10-26T16:11:21.818256: step 3523, loss 0.00358413, acc 1\n",
      "2018-10-26T16:11:22.150264: step 3524, loss 0.0149051, acc 1\n",
      "2018-10-26T16:11:22.529250: step 3525, loss 0.0293816, acc 0.984375\n",
      "2018-10-26T16:11:22.860366: step 3526, loss 0.00577937, acc 1\n",
      "2018-10-26T16:11:23.172585: step 3527, loss 0.00382347, acc 1\n",
      "2018-10-26T16:11:23.493760: step 3528, loss 0.0104254, acc 1\n",
      "2018-10-26T16:11:23.846873: step 3529, loss 0.00651198, acc 1\n",
      "2018-10-26T16:11:24.172863: step 3530, loss 0.000848458, acc 1\n",
      "2018-10-26T16:11:24.511017: step 3531, loss 0.0264257, acc 0.984375\n",
      "2018-10-26T16:11:24.846133: step 3532, loss 0.00490106, acc 1\n",
      "2018-10-26T16:11:25.186175: step 3533, loss 0.0242344, acc 0.984375\n",
      "2018-10-26T16:11:25.514275: step 3534, loss 0.00296035, acc 1\n",
      "2018-10-26T16:11:25.835499: step 3535, loss 0.00262657, acc 1\n",
      "2018-10-26T16:11:26.156564: step 3536, loss 0.0128772, acc 1\n",
      "2018-10-26T16:11:26.542529: step 3537, loss 0.00556969, acc 1\n",
      "2018-10-26T16:11:26.877637: step 3538, loss 0.0126846, acc 1\n",
      "2018-10-26T16:11:27.337407: step 3539, loss 0.01054, acc 1\n",
      "2018-10-26T16:11:27.774243: step 3540, loss 0.00639456, acc 1\n",
      "2018-10-26T16:11:28.160206: step 3541, loss 0.0690419, acc 0.953125\n",
      "2018-10-26T16:11:28.568186: step 3542, loss 0.0383268, acc 0.984375\n",
      "2018-10-26T16:11:28.982716: step 3543, loss 0.00296269, acc 1\n",
      "2018-10-26T16:11:29.411928: step 3544, loss 0.0137643, acc 1\n",
      "2018-10-26T16:11:29.836729: step 3545, loss 0.00695337, acc 1\n",
      "2018-10-26T16:11:30.329412: step 3546, loss 0.0168682, acc 1\n",
      "2018-10-26T16:11:30.771394: step 3547, loss 0.0269758, acc 0.984375\n",
      "2018-10-26T16:11:31.163184: step 3548, loss 0.0133446, acc 1\n",
      "2018-10-26T16:11:31.495297: step 3549, loss 0.0079983, acc 1\n",
      "2018-10-26T16:11:31.896224: step 3550, loss 0.0124179, acc 1\n",
      "2018-10-26T16:11:32.214377: step 3551, loss 0.0031138, acc 1\n",
      "2018-10-26T16:11:32.526540: step 3552, loss 0.00923416, acc 1\n",
      "2018-10-26T16:11:32.862644: step 3553, loss 0.00377169, acc 1\n",
      "2018-10-26T16:11:33.224676: step 3554, loss 0.00525855, acc 1\n",
      "2018-10-26T16:11:33.601668: step 3555, loss 0.0119595, acc 1\n",
      "2018-10-26T16:11:33.925805: step 3556, loss 0.00816638, acc 1\n",
      "2018-10-26T16:11:34.243951: step 3557, loss 0.0031789, acc 1\n",
      "2018-10-26T16:11:34.605986: step 3558, loss 0.00968857, acc 1\n",
      "2018-10-26T16:11:34.956050: step 3559, loss 0.00394024, acc 1\n",
      "2018-10-26T16:11:35.355982: step 3560, loss 0.000760519, acc 1\n",
      "2018-10-26T16:11:35.799811: step 3561, loss 0.00457811, acc 1\n",
      "2018-10-26T16:11:36.263646: step 3562, loss 0.00334234, acc 1\n",
      "2018-10-26T16:11:36.619154: step 3563, loss 0.0023304, acc 1\n",
      "2018-10-26T16:11:36.971715: step 3564, loss 0.0237059, acc 0.984375\n",
      "2018-10-26T16:11:37.304774: step 3565, loss 0.00326637, acc 1\n",
      "2018-10-26T16:11:37.689745: step 3566, loss 0.00558843, acc 1\n",
      "2018-10-26T16:11:38.054552: step 3567, loss 0.00681701, acc 1\n",
      "2018-10-26T16:11:38.505566: step 3568, loss 0.00366933, acc 1\n",
      "2018-10-26T16:11:38.920461: step 3569, loss 0.00833402, acc 1\n",
      "2018-10-26T16:11:39.434085: step 3570, loss 0.00407393, acc 1\n",
      "2018-10-26T16:11:39.910864: step 3571, loss 0.0103503, acc 1\n",
      "2018-10-26T16:11:40.312736: step 3572, loss 0.00346074, acc 1\n",
      "2018-10-26T16:11:40.759614: step 3573, loss 0.00650518, acc 1\n",
      "2018-10-26T16:11:41.189397: step 3574, loss 0.00273964, acc 1\n",
      "2018-10-26T16:11:41.549432: step 3575, loss 0.0239626, acc 0.984375\n",
      "2018-10-26T16:11:41.929418: step 3576, loss 0.00407796, acc 1\n",
      "2018-10-26T16:11:42.347299: step 3577, loss 0.0190497, acc 0.984375\n",
      "2018-10-26T16:11:42.725291: step 3578, loss 0.032502, acc 0.984375\n",
      "2018-10-26T16:11:43.143175: step 3579, loss 0.0463266, acc 0.984375\n",
      "2018-10-26T16:11:43.585990: step 3580, loss 0.0062727, acc 1\n",
      "2018-10-26T16:11:43.917106: step 3581, loss 0.00869541, acc 1\n",
      "2018-10-26T16:11:44.313048: step 3582, loss 0.00831835, acc 1\n",
      "2018-10-26T16:11:44.798873: step 3583, loss 0.00121878, acc 1\n",
      "2018-10-26T16:11:45.158789: step 3584, loss 0.00170027, acc 1\n",
      "2018-10-26T16:11:45.561796: step 3585, loss 0.00225017, acc 1\n",
      "2018-10-26T16:11:45.903843: step 3586, loss 0.0130183, acc 1\n",
      "2018-10-26T16:11:46.234914: step 3587, loss 0.0166436, acc 0.984375\n",
      "2018-10-26T16:11:46.616893: step 3588, loss 0.000399675, acc 1\n",
      "2018-10-26T16:11:47.161436: step 3589, loss 0.00692568, acc 1\n",
      "2018-10-26T16:11:47.476595: step 3590, loss 0.0800302, acc 0.96875\n",
      "2018-10-26T16:11:47.872165: step 3591, loss 0.00921634, acc 1\n",
      "2018-10-26T16:11:48.208640: step 3592, loss 0.00369174, acc 1\n",
      "2018-10-26T16:11:48.525793: step 3593, loss 0.0125865, acc 1\n",
      "2018-10-26T16:11:48.877890: step 3594, loss 0.00124729, acc 1\n",
      "2018-10-26T16:11:49.223927: step 3595, loss 0.00135053, acc 1\n",
      "2018-10-26T16:11:49.557036: step 3596, loss 0.00370414, acc 1\n",
      "2018-10-26T16:11:49.896130: step 3597, loss 0.00645756, acc 1\n",
      "2018-10-26T16:11:50.224254: step 3598, loss 0.0110816, acc 1\n",
      "2018-10-26T16:11:50.578312: step 3599, loss 0.0124863, acc 1\n",
      "2018-10-26T16:11:50.912415: step 3600, loss 0.00289045, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:11:51.665404: step 3600, loss 1.32938, acc 0.723265\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3600\n",
      "\n",
      "2018-10-26T16:11:52.409418: step 3601, loss 0.0052283, acc 1\n",
      "2018-10-26T16:11:52.770452: step 3602, loss 0.00152481, acc 1\n",
      "2018-10-26T16:11:53.219252: step 3603, loss 0.000946241, acc 1\n",
      "2018-10-26T16:11:53.546381: step 3604, loss 0.00236597, acc 1\n",
      "2018-10-26T16:11:53.882482: step 3605, loss 0.00487547, acc 1\n",
      "2018-10-26T16:11:54.217613: step 3606, loss 0.00160506, acc 1\n",
      "2018-10-26T16:11:54.524763: step 3607, loss 0.00225038, acc 1\n",
      "2018-10-26T16:11:54.873886: step 3608, loss 0.00453022, acc 1\n",
      "2018-10-26T16:11:55.293710: step 3609, loss 0.00683388, acc 1\n",
      "2018-10-26T16:11:55.633802: step 3610, loss 0.00184435, acc 1\n",
      "2018-10-26T16:11:55.992841: step 3611, loss 0.00199777, acc 1\n",
      "2018-10-26T16:11:56.328000: step 3612, loss 0.000469142, acc 1\n",
      "2018-10-26T16:11:56.683994: step 3613, loss 0.00147042, acc 1\n",
      "2018-10-26T16:11:57.041130: step 3614, loss 0.0111086, acc 1\n",
      "2018-10-26T16:11:57.383128: step 3615, loss 0.0017374, acc 1\n",
      "2018-10-26T16:11:57.735186: step 3616, loss 0.0063818, acc 1\n",
      "2018-10-26T16:11:58.089408: step 3617, loss 0.00126901, acc 1\n",
      "2018-10-26T16:11:58.429332: step 3618, loss 0.00132119, acc 1\n",
      "2018-10-26T16:11:58.751567: step 3619, loss 0.00484259, acc 1\n",
      "2018-10-26T16:11:59.079598: step 3620, loss 0.0645709, acc 0.96875\n",
      "2018-10-26T16:11:59.410771: step 3621, loss 0.00177133, acc 1\n",
      "2018-10-26T16:11:59.730858: step 3622, loss 0.00566118, acc 1\n",
      "2018-10-26T16:12:00.076929: step 3623, loss 0.00116639, acc 1\n",
      "2018-10-26T16:12:00.399069: step 3624, loss 0.00425182, acc 1\n",
      "2018-10-26T16:12:00.726195: step 3625, loss 0.0032492, acc 1\n",
      "2018-10-26T16:12:01.057311: step 3626, loss 0.00223226, acc 1\n",
      "2018-10-26T16:12:01.369475: step 3627, loss 0.00448854, acc 1\n",
      "2018-10-26T16:12:01.672666: step 3628, loss 0.00348855, acc 1\n",
      "2018-10-26T16:12:01.998003: step 3629, loss 0.00265455, acc 1\n",
      "2018-10-26T16:12:02.350883: step 3630, loss 0.00152403, acc 1\n",
      "2018-10-26T16:12:02.675034: step 3631, loss 0.0199051, acc 0.984375\n",
      "2018-10-26T16:12:03.007103: step 3632, loss 0.0123176, acc 1\n",
      "2018-10-26T16:12:03.355279: step 3633, loss 0.0119261, acc 1\n",
      "2018-10-26T16:12:03.669456: step 3634, loss 0.00422531, acc 1\n",
      "2018-10-26T16:12:03.983492: step 3635, loss 0.00931232, acc 1\n",
      "2018-10-26T16:12:04.301681: step 3636, loss 0.00921999, acc 1\n",
      "2018-10-26T16:12:04.627154: step 3637, loss 0.0070822, acc 1\n",
      "2018-10-26T16:12:04.956895: step 3638, loss 0.0161911, acc 1\n",
      "2018-10-26T16:12:05.312941: step 3639, loss 0.00751783, acc 1\n",
      "2018-10-26T16:12:05.650093: step 3640, loss 0.0178903, acc 1\n",
      "2018-10-26T16:12:06.011073: step 3641, loss 0.00138419, acc 1\n",
      "2018-10-26T16:12:06.335209: step 3642, loss 0.00361104, acc 1\n",
      "2018-10-26T16:12:06.658348: step 3643, loss 0.00322776, acc 1\n",
      "2018-10-26T16:12:06.981482: step 3644, loss 0.00207021, acc 1\n",
      "2018-10-26T16:12:07.302625: step 3645, loss 0.00322767, acc 1\n",
      "2018-10-26T16:12:07.622836: step 3646, loss 0.0139236, acc 1\n",
      "2018-10-26T16:12:07.947942: step 3647, loss 0.000778179, acc 1\n",
      "2018-10-26T16:12:08.268044: step 3648, loss 0.000557943, acc 1\n",
      "2018-10-26T16:12:08.588229: step 3649, loss 0.00588483, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:12:08.923422: step 3650, loss 0.0103353, acc 1\n",
      "2018-10-26T16:12:09.260397: step 3651, loss 0.00195609, acc 1\n",
      "2018-10-26T16:12:09.593503: step 3652, loss 0.00588033, acc 1\n",
      "2018-10-26T16:12:09.927613: step 3653, loss 0.00294859, acc 1\n",
      "2018-10-26T16:12:10.281764: step 3654, loss 0.00373758, acc 1\n",
      "2018-10-26T16:12:10.593924: step 3655, loss 0.00858309, acc 1\n",
      "2018-10-26T16:12:10.911981: step 3656, loss 0.0149082, acc 0.984375\n",
      "2018-10-26T16:12:11.230144: step 3657, loss 0.00354182, acc 1\n",
      "2018-10-26T16:12:11.527486: step 3658, loss 0.00632879, acc 1\n",
      "2018-10-26T16:12:11.852467: step 3659, loss 0.00266591, acc 1\n",
      "2018-10-26T16:12:12.170617: step 3660, loss 0.0315318, acc 0.984375\n",
      "2018-10-26T16:12:12.515695: step 3661, loss 0.00235441, acc 1\n",
      "2018-10-26T16:12:12.890692: step 3662, loss 0.00233649, acc 1\n",
      "2018-10-26T16:12:13.226797: step 3663, loss 0.00581438, acc 1\n",
      "2018-10-26T16:12:13.610770: step 3664, loss 0.0107213, acc 1\n",
      "2018-10-26T16:12:13.942883: step 3665, loss 0.00974291, acc 1\n",
      "2018-10-26T16:12:14.271267: step 3666, loss 0.0038344, acc 1\n",
      "2018-10-26T16:12:14.603119: step 3667, loss 0.0092866, acc 1\n",
      "2018-10-26T16:12:14.928252: step 3668, loss 0.00247141, acc 1\n",
      "2018-10-26T16:12:15.248396: step 3669, loss 0.00237901, acc 1\n",
      "2018-10-26T16:12:15.574525: step 3670, loss 0.00531884, acc 1\n",
      "2018-10-26T16:12:15.897697: step 3671, loss 0.00440748, acc 1\n",
      "2018-10-26T16:12:16.233764: step 3672, loss 0.000549058, acc 1\n",
      "2018-10-26T16:12:16.587818: step 3673, loss 0.0173745, acc 1\n",
      "2018-10-26T16:12:16.917933: step 3674, loss 0.00651703, acc 1\n",
      "2018-10-26T16:12:17.245058: step 3675, loss 0.0360893, acc 0.984375\n",
      "2018-10-26T16:12:17.588146: step 3676, loss 0.015294, acc 0.984375\n",
      "2018-10-26T16:12:17.913304: step 3677, loss 0.00230097, acc 1\n",
      "2018-10-26T16:12:18.216463: step 3678, loss 0.00192231, acc 1\n",
      "2018-10-26T16:12:18.566532: step 3679, loss 0.010377, acc 1\n",
      "2018-10-26T16:12:18.945515: step 3680, loss 0.011438, acc 1\n",
      "2018-10-26T16:12:19.274635: step 3681, loss 0.00131012, acc 1\n",
      "2018-10-26T16:12:19.613781: step 3682, loss 0.00508546, acc 1\n",
      "2018-10-26T16:12:19.942851: step 3683, loss 0.000963697, acc 1\n",
      "2018-10-26T16:12:20.314859: step 3684, loss 0.0081058, acc 1\n",
      "2018-10-26T16:12:20.684909: step 3685, loss 0.00784339, acc 1\n",
      "2018-10-26T16:12:21.000025: step 3686, loss 0.00400952, acc 1\n",
      "2018-10-26T16:12:21.326157: step 3687, loss 0.00370278, acc 1\n",
      "2018-10-26T16:12:21.643308: step 3688, loss 0.00213066, acc 1\n",
      "2018-10-26T16:12:21.970515: step 3689, loss 0.00442393, acc 1\n",
      "2018-10-26T16:12:22.345434: step 3690, loss 0.00139113, acc 1\n",
      "2018-10-26T16:12:22.694499: step 3691, loss 0.0117721, acc 1\n",
      "2018-10-26T16:12:23.068500: step 3692, loss 0.00177569, acc 1\n",
      "2018-10-26T16:12:23.378672: step 3693, loss 0.00477867, acc 1\n",
      "2018-10-26T16:12:23.713779: step 3694, loss 0.00734662, acc 1\n",
      "2018-10-26T16:12:24.059852: step 3695, loss 0.0198502, acc 0.984375\n",
      "2018-10-26T16:12:24.384981: step 3696, loss 0.00801623, acc 1\n",
      "2018-10-26T16:12:24.712108: step 3697, loss 0.002535, acc 1\n",
      "2018-10-26T16:12:25.052239: step 3698, loss 0.0166663, acc 1\n",
      "2018-10-26T16:12:25.390398: step 3699, loss 0.00911671, acc 1\n",
      "2018-10-26T16:12:25.715901: step 3700, loss 0.0199115, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:12:26.489359: step 3700, loss 1.35371, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3700\n",
      "\n",
      "2018-10-26T16:12:27.056843: step 3701, loss 0.035504, acc 0.96875\n",
      "2018-10-26T16:12:27.393943: step 3702, loss 0.0176257, acc 0.984375\n",
      "2018-10-26T16:12:27.830811: step 3703, loss 0.0020047, acc 1\n",
      "2018-10-26T16:12:28.173882: step 3704, loss 0.00140962, acc 1\n",
      "2018-10-26T16:12:28.509962: step 3705, loss 0.00152774, acc 1\n",
      "2018-10-26T16:12:28.890990: step 3706, loss 0.00290391, acc 1\n",
      "2018-10-26T16:12:29.277909: step 3707, loss 0.00540475, acc 1\n",
      "2018-10-26T16:12:29.655900: step 3708, loss 0.0172366, acc 0.984375\n",
      "2018-10-26T16:12:30.004968: step 3709, loss 0.00371958, acc 1\n",
      "2018-10-26T16:12:30.316136: step 3710, loss 0.00202021, acc 1\n",
      "2018-10-26T16:12:30.623316: step 3711, loss 0.00117264, acc 1\n",
      "2018-10-26T16:12:30.944459: step 3712, loss 0.00160414, acc 1\n",
      "2018-10-26T16:12:31.281606: step 3713, loss 0.00232973, acc 1\n",
      "2018-10-26T16:12:31.608681: step 3714, loss 0.00465591, acc 1\n",
      "2018-10-26T16:12:31.945780: step 3715, loss 0.00610483, acc 1\n",
      "2018-10-26T16:12:32.269916: step 3716, loss 0.0064849, acc 1\n",
      "2018-10-26T16:12:32.603023: step 3717, loss 0.0126775, acc 1\n",
      "2018-10-26T16:12:32.999992: step 3718, loss 0.00244311, acc 1\n",
      "2018-10-26T16:12:33.338060: step 3719, loss 0.0104461, acc 1\n",
      "2018-10-26T16:12:33.657211: step 3720, loss 0.00569467, acc 1\n",
      "2018-10-26T16:12:33.965387: step 3721, loss 0.0341988, acc 0.984375\n",
      "2018-10-26T16:12:34.294504: step 3722, loss 0.00159044, acc 1\n",
      "2018-10-26T16:12:34.630609: step 3723, loss 0.00188697, acc 1\n",
      "2018-10-26T16:12:34.958734: step 3724, loss 0.00612846, acc 1\n",
      "2018-10-26T16:12:35.269899: step 3725, loss 0.0140414, acc 0.984375\n",
      "2018-10-26T16:12:35.620996: step 3726, loss 0.00481246, acc 1\n",
      "2018-10-26T16:12:35.937119: step 3727, loss 0.00317958, acc 1\n",
      "2018-10-26T16:12:36.278311: step 3728, loss 0.00453119, acc 1\n",
      "2018-10-26T16:12:36.599460: step 3729, loss 0.00924785, acc 1\n",
      "2018-10-26T16:12:36.931511: step 3730, loss 0.00598427, acc 1\n",
      "2018-10-26T16:12:37.275592: step 3731, loss 0.00300835, acc 1\n",
      "2018-10-26T16:12:37.626726: step 3732, loss 0.00878039, acc 1\n",
      "2018-10-26T16:12:37.954725: step 3733, loss 0.00231401, acc 1\n",
      "2018-10-26T16:12:38.290866: step 3734, loss 0.00548258, acc 1\n",
      "2018-10-26T16:12:38.615959: step 3735, loss 0.00962007, acc 1\n",
      "2018-10-26T16:12:38.943086: step 3736, loss 0.00111057, acc 1\n",
      "2018-10-26T16:12:39.276195: step 3737, loss 0.0049695, acc 1\n",
      "2018-10-26T16:12:39.604320: step 3738, loss 0.0136347, acc 1\n",
      "2018-10-26T16:12:39.971390: step 3739, loss 0.00413014, acc 1\n",
      "2018-10-26T16:12:40.303034: step 3740, loss 0.00927409, acc 1\n",
      "2018-10-26T16:12:40.630576: step 3741, loss 0.00166492, acc 1\n",
      "2018-10-26T16:12:40.957705: step 3742, loss 0.00620622, acc 1\n",
      "2018-10-26T16:12:41.281902: step 3743, loss 0.00292392, acc 1\n",
      "2018-10-26T16:12:41.592009: step 3744, loss 0.0163234, acc 1\n",
      "2018-10-26T16:12:41.902177: step 3745, loss 0.00329937, acc 1\n",
      "2018-10-26T16:12:42.274184: step 3746, loss 0.00505589, acc 1\n",
      "2018-10-26T16:12:42.648184: step 3747, loss 0.00397732, acc 1\n",
      "2018-10-26T16:12:43.014238: step 3748, loss 0.00472168, acc 1\n",
      "2018-10-26T16:12:43.382225: step 3749, loss 0.00747847, acc 1\n",
      "2018-10-26T16:12:43.716405: step 3750, loss 0.00175939, acc 1\n",
      "2018-10-26T16:12:44.125238: step 3751, loss 0.00206237, acc 1\n",
      "2018-10-26T16:12:44.521260: step 3752, loss 0.005215, acc 1\n",
      "2018-10-26T16:12:44.996911: step 3753, loss 0.0138287, acc 1\n",
      "2018-10-26T16:12:45.436735: step 3754, loss 0.0028848, acc 1\n",
      "2018-10-26T16:12:45.885639: step 3755, loss 0.0113523, acc 1\n",
      "2018-10-26T16:12:46.287462: step 3756, loss 0.00607773, acc 1\n",
      "2018-10-26T16:12:46.671510: step 3757, loss 0.00164521, acc 1\n",
      "2018-10-26T16:12:47.093310: step 3758, loss 0.00586865, acc 1\n",
      "2018-10-26T16:12:47.470386: step 3759, loss 0.00326365, acc 1\n",
      "2018-10-26T16:12:47.900153: step 3760, loss 0.0116118, acc 0.984375\n",
      "2018-10-26T16:12:48.329057: step 3761, loss 0.00119325, acc 1\n",
      "2018-10-26T16:12:48.690042: step 3762, loss 0.00484097, acc 1\n",
      "2018-10-26T16:12:49.070027: step 3763, loss 0.00393598, acc 1\n",
      "2018-10-26T16:12:49.464972: step 3764, loss 0.00109021, acc 1\n",
      "2018-10-26T16:12:49.846951: step 3765, loss 0.0358038, acc 0.984375\n",
      "2018-10-26T16:12:50.226938: step 3766, loss 0.00493546, acc 1\n",
      "2018-10-26T16:12:50.718621: step 3767, loss 0.0070622, acc 1\n",
      "2018-10-26T16:12:51.128527: step 3768, loss 0.00873209, acc 1\n",
      "2018-10-26T16:12:51.548405: step 3769, loss 0.00410764, acc 1\n",
      "2018-10-26T16:12:52.012166: step 3770, loss 0.000948462, acc 1\n",
      "2018-10-26T16:12:52.387164: step 3771, loss 0.00133187, acc 1\n",
      "2018-10-26T16:12:52.825423: step 3772, loss 0.00103326, acc 1\n",
      "2018-10-26T16:12:53.177056: step 3773, loss 0.00742297, acc 1\n",
      "2018-10-26T16:12:53.613886: step 3774, loss 0.00611978, acc 1\n",
      "2018-10-26T16:12:53.987890: step 3775, loss 0.00305528, acc 1\n",
      "2018-10-26T16:12:54.352913: step 3776, loss 0.00933508, acc 1\n",
      "2018-10-26T16:12:54.802710: step 3777, loss 0.00123259, acc 1\n",
      "2018-10-26T16:12:55.206631: step 3778, loss 0.00540745, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:12:55.603570: step 3779, loss 0.00314616, acc 1\n",
      "2018-10-26T16:12:56.047385: step 3780, loss 0.0101909, acc 1\n",
      "2018-10-26T16:12:56.430361: step 3781, loss 0.00894524, acc 1\n",
      "2018-10-26T16:12:56.817328: step 3782, loss 0.00207428, acc 1\n",
      "2018-10-26T16:12:57.150480: step 3783, loss 0.00230921, acc 1\n",
      "2018-10-26T16:12:57.516459: step 3784, loss 0.00261247, acc 1\n",
      "2018-10-26T16:12:57.846577: step 3785, loss 0.00184328, acc 1\n",
      "2018-10-26T16:12:58.183678: step 3786, loss 0.00294636, acc 1\n",
      "2018-10-26T16:12:58.506815: step 3787, loss 0.0145621, acc 0.984375\n",
      "2018-10-26T16:12:58.850934: step 3788, loss 0.00629958, acc 1\n",
      "2018-10-26T16:12:59.205944: step 3789, loss 0.0019741, acc 1\n",
      "2018-10-26T16:12:59.532073: step 3790, loss 0.0477, acc 0.984375\n",
      "2018-10-26T16:12:59.852482: step 3791, loss 0.00411217, acc 1\n",
      "2018-10-26T16:13:00.200327: step 3792, loss 0.00994758, acc 1\n",
      "2018-10-26T16:13:00.555340: step 3793, loss 0.00467984, acc 1\n",
      "2018-10-26T16:13:00.879475: step 3794, loss 0.00294377, acc 1\n",
      "2018-10-26T16:13:01.197623: step 3795, loss 0.0148096, acc 1\n",
      "2018-10-26T16:13:01.544696: step 3796, loss 0.0102844, acc 1\n",
      "2018-10-26T16:13:01.884791: step 3797, loss 0.00438694, acc 1\n",
      "2018-10-26T16:13:02.210982: step 3798, loss 0.00149982, acc 1\n",
      "2018-10-26T16:13:02.533056: step 3799, loss 0.00192332, acc 1\n",
      "2018-10-26T16:13:02.871223: step 3800, loss 0.00138444, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:13:03.643089: step 3800, loss 1.41054, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3800\n",
      "\n",
      "2018-10-26T16:13:04.267421: step 3801, loss 0.0132438, acc 1\n",
      "2018-10-26T16:13:04.635438: step 3802, loss 0.0154763, acc 0.984375\n",
      "2018-10-26T16:13:05.097206: step 3803, loss 0.0107935, acc 1\n",
      "2018-10-26T16:13:05.514225: step 3804, loss 0.00121009, acc 1\n",
      "2018-10-26T16:13:05.865153: step 3805, loss 0.00257806, acc 1\n",
      "2018-10-26T16:13:06.202251: step 3806, loss 0.0285705, acc 1\n",
      "2018-10-26T16:13:06.529377: step 3807, loss 0.00368816, acc 1\n",
      "2018-10-26T16:13:06.865479: step 3808, loss 0.00261431, acc 1\n",
      "2018-10-26T16:13:07.199587: step 3809, loss 0.00226155, acc 1\n",
      "2018-10-26T16:13:07.542674: step 3810, loss 0.0158975, acc 0.984375\n",
      "2018-10-26T16:13:07.858828: step 3811, loss 0.00271718, acc 1\n",
      "2018-10-26T16:13:08.188945: step 3812, loss 0.00171094, acc 1\n",
      "2018-10-26T16:13:08.547985: step 3813, loss 0.000608615, acc 1\n",
      "2018-10-26T16:13:08.870124: step 3814, loss 0.00212128, acc 1\n",
      "2018-10-26T16:13:09.206230: step 3815, loss 0.00170189, acc 1\n",
      "2018-10-26T16:13:09.549311: step 3816, loss 0.00367187, acc 1\n",
      "2018-10-26T16:13:09.858484: step 3817, loss 0.00158547, acc 1\n",
      "2018-10-26T16:13:10.186606: step 3818, loss 0.0123034, acc 1\n",
      "2018-10-26T16:13:10.509742: step 3819, loss 0.00132096, acc 1\n",
      "2018-10-26T16:13:10.865792: step 3820, loss 0.00441969, acc 1\n",
      "2018-10-26T16:13:11.187932: step 3821, loss 0.00174328, acc 1\n",
      "2018-10-26T16:13:11.522042: step 3822, loss 0.00167447, acc 1\n",
      "2018-10-26T16:13:11.865121: step 3823, loss 0.00108734, acc 1\n",
      "2018-10-26T16:13:12.204215: step 3824, loss 0.00478085, acc 1\n",
      "2018-10-26T16:13:12.572232: step 3825, loss 0.00462745, acc 1\n",
      "2018-10-26T16:13:12.896370: step 3826, loss 0.00256889, acc 1\n",
      "2018-10-26T16:13:13.227482: step 3827, loss 0.00601682, acc 1\n",
      "2018-10-26T16:13:13.545635: step 3828, loss 0.00211105, acc 1\n",
      "2018-10-26T16:13:13.894699: step 3829, loss 0.00214896, acc 1\n",
      "2018-10-26T16:13:14.240774: step 3830, loss 0.0151389, acc 0.984375\n",
      "2018-10-26T16:13:14.569899: step 3831, loss 0.00702775, acc 1\n",
      "2018-10-26T16:13:14.880069: step 3832, loss 0.0085877, acc 1\n",
      "2018-10-26T16:13:15.199216: step 3833, loss 0.00628929, acc 1\n",
      "2018-10-26T16:13:15.556260: step 3834, loss 0.00147397, acc 1\n",
      "2018-10-26T16:13:15.894401: step 3835, loss 0.00446142, acc 1\n",
      "2018-10-26T16:13:16.215502: step 3836, loss 0.041233, acc 0.96875\n",
      "2018-10-26T16:13:16.544620: step 3837, loss 0.0195424, acc 0.984375\n",
      "2018-10-26T16:13:16.904660: step 3838, loss 0.00840977, acc 1\n",
      "2018-10-26T16:13:17.222881: step 3839, loss 0.00236426, acc 1\n",
      "2018-10-26T16:13:17.541954: step 3840, loss 0.00407656, acc 1\n",
      "2018-10-26T16:13:17.864094: step 3841, loss 0.000748696, acc 1\n",
      "2018-10-26T16:13:18.191346: step 3842, loss 0.00426773, acc 1\n",
      "2018-10-26T16:13:18.575197: step 3843, loss 0.00194765, acc 1\n",
      "2018-10-26T16:13:18.895341: step 3844, loss 0.0103859, acc 1\n",
      "2018-10-26T16:13:19.207508: step 3845, loss 0.0022353, acc 1\n",
      "2018-10-26T16:13:19.544608: step 3846, loss 0.00494276, acc 1\n",
      "2018-10-26T16:13:19.896662: step 3847, loss 0.00321819, acc 1\n",
      "2018-10-26T16:13:20.232765: step 3848, loss 0.000644734, acc 1\n",
      "2018-10-26T16:13:20.543994: step 3849, loss 0.00959761, acc 1\n",
      "2018-10-26T16:13:20.873117: step 3850, loss 0.0213361, acc 0.984375\n",
      "2018-10-26T16:13:21.197188: step 3851, loss 0.0164814, acc 0.984375\n",
      "2018-10-26T16:13:21.507360: step 3852, loss 0.0188843, acc 0.984375\n",
      "2018-10-26T16:13:21.835900: step 3853, loss 0.00814617, acc 1\n",
      "2018-10-26T16:13:22.166599: step 3854, loss 0.00274904, acc 1\n",
      "2018-10-26T16:13:22.531622: step 3855, loss 0.00281898, acc 1\n",
      "2018-10-26T16:13:22.968456: step 3856, loss 0.00249265, acc 1\n",
      "2018-10-26T16:13:23.309547: step 3857, loss 0.00712533, acc 1\n",
      "2018-10-26T16:13:23.668689: step 3858, loss 0.00513266, acc 1\n",
      "2018-10-26T16:13:23.983847: step 3859, loss 0.00074694, acc 1\n",
      "2018-10-26T16:13:24.310872: step 3860, loss 0.00619979, acc 1\n",
      "2018-10-26T16:13:24.683876: step 3861, loss 0.0422813, acc 0.984375\n",
      "2018-10-26T16:13:25.042947: step 3862, loss 0.0028714, acc 1\n",
      "2018-10-26T16:13:25.367050: step 3863, loss 0.0178637, acc 1\n",
      "2018-10-26T16:13:25.712128: step 3864, loss 0.00804665, acc 1\n",
      "2018-10-26T16:13:26.072166: step 3865, loss 0.00482921, acc 1\n",
      "2018-10-26T16:13:26.409474: step 3866, loss 0.0123617, acc 1\n",
      "2018-10-26T16:13:26.744369: step 3867, loss 0.00582372, acc 1\n",
      "2018-10-26T16:13:27.095428: step 3868, loss 0.0630537, acc 0.96875\n",
      "2018-10-26T16:13:27.464137: step 3869, loss 0.00474493, acc 1\n",
      "2018-10-26T16:13:27.793563: step 3870, loss 0.00163801, acc 1\n",
      "2018-10-26T16:13:28.189505: step 3871, loss 0.0032421, acc 1\n",
      "2018-10-26T16:13:28.565501: step 3872, loss 0.00772784, acc 1\n",
      "2018-10-26T16:13:28.916568: step 3873, loss 0.00217723, acc 1\n",
      "2018-10-26T16:13:29.306524: step 3874, loss 0.00791375, acc 1\n",
      "2018-10-26T16:13:29.646616: step 3875, loss 0.0303973, acc 0.96875\n",
      "2018-10-26T16:13:29.991738: step 3876, loss 0.00125834, acc 1\n",
      "2018-10-26T16:13:30.344748: step 3877, loss 0.00694177, acc 1\n",
      "2018-10-26T16:13:30.714230: step 3878, loss 0.00650024, acc 1\n",
      "2018-10-26T16:13:31.055849: step 3879, loss 0.00178124, acc 1\n",
      "2018-10-26T16:13:31.422865: step 3880, loss 0.00208364, acc 1\n",
      "2018-10-26T16:13:31.807838: step 3881, loss 0.00575279, acc 1\n",
      "2018-10-26T16:13:32.171951: step 3882, loss 0.00503651, acc 1\n",
      "2018-10-26T16:13:32.522961: step 3883, loss 0.0156523, acc 0.984375\n",
      "2018-10-26T16:13:32.971812: step 3884, loss 0.0114097, acc 1\n",
      "2018-10-26T16:13:33.346729: step 3885, loss 0.00628901, acc 1\n",
      "2018-10-26T16:13:33.713746: step 3886, loss 0.00413846, acc 1\n",
      "2018-10-26T16:13:34.119799: step 3887, loss 0.0029956, acc 1\n",
      "2018-10-26T16:13:34.507625: step 3888, loss 0.00839393, acc 1\n",
      "2018-10-26T16:13:34.855698: step 3889, loss 0.0284601, acc 0.984375\n",
      "2018-10-26T16:13:35.199775: step 3890, loss 0.00230931, acc 1\n",
      "2018-10-26T16:13:35.552834: step 3891, loss 0.00209098, acc 1\n",
      "2018-10-26T16:13:35.929972: step 3892, loss 0.00922996, acc 1\n",
      "2018-10-26T16:13:36.271911: step 3893, loss 0.000787423, acc 1\n",
      "2018-10-26T16:13:36.642967: step 3894, loss 0.0072456, acc 1\n",
      "2018-10-26T16:13:36.991990: step 3895, loss 0.00192669, acc 1\n",
      "2018-10-26T16:13:37.349034: step 3896, loss 0.00183599, acc 1\n",
      "2018-10-26T16:13:37.735998: step 3897, loss 0.00222729, acc 1\n",
      "2018-10-26T16:13:38.129947: step 3898, loss 0.0028856, acc 1\n",
      "2018-10-26T16:13:38.599691: step 3899, loss 0.00687637, acc 1\n",
      "2018-10-26T16:13:38.933830: step 3900, loss 0.00756357, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:13:39.701746: step 3900, loss 1.40499, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-3900\n",
      "\n",
      "2018-10-26T16:13:40.258260: step 3901, loss 0.00151629, acc 1\n",
      "2018-10-26T16:13:40.578497: step 3902, loss 0.000417326, acc 1\n",
      "2018-10-26T16:13:40.963376: step 3903, loss 0.00292831, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:13:41.344358: step 3904, loss 0.00157408, acc 1\n",
      "2018-10-26T16:13:41.684452: step 3905, loss 0.00114862, acc 1\n",
      "2018-10-26T16:13:42.026539: step 3906, loss 0.00293713, acc 1\n",
      "2018-10-26T16:13:42.363634: step 3907, loss 0.0174782, acc 1\n",
      "2018-10-26T16:13:42.697741: step 3908, loss 0.00219732, acc 1\n",
      "2018-10-26T16:13:43.048805: step 3909, loss 0.00123584, acc 1\n",
      "2018-10-26T16:13:43.406846: step 3910, loss 0.00205372, acc 1\n",
      "2018-10-26T16:13:43.736969: step 3911, loss 0.00766029, acc 1\n",
      "2018-10-26T16:13:44.069272: step 3912, loss 0.00675536, acc 1\n",
      "2018-10-26T16:13:44.412164: step 3913, loss 0.00415128, acc 1\n",
      "2018-10-26T16:13:44.774197: step 3914, loss 0.00401474, acc 1\n",
      "2018-10-26T16:13:45.130282: step 3915, loss 0.00458974, acc 1\n",
      "2018-10-26T16:13:45.486292: step 3916, loss 0.00229766, acc 1\n",
      "2018-10-26T16:13:45.804444: step 3917, loss 0.00265477, acc 1\n",
      "2018-10-26T16:13:46.146599: step 3918, loss 0.00312094, acc 1\n",
      "2018-10-26T16:13:46.465676: step 3919, loss 0.00617484, acc 1\n",
      "2018-10-26T16:13:46.804772: step 3920, loss 0.000717119, acc 1\n",
      "2018-10-26T16:13:47.135886: step 3921, loss 0.00691534, acc 1\n",
      "2018-10-26T16:13:47.467003: step 3922, loss 0.0051926, acc 1\n",
      "2018-10-26T16:13:47.856957: step 3923, loss 0.00165863, acc 1\n",
      "2018-10-26T16:13:48.219987: step 3924, loss 0.00326658, acc 1\n",
      "2018-10-26T16:13:48.589998: step 3925, loss 0.00315604, acc 1\n",
      "2018-10-26T16:13:48.937071: step 3926, loss 0.0248205, acc 0.984375\n",
      "2018-10-26T16:13:49.294116: step 3927, loss 0.0070763, acc 1\n",
      "2018-10-26T16:13:49.676097: step 3928, loss 0.00198755, acc 1\n",
      "2018-10-26T16:13:50.039134: step 3929, loss 0.0304956, acc 0.984375\n",
      "2018-10-26T16:13:50.453057: step 3930, loss 0.0147529, acc 1\n",
      "2018-10-26T16:13:50.885864: step 3931, loss 0.00502768, acc 1\n",
      "2018-10-26T16:13:51.351677: step 3932, loss 0.00223404, acc 1\n",
      "2018-10-26T16:13:51.849290: step 3933, loss 0.00156584, acc 1\n",
      "2018-10-26T16:13:52.257200: step 3934, loss 0.00179118, acc 1\n",
      "2018-10-26T16:13:52.679073: step 3935, loss 0.00290327, acc 1\n",
      "2018-10-26T16:13:53.132861: step 3936, loss 0.0115952, acc 1\n",
      "2018-10-26T16:13:53.526845: step 3937, loss 0.000311779, acc 1\n",
      "2018-10-26T16:13:53.950676: step 3938, loss 0.00394129, acc 1\n",
      "2018-10-26T16:13:54.363624: step 3939, loss 0.00178332, acc 1\n",
      "2018-10-26T16:13:54.792426: step 3940, loss 0.00554009, acc 1\n",
      "2018-10-26T16:13:55.203329: step 3941, loss 0.000800567, acc 1\n",
      "2018-10-26T16:13:55.515565: step 3942, loss 0.0028837, acc 1\n",
      "2018-10-26T16:13:55.832649: step 3943, loss 0.00132323, acc 1\n",
      "2018-10-26T16:13:56.160770: step 3944, loss 0.00790367, acc 1\n",
      "2018-10-26T16:13:56.471943: step 3945, loss 0.00235359, acc 1\n",
      "2018-10-26T16:13:56.791087: step 3946, loss 0.00441257, acc 1\n",
      "2018-10-26T16:13:57.088293: step 3947, loss 0.0128429, acc 1\n",
      "2018-10-26T16:13:57.419407: step 3948, loss 0.00212598, acc 1\n",
      "2018-10-26T16:13:57.751602: step 3949, loss 0.00140548, acc 1\n",
      "2018-10-26T16:13:58.107573: step 3950, loss 0.00596874, acc 1\n",
      "2018-10-26T16:13:58.436693: step 3951, loss 0.012066, acc 1\n",
      "2018-10-26T16:13:58.771839: step 3952, loss 0.00701691, acc 1\n",
      "2018-10-26T16:13:59.109891: step 3953, loss 0.000823217, acc 1\n",
      "2018-10-26T16:13:59.467934: step 3954, loss 0.000937286, acc 1\n",
      "2018-10-26T16:13:59.797054: step 3955, loss 0.00744753, acc 1\n",
      "2018-10-26T16:14:00.141136: step 3956, loss 0.0157345, acc 1\n",
      "2018-10-26T16:14:00.457295: step 3957, loss 0.00196732, acc 1\n",
      "2018-10-26T16:14:00.779433: step 3958, loss 0.0475261, acc 0.984375\n",
      "2018-10-26T16:14:01.131491: step 3959, loss 0.00140827, acc 1\n",
      "2018-10-26T16:14:01.459613: step 3960, loss 0.00519335, acc 1\n",
      "2018-10-26T16:14:01.784745: step 3961, loss 0.00224668, acc 1\n",
      "2018-10-26T16:14:02.127831: step 3962, loss 0.00358646, acc 1\n",
      "2018-10-26T16:14:02.458943: step 3963, loss 0.00140381, acc 1\n",
      "2018-10-26T16:14:02.771157: step 3964, loss 0.00930628, acc 1\n",
      "2018-10-26T16:14:03.101265: step 3965, loss 0.00462755, acc 1\n",
      "2018-10-26T16:14:03.418416: step 3966, loss 0.00499438, acc 1\n",
      "2018-10-26T16:14:03.746506: step 3967, loss 0.00452486, acc 1\n",
      "2018-10-26T16:14:04.071695: step 3968, loss 0.000707412, acc 1\n",
      "2018-10-26T16:14:04.391826: step 3969, loss 0.00209571, acc 1\n",
      "2018-10-26T16:14:04.718970: step 3970, loss 0.00391461, acc 1\n",
      "2018-10-26T16:14:05.087921: step 3971, loss 0.00154781, acc 1\n",
      "2018-10-26T16:14:05.434995: step 3972, loss 0.0246057, acc 0.984375\n",
      "2018-10-26T16:14:05.760123: step 3973, loss 0.00677592, acc 1\n",
      "2018-10-26T16:14:06.096224: step 3974, loss 0.00607789, acc 1\n",
      "2018-10-26T16:14:06.410469: step 3975, loss 0.0101632, acc 1\n",
      "2018-10-26T16:14:06.748483: step 3976, loss 0.000510421, acc 1\n",
      "2018-10-26T16:14:07.085584: step 3977, loss 0.0118332, acc 1\n",
      "2018-10-26T16:14:07.413706: step 3978, loss 0.00206664, acc 1\n",
      "2018-10-26T16:14:07.729861: step 3979, loss 0.00490068, acc 1\n",
      "2018-10-26T16:14:08.060975: step 3980, loss 0.0288036, acc 0.984375\n",
      "2018-10-26T16:14:08.402118: step 3981, loss 0.00131347, acc 1\n",
      "2018-10-26T16:14:08.738203: step 3982, loss 0.0204418, acc 0.984375\n",
      "2018-10-26T16:14:09.068347: step 3983, loss 0.00209848, acc 1\n",
      "2018-10-26T16:14:09.405383: step 3984, loss 0.00261301, acc 1\n",
      "2018-10-26T16:14:09.729520: step 3985, loss 0.00207689, acc 1\n",
      "2018-10-26T16:14:10.077587: step 3986, loss 0.00367904, acc 1\n",
      "2018-10-26T16:14:10.417678: step 3987, loss 0.00919609, acc 1\n",
      "2018-10-26T16:14:10.768742: step 3988, loss 0.00865988, acc 1\n",
      "2018-10-26T16:14:11.120842: step 3989, loss 0.0243165, acc 0.984375\n",
      "2018-10-26T16:14:11.467875: step 3990, loss 0.00319279, acc 1\n",
      "2018-10-26T16:14:11.795141: step 3991, loss 0.000891228, acc 1\n",
      "2018-10-26T16:14:12.123121: step 3992, loss 0.000624775, acc 1\n",
      "2018-10-26T16:14:12.456272: step 3993, loss 0.00295427, acc 1\n",
      "2018-10-26T16:14:12.780366: step 3994, loss 0.00375062, acc 1\n",
      "2018-10-26T16:14:13.111481: step 3995, loss 0.0019432, acc 1\n",
      "2018-10-26T16:14:13.470522: step 3996, loss 0.00731624, acc 1\n",
      "2018-10-26T16:14:13.798648: step 3997, loss 0.0205721, acc 1\n",
      "2018-10-26T16:14:14.111858: step 3998, loss 0.0171886, acc 0.984375\n",
      "2018-10-26T16:14:14.434995: step 3999, loss 0.000494971, acc 1\n",
      "2018-10-26T16:14:14.771047: step 4000, loss 0.00591365, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:14:15.535006: step 4000, loss 1.46277, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4000\n",
      "\n",
      "2018-10-26T16:14:16.150365: step 4001, loss 0.00245167, acc 1\n",
      "2018-10-26T16:14:16.460534: step 4002, loss 0.000510596, acc 1\n",
      "2018-10-26T16:14:16.881476: step 4003, loss 0.00502929, acc 1\n",
      "2018-10-26T16:14:17.306274: step 4004, loss 0.0010745, acc 1\n",
      "2018-10-26T16:14:17.644370: step 4005, loss 0.00519091, acc 1\n",
      "2018-10-26T16:14:17.950562: step 4006, loss 0.00906058, acc 1\n",
      "2018-10-26T16:14:18.273913: step 4007, loss 0.0220108, acc 0.984375\n",
      "2018-10-26T16:14:18.599817: step 4008, loss 0.00308997, acc 1\n",
      "2018-10-26T16:14:18.932930: step 4009, loss 0.00337231, acc 1\n",
      "2018-10-26T16:14:19.262047: step 4010, loss 0.00275638, acc 1\n",
      "2018-10-26T16:14:19.599146: step 4011, loss 0.00358883, acc 1\n",
      "2018-10-26T16:14:19.956192: step 4012, loss 0.00136564, acc 1\n",
      "2018-10-26T16:14:20.311338: step 4013, loss 0.00580829, acc 1\n",
      "2018-10-26T16:14:20.639367: step 4014, loss 0.00223685, acc 1\n",
      "2018-10-26T16:14:20.974471: step 4015, loss 0.00418373, acc 1\n",
      "2018-10-26T16:14:21.285708: step 4016, loss 0.00713747, acc 1\n",
      "2018-10-26T16:14:21.613805: step 4017, loss 0.00346311, acc 1\n",
      "2018-10-26T16:14:21.953856: step 4018, loss 0.00136112, acc 1\n",
      "2018-10-26T16:14:22.281979: step 4019, loss 0.000697718, acc 1\n",
      "2018-10-26T16:14:22.643016: step 4020, loss 0.00197099, acc 1\n",
      "2018-10-26T16:14:22.990086: step 4021, loss 0.0216341, acc 0.984375\n",
      "2018-10-26T16:14:23.321282: step 4022, loss 0.000638196, acc 1\n",
      "2018-10-26T16:14:23.647331: step 4023, loss 0.00349984, acc 1\n",
      "2018-10-26T16:14:23.953535: step 4024, loss 0.0181485, acc 1\n",
      "2018-10-26T16:14:24.279825: step 4025, loss 0.00762534, acc 1\n",
      "2018-10-26T16:14:24.604774: step 4026, loss 0.0142385, acc 1\n",
      "2018-10-26T16:14:24.911955: step 4027, loss 0.0230804, acc 0.984375\n",
      "2018-10-26T16:14:25.236087: step 4028, loss 0.0072291, acc 1\n",
      "2018-10-26T16:14:25.594132: step 4029, loss 0.00293865, acc 1\n",
      "2018-10-26T16:14:25.916272: step 4030, loss 0.0208597, acc 0.984375\n",
      "2018-10-26T16:14:26.244395: step 4031, loss 0.0516001, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:14:26.573514: step 4032, loss 0.00899819, acc 1\n",
      "2018-10-26T16:14:26.902635: step 4033, loss 0.00263911, acc 1\n",
      "2018-10-26T16:14:27.216795: step 4034, loss 0.00739401, acc 1\n",
      "2018-10-26T16:14:27.554892: step 4035, loss 0.0100386, acc 1\n",
      "2018-10-26T16:14:27.922906: step 4036, loss 0.00365705, acc 1\n",
      "2018-10-26T16:14:28.257017: step 4037, loss 0.00628108, acc 1\n",
      "2018-10-26T16:14:28.596108: step 4038, loss 0.00757622, acc 1\n",
      "2018-10-26T16:14:28.912263: step 4039, loss 0.00284264, acc 1\n",
      "2018-10-26T16:14:29.223433: step 4040, loss 0.000871108, acc 1\n",
      "2018-10-26T16:14:29.596435: step 4041, loss 0.0115234, acc 1\n",
      "2018-10-26T16:14:29.916582: step 4042, loss 0.0145789, acc 0.984375\n",
      "2018-10-26T16:14:30.241755: step 4043, loss 0.000505556, acc 1\n",
      "2018-10-26T16:14:30.569835: step 4044, loss 0.0652875, acc 0.984375\n",
      "2018-10-26T16:14:30.939894: step 4045, loss 0.0143158, acc 1\n",
      "2018-10-26T16:14:31.271958: step 4046, loss 0.0116736, acc 1\n",
      "2018-10-26T16:14:31.601080: step 4047, loss 0.00283473, acc 1\n",
      "2018-10-26T16:14:31.922265: step 4048, loss 0.00130516, acc 1\n",
      "2018-10-26T16:14:32.242368: step 4049, loss 0.000901027, acc 1\n",
      "2018-10-26T16:14:32.570503: step 4050, loss 0.0043335, acc 1\n",
      "2018-10-26T16:14:32.881738: step 4051, loss 0.00967722, acc 1\n",
      "2018-10-26T16:14:33.212774: step 4052, loss 0.0019945, acc 1\n",
      "2018-10-26T16:14:33.549873: step 4053, loss 0.00288064, acc 1\n",
      "2018-10-26T16:14:33.874010: step 4054, loss 0.00144708, acc 1\n",
      "2018-10-26T16:14:34.209112: step 4055, loss 0.000781103, acc 1\n",
      "2018-10-26T16:14:34.579125: step 4056, loss 0.00275953, acc 1\n",
      "2018-10-26T16:14:34.926196: step 4057, loss 0.0208465, acc 0.984375\n",
      "2018-10-26T16:14:35.247338: step 4058, loss 0.00106725, acc 1\n",
      "2018-10-26T16:14:35.589427: step 4059, loss 0.00663168, acc 1\n",
      "2018-10-26T16:14:35.917579: step 4060, loss 0.00135169, acc 1\n",
      "2018-10-26T16:14:36.241681: step 4061, loss 0.000427133, acc 1\n",
      "2018-10-26T16:14:36.586758: step 4062, loss 0.010911, acc 1\n",
      "2018-10-26T16:14:36.962756: step 4063, loss 0.00162292, acc 1\n",
      "2018-10-26T16:14:37.324788: step 4064, loss 0.0564506, acc 0.984375\n",
      "2018-10-26T16:14:37.632962: step 4065, loss 0.00143186, acc 1\n",
      "2018-10-26T16:14:37.945133: step 4066, loss 0.000740663, acc 1\n",
      "2018-10-26T16:14:38.300294: step 4067, loss 0.00155941, acc 1\n",
      "2018-10-26T16:14:38.612346: step 4068, loss 0.0018985, acc 1\n",
      "2018-10-26T16:14:38.940518: step 4069, loss 0.00431465, acc 1\n",
      "2018-10-26T16:14:39.264604: step 4070, loss 0.00423044, acc 1\n",
      "2018-10-26T16:14:39.589734: step 4071, loss 0.00478059, acc 1\n",
      "2018-10-26T16:14:39.910907: step 4072, loss 0.00345356, acc 1\n",
      "2018-10-26T16:14:40.239998: step 4073, loss 0.00329078, acc 1\n",
      "2018-10-26T16:14:40.560235: step 4074, loss 0.00471556, acc 1\n",
      "2018-10-26T16:14:40.859437: step 4075, loss 0.00667919, acc 1\n",
      "2018-10-26T16:14:41.169515: step 4076, loss 0.000823658, acc 1\n",
      "2018-10-26T16:14:41.494704: step 4077, loss 0.00287558, acc 1\n",
      "2018-10-26T16:14:41.828753: step 4078, loss 0.00258886, acc 1\n",
      "2018-10-26T16:14:42.160866: step 4079, loss 0.00105513, acc 1\n",
      "2018-10-26T16:14:42.479016: step 4080, loss 0.0459531, acc 0.984375\n",
      "2018-10-26T16:14:42.794172: step 4081, loss 0.000703666, acc 1\n",
      "2018-10-26T16:14:43.122300: step 4082, loss 0.00184619, acc 1\n",
      "2018-10-26T16:14:43.451464: step 4083, loss 0.00473919, acc 1\n",
      "2018-10-26T16:14:43.782534: step 4084, loss 0.00958523, acc 1\n",
      "2018-10-26T16:14:44.118766: step 4085, loss 0.000643376, acc 1\n",
      "2018-10-26T16:14:44.449753: step 4086, loss 0.00590636, acc 1\n",
      "2018-10-26T16:14:44.758923: step 4087, loss 0.0141078, acc 1\n",
      "2018-10-26T16:14:45.072126: step 4088, loss 0.00314221, acc 1\n",
      "2018-10-26T16:14:45.414176: step 4089, loss 0.0222607, acc 0.984375\n",
      "2018-10-26T16:14:45.738662: step 4090, loss 0.00696226, acc 1\n",
      "2018-10-26T16:14:46.049562: step 4091, loss 0.00252748, acc 1\n",
      "2018-10-26T16:14:46.365632: step 4092, loss 0.00113543, acc 1\n",
      "2018-10-26T16:14:46.681847: step 4093, loss 0.00170059, acc 1\n",
      "2018-10-26T16:14:46.987110: step 4094, loss 0.0150748, acc 0.984375\n",
      "2018-10-26T16:14:47.312102: step 4095, loss 0.00232728, acc 1\n",
      "2018-10-26T16:14:47.624268: step 4096, loss 0.0076956, acc 1\n",
      "2018-10-26T16:14:47.939426: step 4097, loss 0.000654552, acc 1\n",
      "2018-10-26T16:14:48.263559: step 4098, loss 0.000946865, acc 1\n",
      "2018-10-26T16:14:48.590686: step 4099, loss 0.00220068, acc 1\n",
      "2018-10-26T16:14:48.926092: step 4100, loss 0.046376, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:14:49.717675: step 4100, loss 1.45694, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4100\n",
      "\n",
      "2018-10-26T16:14:50.371926: step 4101, loss 0.00111849, acc 1\n",
      "2018-10-26T16:14:50.889670: step 4102, loss 0.00541554, acc 1\n",
      "2018-10-26T16:14:51.405165: step 4103, loss 0.00051108, acc 1\n",
      "2018-10-26T16:14:51.733289: step 4104, loss 0.00527623, acc 1\n",
      "2018-10-26T16:14:52.071385: step 4105, loss 0.00330749, acc 1\n",
      "2018-10-26T16:14:52.441493: step 4106, loss 0.00435827, acc 1\n",
      "2018-10-26T16:14:52.834348: step 4107, loss 0.00961957, acc 1\n",
      "2018-10-26T16:14:53.185411: step 4108, loss 0.0398061, acc 0.984375\n",
      "2018-10-26T16:14:53.572418: step 4109, loss 0.00111698, acc 1\n",
      "2018-10-26T16:14:53.958347: step 4110, loss 0.00144261, acc 1\n",
      "2018-10-26T16:14:54.353290: step 4111, loss 0.00207191, acc 1\n",
      "2018-10-26T16:14:54.694378: step 4112, loss 0.00463063, acc 1\n",
      "2018-10-26T16:14:55.084336: step 4113, loss 0.0095645, acc 1\n",
      "2018-10-26T16:14:55.446368: step 4114, loss 0.00138025, acc 1\n",
      "2018-10-26T16:14:55.830345: step 4115, loss 0.00373947, acc 1\n",
      "2018-10-26T16:14:56.340983: step 4116, loss 0.00187908, acc 1\n",
      "2018-10-26T16:14:56.837651: step 4117, loss 0.00197711, acc 1\n",
      "2018-10-26T16:14:57.325348: step 4118, loss 0.00284502, acc 1\n",
      "2018-10-26T16:14:57.794096: step 4119, loss 0.0110887, acc 1\n",
      "2018-10-26T16:14:58.231925: step 4120, loss 0.00325331, acc 1\n",
      "2018-10-26T16:14:58.629862: step 4121, loss 0.00301193, acc 1\n",
      "2018-10-26T16:14:59.023810: step 4122, loss 0.00131775, acc 1\n",
      "2018-10-26T16:14:59.498541: step 4123, loss 0.00138284, acc 1\n",
      "2018-10-26T16:14:59.943352: step 4124, loss 0.0110052, acc 1\n",
      "2018-10-26T16:15:00.397141: step 4125, loss 0.00412119, acc 1\n",
      "2018-10-26T16:15:00.796074: step 4126, loss 0.00662206, acc 1\n",
      "2018-10-26T16:15:01.221936: step 4127, loss 0.00738207, acc 1\n",
      "2018-10-26T16:15:01.568013: step 4128, loss 0.00213859, acc 1\n",
      "2018-10-26T16:15:01.921070: step 4129, loss 0.00812009, acc 1\n",
      "2018-10-26T16:15:02.262158: step 4130, loss 0.000851007, acc 1\n",
      "2018-10-26T16:15:02.635161: step 4131, loss 0.0166011, acc 0.984375\n",
      "2018-10-26T16:15:02.972260: step 4132, loss 0.00352199, acc 1\n",
      "2018-10-26T16:15:03.316665: step 4133, loss 0.00376098, acc 1\n",
      "2018-10-26T16:15:03.675456: step 4134, loss 0.010717, acc 1\n",
      "2018-10-26T16:15:04.051473: step 4135, loss 0.000250232, acc 1\n",
      "2018-10-26T16:15:04.417402: step 4136, loss 0.00509252, acc 1\n",
      "2018-10-26T16:15:04.767467: step 4137, loss 0.00384817, acc 1\n",
      "2018-10-26T16:15:05.191336: step 4138, loss 0.0157501, acc 0.984375\n",
      "2018-10-26T16:15:05.564334: step 4139, loss 0.0132323, acc 1\n",
      "2018-10-26T16:15:05.909412: step 4140, loss 0.000849647, acc 1\n",
      "2018-10-26T16:15:06.252579: step 4141, loss 0.0193432, acc 0.984375\n",
      "2018-10-26T16:15:06.630486: step 4142, loss 0.00259841, acc 1\n",
      "2018-10-26T16:15:06.966588: step 4143, loss 0.000980643, acc 1\n",
      "2018-10-26T16:15:07.318647: step 4144, loss 0.000968224, acc 1\n",
      "2018-10-26T16:15:07.689736: step 4145, loss 0.0209294, acc 0.984375\n",
      "2018-10-26T16:15:08.103550: step 4146, loss 0.000980821, acc 1\n",
      "2018-10-26T16:15:08.439680: step 4147, loss 0.00202593, acc 1\n",
      "2018-10-26T16:15:08.817885: step 4148, loss 0.00111826, acc 1\n",
      "2018-10-26T16:15:09.200661: step 4149, loss 0.00484152, acc 1\n",
      "2018-10-26T16:15:09.581602: step 4150, loss 0.00234648, acc 1\n",
      "2018-10-26T16:15:10.046359: step 4151, loss 0.00212865, acc 1\n",
      "2018-10-26T16:15:10.378471: step 4152, loss 0.00235274, acc 1\n",
      "2018-10-26T16:15:10.756466: step 4153, loss 0.00211079, acc 1\n",
      "2018-10-26T16:15:11.149413: step 4154, loss 0.012139, acc 1\n",
      "2018-10-26T16:15:11.494491: step 4155, loss 0.0141446, acc 1\n",
      "2018-10-26T16:15:11.858516: step 4156, loss 0.00414074, acc 1\n",
      "2018-10-26T16:15:12.228530: step 4157, loss 0.00314177, acc 1\n",
      "2018-10-26T16:15:12.626468: step 4158, loss 0.00386472, acc 1\n",
      "2018-10-26T16:15:12.980553: step 4159, loss 0.00611205, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:15:13.344548: step 4160, loss 0.00121169, acc 1\n",
      "2018-10-26T16:15:13.718562: step 4161, loss 0.000609072, acc 1\n",
      "2018-10-26T16:15:14.069611: step 4162, loss 0.0196046, acc 0.984375\n",
      "2018-10-26T16:15:14.420672: step 4163, loss 0.0049415, acc 1\n",
      "2018-10-26T16:15:14.765753: step 4164, loss 0.00424876, acc 1\n",
      "2018-10-26T16:15:15.132768: step 4165, loss 0.0434089, acc 0.984375\n",
      "2018-10-26T16:15:15.506770: step 4166, loss 0.00156678, acc 1\n",
      "2018-10-26T16:15:15.859826: step 4167, loss 0.0384574, acc 0.984375\n",
      "2018-10-26T16:15:16.202910: step 4168, loss 0.00597553, acc 1\n",
      "2018-10-26T16:15:16.592003: step 4169, loss 0.00239231, acc 1\n",
      "2018-10-26T16:15:16.936949: step 4170, loss 0.0136368, acc 1\n",
      "2018-10-26T16:15:17.267070: step 4171, loss 0.0030101, acc 1\n",
      "2018-10-26T16:15:17.617166: step 4172, loss 0.00288824, acc 1\n",
      "2018-10-26T16:15:17.988140: step 4173, loss 0.00138947, acc 1\n",
      "2018-10-26T16:15:18.339265: step 4174, loss 0.00285031, acc 1\n",
      "2018-10-26T16:15:18.735144: step 4175, loss 0.00591245, acc 1\n",
      "2018-10-26T16:15:19.064384: step 4176, loss 0.00139958, acc 1\n",
      "2018-10-26T16:15:19.435275: step 4177, loss 0.00744058, acc 1\n",
      "2018-10-26T16:15:19.752426: step 4178, loss 0.00626977, acc 1\n",
      "2018-10-26T16:15:20.102491: step 4179, loss 0.00221337, acc 1\n",
      "2018-10-26T16:15:20.433608: step 4180, loss 0.0460618, acc 0.96875\n",
      "2018-10-26T16:15:20.804618: step 4181, loss 0.00623071, acc 1\n",
      "2018-10-26T16:15:21.167647: step 4182, loss 0.000829899, acc 1\n",
      "2018-10-26T16:15:21.540648: step 4183, loss 0.00632137, acc 1\n",
      "2018-10-26T16:15:21.847892: step 4184, loss 0.00633444, acc 1\n",
      "2018-10-26T16:15:22.212856: step 4185, loss 0.0744915, acc 0.984375\n",
      "2018-10-26T16:15:22.621759: step 4186, loss 0.000669478, acc 1\n",
      "2018-10-26T16:15:22.946891: step 4187, loss 0.0118639, acc 1\n",
      "2018-10-26T16:15:23.283991: step 4188, loss 0.00427357, acc 1\n",
      "2018-10-26T16:15:23.639044: step 4189, loss 0.000742942, acc 1\n",
      "2018-10-26T16:15:23.991222: step 4190, loss 0.00314257, acc 1\n",
      "2018-10-26T16:15:24.403004: step 4191, loss 0.000749335, acc 1\n",
      "2018-10-26T16:15:24.750074: step 4192, loss 0.00501512, acc 1\n",
      "2018-10-26T16:15:25.104128: step 4193, loss 0.00790513, acc 1\n",
      "2018-10-26T16:15:25.487103: step 4194, loss 0.0114666, acc 1\n",
      "2018-10-26T16:15:25.902992: step 4195, loss 0.00321808, acc 1\n",
      "2018-10-26T16:15:26.255051: step 4196, loss 0.00370094, acc 1\n",
      "2018-10-26T16:15:26.610104: step 4197, loss 0.00175568, acc 1\n",
      "2018-10-26T16:15:26.956179: step 4198, loss 0.0132792, acc 0.984375\n",
      "2018-10-26T16:15:27.328200: step 4199, loss 0.0284552, acc 0.984375\n",
      "2018-10-26T16:15:27.664636: step 4200, loss 0.00510112, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:15:28.556902: step 4200, loss 1.48613, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4200\n",
      "\n",
      "2018-10-26T16:15:29.282964: step 4201, loss 0.000367832, acc 1\n",
      "2018-10-26T16:15:29.662947: step 4202, loss 0.00275922, acc 1\n",
      "2018-10-26T16:15:30.132691: step 4203, loss 0.0184283, acc 0.984375\n",
      "2018-10-26T16:15:30.493727: step 4204, loss 0.00328496, acc 1\n",
      "2018-10-26T16:15:30.844788: step 4205, loss 0.0121312, acc 1\n",
      "2018-10-26T16:15:31.263722: step 4206, loss 0.00360992, acc 1\n",
      "2018-10-26T16:15:31.621768: step 4207, loss 0.00245615, acc 1\n",
      "2018-10-26T16:15:32.034986: step 4208, loss 0.00422792, acc 1\n",
      "2018-10-26T16:15:32.405619: step 4209, loss 0.00306187, acc 1\n",
      "2018-10-26T16:15:32.803649: step 4210, loss 0.00047733, acc 1\n",
      "2018-10-26T16:15:33.160680: step 4211, loss 0.01437, acc 1\n",
      "2018-10-26T16:15:33.516651: step 4212, loss 0.00297202, acc 1\n",
      "2018-10-26T16:15:33.862726: step 4213, loss 0.00141755, acc 1\n",
      "2018-10-26T16:15:34.211793: step 4214, loss 0.00135956, acc 1\n",
      "2018-10-26T16:15:34.556875: step 4215, loss 0.00441941, acc 1\n",
      "2018-10-26T16:15:34.918903: step 4216, loss 0.00376359, acc 1\n",
      "2018-10-26T16:15:35.256006: step 4217, loss 0.019105, acc 0.984375\n",
      "2018-10-26T16:15:35.606112: step 4218, loss 0.00969303, acc 1\n",
      "2018-10-26T16:15:35.949152: step 4219, loss 0.00552952, acc 1\n",
      "2018-10-26T16:15:36.289244: step 4220, loss 0.0122653, acc 1\n",
      "2018-10-26T16:15:36.635318: step 4221, loss 0.000525036, acc 1\n",
      "2018-10-26T16:15:36.979397: step 4222, loss 0.000317939, acc 1\n",
      "2018-10-26T16:15:37.331460: step 4223, loss 0.00588761, acc 1\n",
      "2018-10-26T16:15:37.708452: step 4224, loss 0.00947357, acc 1\n",
      "2018-10-26T16:15:38.060512: step 4225, loss 0.0404815, acc 0.96875\n",
      "2018-10-26T16:15:38.406628: step 4226, loss 0.00651477, acc 1\n",
      "2018-10-26T16:15:38.748670: step 4227, loss 0.0103733, acc 1\n",
      "2018-10-26T16:15:39.132645: step 4228, loss 0.00560208, acc 1\n",
      "2018-10-26T16:15:39.462763: step 4229, loss 0.00644075, acc 1\n",
      "2018-10-26T16:15:39.811903: step 4230, loss 0.00925286, acc 1\n",
      "2018-10-26T16:15:40.186831: step 4231, loss 0.00164781, acc 1\n",
      "2018-10-26T16:15:40.537891: step 4232, loss 0.00466745, acc 1\n",
      "2018-10-26T16:15:40.870134: step 4233, loss 0.00161825, acc 1\n",
      "2018-10-26T16:15:41.223094: step 4234, loss 0.001883, acc 1\n",
      "2018-10-26T16:15:41.570169: step 4235, loss 0.00509886, acc 1\n",
      "2018-10-26T16:15:41.947125: step 4236, loss 0.0106036, acc 1\n",
      "2018-10-26T16:15:42.308160: step 4237, loss 0.000896679, acc 1\n",
      "2018-10-26T16:15:42.665208: step 4238, loss 0.00539935, acc 1\n",
      "2018-10-26T16:15:43.007292: step 4239, loss 0.00164079, acc 1\n",
      "2018-10-26T16:15:43.353370: step 4240, loss 0.0137031, acc 0.984375\n",
      "2018-10-26T16:15:43.711411: step 4241, loss 0.0304703, acc 0.984375\n",
      "2018-10-26T16:15:44.038540: step 4242, loss 0.0380324, acc 0.984375\n",
      "2018-10-26T16:15:44.419518: step 4243, loss 0.00227835, acc 1\n",
      "2018-10-26T16:15:44.777568: step 4244, loss 0.00299671, acc 1\n",
      "2018-10-26T16:15:45.177496: step 4245, loss 0.0163167, acc 1\n",
      "2018-10-26T16:15:45.517625: step 4246, loss 0.000582541, acc 1\n",
      "2018-10-26T16:15:45.878624: step 4247, loss 0.0012909, acc 1\n",
      "2018-10-26T16:15:46.254621: step 4248, loss 0.0025627, acc 1\n",
      "2018-10-26T16:15:46.657577: step 4249, loss 0.0173398, acc 0.984375\n",
      "2018-10-26T16:15:47.045505: step 4250, loss 0.0037058, acc 1\n",
      "2018-10-26T16:15:47.402550: step 4251, loss 0.0061135, acc 1\n",
      "2018-10-26T16:15:47.754823: step 4252, loss 0.00595677, acc 1\n",
      "2018-10-26T16:15:48.113683: step 4253, loss 0.00356914, acc 1\n",
      "2018-10-26T16:15:48.487685: step 4254, loss 0.00115077, acc 1\n",
      "2018-10-26T16:15:48.868633: step 4255, loss 0.00112227, acc 1\n",
      "2018-10-26T16:15:49.258589: step 4256, loss 0.00206056, acc 1\n",
      "2018-10-26T16:15:49.617632: step 4257, loss 0.00371492, acc 1\n",
      "2018-10-26T16:15:50.000638: step 4258, loss 0.00216895, acc 1\n",
      "2018-10-26T16:15:50.352679: step 4259, loss 0.00225012, acc 1\n",
      "2018-10-26T16:15:50.745623: step 4260, loss 0.000831136, acc 1\n",
      "2018-10-26T16:15:51.138566: step 4261, loss 0.00817985, acc 1\n",
      "2018-10-26T16:15:51.482732: step 4262, loss 0.000930564, acc 1\n",
      "2018-10-26T16:15:51.845765: step 4263, loss 0.00415963, acc 1\n",
      "2018-10-26T16:15:52.210706: step 4264, loss 0.00973692, acc 1\n",
      "2018-10-26T16:15:52.565756: step 4265, loss 0.00452356, acc 1\n",
      "2018-10-26T16:15:52.902127: step 4266, loss 0.000730957, acc 1\n",
      "2018-10-26T16:15:53.262951: step 4267, loss 0.00164824, acc 1\n",
      "2018-10-26T16:15:53.611995: step 4268, loss 0.0272119, acc 0.984375\n",
      "2018-10-26T16:15:53.970153: step 4269, loss 0.00101747, acc 1\n",
      "2018-10-26T16:15:54.336027: step 4270, loss 0.00542665, acc 1\n",
      "2018-10-26T16:15:54.674120: step 4271, loss 0.00408976, acc 1\n",
      "2018-10-26T16:15:55.062085: step 4272, loss 0.000333706, acc 1\n",
      "2018-10-26T16:15:55.394197: step 4273, loss 0.00882697, acc 1\n",
      "2018-10-26T16:15:55.747253: step 4274, loss 0.00310098, acc 1\n",
      "2018-10-26T16:15:56.127241: step 4275, loss 0.0251551, acc 0.984375\n",
      "2018-10-26T16:15:56.454363: step 4276, loss 0.00305038, acc 1\n",
      "2018-10-26T16:15:56.790465: step 4277, loss 0.00807007, acc 1\n",
      "2018-10-26T16:15:57.166543: step 4278, loss 0.00425356, acc 1\n",
      "2018-10-26T16:15:57.519519: step 4279, loss 0.00554644, acc 1\n",
      "2018-10-26T16:15:57.873571: step 4280, loss 0.0329091, acc 0.984375\n",
      "2018-10-26T16:15:58.222646: step 4281, loss 0.00516714, acc 1\n",
      "2018-10-26T16:15:58.591686: step 4282, loss 0.0018069, acc 1\n",
      "2018-10-26T16:15:59.005591: step 4283, loss 0.00243818, acc 1\n",
      "2018-10-26T16:15:59.380594: step 4284, loss 0.00169789, acc 1\n",
      "2018-10-26T16:15:59.797432: step 4285, loss 0.00465822, acc 1\n",
      "2018-10-26T16:16:00.224292: step 4286, loss 0.00046411, acc 1\n",
      "2018-10-26T16:16:00.580341: step 4287, loss 0.00235381, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:16:00.971337: step 4288, loss 0.00120808, acc 1\n",
      "2018-10-26T16:16:01.344300: step 4289, loss 0.00596142, acc 1\n",
      "2018-10-26T16:16:01.743333: step 4290, loss 0.0176231, acc 0.984375\n",
      "2018-10-26T16:16:02.225943: step 4291, loss 0.0091533, acc 1\n",
      "2018-10-26T16:16:02.645822: step 4292, loss 0.0177534, acc 0.984375\n",
      "2018-10-26T16:16:03.109583: step 4293, loss 0.00179503, acc 1\n",
      "2018-10-26T16:16:03.573449: step 4294, loss 0.00411529, acc 1\n",
      "2018-10-26T16:16:04.026133: step 4295, loss 0.0268789, acc 0.984375\n",
      "2018-10-26T16:16:04.483911: step 4296, loss 0.000623779, acc 1\n",
      "2018-10-26T16:16:04.875865: step 4297, loss 0.000921937, acc 1\n",
      "2018-10-26T16:16:05.296738: step 4298, loss 0.0075987, acc 1\n",
      "2018-10-26T16:16:05.673749: step 4299, loss 0.00739356, acc 1\n",
      "2018-10-26T16:16:06.082638: step 4300, loss 0.00137168, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:16:07.031105: step 4300, loss 1.52126, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4300\n",
      "\n",
      "2018-10-26T16:16:07.660426: step 4301, loss 0.00253983, acc 1\n",
      "2018-10-26T16:16:08.020465: step 4302, loss 0.00113321, acc 1\n",
      "2018-10-26T16:16:08.530098: step 4303, loss 0.000637519, acc 1\n",
      "2018-10-26T16:16:08.902105: step 4304, loss 0.00070942, acc 1\n",
      "2018-10-26T16:16:09.270125: step 4305, loss 0.0022569, acc 1\n",
      "2018-10-26T16:16:09.629162: step 4306, loss 0.0016843, acc 1\n",
      "2018-10-26T16:16:09.980228: step 4307, loss 0.00173924, acc 1\n",
      "2018-10-26T16:16:10.348286: step 4308, loss 0.00475217, acc 1\n",
      "2018-10-26T16:16:10.698386: step 4309, loss 0.00873296, acc 1\n",
      "2018-10-26T16:16:11.056352: step 4310, loss 0.0126943, acc 1\n",
      "2018-10-26T16:16:11.383476: step 4311, loss 0.00834021, acc 1\n",
      "2018-10-26T16:16:11.745509: step 4312, loss 0.00259416, acc 1\n",
      "2018-10-26T16:16:12.063657: step 4313, loss 0.0010112, acc 1\n",
      "2018-10-26T16:16:12.431674: step 4314, loss 0.00761706, acc 1\n",
      "2018-10-26T16:16:12.783737: step 4315, loss 0.0158268, acc 1\n",
      "2018-10-26T16:16:13.156217: step 4316, loss 0.00496862, acc 1\n",
      "2018-10-26T16:16:13.525764: step 4317, loss 0.00253061, acc 1\n",
      "2018-10-26T16:16:13.863978: step 4318, loss 0.0389787, acc 0.984375\n",
      "2018-10-26T16:16:14.205934: step 4319, loss 0.014921, acc 0.984375\n",
      "2018-10-26T16:16:14.576943: step 4320, loss 0.000615198, acc 1\n",
      "2018-10-26T16:16:14.900079: step 4321, loss 0.00239125, acc 1\n",
      "2018-10-26T16:16:15.255233: step 4322, loss 0.000660122, acc 1\n",
      "2018-10-26T16:16:15.640103: step 4323, loss 0.00131714, acc 1\n",
      "2018-10-26T16:16:15.997149: step 4324, loss 0.000792056, acc 1\n",
      "2018-10-26T16:16:16.339237: step 4325, loss 0.00382257, acc 1\n",
      "2018-10-26T16:16:16.697278: step 4326, loss 0.00232287, acc 1\n",
      "2018-10-26T16:16:17.019420: step 4327, loss 0.00147543, acc 1\n",
      "2018-10-26T16:16:17.364495: step 4328, loss 0.00117857, acc 1\n",
      "2018-10-26T16:16:17.754453: step 4329, loss 0.00196224, acc 1\n",
      "2018-10-26T16:16:18.169344: step 4330, loss 0.00243018, acc 1\n",
      "2018-10-26T16:16:18.561297: step 4331, loss 0.0275899, acc 0.984375\n",
      "2018-10-26T16:16:18.952364: step 4332, loss 0.00194663, acc 1\n",
      "2018-10-26T16:16:19.301385: step 4333, loss 0.0076314, acc 1\n",
      "2018-10-26T16:16:19.690280: step 4334, loss 0.01833, acc 0.984375\n",
      "2018-10-26T16:16:20.078565: step 4335, loss 0.00223771, acc 1\n",
      "2018-10-26T16:16:20.458297: step 4336, loss 0.000738131, acc 1\n",
      "2018-10-26T16:16:20.796330: step 4337, loss 0.00396213, acc 1\n",
      "2018-10-26T16:16:21.155366: step 4338, loss 0.000646727, acc 1\n",
      "2018-10-26T16:16:21.554300: step 4339, loss 0.000523503, acc 1\n",
      "2018-10-26T16:16:21.908358: step 4340, loss 0.0044926, acc 1\n",
      "2018-10-26T16:16:22.283359: step 4341, loss 0.00417037, acc 1\n",
      "2018-10-26T16:16:22.677301: step 4342, loss 0.000798452, acc 1\n",
      "2018-10-26T16:16:23.165996: step 4343, loss 0.0116004, acc 1\n",
      "2018-10-26T16:16:23.562933: step 4344, loss 0.00206459, acc 1\n",
      "2018-10-26T16:16:23.962867: step 4345, loss 0.0012327, acc 1\n",
      "2018-10-26T16:16:24.354817: step 4346, loss 0.00321947, acc 1\n",
      "2018-10-26T16:16:24.680948: step 4347, loss 0.00838715, acc 1\n",
      "2018-10-26T16:16:25.090851: step 4348, loss 0.00622458, acc 1\n",
      "2018-10-26T16:16:25.491824: step 4349, loss 0.00784887, acc 1\n",
      "2018-10-26T16:16:25.824898: step 4350, loss 0.00737149, acc 1\n",
      "2018-10-26T16:16:26.199891: step 4351, loss 0.00161678, acc 1\n",
      "2018-10-26T16:16:26.675618: step 4352, loss 0.000480063, acc 1\n",
      "2018-10-26T16:16:27.019699: step 4353, loss 0.00113422, acc 1\n",
      "2018-10-26T16:16:27.501411: step 4354, loss 0.0035159, acc 1\n",
      "2018-10-26T16:16:27.888378: step 4355, loss 0.000360747, acc 1\n",
      "2018-10-26T16:16:28.221561: step 4356, loss 0.00665126, acc 1\n",
      "2018-10-26T16:16:28.563572: step 4357, loss 0.000724353, acc 1\n",
      "2018-10-26T16:16:28.927648: step 4358, loss 0.00370279, acc 1\n",
      "2018-10-26T16:16:29.432252: step 4359, loss 0.0088433, acc 1\n",
      "2018-10-26T16:16:29.756389: step 4360, loss 0.00803956, acc 1\n",
      "2018-10-26T16:16:30.191290: step 4361, loss 0.00560389, acc 1\n",
      "2018-10-26T16:16:30.554258: step 4362, loss 0.000688695, acc 1\n",
      "2018-10-26T16:16:30.902324: step 4363, loss 0.00658155, acc 1\n",
      "2018-10-26T16:16:31.252388: step 4364, loss 0.00217389, acc 1\n",
      "2018-10-26T16:16:31.622399: step 4365, loss 0.00393584, acc 1\n",
      "2018-10-26T16:16:31.978452: step 4366, loss 0.00560627, acc 1\n",
      "2018-10-26T16:16:32.352449: step 4367, loss 0.00153673, acc 1\n",
      "2018-10-26T16:16:32.750389: step 4368, loss 0.0045756, acc 1\n",
      "2018-10-26T16:16:33.208242: step 4369, loss 0.000440676, acc 1\n",
      "2018-10-26T16:16:33.626046: step 4370, loss 0.00128824, acc 1\n",
      "2018-10-26T16:16:33.967182: step 4371, loss 0.00830145, acc 1\n",
      "2018-10-26T16:16:34.338234: step 4372, loss 0.00629135, acc 1\n",
      "2018-10-26T16:16:34.687212: step 4373, loss 0.00517852, acc 1\n",
      "2018-10-26T16:16:35.061253: step 4374, loss 0.0352667, acc 0.984375\n",
      "2018-10-26T16:16:35.402303: step 4375, loss 0.000745777, acc 1\n",
      "2018-10-26T16:16:35.847114: step 4376, loss 0.000426039, acc 1\n",
      "2018-10-26T16:16:36.184211: step 4377, loss 0.0170544, acc 0.984375\n",
      "2018-10-26T16:16:36.550237: step 4378, loss 0.0056671, acc 1\n",
      "2018-10-26T16:16:36.909277: step 4379, loss 0.0016256, acc 1\n",
      "2018-10-26T16:16:37.288262: step 4380, loss 0.00185148, acc 1\n",
      "2018-10-26T16:16:37.630433: step 4381, loss 0.00204245, acc 1\n",
      "2018-10-26T16:16:38.000360: step 4382, loss 0.00162149, acc 1\n",
      "2018-10-26T16:16:38.418242: step 4383, loss 0.00572406, acc 1\n",
      "2018-10-26T16:16:38.795241: step 4384, loss 0.0234672, acc 0.984375\n",
      "2018-10-26T16:16:39.195167: step 4385, loss 0.0071066, acc 1\n",
      "2018-10-26T16:16:39.577147: step 4386, loss 0.00051576, acc 1\n",
      "2018-10-26T16:16:40.010986: step 4387, loss 0.000579103, acc 1\n",
      "2018-10-26T16:16:40.354169: step 4388, loss 0.000726865, acc 1\n",
      "2018-10-26T16:16:40.729146: step 4389, loss 0.00244129, acc 1\n",
      "2018-10-26T16:16:41.077139: step 4390, loss 0.00136871, acc 1\n",
      "2018-10-26T16:16:41.442164: step 4391, loss 0.00320029, acc 1\n",
      "2018-10-26T16:16:41.844092: step 4392, loss 0.000837756, acc 1\n",
      "2018-10-26T16:16:42.199141: step 4393, loss 0.000313078, acc 1\n",
      "2018-10-26T16:16:42.554192: step 4394, loss 0.00156561, acc 1\n",
      "2018-10-26T16:16:42.906253: step 4395, loss 0.00189841, acc 1\n",
      "2018-10-26T16:16:43.289228: step 4396, loss 0.00334583, acc 1\n",
      "2018-10-26T16:16:43.639338: step 4397, loss 0.00776805, acc 1\n",
      "2018-10-26T16:16:43.972402: step 4398, loss 0.00165518, acc 1\n",
      "2018-10-26T16:16:44.336443: step 4399, loss 0.000663232, acc 1\n",
      "2018-10-26T16:16:44.699566: step 4400, loss 0.000576451, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:16:45.550187: step 4400, loss 1.56036, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4400\n",
      "\n",
      "2018-10-26T16:16:46.201522: step 4401, loss 0.00170452, acc 1\n",
      "2018-10-26T16:16:46.583531: step 4402, loss 0.000974212, acc 1\n",
      "2018-10-26T16:16:47.131963: step 4403, loss 0.0276108, acc 0.984375\n",
      "2018-10-26T16:16:47.471091: step 4404, loss 0.00121706, acc 1\n",
      "2018-10-26T16:16:47.838113: step 4405, loss 0.00131925, acc 1\n",
      "2018-10-26T16:16:48.181157: step 4406, loss 0.000616865, acc 1\n",
      "2018-10-26T16:16:48.634946: step 4407, loss 0.0010633, acc 1\n",
      "2018-10-26T16:16:49.049839: step 4408, loss 0.000278918, acc 1\n",
      "2018-10-26T16:16:49.448775: step 4409, loss 0.00176507, acc 1\n",
      "2018-10-26T16:16:49.828755: step 4410, loss 0.00206393, acc 1\n",
      "2018-10-26T16:16:50.209741: step 4411, loss 0.00106381, acc 1\n",
      "2018-10-26T16:16:50.561850: step 4412, loss 0.000956252, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:16:50.958737: step 4413, loss 0.00233063, acc 1\n",
      "2018-10-26T16:16:51.349692: step 4414, loss 0.00120511, acc 1\n",
      "2018-10-26T16:16:51.766578: step 4415, loss 0.00163394, acc 1\n",
      "2018-10-26T16:16:52.110695: step 4416, loss 0.001061, acc 1\n",
      "2018-10-26T16:16:52.475683: step 4417, loss 0.0295083, acc 0.984375\n",
      "2018-10-26T16:16:52.850711: step 4418, loss 0.00227107, acc 1\n",
      "2018-10-26T16:16:53.196757: step 4419, loss 0.00232806, acc 1\n",
      "2018-10-26T16:16:53.539881: step 4420, loss 0.00559851, acc 1\n",
      "2018-10-26T16:16:53.894893: step 4421, loss 0.000881775, acc 1\n",
      "2018-10-26T16:16:54.262908: step 4422, loss 0.000671912, acc 1\n",
      "2018-10-26T16:16:54.645888: step 4423, loss 0.00195711, acc 1\n",
      "2018-10-26T16:16:55.005925: step 4424, loss 0.0187511, acc 0.984375\n",
      "2018-10-26T16:16:55.345019: step 4425, loss 0.00324501, acc 1\n",
      "2018-10-26T16:16:55.694147: step 4426, loss 0.00106074, acc 1\n",
      "2018-10-26T16:16:56.059108: step 4427, loss 0.00189787, acc 1\n",
      "2018-10-26T16:16:56.504917: step 4428, loss 0.00911391, acc 1\n",
      "2018-10-26T16:16:56.909962: step 4429, loss 0.0510326, acc 0.984375\n",
      "2018-10-26T16:16:57.251921: step 4430, loss 0.00811901, acc 1\n",
      "2018-10-26T16:16:57.590019: step 4431, loss 0.00113706, acc 1\n",
      "2018-10-26T16:16:57.920260: step 4432, loss 0.00342762, acc 1\n",
      "2018-10-26T16:16:58.258233: step 4433, loss 0.000593868, acc 1\n",
      "2018-10-26T16:16:58.592340: step 4434, loss 0.0030187, acc 1\n",
      "2018-10-26T16:16:58.934495: step 4435, loss 0.00708832, acc 1\n",
      "2018-10-26T16:16:59.279533: step 4436, loss 0.0068065, acc 1\n",
      "2018-10-26T16:16:59.620597: step 4437, loss 0.00520406, acc 1\n",
      "2018-10-26T16:16:59.940738: step 4438, loss 0.00159241, acc 1\n",
      "2018-10-26T16:17:00.295835: step 4439, loss 0.00279821, acc 1\n",
      "2018-10-26T16:17:00.623913: step 4440, loss 0.0014284, acc 1\n",
      "2018-10-26T16:17:00.952040: step 4441, loss 0.0019188, acc 1\n",
      "2018-10-26T16:17:01.283153: step 4442, loss 0.00845045, acc 1\n",
      "2018-10-26T16:17:01.607288: step 4443, loss 0.00120239, acc 1\n",
      "2018-10-26T16:17:01.935409: step 4444, loss 0.00118935, acc 1\n",
      "2018-10-26T16:17:02.273506: step 4445, loss 0.0134636, acc 0.984375\n",
      "2018-10-26T16:17:02.601629: step 4446, loss 0.0067405, acc 1\n",
      "2018-10-26T16:17:02.942720: step 4447, loss 0.00142596, acc 1\n",
      "2018-10-26T16:17:03.271943: step 4448, loss 0.00128775, acc 1\n",
      "2018-10-26T16:17:03.608939: step 4449, loss 0.00101562, acc 1\n",
      "2018-10-26T16:17:03.929082: step 4450, loss 0.00622664, acc 1\n",
      "2018-10-26T16:17:04.248230: step 4451, loss 0.00218495, acc 1\n",
      "2018-10-26T16:17:04.585328: step 4452, loss 0.000621052, acc 1\n",
      "2018-10-26T16:17:04.909465: step 4453, loss 0.00441767, acc 1\n",
      "2018-10-26T16:17:05.320426: step 4454, loss 0.00229934, acc 1\n",
      "2018-10-26T16:17:05.684395: step 4455, loss 0.000652799, acc 1\n",
      "2018-10-26T16:17:06.035453: step 4456, loss 0.00617756, acc 1\n",
      "2018-10-26T16:17:06.428404: step 4457, loss 0.0066946, acc 1\n",
      "2018-10-26T16:17:06.788442: step 4458, loss 0.00406833, acc 1\n",
      "2018-10-26T16:17:07.130528: step 4459, loss 0.00214695, acc 1\n",
      "2018-10-26T16:17:07.503532: step 4460, loss 0.00575806, acc 1\n",
      "2018-10-26T16:17:07.913436: step 4461, loss 0.00498785, acc 1\n",
      "2018-10-26T16:17:08.353262: step 4462, loss 0.0135296, acc 0.984375\n",
      "2018-10-26T16:17:08.836969: step 4463, loss 0.026877, acc 0.984375\n",
      "2018-10-26T16:17:09.264826: step 4464, loss 0.000747039, acc 1\n",
      "2018-10-26T16:17:09.723600: step 4465, loss 0.00237987, acc 1\n",
      "2018-10-26T16:17:10.126523: step 4466, loss 0.0559915, acc 0.984375\n",
      "2018-10-26T16:17:10.536427: step 4467, loss 0.0011498, acc 1\n",
      "2018-10-26T16:17:10.896503: step 4468, loss 0.000896718, acc 1\n",
      "2018-10-26T16:17:11.289415: step 4469, loss 0.00164283, acc 1\n",
      "2018-10-26T16:17:11.653677: step 4470, loss 0.0150395, acc 1\n",
      "2018-10-26T16:17:12.073322: step 4471, loss 0.00229347, acc 1\n",
      "2018-10-26T16:17:12.397455: step 4472, loss 0.00434653, acc 1\n",
      "2018-10-26T16:17:12.814343: step 4473, loss 0.0247836, acc 0.984375\n",
      "2018-10-26T16:17:13.222252: step 4474, loss 0.000787961, acc 1\n",
      "2018-10-26T16:17:13.608221: step 4475, loss 0.00248186, acc 1\n",
      "2018-10-26T16:17:13.947314: step 4476, loss 0.0021756, acc 1\n",
      "2018-10-26T16:17:14.282427: step 4477, loss 0.00380065, acc 1\n",
      "2018-10-26T16:17:14.607550: step 4478, loss 0.00868517, acc 1\n",
      "2018-10-26T16:17:14.943653: step 4479, loss 0.000699031, acc 1\n",
      "2018-10-26T16:17:15.288736: step 4480, loss 0.00731654, acc 1\n",
      "2018-10-26T16:17:15.654753: step 4481, loss 0.0197339, acc 0.984375\n",
      "2018-10-26T16:17:16.014816: step 4482, loss 0.00241809, acc 1\n",
      "2018-10-26T16:17:16.342916: step 4483, loss 0.000937874, acc 1\n",
      "2018-10-26T16:17:16.688989: step 4484, loss 0.00701889, acc 1\n",
      "2018-10-26T16:17:17.009132: step 4485, loss 0.0111525, acc 1\n",
      "2018-10-26T16:17:17.339251: step 4486, loss 0.000684633, acc 1\n",
      "2018-10-26T16:17:17.672362: step 4487, loss 0.00122002, acc 1\n",
      "2018-10-26T16:17:18.014451: step 4488, loss 0.0057365, acc 1\n",
      "2018-10-26T16:17:18.371552: step 4489, loss 0.000713543, acc 1\n",
      "2018-10-26T16:17:18.715574: step 4490, loss 0.000589736, acc 1\n",
      "2018-10-26T16:17:19.043699: step 4491, loss 0.00151984, acc 1\n",
      "2018-10-26T16:17:19.373839: step 4492, loss 0.000548868, acc 1\n",
      "2018-10-26T16:17:19.696956: step 4493, loss 0.00162536, acc 1\n",
      "2018-10-26T16:17:20.017100: step 4494, loss 0.00620916, acc 1\n",
      "2018-10-26T16:17:20.351240: step 4495, loss 0.0235253, acc 0.984375\n",
      "2018-10-26T16:17:20.668360: step 4496, loss 0.00388838, acc 1\n",
      "2018-10-26T16:17:21.001552: step 4497, loss 0.00183052, acc 1\n",
      "2018-10-26T16:17:21.322608: step 4498, loss 0.00141667, acc 1\n",
      "2018-10-26T16:17:21.640757: step 4499, loss 0.00359556, acc 1\n",
      "2018-10-26T16:17:21.937964: step 4500, loss 0.00262094, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:17:22.717113: step 4500, loss 1.59636, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4500\n",
      "\n",
      "2018-10-26T16:17:23.328250: step 4501, loss 0.00309005, acc 1\n",
      "2018-10-26T16:17:23.655376: step 4502, loss 0.000880799, acc 1\n",
      "2018-10-26T16:17:24.077676: step 4503, loss 0.00387031, acc 1\n",
      "2018-10-26T16:17:24.432299: step 4504, loss 0.00674626, acc 1\n",
      "2018-10-26T16:17:24.771436: step 4505, loss 0.0133677, acc 1\n",
      "2018-10-26T16:17:25.091539: step 4506, loss 0.00460215, acc 1\n",
      "2018-10-26T16:17:25.440610: step 4507, loss 0.00116576, acc 1\n",
      "2018-10-26T16:17:25.784687: step 4508, loss 0.000602566, acc 1\n",
      "2018-10-26T16:17:26.116798: step 4509, loss 0.000978126, acc 1\n",
      "2018-10-26T16:17:26.464870: step 4510, loss 0.00544703, acc 1\n",
      "2018-10-26T16:17:26.805960: step 4511, loss 0.00239589, acc 1\n",
      "2018-10-26T16:17:27.121179: step 4512, loss 0.00513493, acc 1\n",
      "2018-10-26T16:17:27.442261: step 4513, loss 0.00218715, acc 1\n",
      "2018-10-26T16:17:27.762404: step 4514, loss 0.000347839, acc 1\n",
      "2018-10-26T16:17:28.084583: step 4515, loss 0.00228917, acc 1\n",
      "2018-10-26T16:17:28.387734: step 4516, loss 0.00499225, acc 1\n",
      "2018-10-26T16:17:28.718847: step 4517, loss 0.0013482, acc 1\n",
      "2018-10-26T16:17:29.035004: step 4518, loss 0.00323196, acc 1\n",
      "2018-10-26T16:17:29.363125: step 4519, loss 0.000809988, acc 1\n",
      "2018-10-26T16:17:29.701223: step 4520, loss 0.000827504, acc 1\n",
      "2018-10-26T16:17:30.009398: step 4521, loss 0.000616192, acc 1\n",
      "2018-10-26T16:17:30.324601: step 4522, loss 0.0017281, acc 1\n",
      "2018-10-26T16:17:30.648691: step 4523, loss 0.00256564, acc 1\n",
      "2018-10-26T16:17:30.964847: step 4524, loss 0.0023406, acc 1\n",
      "2018-10-26T16:17:31.274093: step 4525, loss 0.00257088, acc 1\n",
      "2018-10-26T16:17:31.591279: step 4526, loss 0.000461054, acc 1\n",
      "2018-10-26T16:17:31.912314: step 4527, loss 0.000580057, acc 1\n",
      "2018-10-26T16:17:32.229468: step 4528, loss 0.00483676, acc 1\n",
      "2018-10-26T16:17:32.560582: step 4529, loss 0.00132269, acc 1\n",
      "2018-10-26T16:17:32.877736: step 4530, loss 0.000717286, acc 1\n",
      "2018-10-26T16:17:33.205893: step 4531, loss 0.00227789, acc 1\n",
      "2018-10-26T16:17:33.536976: step 4532, loss 0.00185129, acc 1\n",
      "2018-10-26T16:17:33.865098: step 4533, loss 0.000752958, acc 1\n",
      "2018-10-26T16:17:34.193221: step 4534, loss 0.00166166, acc 1\n",
      "2018-10-26T16:17:34.512368: step 4535, loss 0.0028967, acc 1\n",
      "2018-10-26T16:17:34.831565: step 4536, loss 0.00478048, acc 1\n",
      "2018-10-26T16:17:35.150700: step 4537, loss 0.0090919, acc 1\n",
      "2018-10-26T16:17:35.473798: step 4538, loss 0.00179326, acc 1\n",
      "2018-10-26T16:17:35.808905: step 4539, loss 0.00507476, acc 1\n",
      "2018-10-26T16:17:36.123064: step 4540, loss 0.00735445, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:17:36.450194: step 4541, loss 0.00305853, acc 1\n",
      "2018-10-26T16:17:36.788328: step 4542, loss 0.0165732, acc 0.984375\n",
      "2018-10-26T16:17:37.119403: step 4543, loss 0.00131659, acc 1\n",
      "2018-10-26T16:17:37.465481: step 4544, loss 0.00140901, acc 1\n",
      "2018-10-26T16:17:37.780637: step 4545, loss 0.00587072, acc 1\n",
      "2018-10-26T16:17:38.103773: step 4546, loss 0.0272107, acc 0.984375\n",
      "2018-10-26T16:17:38.440875: step 4547, loss 0.000307213, acc 1\n",
      "2018-10-26T16:17:38.773983: step 4548, loss 0.00123745, acc 1\n",
      "2018-10-26T16:17:39.098122: step 4549, loss 0.0051063, acc 1\n",
      "2018-10-26T16:17:39.427372: step 4550, loss 0.0026172, acc 1\n",
      "2018-10-26T16:17:39.803271: step 4551, loss 0.0153822, acc 1\n",
      "2018-10-26T16:17:40.107421: step 4552, loss 0.00219026, acc 1\n",
      "2018-10-26T16:17:40.437540: step 4553, loss 0.000111779, acc 1\n",
      "2018-10-26T16:17:40.758678: step 4554, loss 0.0030414, acc 1\n",
      "2018-10-26T16:17:41.074833: step 4555, loss 0.0117714, acc 1\n",
      "2018-10-26T16:17:41.401959: step 4556, loss 0.00106117, acc 1\n",
      "2018-10-26T16:17:41.728092: step 4557, loss 0.00367084, acc 1\n",
      "2018-10-26T16:17:42.048263: step 4558, loss 0.0182472, acc 0.984375\n",
      "2018-10-26T16:17:42.367383: step 4559, loss 0.00315418, acc 1\n",
      "2018-10-26T16:17:42.705479: step 4560, loss 0.00125585, acc 1\n",
      "2018-10-26T16:17:43.056542: step 4561, loss 0.000495725, acc 1\n",
      "2018-10-26T16:17:43.423559: step 4562, loss 0.00538646, acc 1\n",
      "2018-10-26T16:17:43.799617: step 4563, loss 0.000734561, acc 1\n",
      "2018-10-26T16:17:44.130672: step 4564, loss 0.0339173, acc 0.984375\n",
      "2018-10-26T16:17:44.464777: step 4565, loss 0.00149105, acc 1\n",
      "2018-10-26T16:17:44.806862: step 4566, loss 0.000468775, acc 1\n",
      "2018-10-26T16:17:45.142965: step 4567, loss 0.000620888, acc 1\n",
      "2018-10-26T16:17:45.475078: step 4568, loss 0.0285968, acc 0.984375\n",
      "2018-10-26T16:17:45.845158: step 4569, loss 0.0773937, acc 0.96875\n",
      "2018-10-26T16:17:46.165233: step 4570, loss 0.00185748, acc 1\n",
      "2018-10-26T16:17:46.528263: step 4571, loss 0.00147107, acc 1\n",
      "2018-10-26T16:17:46.823475: step 4572, loss 0.00520435, acc 1\n",
      "2018-10-26T16:17:47.140630: step 4573, loss 0.00352312, acc 1\n",
      "2018-10-26T16:17:47.521609: step 4574, loss 0.00470658, acc 1\n",
      "2018-10-26T16:17:47.865691: step 4575, loss 0.00212521, acc 1\n",
      "2018-10-26T16:17:48.203785: step 4576, loss 0.0129163, acc 1\n",
      "2018-10-26T16:17:48.543897: step 4577, loss 0.00329605, acc 1\n",
      "2018-10-26T16:17:48.873994: step 4578, loss 0.00396867, acc 1\n",
      "2018-10-26T16:17:49.208106: step 4579, loss 0.00132649, acc 1\n",
      "2018-10-26T16:17:49.539221: step 4580, loss 0.000932084, acc 1\n",
      "2018-10-26T16:17:49.873324: step 4581, loss 0.00136838, acc 1\n",
      "2018-10-26T16:17:50.205437: step 4582, loss 0.000319933, acc 1\n",
      "2018-10-26T16:17:50.578442: step 4583, loss 0.00170707, acc 1\n",
      "2018-10-26T16:17:50.908561: step 4584, loss 0.000546455, acc 1\n",
      "2018-10-26T16:17:51.246750: step 4585, loss 0.0239962, acc 0.984375\n",
      "2018-10-26T16:17:51.565805: step 4586, loss 0.0759424, acc 0.984375\n",
      "2018-10-26T16:17:51.889984: step 4587, loss 0.00294448, acc 1\n",
      "2018-10-26T16:17:52.221100: step 4588, loss 0.00737168, acc 1\n",
      "2018-10-26T16:17:52.545186: step 4589, loss 0.00243719, acc 1\n",
      "2018-10-26T16:17:52.895254: step 4590, loss 0.00171831, acc 1\n",
      "2018-10-26T16:17:53.225368: step 4591, loss 0.0063829, acc 1\n",
      "2018-10-26T16:17:53.567458: step 4592, loss 0.00274108, acc 1\n",
      "2018-10-26T16:17:53.897577: step 4593, loss 0.00147118, acc 1\n",
      "2018-10-26T16:17:54.217748: step 4594, loss 0.0195668, acc 0.984375\n",
      "2018-10-26T16:17:54.549831: step 4595, loss 0.0157633, acc 1\n",
      "2018-10-26T16:17:54.875963: step 4596, loss 0.0009956, acc 1\n",
      "2018-10-26T16:17:55.219044: step 4597, loss 0.000255078, acc 1\n",
      "2018-10-26T16:17:55.556145: step 4598, loss 0.0150714, acc 0.984375\n",
      "2018-10-26T16:17:55.910196: step 4599, loss 0.000308401, acc 1\n",
      "2018-10-26T16:17:56.236327: step 4600, loss 0.0014776, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:17:57.040176: step 4600, loss 1.61646, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4600\n",
      "\n",
      "2018-10-26T16:17:57.651543: step 4601, loss 0.00137351, acc 1\n",
      "2018-10-26T16:17:57.998840: step 4602, loss 0.000731349, acc 1\n",
      "2018-10-26T16:17:58.453447: step 4603, loss 0.000475294, acc 1\n",
      "2018-10-26T16:17:58.782520: step 4604, loss 0.00208836, acc 1\n",
      "2018-10-26T16:17:59.121688: step 4605, loss 0.0046341, acc 1\n",
      "2018-10-26T16:17:59.461706: step 4606, loss 0.00862147, acc 1\n",
      "2018-10-26T16:17:59.820803: step 4607, loss 0.00236032, acc 1\n",
      "2018-10-26T16:18:00.161942: step 4608, loss 0.000595441, acc 1\n",
      "2018-10-26T16:18:00.483977: step 4609, loss 0.0189154, acc 1\n",
      "2018-10-26T16:18:00.808110: step 4610, loss 0.0326335, acc 0.984375\n",
      "2018-10-26T16:18:01.124265: step 4611, loss 0.00273959, acc 1\n",
      "2018-10-26T16:18:01.462360: step 4612, loss 0.00649155, acc 1\n",
      "2018-10-26T16:18:01.783506: step 4613, loss 0.000638222, acc 1\n",
      "2018-10-26T16:18:02.103648: step 4614, loss 0.000639238, acc 1\n",
      "2018-10-26T16:18:02.512558: step 4615, loss 0.000920848, acc 1\n",
      "2018-10-26T16:18:02.840677: step 4616, loss 0.00213458, acc 1\n",
      "2018-10-26T16:18:03.162916: step 4617, loss 0.0403303, acc 0.984375\n",
      "2018-10-26T16:18:03.467004: step 4618, loss 0.00325116, acc 1\n",
      "2018-10-26T16:18:03.806099: step 4619, loss 0.00345368, acc 1\n",
      "2018-10-26T16:18:04.147188: step 4620, loss 0.00500759, acc 1\n",
      "2018-10-26T16:18:04.498315: step 4621, loss 0.00656539, acc 1\n",
      "2018-10-26T16:18:04.820393: step 4622, loss 0.0119308, acc 1\n",
      "2018-10-26T16:18:05.142528: step 4623, loss 0.0116112, acc 0.984375\n",
      "2018-10-26T16:18:05.472648: step 4624, loss 0.00071962, acc 1\n",
      "2018-10-26T16:18:05.824705: step 4625, loss 0.00169405, acc 1\n",
      "2018-10-26T16:18:06.181821: step 4626, loss 0.000949213, acc 1\n",
      "2018-10-26T16:18:06.587669: step 4627, loss 0.000956063, acc 1\n",
      "2018-10-26T16:18:06.939150: step 4628, loss 0.00416823, acc 1\n",
      "2018-10-26T16:18:07.306745: step 4629, loss 0.00801255, acc 1\n",
      "2018-10-26T16:18:07.629882: step 4630, loss 0.0182327, acc 0.984375\n",
      "2018-10-26T16:18:07.995905: step 4631, loss 0.00114306, acc 1\n",
      "2018-10-26T16:18:08.330110: step 4632, loss 0.00554204, acc 1\n",
      "2018-10-26T16:18:08.687057: step 4633, loss 0.0274338, acc 0.984375\n",
      "2018-10-26T16:18:09.041110: step 4634, loss 0.00101979, acc 1\n",
      "2018-10-26T16:18:09.367240: step 4635, loss 0.000278509, acc 1\n",
      "2018-10-26T16:18:09.721295: step 4636, loss 0.00112534, acc 1\n",
      "2018-10-26T16:18:10.050774: step 4637, loss 0.000430953, acc 1\n",
      "2018-10-26T16:18:10.387520: step 4638, loss 0.0027382, acc 1\n",
      "2018-10-26T16:18:10.764506: step 4639, loss 0.000606282, acc 1\n",
      "2018-10-26T16:18:11.106630: step 4640, loss 0.0150618, acc 1\n",
      "2018-10-26T16:18:11.458655: step 4641, loss 0.0183806, acc 0.984375\n",
      "2018-10-26T16:18:11.849608: step 4642, loss 0.00450991, acc 1\n",
      "2018-10-26T16:18:12.195683: step 4643, loss 0.000539457, acc 1\n",
      "2018-10-26T16:18:12.545747: step 4644, loss 0.000497776, acc 1\n",
      "2018-10-26T16:18:12.896810: step 4645, loss 0.0199193, acc 0.984375\n",
      "2018-10-26T16:18:13.266820: step 4646, loss 0.00198229, acc 1\n",
      "2018-10-26T16:18:13.656780: step 4647, loss 0.00116689, acc 1\n",
      "2018-10-26T16:18:14.124531: step 4648, loss 0.00469351, acc 1\n",
      "2018-10-26T16:18:14.649128: step 4649, loss 0.00152579, acc 1\n",
      "2018-10-26T16:18:15.052051: step 4650, loss 0.0032351, acc 1\n",
      "2018-10-26T16:18:15.479930: step 4651, loss 0.027493, acc 0.984375\n",
      "2018-10-26T16:18:15.883830: step 4652, loss 0.00147569, acc 1\n",
      "2018-10-26T16:18:16.289745: step 4653, loss 0.00807258, acc 1\n",
      "2018-10-26T16:18:16.678753: step 4654, loss 0.000555959, acc 1\n",
      "2018-10-26T16:18:17.120117: step 4655, loss 0.0102885, acc 1\n",
      "2018-10-26T16:18:17.523448: step 4656, loss 0.00260365, acc 1\n",
      "2018-10-26T16:18:17.901439: step 4657, loss 0.00556248, acc 1\n",
      "2018-10-26T16:18:18.291396: step 4658, loss 0.00386799, acc 1\n",
      "2018-10-26T16:18:18.673376: step 4659, loss 0.00522911, acc 1\n",
      "2018-10-26T16:18:18.979557: step 4660, loss 0.0257311, acc 0.984375\n",
      "2018-10-26T16:18:19.306740: step 4661, loss 0.00551987, acc 1\n",
      "2018-10-26T16:18:19.630999: step 4662, loss 0.00161177, acc 1\n",
      "2018-10-26T16:18:19.955952: step 4663, loss 0.000398679, acc 1\n",
      "2018-10-26T16:18:20.282121: step 4664, loss 0.00180748, acc 1\n",
      "2018-10-26T16:18:20.618226: step 4665, loss 0.00207151, acc 1\n",
      "2018-10-26T16:18:20.927355: step 4666, loss 0.00611289, acc 1\n",
      "2018-10-26T16:18:21.237528: step 4667, loss 0.00102466, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:18:21.564651: step 4668, loss 0.000697283, acc 1\n",
      "2018-10-26T16:18:21.871831: step 4669, loss 0.00435071, acc 1\n",
      "2018-10-26T16:18:22.207998: step 4670, loss 0.0156431, acc 0.984375\n",
      "2018-10-26T16:18:22.537053: step 4671, loss 0.00231055, acc 1\n",
      "2018-10-26T16:18:22.867173: step 4672, loss 0.00486104, acc 1\n",
      "2018-10-26T16:18:23.196291: step 4673, loss 0.00214906, acc 1\n",
      "2018-10-26T16:18:23.537383: step 4674, loss 0.000363893, acc 1\n",
      "2018-10-26T16:18:23.881563: step 4675, loss 0.000493557, acc 1\n",
      "2018-10-26T16:18:24.217602: step 4676, loss 0.00766178, acc 1\n",
      "2018-10-26T16:18:24.546683: step 4677, loss 0.00216472, acc 1\n",
      "2018-10-26T16:18:24.893756: step 4678, loss 0.00208129, acc 1\n",
      "2018-10-26T16:18:25.232851: step 4679, loss 0.00142982, acc 1\n",
      "2018-10-26T16:18:25.552996: step 4680, loss 0.00282559, acc 1\n",
      "2018-10-26T16:18:25.889097: step 4681, loss 0.00535304, acc 1\n",
      "2018-10-26T16:18:26.228190: step 4682, loss 0.0020654, acc 1\n",
      "2018-10-26T16:18:26.551328: step 4683, loss 0.00736865, acc 1\n",
      "2018-10-26T16:18:26.902391: step 4684, loss 0.000989209, acc 1\n",
      "2018-10-26T16:18:27.246512: step 4685, loss 0.000346388, acc 1\n",
      "2018-10-26T16:18:27.577585: step 4686, loss 0.000458676, acc 1\n",
      "2018-10-26T16:18:27.902732: step 4687, loss 0.000745981, acc 1\n",
      "2018-10-26T16:18:28.276721: step 4688, loss 0.00109775, acc 1\n",
      "2018-10-26T16:18:28.607833: step 4689, loss 0.00295122, acc 1\n",
      "2018-10-26T16:18:28.934061: step 4690, loss 0.00183592, acc 1\n",
      "2018-10-26T16:18:29.258098: step 4691, loss 0.00417334, acc 1\n",
      "2018-10-26T16:18:29.587216: step 4692, loss 0.00241208, acc 1\n",
      "2018-10-26T16:18:29.907402: step 4693, loss 0.00117728, acc 1\n",
      "2018-10-26T16:18:30.251479: step 4694, loss 0.00100221, acc 1\n",
      "2018-10-26T16:18:30.561710: step 4695, loss 0.000838185, acc 1\n",
      "2018-10-26T16:18:30.886744: step 4696, loss 0.000905151, acc 1\n",
      "2018-10-26T16:18:31.215013: step 4697, loss 0.00122183, acc 1\n",
      "2018-10-26T16:18:31.537024: step 4698, loss 0.00853747, acc 1\n",
      "2018-10-26T16:18:31.882083: step 4699, loss 0.00467957, acc 1\n",
      "2018-10-26T16:18:32.227165: step 4700, loss 0.00303667, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:18:33.008076: step 4700, loss 1.67057, acc 0.707317\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4700\n",
      "\n",
      "2018-10-26T16:18:33.589597: step 4701, loss 0.000930462, acc 1\n",
      "2018-10-26T16:18:33.902685: step 4702, loss 0.00329412, acc 1\n",
      "2018-10-26T16:18:34.338522: step 4703, loss 0.00513088, acc 1\n",
      "2018-10-26T16:18:34.683601: step 4704, loss 0.00602496, acc 1\n",
      "2018-10-26T16:18:35.033665: step 4705, loss 0.00127726, acc 1\n",
      "2018-10-26T16:18:35.368767: step 4706, loss 0.00108437, acc 1\n",
      "2018-10-26T16:18:35.699884: step 4707, loss 0.00984183, acc 1\n",
      "2018-10-26T16:18:36.034034: step 4708, loss 0.00030501, acc 1\n",
      "2018-10-26T16:18:36.387047: step 4709, loss 0.00146027, acc 1\n",
      "2018-10-26T16:18:36.744093: step 4710, loss 0.00273977, acc 1\n",
      "2018-10-26T16:18:37.072225: step 4711, loss 0.000762703, acc 1\n",
      "2018-10-26T16:18:37.401341: step 4712, loss 0.00410039, acc 1\n",
      "2018-10-26T16:18:37.713503: step 4713, loss 0.00377703, acc 1\n",
      "2018-10-26T16:18:38.055592: step 4714, loss 0.00344817, acc 1\n",
      "2018-10-26T16:18:38.383716: step 4715, loss 0.00140312, acc 1\n",
      "2018-10-26T16:18:38.709080: step 4716, loss 0.00606815, acc 1\n",
      "2018-10-26T16:18:39.064892: step 4717, loss 0.00161862, acc 1\n",
      "2018-10-26T16:18:39.390027: step 4718, loss 0.0076167, acc 1\n",
      "2018-10-26T16:18:39.730118: step 4719, loss 0.00362437, acc 1\n",
      "2018-10-26T16:18:40.070206: step 4720, loss 0.00390434, acc 1\n",
      "2018-10-26T16:18:40.402371: step 4721, loss 0.00479233, acc 1\n",
      "2018-10-26T16:18:40.723461: step 4722, loss 0.00171765, acc 1\n",
      "2018-10-26T16:18:41.058567: step 4723, loss 0.00295261, acc 1\n",
      "2018-10-26T16:18:41.375722: step 4724, loss 0.000592122, acc 1\n",
      "2018-10-26T16:18:41.687884: step 4725, loss 0.00758245, acc 1\n",
      "2018-10-26T16:18:42.012018: step 4726, loss 0.00794572, acc 1\n",
      "2018-10-26T16:18:42.337153: step 4727, loss 0.00865977, acc 1\n",
      "2018-10-26T16:18:42.643331: step 4728, loss 0.00212226, acc 1\n",
      "2018-10-26T16:18:42.971455: step 4729, loss 0.016968, acc 0.984375\n",
      "2018-10-26T16:18:43.288610: step 4730, loss 0.000533952, acc 1\n",
      "2018-10-26T16:18:43.611841: step 4731, loss 0.00337536, acc 1\n",
      "2018-10-26T16:18:43.928896: step 4732, loss 0.00325511, acc 1\n",
      "2018-10-26T16:18:44.257022: step 4733, loss 0.0144641, acc 0.984375\n",
      "2018-10-26T16:18:44.603099: step 4734, loss 0.000703562, acc 1\n",
      "2018-10-26T16:18:44.936204: step 4735, loss 0.000882415, acc 1\n",
      "2018-10-26T16:18:45.257347: step 4736, loss 0.00148791, acc 1\n",
      "2018-10-26T16:18:45.620523: step 4737, loss 0.00427647, acc 1\n",
      "2018-10-26T16:18:45.953556: step 4738, loss 0.00199058, acc 1\n",
      "2018-10-26T16:18:46.311586: step 4739, loss 0.0018217, acc 1\n",
      "2018-10-26T16:18:46.630709: step 4740, loss 0.00180282, acc 1\n",
      "2018-10-26T16:18:46.985729: step 4741, loss 0.00047902, acc 1\n",
      "2018-10-26T16:18:47.284944: step 4742, loss 0.000150253, acc 1\n",
      "2018-10-26T16:18:47.615051: step 4743, loss 0.00077238, acc 1\n",
      "2018-10-26T16:18:47.947377: step 4744, loss 0.00316265, acc 1\n",
      "2018-10-26T16:18:48.254340: step 4745, loss 0.000309716, acc 1\n",
      "2018-10-26T16:18:48.576480: step 4746, loss 0.0020504, acc 1\n",
      "2018-10-26T16:18:48.897622: step 4747, loss 0.000204191, acc 1\n",
      "2018-10-26T16:18:49.239710: step 4748, loss 0.000855313, acc 1\n",
      "2018-10-26T16:18:49.561917: step 4749, loss 0.00320906, acc 1\n",
      "2018-10-26T16:18:49.866033: step 4750, loss 0.00145753, acc 1\n",
      "2018-10-26T16:18:50.178202: step 4751, loss 0.00073355, acc 1\n",
      "2018-10-26T16:18:50.507320: step 4752, loss 0.00349126, acc 1\n",
      "2018-10-26T16:18:50.870351: step 4753, loss 0.00279606, acc 1\n",
      "2018-10-26T16:18:51.215428: step 4754, loss 0.000366327, acc 1\n",
      "2018-10-26T16:18:51.529711: step 4755, loss 0.00148889, acc 1\n",
      "2018-10-26T16:18:51.846744: step 4756, loss 0.000820938, acc 1\n",
      "2018-10-26T16:18:52.171873: step 4757, loss 0.000607722, acc 1\n",
      "2018-10-26T16:18:52.492019: step 4758, loss 0.000821925, acc 1\n",
      "2018-10-26T16:18:52.805182: step 4759, loss 0.000730854, acc 1\n",
      "2018-10-26T16:18:53.132307: step 4760, loss 0.000766651, acc 1\n",
      "2018-10-26T16:18:53.465416: step 4761, loss 0.00284095, acc 1\n",
      "2018-10-26T16:18:53.788553: step 4762, loss 0.00102973, acc 1\n",
      "2018-10-26T16:18:54.124658: step 4763, loss 0.00779183, acc 1\n",
      "2018-10-26T16:18:54.424857: step 4764, loss 0.0012237, acc 1\n",
      "2018-10-26T16:18:54.753981: step 4765, loss 0.00337122, acc 1\n",
      "2018-10-26T16:18:55.064145: step 4766, loss 0.00175784, acc 1\n",
      "2018-10-26T16:18:55.383294: step 4767, loss 0.00653048, acc 1\n",
      "2018-10-26T16:18:55.713411: step 4768, loss 0.00550193, acc 1\n",
      "2018-10-26T16:18:56.038542: step 4769, loss 0.00329218, acc 1\n",
      "2018-10-26T16:18:56.373645: step 4770, loss 0.000806095, acc 1\n",
      "2018-10-26T16:18:56.698779: step 4771, loss 0.00090072, acc 1\n",
      "2018-10-26T16:18:57.029893: step 4772, loss 0.00208162, acc 1\n",
      "2018-10-26T16:18:57.342063: step 4773, loss 0.00168792, acc 1\n",
      "2018-10-26T16:18:57.663201: step 4774, loss 0.000487915, acc 1\n",
      "2018-10-26T16:18:58.008280: step 4775, loss 0.00251754, acc 1\n",
      "2018-10-26T16:18:58.314461: step 4776, loss 0.00539279, acc 1\n",
      "2018-10-26T16:18:58.639633: step 4777, loss 0.00260282, acc 1\n",
      "2018-10-26T16:18:58.963728: step 4778, loss 0.014425, acc 0.984375\n",
      "2018-10-26T16:18:59.291951: step 4779, loss 0.000561786, acc 1\n",
      "2018-10-26T16:18:59.600027: step 4780, loss 0.00194945, acc 1\n",
      "2018-10-26T16:18:59.905210: step 4781, loss 0.00153636, acc 1\n",
      "2018-10-26T16:19:00.249290: step 4782, loss 0.00259673, acc 1\n",
      "2018-10-26T16:19:00.573424: step 4783, loss 0.0048056, acc 1\n",
      "2018-10-26T16:19:00.891619: step 4784, loss 0.000111926, acc 1\n",
      "2018-10-26T16:19:01.220696: step 4785, loss 0.000489771, acc 1\n",
      "2018-10-26T16:19:01.549818: step 4786, loss 0.00102418, acc 1\n",
      "2018-10-26T16:19:01.872952: step 4787, loss 0.00431665, acc 1\n",
      "2018-10-26T16:19:02.211099: step 4788, loss 0.000741623, acc 1\n",
      "2018-10-26T16:19:02.534187: step 4789, loss 0.00143848, acc 1\n",
      "2018-10-26T16:19:02.856330: step 4790, loss 0.000709703, acc 1\n",
      "2018-10-26T16:19:03.183452: step 4791, loss 0.00277037, acc 1\n",
      "2018-10-26T16:19:03.501600: step 4792, loss 0.00172732, acc 1\n",
      "2018-10-26T16:19:03.810779: step 4793, loss 0.00185135, acc 1\n",
      "2018-10-26T16:19:04.107982: step 4794, loss 0.0236496, acc 0.984375\n",
      "2018-10-26T16:19:04.418266: step 4795, loss 0.00396405, acc 1\n",
      "2018-10-26T16:19:04.719350: step 4796, loss 0.00312907, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:19:05.047505: step 4797, loss 0.0062611, acc 1\n",
      "2018-10-26T16:19:05.400527: step 4798, loss 0.00294997, acc 1\n",
      "2018-10-26T16:19:05.773586: step 4799, loss 0.0138326, acc 0.984375\n",
      "2018-10-26T16:19:06.095674: step 4800, loss 0.00132302, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:19:06.870644: step 4800, loss 1.66804, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4800\n",
      "\n",
      "2018-10-26T16:19:07.520897: step 4801, loss 0.00256776, acc 1\n",
      "2018-10-26T16:19:07.848988: step 4802, loss 0.00118006, acc 1\n",
      "2018-10-26T16:19:08.312746: step 4803, loss 0.0067635, acc 1\n",
      "2018-10-26T16:19:08.672868: step 4804, loss 0.00977665, acc 1\n",
      "2018-10-26T16:19:09.016864: step 4805, loss 0.00056578, acc 1\n",
      "2018-10-26T16:19:09.343991: step 4806, loss 0.000953012, acc 1\n",
      "2018-10-26T16:19:09.704030: step 4807, loss 0.00298201, acc 1\n",
      "2018-10-26T16:19:10.037140: step 4808, loss 0.000951168, acc 1\n",
      "2018-10-26T16:19:10.358282: step 4809, loss 0.000743036, acc 1\n",
      "2018-10-26T16:19:10.694387: step 4810, loss 0.000984574, acc 1\n",
      "2018-10-26T16:19:11.028491: step 4811, loss 0.00142396, acc 1\n",
      "2018-10-26T16:19:11.347667: step 4812, loss 0.00133241, acc 1\n",
      "2018-10-26T16:19:11.669781: step 4813, loss 0.00126918, acc 1\n",
      "2018-10-26T16:19:12.006880: step 4814, loss 0.00675592, acc 1\n",
      "2018-10-26T16:19:12.359933: step 4815, loss 0.00749606, acc 1\n",
      "2018-10-26T16:19:12.708003: step 4816, loss 0.00195917, acc 1\n",
      "2018-10-26T16:19:13.031141: step 4817, loss 0.0041854, acc 1\n",
      "2018-10-26T16:19:13.338322: step 4818, loss 0.00312624, acc 1\n",
      "2018-10-26T16:19:13.672457: step 4819, loss 0.00057208, acc 1\n",
      "2018-10-26T16:19:13.999572: step 4820, loss 0.000903934, acc 1\n",
      "2018-10-26T16:19:14.317706: step 4821, loss 0.000396615, acc 1\n",
      "2018-10-26T16:19:14.660786: step 4822, loss 0.047794, acc 0.984375\n",
      "2018-10-26T16:19:14.987957: step 4823, loss 0.000789395, acc 1\n",
      "2018-10-26T16:19:15.321129: step 4824, loss 0.00232306, acc 1\n",
      "2018-10-26T16:19:15.640172: step 4825, loss 0.00332791, acc 1\n",
      "2018-10-26T16:19:16.014169: step 4826, loss 0.000847392, acc 1\n",
      "2018-10-26T16:19:16.356257: step 4827, loss 0.00266354, acc 1\n",
      "2018-10-26T16:19:16.726267: step 4828, loss 0.00658119, acc 1\n",
      "2018-10-26T16:19:17.113232: step 4829, loss 0.000740357, acc 1\n",
      "2018-10-26T16:19:17.473275: step 4830, loss 0.000557675, acc 1\n",
      "2018-10-26T16:19:17.823335: step 4831, loss 0.00044277, acc 1\n",
      "2018-10-26T16:19:18.186365: step 4832, loss 0.0015167, acc 1\n",
      "2018-10-26T16:19:18.569343: step 4833, loss 0.00105843, acc 1\n",
      "2018-10-26T16:19:18.913469: step 4834, loss 0.00224304, acc 1\n",
      "2018-10-26T16:19:19.310363: step 4835, loss 0.00106034, acc 1\n",
      "2018-10-26T16:19:19.782102: step 4836, loss 0.00126315, acc 1\n",
      "2018-10-26T16:19:20.266807: step 4837, loss 0.0250531, acc 0.984375\n",
      "2018-10-26T16:19:20.718599: step 4838, loss 0.000675955, acc 1\n",
      "2018-10-26T16:19:21.128506: step 4839, loss 0.00158291, acc 1\n",
      "2018-10-26T16:19:21.535418: step 4840, loss 0.00420345, acc 1\n",
      "2018-10-26T16:19:21.938405: step 4841, loss 0.00166451, acc 1\n",
      "2018-10-26T16:19:22.292394: step 4842, loss 0.00064691, acc 1\n",
      "2018-10-26T16:19:22.730225: step 4843, loss 0.00411411, acc 1\n",
      "2018-10-26T16:19:23.121378: step 4844, loss 0.000479511, acc 1\n",
      "2018-10-26T16:19:23.441380: step 4845, loss 0.00176678, acc 1\n",
      "2018-10-26T16:19:23.862201: step 4846, loss 0.00360248, acc 1\n",
      "2018-10-26T16:19:24.207281: step 4847, loss 0.00280552, acc 1\n",
      "2018-10-26T16:19:24.613194: step 4848, loss 0.000715704, acc 1\n",
      "2018-10-26T16:19:24.946303: step 4849, loss 0.000493778, acc 1\n",
      "2018-10-26T16:19:25.295375: step 4850, loss 0.00146595, acc 1\n",
      "2018-10-26T16:19:25.628485: step 4851, loss 0.00329328, acc 1\n",
      "2018-10-26T16:19:25.962589: step 4852, loss 0.00208314, acc 1\n",
      "2018-10-26T16:19:26.273794: step 4853, loss 0.00453238, acc 1\n",
      "2018-10-26T16:19:26.603879: step 4854, loss 0.000171803, acc 1\n",
      "2018-10-26T16:19:26.950948: step 4855, loss 0.0106155, acc 1\n",
      "2018-10-26T16:19:27.314277: step 4856, loss 0.000250126, acc 1\n",
      "2018-10-26T16:19:27.644155: step 4857, loss 0.00188724, acc 1\n",
      "2018-10-26T16:19:27.979419: step 4858, loss 0.00172813, acc 1\n",
      "2018-10-26T16:19:28.280397: step 4859, loss 0.000573976, acc 1\n",
      "2018-10-26T16:19:28.605652: step 4860, loss 0.00179172, acc 1\n",
      "2018-10-26T16:19:28.944622: step 4861, loss 0.00200824, acc 1\n",
      "2018-10-26T16:19:29.284713: step 4862, loss 0.00101774, acc 1\n",
      "2018-10-26T16:19:29.612836: step 4863, loss 0.00609753, acc 1\n",
      "2018-10-26T16:19:29.931988: step 4864, loss 0.00136702, acc 1\n",
      "2018-10-26T16:19:30.259127: step 4865, loss 0.000503269, acc 1\n",
      "2018-10-26T16:19:30.596211: step 4866, loss 0.017267, acc 0.984375\n",
      "2018-10-26T16:19:30.918351: step 4867, loss 0.00776159, acc 1\n",
      "2018-10-26T16:19:31.254453: step 4868, loss 0.00568454, acc 1\n",
      "2018-10-26T16:19:31.597559: step 4869, loss 0.0541595, acc 0.984375\n",
      "2018-10-26T16:19:31.924658: step 4870, loss 0.00224869, acc 1\n",
      "2018-10-26T16:19:32.252787: step 4871, loss 0.00083266, acc 1\n",
      "2018-10-26T16:19:32.580907: step 4872, loss 0.000672981, acc 1\n",
      "2018-10-26T16:19:32.929976: step 4873, loss 0.00123724, acc 1\n",
      "2018-10-26T16:19:33.245133: step 4874, loss 0.0128036, acc 0.984375\n",
      "2018-10-26T16:19:33.569265: step 4875, loss 0.00204664, acc 1\n",
      "2018-10-26T16:19:33.891408: step 4876, loss 0.00335834, acc 1\n",
      "2018-10-26T16:19:34.223521: step 4877, loss 0.00365174, acc 1\n",
      "2018-10-26T16:19:34.546656: step 4878, loss 0.00480772, acc 1\n",
      "2018-10-26T16:19:34.864803: step 4879, loss 0.0076641, acc 1\n",
      "2018-10-26T16:19:35.177970: step 4880, loss 0.00289515, acc 1\n",
      "2018-10-26T16:19:35.502100: step 4881, loss 0.0587062, acc 0.984375\n",
      "2018-10-26T16:19:35.835211: step 4882, loss 0.00662557, acc 1\n",
      "2018-10-26T16:19:36.165329: step 4883, loss 0.00196862, acc 1\n",
      "2018-10-26T16:19:36.502429: step 4884, loss 0.0227347, acc 0.984375\n",
      "2018-10-26T16:19:36.810604: step 4885, loss 0.000897316, acc 1\n",
      "2018-10-26T16:19:37.128756: step 4886, loss 0.0119286, acc 1\n",
      "2018-10-26T16:19:37.462865: step 4887, loss 0.000799606, acc 1\n",
      "2018-10-26T16:19:37.848947: step 4888, loss 0.0176998, acc 0.984375\n",
      "2018-10-26T16:19:38.196901: step 4889, loss 0.000573575, acc 1\n",
      "2018-10-26T16:19:38.558983: step 4890, loss 0.0493393, acc 0.984375\n",
      "2018-10-26T16:19:38.916978: step 4891, loss 0.00117851, acc 1\n",
      "2018-10-26T16:19:39.286991: step 4892, loss 0.0150901, acc 0.984375\n",
      "2018-10-26T16:19:39.622114: step 4893, loss 0.000349822, acc 1\n",
      "2018-10-26T16:19:39.937250: step 4894, loss 0.00432156, acc 1\n",
      "2018-10-26T16:19:40.253981: step 4895, loss 0.00291854, acc 1\n",
      "2018-10-26T16:19:40.578538: step 4896, loss 0.00154048, acc 1\n",
      "2018-10-26T16:19:40.895694: step 4897, loss 0.0303701, acc 0.984375\n",
      "2018-10-26T16:19:41.192896: step 4898, loss 0.00174824, acc 1\n",
      "2018-10-26T16:19:41.502069: step 4899, loss 0.00491636, acc 1\n",
      "2018-10-26T16:19:41.850139: step 4900, loss 0.000649265, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:19:42.619086: step 4900, loss 1.66284, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-4900\n",
      "\n",
      "2018-10-26T16:19:43.213498: step 4901, loss 0.00422781, acc 1\n",
      "2018-10-26T16:19:43.536638: step 4902, loss 0.00244872, acc 1\n",
      "2018-10-26T16:19:43.979450: step 4903, loss 0.000551193, acc 1\n",
      "2018-10-26T16:19:44.339489: step 4904, loss 0.00244353, acc 1\n",
      "2018-10-26T16:19:44.665618: step 4905, loss 0.000647805, acc 1\n",
      "2018-10-26T16:19:44.984764: step 4906, loss 0.0036301, acc 1\n",
      "2018-10-26T16:19:45.306908: step 4907, loss 0.00360169, acc 1\n",
      "2018-10-26T16:19:45.671928: step 4908, loss 0.000192462, acc 1\n",
      "2018-10-26T16:19:45.987099: step 4909, loss 0.000280228, acc 1\n",
      "2018-10-26T16:19:46.312217: step 4910, loss 0.00135699, acc 1\n",
      "2018-10-26T16:19:46.635358: step 4911, loss 0.00440722, acc 1\n",
      "2018-10-26T16:19:46.948517: step 4912, loss 0.00611283, acc 1\n",
      "2018-10-26T16:19:47.270700: step 4913, loss 0.000587239, acc 1\n",
      "2018-10-26T16:19:47.621718: step 4914, loss 0.000419748, acc 1\n",
      "2018-10-26T16:19:47.990733: step 4915, loss 0.00520988, acc 1\n",
      "2018-10-26T16:19:48.341839: step 4916, loss 0.000374895, acc 1\n",
      "2018-10-26T16:19:48.698840: step 4917, loss 0.00583761, acc 1\n",
      "2018-10-26T16:19:49.021977: step 4918, loss 0.000804598, acc 1\n",
      "2018-10-26T16:19:49.337137: step 4919, loss 0.000414184, acc 1\n",
      "2018-10-26T16:19:49.654363: step 4920, loss 0.00170004, acc 1\n",
      "2018-10-26T16:19:49.977424: step 4921, loss 0.00269462, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:19:50.295577: step 4922, loss 0.00240342, acc 1\n",
      "2018-10-26T16:19:50.615720: step 4923, loss 0.001862, acc 1\n",
      "2018-10-26T16:19:50.943845: step 4924, loss 0.0309652, acc 0.984375\n",
      "2018-10-26T16:19:51.280968: step 4925, loss 0.000941803, acc 1\n",
      "2018-10-26T16:19:51.623124: step 4926, loss 0.00363699, acc 1\n",
      "2018-10-26T16:19:51.957139: step 4927, loss 0.00307588, acc 1\n",
      "2018-10-26T16:19:52.293241: step 4928, loss 0.0121491, acc 1\n",
      "2018-10-26T16:19:52.618369: step 4929, loss 0.000176483, acc 1\n",
      "2018-10-26T16:19:52.955469: step 4930, loss 0.00617568, acc 1\n",
      "2018-10-26T16:19:53.299553: step 4931, loss 0.00283362, acc 1\n",
      "2018-10-26T16:19:53.659748: step 4932, loss 0.00634385, acc 1\n",
      "2018-10-26T16:19:53.986716: step 4933, loss 0.0124713, acc 1\n",
      "2018-10-26T16:19:54.323019: step 4934, loss 0.000708652, acc 1\n",
      "2018-10-26T16:19:54.653931: step 4935, loss 0.0192032, acc 1\n",
      "2018-10-26T16:19:54.991031: step 4936, loss 0.000968412, acc 1\n",
      "2018-10-26T16:19:55.322234: step 4937, loss 0.00761437, acc 1\n",
      "2018-10-26T16:19:55.656252: step 4938, loss 0.00177844, acc 1\n",
      "2018-10-26T16:19:55.993351: step 4939, loss 0.00150076, acc 1\n",
      "2018-10-26T16:19:56.318485: step 4940, loss 0.0048022, acc 1\n",
      "2018-10-26T16:19:56.655582: step 4941, loss 0.000696986, acc 1\n",
      "2018-10-26T16:19:56.981710: step 4942, loss 0.00517347, acc 1\n",
      "2018-10-26T16:19:57.315821: step 4943, loss 0.0134778, acc 0.984375\n",
      "2018-10-26T16:19:57.685829: step 4944, loss 0.00380584, acc 1\n",
      "2018-10-26T16:19:58.034896: step 4945, loss 0.0020439, acc 1\n",
      "2018-10-26T16:19:58.367010: step 4946, loss 0.000948804, acc 1\n",
      "2018-10-26T16:19:58.689190: step 4947, loss 0.00181071, acc 1\n",
      "2018-10-26T16:19:59.009343: step 4948, loss 0.00974235, acc 1\n",
      "2018-10-26T16:19:59.352583: step 4949, loss 0.00595638, acc 1\n",
      "2018-10-26T16:19:59.673580: step 4950, loss 0.00857292, acc 1\n",
      "2018-10-26T16:20:00.069494: step 4951, loss 0.0014029, acc 1\n",
      "2018-10-26T16:20:00.403567: step 4952, loss 0.000264751, acc 1\n",
      "2018-10-26T16:20:00.752636: step 4953, loss 0.00128713, acc 1\n",
      "2018-10-26T16:20:01.067828: step 4954, loss 0.00324881, acc 1\n",
      "2018-10-26T16:20:01.391927: step 4955, loss 0.000993187, acc 1\n",
      "2018-10-26T16:20:01.746979: step 4956, loss 0.00253552, acc 1\n",
      "2018-10-26T16:20:02.083110: step 4957, loss 0.00255524, acc 1\n",
      "2018-10-26T16:20:02.418185: step 4958, loss 0.00679706, acc 1\n",
      "2018-10-26T16:20:02.769250: step 4959, loss 0.000434737, acc 1\n",
      "2018-10-26T16:20:03.101442: step 4960, loss 0.00106218, acc 1\n",
      "2018-10-26T16:20:03.431524: step 4961, loss 0.0012798, acc 1\n",
      "2018-10-26T16:20:03.749628: step 4962, loss 0.00123746, acc 1\n",
      "2018-10-26T16:20:04.074758: step 4963, loss 0.0358453, acc 0.984375\n",
      "2018-10-26T16:20:04.408897: step 4964, loss 0.00252955, acc 1\n",
      "2018-10-26T16:20:04.716046: step 4965, loss 0.0191226, acc 0.984375\n",
      "2018-10-26T16:20:05.060128: step 4966, loss 0.000795231, acc 1\n",
      "2018-10-26T16:20:05.443103: step 4967, loss 0.000940685, acc 1\n",
      "2018-10-26T16:20:05.759285: step 4968, loss 0.000721016, acc 1\n",
      "2018-10-26T16:20:06.185128: step 4969, loss 0.000173671, acc 1\n",
      "2018-10-26T16:20:06.551147: step 4970, loss 0.00261688, acc 1\n",
      "2018-10-26T16:20:06.896221: step 4971, loss 0.000825455, acc 1\n",
      "2018-10-26T16:20:07.353020: step 4972, loss 0.00436927, acc 1\n",
      "2018-10-26T16:20:07.682169: step 4973, loss 0.00402835, acc 1\n",
      "2018-10-26T16:20:08.032187: step 4974, loss 0.00192631, acc 1\n",
      "2018-10-26T16:20:08.523873: step 4975, loss 0.000780281, acc 1\n",
      "2018-10-26T16:20:08.885907: step 4976, loss 0.00122692, acc 1\n",
      "2018-10-26T16:20:09.224999: step 4977, loss 0.0329266, acc 0.984375\n",
      "2018-10-26T16:20:09.556117: step 4978, loss 0.00434017, acc 1\n",
      "2018-10-26T16:20:09.883240: step 4979, loss 0.00406124, acc 1\n",
      "2018-10-26T16:20:10.213361: step 4980, loss 0.00130528, acc 1\n",
      "2018-10-26T16:20:10.535497: step 4981, loss 9.2527e-05, acc 1\n",
      "2018-10-26T16:20:10.854646: step 4982, loss 0.0198714, acc 0.984375\n",
      "2018-10-26T16:20:11.197731: step 4983, loss 0.05811, acc 0.984375\n",
      "2018-10-26T16:20:11.524853: step 4984, loss 0.00650162, acc 1\n",
      "2018-10-26T16:20:11.850081: step 4985, loss 0.0262855, acc 0.984375\n",
      "2018-10-26T16:20:12.220997: step 4986, loss 0.00225778, acc 1\n",
      "2018-10-26T16:20:12.587017: step 4987, loss 0.00276066, acc 1\n",
      "2018-10-26T16:20:12.941185: step 4988, loss 0.0158153, acc 0.984375\n",
      "2018-10-26T16:20:13.268196: step 4989, loss 0.00589428, acc 1\n",
      "2018-10-26T16:20:13.571424: step 4990, loss 0.00146725, acc 1\n",
      "2018-10-26T16:20:13.948380: step 4991, loss 0.00309303, acc 1\n",
      "2018-10-26T16:20:14.312408: step 4992, loss 0.0117659, acc 1\n",
      "2018-10-26T16:20:14.637540: step 4993, loss 0.00333273, acc 1\n",
      "2018-10-26T16:20:14.958678: step 4994, loss 0.000504751, acc 1\n",
      "2018-10-26T16:20:15.370639: step 4995, loss 0.000599454, acc 1\n",
      "2018-10-26T16:20:15.767519: step 4996, loss 0.000572828, acc 1\n",
      "2018-10-26T16:20:16.185405: step 4997, loss 0.0134128, acc 1\n",
      "2018-10-26T16:20:16.516789: step 4998, loss 0.00583671, acc 1\n",
      "2018-10-26T16:20:16.837657: step 4999, loss 0.00415375, acc 1\n",
      "2018-10-26T16:20:17.171767: step 5000, loss 0.00420167, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:20:17.951753: step 5000, loss 1.72076, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5000\n",
      "\n",
      "2018-10-26T16:20:18.529139: step 5001, loss 0.00217994, acc 1\n",
      "2018-10-26T16:20:18.852469: step 5002, loss 0.0159501, acc 0.984375\n",
      "2018-10-26T16:20:19.282127: step 5003, loss 0.0025977, acc 1\n",
      "2018-10-26T16:20:19.632193: step 5004, loss 0.000748109, acc 1\n",
      "2018-10-26T16:20:19.957323: step 5005, loss 0.00091723, acc 1\n",
      "2018-10-26T16:20:20.266500: step 5006, loss 0.00064926, acc 1\n",
      "2018-10-26T16:20:20.578793: step 5007, loss 0.139514, acc 0.984375\n",
      "2018-10-26T16:20:20.889875: step 5008, loss 0.00105132, acc 1\n",
      "2018-10-26T16:20:21.205986: step 5009, loss 0.000835363, acc 1\n",
      "2018-10-26T16:20:21.622876: step 5010, loss 0.00583884, acc 1\n",
      "2018-10-26T16:20:22.015824: step 5011, loss 0.000694515, acc 1\n",
      "2018-10-26T16:20:22.392815: step 5012, loss 0.00189429, acc 1\n",
      "2018-10-26T16:20:22.783799: step 5013, loss 0.000731588, acc 1\n",
      "2018-10-26T16:20:23.136865: step 5014, loss 0.00220459, acc 1\n",
      "2018-10-26T16:20:23.507836: step 5015, loss 0.000374619, acc 1\n",
      "2018-10-26T16:20:23.928713: step 5016, loss 0.00671921, acc 1\n",
      "2018-10-26T16:20:24.270894: step 5017, loss 0.000296493, acc 1\n",
      "2018-10-26T16:20:24.729576: step 5018, loss 0.00186819, acc 1\n",
      "2018-10-26T16:20:25.181365: step 5019, loss 0.00124564, acc 1\n",
      "2018-10-26T16:20:25.656196: step 5020, loss 0.00446219, acc 1\n",
      "2018-10-26T16:20:26.054034: step 5021, loss 0.00431428, acc 1\n",
      "2018-10-26T16:20:26.471918: step 5022, loss 0.0243056, acc 0.984375\n",
      "2018-10-26T16:20:26.885811: step 5023, loss 0.00779198, acc 1\n",
      "2018-10-26T16:20:27.348575: step 5024, loss 0.000706494, acc 1\n",
      "2018-10-26T16:20:27.743519: step 5025, loss 0.00150725, acc 1\n",
      "2018-10-26T16:20:28.138464: step 5026, loss 0.000903729, acc 1\n",
      "2018-10-26T16:20:28.543413: step 5027, loss 0.00378138, acc 1\n",
      "2018-10-26T16:20:28.954283: step 5028, loss 0.000354442, acc 1\n",
      "2018-10-26T16:20:29.419449: step 5029, loss 0.00137464, acc 1\n",
      "2018-10-26T16:20:29.809000: step 5030, loss 0.000269138, acc 1\n",
      "2018-10-26T16:20:30.151271: step 5031, loss 0.000404072, acc 1\n",
      "2018-10-26T16:20:30.454292: step 5032, loss 0.00668279, acc 1\n",
      "2018-10-26T16:20:30.858237: step 5033, loss 0.000508927, acc 1\n",
      "2018-10-26T16:20:31.179367: step 5034, loss 0.000946403, acc 1\n",
      "2018-10-26T16:20:31.463580: step 5035, loss 0.0118751, acc 1\n",
      "2018-10-26T16:20:31.767768: step 5036, loss 0.00151271, acc 1\n",
      "2018-10-26T16:20:32.039067: step 5037, loss 0.000674063, acc 1\n",
      "2018-10-26T16:20:32.332258: step 5038, loss 0.0021836, acc 1\n",
      "2018-10-26T16:20:32.647485: step 5039, loss 0.0158961, acc 0.984375\n",
      "2018-10-26T16:20:33.005502: step 5040, loss 0.0165239, acc 0.984375\n",
      "2018-10-26T16:20:33.322612: step 5041, loss 0.00027258, acc 1\n",
      "2018-10-26T16:20:33.602863: step 5042, loss 0.0218754, acc 0.984375\n",
      "2018-10-26T16:20:33.938966: step 5043, loss 0.00410161, acc 1\n",
      "2018-10-26T16:20:34.254124: step 5044, loss 0.0137928, acc 0.984375\n",
      "2018-10-26T16:20:34.544347: step 5045, loss 0.00220373, acc 1\n",
      "2018-10-26T16:20:34.869479: step 5046, loss 0.0572644, acc 0.984375\n",
      "2018-10-26T16:20:35.176694: step 5047, loss 0.000459168, acc 1\n",
      "2018-10-26T16:20:35.492813: step 5048, loss 0.00265645, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:20:35.816986: step 5049, loss 0.0353704, acc 0.984375\n",
      "2018-10-26T16:20:36.103185: step 5050, loss 0.000193437, acc 1\n",
      "2018-10-26T16:20:36.386442: step 5051, loss 0.00127298, acc 1\n",
      "2018-10-26T16:20:36.696598: step 5052, loss 0.0118318, acc 1\n",
      "2018-10-26T16:20:37.011842: step 5053, loss 0.00167295, acc 1\n",
      "2018-10-26T16:20:37.268102: step 5054, loss 0.00527862, acc 1\n",
      "2018-10-26T16:20:37.548322: step 5055, loss 0.00116052, acc 1\n",
      "2018-10-26T16:20:37.836552: step 5056, loss 0.00034086, acc 1\n",
      "2018-10-26T16:20:38.119796: step 5057, loss 0.00026884, acc 1\n",
      "2018-10-26T16:20:38.408026: step 5058, loss 0.00364011, acc 1\n",
      "2018-10-26T16:20:38.723218: step 5059, loss 0.0094243, acc 1\n",
      "2018-10-26T16:20:39.057344: step 5060, loss 0.00128184, acc 1\n",
      "2018-10-26T16:20:39.352522: step 5061, loss 0.0445248, acc 0.984375\n",
      "2018-10-26T16:20:39.688603: step 5062, loss 0.0016404, acc 1\n",
      "2018-10-26T16:20:39.940929: step 5063, loss 0.00280156, acc 1\n",
      "2018-10-26T16:20:40.223333: step 5064, loss 0.00548287, acc 1\n",
      "2018-10-26T16:20:40.522494: step 5065, loss 0.00359438, acc 1\n",
      "2018-10-26T16:20:40.839560: step 5066, loss 0.000288558, acc 1\n",
      "2018-10-26T16:20:41.156684: step 5067, loss 0.00365749, acc 1\n",
      "2018-10-26T16:20:41.409030: step 5068, loss 0.00486232, acc 1\n",
      "2018-10-26T16:20:41.712224: step 5069, loss 0.00342553, acc 1\n",
      "2018-10-26T16:20:42.030347: step 5070, loss 0.00156153, acc 1\n",
      "2018-10-26T16:20:42.350661: step 5071, loss 0.00139625, acc 1\n",
      "2018-10-26T16:20:42.638721: step 5072, loss 0.0137762, acc 1\n",
      "2018-10-26T16:20:42.927948: step 5073, loss 0.0131879, acc 1\n",
      "2018-10-26T16:20:43.232136: step 5074, loss 0.000409171, acc 1\n",
      "2018-10-26T16:20:43.549291: step 5075, loss 0.028589, acc 0.984375\n",
      "2018-10-26T16:20:43.847493: step 5076, loss 0.00337881, acc 1\n",
      "2018-10-26T16:20:44.146692: step 5077, loss 0.0883595, acc 0.984375\n",
      "2018-10-26T16:20:44.437951: step 5078, loss 0.00235329, acc 1\n",
      "2018-10-26T16:20:44.756094: step 5079, loss 9.19773e-05, acc 1\n",
      "2018-10-26T16:20:45.048792: step 5080, loss 0.000268516, acc 1\n",
      "2018-10-26T16:20:45.317593: step 5081, loss 0.0246672, acc 0.984375\n",
      "2018-10-26T16:20:45.585865: step 5082, loss 0.000215406, acc 1\n",
      "2018-10-26T16:20:45.944891: step 5083, loss 0.00719093, acc 1\n",
      "2018-10-26T16:20:46.288969: step 5084, loss 0.00614019, acc 1\n",
      "2018-10-26T16:20:46.594627: step 5085, loss 0.00356002, acc 1\n",
      "2018-10-26T16:20:46.878423: step 5086, loss 0.0108528, acc 1\n",
      "2018-10-26T16:20:47.195671: step 5087, loss 0.000655692, acc 1\n",
      "2018-10-26T16:20:47.485770: step 5088, loss 0.000396575, acc 1\n",
      "2018-10-26T16:20:47.786990: step 5089, loss 0.000598994, acc 1\n",
      "2018-10-26T16:20:48.108111: step 5090, loss 0.000863159, acc 1\n",
      "2018-10-26T16:20:48.430247: step 5091, loss 0.0119874, acc 0.984375\n",
      "2018-10-26T16:20:48.722467: step 5092, loss 0.00952673, acc 1\n",
      "2018-10-26T16:20:49.088489: step 5093, loss 0.000654996, acc 1\n",
      "2018-10-26T16:20:49.440724: step 5094, loss 0.00573736, acc 1\n",
      "2018-10-26T16:20:49.719915: step 5095, loss 0.00226579, acc 1\n",
      "2018-10-26T16:20:50.051916: step 5096, loss 0.0310542, acc 0.984375\n",
      "2018-10-26T16:20:50.365110: step 5097, loss 0.00623755, acc 1\n",
      "2018-10-26T16:20:50.756106: step 5098, loss 0.00153643, acc 1\n",
      "2018-10-26T16:20:51.086152: step 5099, loss 0.0033198, acc 1\n",
      "2018-10-26T16:20:51.383419: step 5100, loss 0.00489075, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:20:52.109446: step 5100, loss 1.73577, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5100\n",
      "\n",
      "2018-10-26T16:20:52.666928: step 5101, loss 0.000975535, acc 1\n",
      "2018-10-26T16:20:53.163703: step 5102, loss 0.00265305, acc 1\n",
      "2018-10-26T16:20:53.460853: step 5103, loss 0.00254689, acc 1\n",
      "2018-10-26T16:20:53.789926: step 5104, loss 0.00242147, acc 1\n",
      "2018-10-26T16:20:54.068236: step 5105, loss 0.0149079, acc 0.984375\n",
      "2018-10-26T16:20:54.379352: step 5106, loss 0.00228421, acc 1\n",
      "2018-10-26T16:20:54.694545: step 5107, loss 0.00146936, acc 1\n",
      "2018-10-26T16:20:55.087461: step 5108, loss 0.00112545, acc 1\n",
      "2018-10-26T16:20:55.434823: step 5109, loss 0.0203578, acc 0.984375\n",
      "2018-10-26T16:20:55.729746: step 5110, loss 0.0018114, acc 1\n",
      "2018-10-26T16:20:56.035925: step 5111, loss 0.0679172, acc 0.984375\n",
      "2018-10-26T16:20:56.350119: step 5112, loss 0.012573, acc 1\n",
      "2018-10-26T16:20:56.649289: step 5113, loss 0.0282966, acc 0.96875\n",
      "2018-10-26T16:20:57.056201: step 5114, loss 0.00676062, acc 1\n",
      "2018-10-26T16:20:57.369363: step 5115, loss 0.000928865, acc 1\n",
      "2018-10-26T16:20:57.643665: step 5116, loss 0.000936016, acc 1\n",
      "2018-10-26T16:20:57.916983: step 5117, loss 0.00624197, acc 1\n",
      "2018-10-26T16:20:58.199173: step 5118, loss 0.00143082, acc 1\n",
      "2018-10-26T16:20:58.483387: step 5119, loss 0.00132569, acc 1\n",
      "2018-10-26T16:20:58.801538: step 5120, loss 0.00614569, acc 1\n",
      "2018-10-26T16:20:59.113703: step 5121, loss 0.00118915, acc 1\n",
      "2018-10-26T16:20:59.442824: step 5122, loss 0.0025889, acc 1\n",
      "2018-10-26T16:20:59.738081: step 5123, loss 0.00160631, acc 1\n",
      "2018-10-26T16:21:00.041306: step 5124, loss 0.019313, acc 0.984375\n",
      "2018-10-26T16:21:00.364552: step 5125, loss 0.00506422, acc 1\n",
      "2018-10-26T16:21:00.683536: step 5126, loss 0.00352282, acc 1\n",
      "2018-10-26T16:21:00.969792: step 5127, loss 0.00115244, acc 1\n",
      "2018-10-26T16:21:01.276923: step 5128, loss 0.000231645, acc 1\n",
      "2018-10-26T16:21:01.585137: step 5129, loss 0.0100869, acc 1\n",
      "2018-10-26T16:21:01.895323: step 5130, loss 0.000540875, acc 1\n",
      "2018-10-26T16:21:02.156600: step 5131, loss 0.00122364, acc 1\n",
      "2018-10-26T16:21:02.463892: step 5132, loss 0.00448011, acc 1\n",
      "2018-10-26T16:21:02.776916: step 5133, loss 0.015678, acc 0.984375\n",
      "2018-10-26T16:21:03.088161: step 5134, loss 0.00138702, acc 1\n",
      "2018-10-26T16:21:03.378308: step 5135, loss 0.00987494, acc 1\n",
      "2018-10-26T16:21:03.701475: step 5136, loss 0.00641573, acc 1\n",
      "2018-10-26T16:21:04.012616: step 5137, loss 0.0190227, acc 0.984375\n",
      "2018-10-26T16:21:04.308822: step 5138, loss 0.000985601, acc 1\n",
      "2018-10-26T16:21:04.640994: step 5139, loss 0.00167986, acc 1\n",
      "2018-10-26T16:21:04.964070: step 5140, loss 0.00781549, acc 1\n",
      "2018-10-26T16:21:05.247314: step 5141, loss 0.0115659, acc 1\n",
      "2018-10-26T16:21:05.559480: step 5142, loss 0.0010214, acc 1\n",
      "2018-10-26T16:21:05.848707: step 5143, loss 0.00186948, acc 1\n",
      "2018-10-26T16:21:06.129985: step 5144, loss 0.0106242, acc 1\n",
      "2018-10-26T16:21:06.489096: step 5145, loss 0.0355177, acc 0.984375\n",
      "2018-10-26T16:21:06.830085: step 5146, loss 0.00831346, acc 1\n",
      "2018-10-26T16:21:07.126294: step 5147, loss 0.0268714, acc 0.984375\n",
      "2018-10-26T16:21:07.443489: step 5148, loss 0.00046815, acc 1\n",
      "2018-10-26T16:21:07.763592: step 5149, loss 0.000246537, acc 1\n",
      "2018-10-26T16:21:08.123629: step 5150, loss 0.00148776, acc 1\n",
      "2018-10-26T16:21:08.458733: step 5151, loss 0.0135699, acc 1\n",
      "2018-10-26T16:21:08.786883: step 5152, loss 0.00488401, acc 1\n",
      "2018-10-26T16:21:09.103015: step 5153, loss 0.00277951, acc 1\n",
      "2018-10-26T16:21:09.456133: step 5154, loss 0.0123092, acc 0.984375\n",
      "2018-10-26T16:21:09.768460: step 5155, loss 0.00120946, acc 1\n",
      "2018-10-26T16:21:10.104379: step 5156, loss 0.00713048, acc 1\n",
      "2018-10-26T16:21:10.417538: step 5157, loss 0.0017408, acc 1\n",
      "2018-10-26T16:21:10.742633: step 5158, loss 0.00581137, acc 1\n",
      "2018-10-26T16:21:11.049813: step 5159, loss 0.0013647, acc 1\n",
      "2018-10-26T16:21:11.366987: step 5160, loss 0.00175949, acc 1\n",
      "2018-10-26T16:21:11.670154: step 5161, loss 0.00140488, acc 1\n",
      "2018-10-26T16:21:11.992292: step 5162, loss 0.00578401, acc 1\n",
      "2018-10-26T16:21:12.333391: step 5163, loss 0.00103708, acc 1\n",
      "2018-10-26T16:21:12.675468: step 5164, loss 0.00429049, acc 1\n",
      "2018-10-26T16:21:13.014561: step 5165, loss 0.0423411, acc 0.984375\n",
      "2018-10-26T16:21:13.361674: step 5166, loss 0.000631515, acc 1\n",
      "2018-10-26T16:21:13.698733: step 5167, loss 0.0156111, acc 1\n",
      "2018-10-26T16:21:14.055780: step 5168, loss 0.00316487, acc 1\n",
      "2018-10-26T16:21:14.382905: step 5169, loss 0.000235201, acc 1\n",
      "2018-10-26T16:21:14.734968: step 5170, loss 0.000317846, acc 1\n",
      "2018-10-26T16:21:15.087027: step 5171, loss 9.81536e-05, acc 1\n",
      "2018-10-26T16:21:15.451054: step 5172, loss 0.01395, acc 1\n",
      "2018-10-26T16:21:15.798125: step 5173, loss 0.000115939, acc 1\n",
      "2018-10-26T16:21:16.137219: step 5174, loss 0.000433038, acc 1\n",
      "2018-10-26T16:21:16.468338: step 5175, loss 0.00698099, acc 1\n",
      "2018-10-26T16:21:16.817401: step 5176, loss 0.000255723, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:21:17.160522: step 5177, loss 0.000676255, acc 1\n",
      "2018-10-26T16:21:17.499579: step 5178, loss 0.00312408, acc 1\n",
      "2018-10-26T16:21:17.847684: step 5179, loss 0.00147281, acc 1\n",
      "2018-10-26T16:21:18.180759: step 5180, loss 0.00356585, acc 1\n",
      "2018-10-26T16:21:18.527834: step 5181, loss 0.000390143, acc 1\n",
      "2018-10-26T16:21:18.887873: step 5182, loss 0.00180283, acc 1\n",
      "2018-10-26T16:21:19.222977: step 5183, loss 0.00476295, acc 1\n",
      "2018-10-26T16:21:19.605952: step 5184, loss 0.000805955, acc 1\n",
      "2018-10-26T16:21:19.963993: step 5185, loss 0.0308286, acc 0.984375\n",
      "2018-10-26T16:21:20.333008: step 5186, loss 0.00092071, acc 1\n",
      "2018-10-26T16:21:20.687063: step 5187, loss 0.00185967, acc 1\n",
      "2018-10-26T16:21:21.057074: step 5188, loss 0.00700114, acc 1\n",
      "2018-10-26T16:21:21.400158: step 5189, loss 0.000506847, acc 1\n",
      "2018-10-26T16:21:21.760198: step 5190, loss 0.0112814, acc 1\n",
      "2018-10-26T16:21:22.133200: step 5191, loss 0.00161575, acc 1\n",
      "2018-10-26T16:21:22.471365: step 5192, loss 0.000409428, acc 1\n",
      "2018-10-26T16:21:22.795433: step 5193, loss 0.00114625, acc 1\n",
      "2018-10-26T16:21:23.124551: step 5194, loss 0.00507979, acc 1\n",
      "2018-10-26T16:21:23.436842: step 5195, loss 0.000273412, acc 1\n",
      "2018-10-26T16:21:23.784789: step 5196, loss 0.00370965, acc 1\n",
      "2018-10-26T16:21:24.106923: step 5197, loss 0.00210889, acc 1\n",
      "2018-10-26T16:21:24.460031: step 5198, loss 0.058127, acc 0.984375\n",
      "2018-10-26T16:21:24.798077: step 5199, loss 0.000400225, acc 1\n",
      "2018-10-26T16:21:25.134181: step 5200, loss 0.00809362, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:21:25.925067: step 5200, loss 1.74306, acc 0.726079\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5200\n",
      "\n",
      "2018-10-26T16:21:26.607245: step 5201, loss 0.000581871, acc 1\n",
      "2018-10-26T16:21:26.933373: step 5202, loss 0.00391749, acc 1\n",
      "2018-10-26T16:21:27.343283: step 5203, loss 0.00129291, acc 1\n",
      "2018-10-26T16:21:27.676462: step 5204, loss 0.000149085, acc 1\n",
      "2018-10-26T16:21:28.004512: step 5205, loss 0.0011889, acc 1\n",
      "2018-10-26T16:21:28.350587: step 5206, loss 0.000540194, acc 1\n",
      "2018-10-26T16:21:28.750581: step 5207, loss 0.00155802, acc 1\n",
      "2018-10-26T16:21:29.129505: step 5208, loss 0.000511384, acc 1\n",
      "2018-10-26T16:21:29.493532: step 5209, loss 0.00135271, acc 1\n",
      "2018-10-26T16:21:29.912462: step 5210, loss 0.00473372, acc 1\n",
      "2018-10-26T16:21:30.306364: step 5211, loss 0.00184639, acc 1\n",
      "2018-10-26T16:21:30.759150: step 5212, loss 0.000347855, acc 1\n",
      "2018-10-26T16:21:31.233884: step 5213, loss 0.000765055, acc 1\n",
      "2018-10-26T16:21:31.673820: step 5214, loss 0.000381242, acc 1\n",
      "2018-10-26T16:21:32.138503: step 5215, loss 0.000764008, acc 1\n",
      "2018-10-26T16:21:32.602227: step 5216, loss 0.0272666, acc 0.984375\n",
      "2018-10-26T16:21:33.050030: step 5217, loss 0.000257581, acc 1\n",
      "2018-10-26T16:21:33.433007: step 5218, loss 0.00037768, acc 1\n",
      "2018-10-26T16:21:33.824958: step 5219, loss 0.001418, acc 1\n",
      "2018-10-26T16:21:34.230874: step 5220, loss 0.0305787, acc 0.984375\n",
      "2018-10-26T16:21:34.636792: step 5221, loss 0.00209501, acc 1\n",
      "2018-10-26T16:21:34.986856: step 5222, loss 0.000457029, acc 1\n",
      "2018-10-26T16:21:35.439648: step 5223, loss 0.00181629, acc 1\n",
      "2018-10-26T16:21:35.852542: step 5224, loss 0.00159004, acc 1\n",
      "2018-10-26T16:21:36.193842: step 5225, loss 0.000493292, acc 1\n",
      "2018-10-26T16:21:36.529732: step 5226, loss 0.00541274, acc 1\n",
      "2018-10-26T16:21:36.900779: step 5227, loss 0.00122487, acc 1\n",
      "2018-10-26T16:21:37.245820: step 5228, loss 0.00145728, acc 1\n",
      "2018-10-26T16:21:37.593925: step 5229, loss 0.000594826, acc 1\n",
      "2018-10-26T16:21:37.944952: step 5230, loss 0.00166868, acc 1\n",
      "2018-10-26T16:21:38.296014: step 5231, loss 0.00571472, acc 1\n",
      "2018-10-26T16:21:38.639100: step 5232, loss 0.000505409, acc 1\n",
      "2018-10-26T16:21:38.983183: step 5233, loss 0.0183557, acc 0.984375\n",
      "2018-10-26T16:21:39.323272: step 5234, loss 0.00238657, acc 1\n",
      "2018-10-26T16:21:39.690292: step 5235, loss 0.00389568, acc 1\n",
      "2018-10-26T16:21:40.043346: step 5236, loss 0.00569958, acc 1\n",
      "2018-10-26T16:21:40.393412: step 5237, loss 0.00113725, acc 1\n",
      "2018-10-26T16:21:40.747465: step 5238, loss 0.00569592, acc 1\n",
      "2018-10-26T16:21:41.093538: step 5239, loss 0.000378136, acc 1\n",
      "2018-10-26T16:21:41.433632: step 5240, loss 0.00120994, acc 1\n",
      "2018-10-26T16:21:41.781702: step 5241, loss 0.00686432, acc 1\n",
      "2018-10-26T16:21:42.119796: step 5242, loss 0.00377682, acc 1\n",
      "2018-10-26T16:21:42.459889: step 5243, loss 0.000271214, acc 1\n",
      "2018-10-26T16:21:42.809954: step 5244, loss 0.000893242, acc 1\n",
      "2018-10-26T16:21:43.169990: step 5245, loss 0.00271428, acc 1\n",
      "2018-10-26T16:21:43.529031: step 5246, loss 0.00275732, acc 1\n",
      "2018-10-26T16:21:43.858153: step 5247, loss 0.00192886, acc 1\n",
      "2018-10-26T16:21:44.202354: step 5248, loss 0.00163986, acc 1\n",
      "2018-10-26T16:21:44.557287: step 5249, loss 0.000602465, acc 1\n",
      "2018-10-26T16:21:44.894384: step 5250, loss 0.000393776, acc 1\n",
      "2018-10-26T16:21:45.253427: step 5251, loss 0.00672881, acc 1\n",
      "2018-10-26T16:21:45.599502: step 5252, loss 0.000661907, acc 1\n",
      "2018-10-26T16:21:45.948567: step 5253, loss 0.00112883, acc 1\n",
      "2018-10-26T16:21:46.333539: step 5254, loss 0.000261533, acc 1\n",
      "2018-10-26T16:21:46.739455: step 5255, loss 0.00157549, acc 1\n",
      "2018-10-26T16:21:47.158335: step 5256, loss 0.00106924, acc 1\n",
      "2018-10-26T16:21:47.506409: step 5257, loss 0.00227291, acc 1\n",
      "2018-10-26T16:21:47.901349: step 5258, loss 0.00636643, acc 1\n",
      "2018-10-26T16:21:48.236458: step 5259, loss 0.000494811, acc 1\n",
      "2018-10-26T16:21:48.578608: step 5260, loss 0.000970291, acc 1\n",
      "2018-10-26T16:21:48.893802: step 5261, loss 0.0105707, acc 1\n",
      "2018-10-26T16:21:49.281662: step 5262, loss 0.0018313, acc 1\n",
      "2018-10-26T16:21:49.644693: step 5263, loss 0.0113402, acc 1\n",
      "2018-10-26T16:21:49.972814: step 5264, loss 0.00203691, acc 1\n",
      "2018-10-26T16:21:50.302934: step 5265, loss 0.00118039, acc 1\n",
      "2018-10-26T16:21:50.644022: step 5266, loss 0.00471741, acc 1\n",
      "2018-10-26T16:21:50.980124: step 5267, loss 0.000665309, acc 1\n",
      "2018-10-26T16:21:51.316745: step 5268, loss 0.00020289, acc 1\n",
      "2018-10-26T16:21:51.646342: step 5269, loss 0.000566527, acc 1\n",
      "2018-10-26T16:21:51.974468: step 5270, loss 0.000248479, acc 1\n",
      "2018-10-26T16:21:52.287721: step 5271, loss 0.0015763, acc 1\n",
      "2018-10-26T16:21:52.603785: step 5272, loss 0.00142408, acc 1\n",
      "2018-10-26T16:21:52.932907: step 5273, loss 0.000392347, acc 1\n",
      "2018-10-26T16:21:53.247068: step 5274, loss 0.00118148, acc 1\n",
      "2018-10-26T16:21:53.561331: step 5275, loss 0.000367275, acc 1\n",
      "2018-10-26T16:21:53.901320: step 5276, loss 0.00335444, acc 1\n",
      "2018-10-26T16:21:54.218504: step 5277, loss 0.00330019, acc 1\n",
      "2018-10-26T16:21:54.540613: step 5278, loss 0.00223144, acc 1\n",
      "2018-10-26T16:21:54.872723: step 5279, loss 0.00247883, acc 1\n",
      "2018-10-26T16:21:55.203838: step 5280, loss 0.00246327, acc 1\n",
      "2018-10-26T16:21:55.522054: step 5281, loss 0.00175398, acc 1\n",
      "2018-10-26T16:21:55.919928: step 5282, loss 0.003784, acc 1\n",
      "2018-10-26T16:21:56.348778: step 5283, loss 0.00174626, acc 1\n",
      "2018-10-26T16:21:56.660945: step 5284, loss 0.000719633, acc 1\n",
      "2018-10-26T16:21:57.008018: step 5285, loss 0.000259157, acc 1\n",
      "2018-10-26T16:21:57.343122: step 5286, loss 0.000860076, acc 1\n",
      "2018-10-26T16:21:57.731165: step 5287, loss 0.00110574, acc 1\n",
      "2018-10-26T16:21:58.143982: step 5288, loss 0.00170794, acc 1\n",
      "2018-10-26T16:21:58.512065: step 5289, loss 0.00262547, acc 1\n",
      "2018-10-26T16:21:58.855084: step 5290, loss 0.00154916, acc 1\n",
      "2018-10-26T16:21:59.172235: step 5291, loss 0.00326087, acc 1\n",
      "2018-10-26T16:21:59.514322: step 5292, loss 0.000259103, acc 1\n",
      "2018-10-26T16:21:59.834503: step 5293, loss 0.000380719, acc 1\n",
      "2018-10-26T16:22:00.209464: step 5294, loss 0.00305639, acc 1\n",
      "2018-10-26T16:22:00.557861: step 5295, loss 0.00369545, acc 1\n",
      "2018-10-26T16:22:00.889717: step 5296, loss 0.000415552, acc 1\n",
      "2018-10-26T16:22:01.187850: step 5297, loss 0.00144437, acc 1\n",
      "2018-10-26T16:22:01.530042: step 5298, loss 0.00156413, acc 1\n",
      "2018-10-26T16:22:01.934854: step 5299, loss 0.00076415, acc 1\n",
      "2018-10-26T16:22:02.282924: step 5300, loss 0.00302342, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:22:03.092761: step 5300, loss 1.79187, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5300\n",
      "\n",
      "2018-10-26T16:22:03.750004: step 5301, loss 0.00399692, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:22:04.059180: step 5302, loss 0.0030713, acc 1\n",
      "2018-10-26T16:22:04.564859: step 5303, loss 0.000990351, acc 1\n",
      "2018-10-26T16:22:04.982081: step 5304, loss 0.00947117, acc 1\n",
      "2018-10-26T16:22:05.300860: step 5305, loss 0.000532852, acc 1\n",
      "2018-10-26T16:22:05.675858: step 5306, loss 0.000348936, acc 1\n",
      "2018-10-26T16:22:06.049860: step 5307, loss 0.00303072, acc 1\n",
      "2018-10-26T16:22:06.402915: step 5308, loss 0.00401116, acc 1\n",
      "2018-10-26T16:22:06.750037: step 5309, loss 0.00796029, acc 1\n",
      "2018-10-26T16:22:07.152949: step 5310, loss 0.000129618, acc 1\n",
      "2018-10-26T16:22:07.515943: step 5311, loss 0.00267011, acc 1\n",
      "2018-10-26T16:22:07.844066: step 5312, loss 0.0441882, acc 0.96875\n",
      "2018-10-26T16:22:08.180168: step 5313, loss 0.00477932, acc 1\n",
      "2018-10-26T16:22:08.535239: step 5314, loss 0.000235825, acc 1\n",
      "2018-10-26T16:22:08.900247: step 5315, loss 0.0178603, acc 0.984375\n",
      "2018-10-26T16:22:09.257289: step 5316, loss 0.00304759, acc 1\n",
      "2018-10-26T16:22:09.573446: step 5317, loss 0.00192038, acc 1\n",
      "2018-10-26T16:22:09.949442: step 5318, loss 0.00026001, acc 1\n",
      "2018-10-26T16:22:10.338401: step 5319, loss 0.00118878, acc 1\n",
      "2018-10-26T16:22:10.713438: step 5320, loss 0.00201104, acc 1\n",
      "2018-10-26T16:22:11.119320: step 5321, loss 0.00377857, acc 1\n",
      "2018-10-26T16:22:11.533210: step 5322, loss 0.0190103, acc 0.984375\n",
      "2018-10-26T16:22:11.968048: step 5323, loss 0.000773485, acc 1\n",
      "2018-10-26T16:22:12.318150: step 5324, loss 0.00125062, acc 1\n",
      "2018-10-26T16:22:12.734004: step 5325, loss 0.00201532, acc 1\n",
      "2018-10-26T16:22:13.142909: step 5326, loss 0.00676818, acc 1\n",
      "2018-10-26T16:22:13.557800: step 5327, loss 0.0048908, acc 1\n",
      "2018-10-26T16:22:13.970808: step 5328, loss 0.00517546, acc 1\n",
      "2018-10-26T16:22:14.333729: step 5329, loss 0.00202214, acc 1\n",
      "2018-10-26T16:22:14.674819: step 5330, loss 0.00371253, acc 1\n",
      "2018-10-26T16:22:15.015905: step 5331, loss 0.0263017, acc 0.984375\n",
      "2018-10-26T16:22:15.375944: step 5332, loss 0.00244128, acc 1\n",
      "2018-10-26T16:22:15.716036: step 5333, loss 0.0135509, acc 1\n",
      "2018-10-26T16:22:16.134916: step 5334, loss 0.00797804, acc 1\n",
      "2018-10-26T16:22:16.584712: step 5335, loss 0.0179302, acc 0.984375\n",
      "2018-10-26T16:22:16.926905: step 5336, loss 0.00217874, acc 1\n",
      "2018-10-26T16:22:17.244947: step 5337, loss 0.000261636, acc 1\n",
      "2018-10-26T16:22:17.644884: step 5338, loss 0.000582415, acc 1\n",
      "2018-10-26T16:22:18.075728: step 5339, loss 0.0010515, acc 1\n",
      "2018-10-26T16:22:18.408842: step 5340, loss 0.00740487, acc 1\n",
      "2018-10-26T16:22:18.725989: step 5341, loss 0.0154474, acc 1\n",
      "2018-10-26T16:22:19.054157: step 5342, loss 0.0101264, acc 1\n",
      "2018-10-26T16:22:19.436097: step 5343, loss 0.00112731, acc 1\n",
      "2018-10-26T16:22:19.783168: step 5344, loss 0.00542337, acc 1\n",
      "2018-10-26T16:22:20.197062: step 5345, loss 0.00227547, acc 1\n",
      "2018-10-26T16:22:20.654839: step 5346, loss 0.00204997, acc 1\n",
      "2018-10-26T16:22:21.030832: step 5347, loss 0.00784306, acc 1\n",
      "2018-10-26T16:22:21.376907: step 5348, loss 0.0019425, acc 1\n",
      "2018-10-26T16:22:21.743931: step 5349, loss 0.00487507, acc 1\n",
      "2018-10-26T16:22:22.143859: step 5350, loss 0.000685792, acc 1\n",
      "2018-10-26T16:22:22.487033: step 5351, loss 0.00306843, acc 1\n",
      "2018-10-26T16:22:22.819058: step 5352, loss 0.000805619, acc 1\n",
      "2018-10-26T16:22:23.137301: step 5353, loss 0.00326765, acc 1\n",
      "2018-10-26T16:22:23.459455: step 5354, loss 0.00070559, acc 1\n",
      "2018-10-26T16:22:23.772506: step 5355, loss 0.00104512, acc 1\n",
      "2018-10-26T16:22:24.097639: step 5356, loss 0.00262569, acc 1\n",
      "2018-10-26T16:22:24.429751: step 5357, loss 0.00213602, acc 1\n",
      "2018-10-26T16:22:24.808738: step 5358, loss 0.000463978, acc 1\n",
      "2018-10-26T16:22:25.126893: step 5359, loss 0.00136576, acc 1\n",
      "2018-10-26T16:22:25.501891: step 5360, loss 0.000407429, acc 1\n",
      "2018-10-26T16:22:25.825026: step 5361, loss 0.000573315, acc 1\n",
      "2018-10-26T16:22:26.239915: step 5362, loss 0.000557405, acc 1\n",
      "2018-10-26T16:22:26.716641: step 5363, loss 0.00158563, acc 1\n",
      "2018-10-26T16:22:27.177410: step 5364, loss 0.000422807, acc 1\n",
      "2018-10-26T16:22:27.493566: step 5365, loss 0.00518553, acc 1\n",
      "2018-10-26T16:22:27.824708: step 5366, loss 0.000640876, acc 1\n",
      "2018-10-26T16:22:28.149814: step 5367, loss 0.00153581, acc 1\n",
      "2018-10-26T16:22:28.474043: step 5368, loss 0.00152829, acc 1\n",
      "2018-10-26T16:22:28.858920: step 5369, loss 0.000199771, acc 1\n",
      "2018-10-26T16:22:29.186080: step 5370, loss 0.0014329, acc 1\n",
      "2018-10-26T16:22:29.508183: step 5371, loss 0.0192917, acc 0.984375\n",
      "2018-10-26T16:22:29.844285: step 5372, loss 0.00432644, acc 1\n",
      "2018-10-26T16:22:30.173404: step 5373, loss 0.000417905, acc 1\n",
      "2018-10-26T16:22:30.493550: step 5374, loss 0.0125904, acc 1\n",
      "2018-10-26T16:22:30.840659: step 5375, loss 0.000286844, acc 1\n",
      "2018-10-26T16:22:31.171801: step 5376, loss 0.000577351, acc 1\n",
      "2018-10-26T16:22:31.550758: step 5377, loss 0.0140017, acc 0.984375\n",
      "2018-10-26T16:22:31.874860: step 5378, loss 0.00888931, acc 1\n",
      "2018-10-26T16:22:32.228912: step 5379, loss 0.00164337, acc 1\n",
      "2018-10-26T16:22:32.544070: step 5380, loss 0.000925629, acc 1\n",
      "2018-10-26T16:22:32.870294: step 5381, loss 0.000388518, acc 1\n",
      "2018-10-26T16:22:33.201354: step 5382, loss 0.0012137, acc 1\n",
      "2018-10-26T16:22:33.574400: step 5383, loss 0.00300027, acc 1\n",
      "2018-10-26T16:22:33.961340: step 5384, loss 0.00115149, acc 1\n",
      "2018-10-26T16:22:34.346256: step 5385, loss 0.00131163, acc 1\n",
      "2018-10-26T16:22:34.709287: step 5386, loss 0.00211266, acc 1\n",
      "2018-10-26T16:22:35.086278: step 5387, loss 0.00127101, acc 1\n",
      "2018-10-26T16:22:35.465327: step 5388, loss 0.000718464, acc 1\n",
      "2018-10-26T16:22:35.880158: step 5389, loss 0.000702458, acc 1\n",
      "2018-10-26T16:22:36.301032: step 5390, loss 0.000971016, acc 1\n",
      "2018-10-26T16:22:36.773769: step 5391, loss 0.00369918, acc 1\n",
      "2018-10-26T16:22:37.287398: step 5392, loss 0.00505226, acc 1\n",
      "2018-10-26T16:22:37.819975: step 5393, loss 0.00175825, acc 1\n",
      "2018-10-26T16:22:38.277750: step 5394, loss 0.000486842, acc 1\n",
      "2018-10-26T16:22:38.718572: step 5395, loss 0.00466404, acc 1\n",
      "2018-10-26T16:22:39.142441: step 5396, loss 0.000495611, acc 1\n",
      "2018-10-26T16:22:39.575285: step 5397, loss 0.00305259, acc 1\n",
      "2018-10-26T16:22:40.011124: step 5398, loss 0.00370163, acc 1\n",
      "2018-10-26T16:22:40.437980: step 5399, loss 0.000344614, acc 1\n",
      "2018-10-26T16:22:40.830930: step 5400, loss 0.00151282, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:22:41.709582: step 5400, loss 1.80298, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5400\n",
      "\n",
      "2018-10-26T16:22:42.364832: step 5401, loss 0.00788633, acc 1\n",
      "2018-10-26T16:22:42.702004: step 5402, loss 0.00150606, acc 1\n",
      "2018-10-26T16:22:43.191669: step 5403, loss 0.000933943, acc 1\n",
      "2018-10-26T16:22:43.560636: step 5404, loss 0.000663644, acc 1\n",
      "2018-10-26T16:22:43.890754: step 5405, loss 0.00605143, acc 1\n",
      "2018-10-26T16:22:44.248831: step 5406, loss 0.00020346, acc 1\n",
      "2018-10-26T16:22:44.598861: step 5407, loss 0.000991542, acc 1\n",
      "2018-10-26T16:22:44.938956: step 5408, loss 0.00493758, acc 1\n",
      "2018-10-26T16:22:45.282107: step 5409, loss 0.000213923, acc 1\n",
      "2018-10-26T16:22:45.647087: step 5410, loss 0.01259, acc 1\n",
      "2018-10-26T16:22:45.973193: step 5411, loss 6.31508e-05, acc 1\n",
      "2018-10-26T16:22:46.392070: step 5412, loss 0.00122571, acc 1\n",
      "2018-10-26T16:22:46.844861: step 5413, loss 0.00138715, acc 1\n",
      "2018-10-26T16:22:47.190985: step 5414, loss 0.0355647, acc 0.984375\n",
      "2018-10-26T16:22:47.541193: step 5415, loss 0.0252967, acc 0.984375\n",
      "2018-10-26T16:22:47.915002: step 5416, loss 0.00442971, acc 1\n",
      "2018-10-26T16:22:48.296981: step 5417, loss 0.00670052, acc 1\n",
      "2018-10-26T16:22:48.637072: step 5418, loss 0.000642768, acc 1\n",
      "2018-10-26T16:22:48.987137: step 5419, loss 0.000298892, acc 1\n",
      "2018-10-26T16:22:49.355155: step 5420, loss 0.00198828, acc 1\n",
      "2018-10-26T16:22:49.715193: step 5421, loss 0.00034417, acc 1\n",
      "2018-10-26T16:22:50.094179: step 5422, loss 0.000343684, acc 1\n",
      "2018-10-26T16:22:50.469177: step 5423, loss 0.00230053, acc 1\n",
      "2018-10-26T16:22:50.923963: step 5424, loss 0.00234881, acc 1\n",
      "2018-10-26T16:22:51.289985: step 5425, loss 0.000817986, acc 1\n",
      "2018-10-26T16:22:51.661991: step 5426, loss 0.0119328, acc 0.984375\n",
      "2018-10-26T16:22:52.033002: step 5427, loss 0.00550105, acc 1\n",
      "2018-10-26T16:22:52.401035: step 5428, loss 0.0135171, acc 0.984375\n",
      "2018-10-26T16:22:52.793967: step 5429, loss 0.00103619, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:22:53.190941: step 5430, loss 0.00246338, acc 1\n",
      "2018-10-26T16:22:53.546955: step 5431, loss 0.0118613, acc 0.984375\n",
      "2018-10-26T16:22:53.884053: step 5432, loss 0.00145752, acc 1\n",
      "2018-10-26T16:22:54.274011: step 5433, loss 0.00110927, acc 1\n",
      "2018-10-26T16:22:54.647015: step 5434, loss 0.00342075, acc 1\n",
      "2018-10-26T16:22:54.998134: step 5435, loss 0.00118231, acc 1\n",
      "2018-10-26T16:22:55.385044: step 5436, loss 0.000512386, acc 1\n",
      "2018-10-26T16:22:55.730155: step 5437, loss 0.00153529, acc 1\n",
      "2018-10-26T16:22:56.064231: step 5438, loss 0.000482207, acc 1\n",
      "2018-10-26T16:22:56.406315: step 5439, loss 0.0514604, acc 0.96875\n",
      "2018-10-26T16:22:56.754385: step 5440, loss 0.00107234, acc 1\n",
      "2018-10-26T16:22:57.095504: step 5441, loss 0.000644107, acc 1\n",
      "2018-10-26T16:22:57.453515: step 5442, loss 0.00113961, acc 1\n",
      "2018-10-26T16:22:57.770670: step 5443, loss 0.000485956, acc 1\n",
      "2018-10-26T16:22:58.112756: step 5444, loss 0.000257299, acc 1\n",
      "2018-10-26T16:22:58.490950: step 5445, loss 0.00152658, acc 1\n",
      "2018-10-26T16:22:58.875715: step 5446, loss 0.000270912, acc 1\n",
      "2018-10-26T16:22:59.226779: step 5447, loss 0.0018242, acc 1\n",
      "2018-10-26T16:22:59.579835: step 5448, loss 0.00238672, acc 1\n",
      "2018-10-26T16:22:59.936882: step 5449, loss 0.000170587, acc 1\n",
      "2018-10-26T16:23:00.317076: step 5450, loss 0.000855639, acc 1\n",
      "2018-10-26T16:23:00.665934: step 5451, loss 0.000216388, acc 1\n",
      "2018-10-26T16:23:01.041929: step 5452, loss 0.000766408, acc 1\n",
      "2018-10-26T16:23:01.385412: step 5453, loss 0.000607653, acc 1\n",
      "2018-10-26T16:23:01.740062: step 5454, loss 0.00360384, acc 1\n",
      "2018-10-26T16:23:02.104092: step 5455, loss 0.000521963, acc 1\n",
      "2018-10-26T16:23:02.447173: step 5456, loss 0.00135579, acc 1\n",
      "2018-10-26T16:23:02.810247: step 5457, loss 0.00444609, acc 1\n",
      "2018-10-26T16:23:03.169245: step 5458, loss 0.00164895, acc 1\n",
      "2018-10-26T16:23:03.504349: step 5459, loss 0.00366303, acc 1\n",
      "2018-10-26T16:23:03.852419: step 5460, loss 0.00231932, acc 1\n",
      "2018-10-26T16:23:04.214453: step 5461, loss 9.73196e-05, acc 1\n",
      "2018-10-26T16:23:04.563519: step 5462, loss 0.000627863, acc 1\n",
      "2018-10-26T16:23:04.932535: step 5463, loss 0.000551381, acc 1\n",
      "2018-10-26T16:23:05.302544: step 5464, loss 0.000559944, acc 1\n",
      "2018-10-26T16:23:05.671606: step 5465, loss 0.00888109, acc 1\n",
      "2018-10-26T16:23:06.016638: step 5466, loss 0.000525541, acc 1\n",
      "2018-10-26T16:23:06.346786: step 5467, loss 0.00517873, acc 1\n",
      "2018-10-26T16:23:06.766634: step 5468, loss 0.00173762, acc 1\n",
      "2018-10-26T16:23:07.158586: step 5469, loss 0.0017836, acc 1\n",
      "2018-10-26T16:23:07.550539: step 5470, loss 0.000513025, acc 1\n",
      "2018-10-26T16:23:07.903596: step 5471, loss 0.000794973, acc 1\n",
      "2018-10-26T16:23:08.244684: step 5472, loss 0.00196001, acc 1\n",
      "2018-10-26T16:23:08.578791: step 5473, loss 0.00943135, acc 1\n",
      "2018-10-26T16:23:08.929882: step 5474, loss 0.00144371, acc 1\n",
      "2018-10-26T16:23:09.294877: step 5475, loss 0.000800626, acc 1\n",
      "2018-10-26T16:23:09.653919: step 5476, loss 0.00808981, acc 1\n",
      "2018-10-26T16:23:09.997998: step 5477, loss 0.00120481, acc 1\n",
      "2018-10-26T16:23:10.318144: step 5478, loss 0.000337534, acc 1\n",
      "2018-10-26T16:23:10.668207: step 5479, loss 0.00126979, acc 1\n",
      "2018-10-26T16:23:11.044314: step 5480, loss 0.000597576, acc 1\n",
      "2018-10-26T16:23:11.427180: step 5481, loss 0.00227348, acc 1\n",
      "2018-10-26T16:23:11.799188: step 5482, loss 0.00210707, acc 1\n",
      "2018-10-26T16:23:12.249983: step 5483, loss 0.00217208, acc 1\n",
      "2018-10-26T16:23:12.592343: step 5484, loss 0.000919098, acc 1\n",
      "2018-10-26T16:23:12.917334: step 5485, loss 0.00448334, acc 1\n",
      "2018-10-26T16:23:13.244366: step 5486, loss 0.00031563, acc 1\n",
      "2018-10-26T16:23:13.576674: step 5487, loss 0.000121041, acc 1\n",
      "2018-10-26T16:23:13.976374: step 5488, loss 0.00876882, acc 1\n",
      "2018-10-26T16:23:14.348376: step 5489, loss 0.00092746, acc 1\n",
      "2018-10-26T16:23:14.684481: step 5490, loss 0.00422424, acc 1\n",
      "2018-10-26T16:23:15.014599: step 5491, loss 0.000797927, acc 1\n",
      "2018-10-26T16:23:15.332747: step 5492, loss 0.000747599, acc 1\n",
      "2018-10-26T16:23:15.638929: step 5493, loss 0.0216402, acc 0.984375\n",
      "2018-10-26T16:23:15.978057: step 5494, loss 0.000194926, acc 1\n",
      "2018-10-26T16:23:16.308181: step 5495, loss 0.00167415, acc 1\n",
      "2018-10-26T16:23:16.609335: step 5496, loss 0.023341, acc 0.984375\n",
      "2018-10-26T16:23:16.936460: step 5497, loss 0.000480838, acc 1\n",
      "2018-10-26T16:23:17.273560: step 5498, loss 0.0234518, acc 0.984375\n",
      "2018-10-26T16:23:17.612657: step 5499, loss 0.00134235, acc 1\n",
      "2018-10-26T16:23:17.972812: step 5500, loss 0.00056279, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:23:18.776675: step 5500, loss 1.86641, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5500\n",
      "\n",
      "2018-10-26T16:23:19.488682: step 5501, loss 0.00162559, acc 1\n",
      "2018-10-26T16:23:19.834717: step 5502, loss 0.0531375, acc 0.984375\n",
      "2018-10-26T16:23:20.252720: step 5503, loss 0.00327235, acc 1\n",
      "2018-10-26T16:23:20.635579: step 5504, loss 0.00240995, acc 1\n",
      "2018-10-26T16:23:20.966695: step 5505, loss 0.000371815, acc 1\n",
      "2018-10-26T16:23:21.285841: step 5506, loss 0.000444392, acc 1\n",
      "2018-10-26T16:23:21.618949: step 5507, loss 0.00169027, acc 1\n",
      "2018-10-26T16:23:21.975001: step 5508, loss 0.000875087, acc 1\n",
      "2018-10-26T16:23:22.515783: step 5509, loss 0.00204353, acc 1\n",
      "2018-10-26T16:23:23.015219: step 5510, loss 0.000546863, acc 1\n",
      "2018-10-26T16:23:23.372326: step 5511, loss 0.00209347, acc 1\n",
      "2018-10-26T16:23:23.780177: step 5512, loss 0.000395794, acc 1\n",
      "2018-10-26T16:23:24.171130: step 5513, loss 0.000435202, acc 1\n",
      "2018-10-26T16:23:24.500682: step 5514, loss 0.000745623, acc 1\n",
      "2018-10-26T16:23:24.894374: step 5515, loss 0.00118788, acc 1\n",
      "2018-10-26T16:23:25.279173: step 5516, loss 0.001615, acc 1\n",
      "2018-10-26T16:23:25.673125: step 5517, loss 0.000106457, acc 1\n",
      "2018-10-26T16:23:26.071167: step 5518, loss 0.00283769, acc 1\n",
      "2018-10-26T16:23:26.538806: step 5519, loss 0.00178449, acc 1\n",
      "2018-10-26T16:23:27.002568: step 5520, loss 0.00367045, acc 1\n",
      "2018-10-26T16:23:27.439399: step 5521, loss 0.00205413, acc 1\n",
      "2018-10-26T16:23:27.902210: step 5522, loss 0.000520692, acc 1\n",
      "2018-10-26T16:23:28.371958: step 5523, loss 0.00265779, acc 1\n",
      "2018-10-26T16:23:28.837355: step 5524, loss 0.00106716, acc 1\n",
      "2018-10-26T16:23:29.230660: step 5525, loss 0.00217288, acc 1\n",
      "2018-10-26T16:23:29.625669: step 5526, loss 0.00298295, acc 1\n",
      "2018-10-26T16:23:30.041565: step 5527, loss 0.000655129, acc 1\n",
      "2018-10-26T16:23:30.551131: step 5528, loss 0.000825541, acc 1\n",
      "2018-10-26T16:23:31.043820: step 5529, loss 0.00154142, acc 1\n",
      "2018-10-26T16:23:31.448789: step 5530, loss 0.00710775, acc 1\n",
      "2018-10-26T16:23:31.790819: step 5531, loss 0.00132698, acc 1\n",
      "2018-10-26T16:23:32.099995: step 5532, loss 0.000569601, acc 1\n",
      "2018-10-26T16:23:32.445070: step 5533, loss 0.0116502, acc 1\n",
      "2018-10-26T16:23:32.853979: step 5534, loss 0.00100495, acc 1\n",
      "2018-10-26T16:23:33.184144: step 5535, loss 0.000825893, acc 1\n",
      "2018-10-26T16:23:33.534163: step 5536, loss 0.00728191, acc 1\n",
      "2018-10-26T16:23:33.956034: step 5537, loss 0.0403732, acc 0.984375\n",
      "2018-10-26T16:23:34.339013: step 5538, loss 0.000229866, acc 1\n",
      "2018-10-26T16:23:34.763881: step 5539, loss 0.000401397, acc 1\n",
      "2018-10-26T16:23:35.200764: step 5540, loss 0.00104226, acc 1\n",
      "2018-10-26T16:23:35.620586: step 5541, loss 0.000441594, acc 1\n",
      "2018-10-26T16:23:36.046460: step 5542, loss 0.00127186, acc 1\n",
      "2018-10-26T16:23:36.551155: step 5543, loss 0.0129783, acc 0.984375\n",
      "2018-10-26T16:23:37.046329: step 5544, loss 0.000605474, acc 1\n",
      "2018-10-26T16:23:37.435798: step 5545, loss 0.000899898, acc 1\n",
      "2018-10-26T16:23:37.816719: step 5546, loss 0.00180503, acc 1\n",
      "2018-10-26T16:23:38.170814: step 5547, loss 0.0855661, acc 0.984375\n",
      "2018-10-26T16:23:38.480947: step 5548, loss 0.00622795, acc 1\n",
      "2018-10-26T16:23:38.833003: step 5549, loss 0.0188624, acc 1\n",
      "2018-10-26T16:23:39.333667: step 5550, loss 0.00193409, acc 1\n",
      "2018-10-26T16:23:39.737627: step 5551, loss 0.000968974, acc 1\n",
      "2018-10-26T16:23:40.236269: step 5552, loss 0.000365407, acc 1\n",
      "2018-10-26T16:23:40.672162: step 5553, loss 0.00112355, acc 1\n",
      "2018-10-26T16:23:41.168763: step 5554, loss 0.000119795, acc 1\n",
      "2018-10-26T16:23:41.605596: step 5555, loss 0.000155176, acc 1\n",
      "2018-10-26T16:23:41.979598: step 5556, loss 0.000484834, acc 1\n",
      "2018-10-26T16:23:42.451336: step 5557, loss 0.000361458, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:23:42.974938: step 5558, loss 0.0024683, acc 1\n",
      "2018-10-26T16:23:43.478591: step 5559, loss 0.00432693, acc 1\n",
      "2018-10-26T16:23:43.946352: step 5560, loss 0.000282557, acc 1\n",
      "2018-10-26T16:23:44.437030: step 5561, loss 0.000980759, acc 1\n",
      "2018-10-26T16:23:44.916762: step 5562, loss 0.00361133, acc 1\n",
      "2018-10-26T16:23:45.382964: step 5563, loss 0.0500549, acc 0.984375\n",
      "2018-10-26T16:23:45.828356: step 5564, loss 0.00056627, acc 1\n",
      "2018-10-26T16:23:46.358897: step 5565, loss 0.0110129, acc 1\n",
      "2018-10-26T16:23:46.877510: step 5566, loss 0.00272132, acc 1\n",
      "2018-10-26T16:23:47.338280: step 5567, loss 0.00206709, acc 1\n",
      "2018-10-26T16:23:47.790072: step 5568, loss 0.00358203, acc 1\n",
      "2018-10-26T16:23:48.310825: step 5569, loss 0.000559515, acc 1\n",
      "2018-10-26T16:23:48.684684: step 5570, loss 0.000260656, acc 1\n",
      "2018-10-26T16:23:49.108549: step 5571, loss 0.000400149, acc 1\n",
      "2018-10-26T16:23:49.484546: step 5572, loss 0.0020762, acc 1\n",
      "2018-10-26T16:23:49.907476: step 5573, loss 0.00120169, acc 1\n",
      "2018-10-26T16:23:50.296376: step 5574, loss 0.030524, acc 0.984375\n",
      "2018-10-26T16:23:50.727225: step 5575, loss 0.000877782, acc 1\n",
      "2018-10-26T16:23:51.130193: step 5576, loss 0.00117856, acc 1\n",
      "2018-10-26T16:23:51.568975: step 5577, loss 0.000528732, acc 1\n",
      "2018-10-26T16:23:51.901137: step 5578, loss 0.000302213, acc 1\n",
      "2018-10-26T16:23:52.237191: step 5579, loss 0.00311427, acc 1\n",
      "2018-10-26T16:23:52.617175: step 5580, loss 0.0030488, acc 1\n",
      "2018-10-26T16:23:53.042072: step 5581, loss 0.000379668, acc 1\n",
      "2018-10-26T16:23:53.549689: step 5582, loss 0.00016971, acc 1\n",
      "2018-10-26T16:23:53.873817: step 5583, loss 0.00833903, acc 1\n",
      "2018-10-26T16:23:54.242830: step 5584, loss 0.00107301, acc 1\n",
      "2018-10-26T16:23:54.584917: step 5585, loss 0.00676244, acc 1\n",
      "2018-10-26T16:23:54.926007: step 5586, loss 0.00336121, acc 1\n",
      "2018-10-26T16:23:55.322076: step 5587, loss 0.000791081, acc 1\n",
      "2018-10-26T16:23:55.693955: step 5588, loss 0.000365687, acc 1\n",
      "2018-10-26T16:23:56.096877: step 5589, loss 0.000994628, acc 1\n",
      "2018-10-26T16:23:56.479854: step 5590, loss 0.000792843, acc 1\n",
      "2018-10-26T16:23:56.894833: step 5591, loss 0.00139128, acc 1\n",
      "2018-10-26T16:23:57.218892: step 5592, loss 0.00101098, acc 1\n",
      "2018-10-26T16:23:57.583905: step 5593, loss 0.001187, acc 1\n",
      "2018-10-26T16:23:57.917014: step 5594, loss 0.00118583, acc 1\n",
      "2018-10-26T16:23:58.387082: step 5595, loss 0.000636013, acc 1\n",
      "2018-10-26T16:23:58.780707: step 5596, loss 0.000372234, acc 1\n",
      "2018-10-26T16:23:59.103929: step 5597, loss 0.000701298, acc 1\n",
      "2018-10-26T16:23:59.432967: step 5598, loss 0.00168812, acc 1\n",
      "2018-10-26T16:23:59.759092: step 5599, loss 0.00113617, acc 1\n",
      "2018-10-26T16:24:00.109194: step 5600, loss 0.00203729, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:24:00.885083: step 5600, loss 1.88365, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5600\n",
      "\n",
      "2018-10-26T16:24:01.517399: step 5601, loss 0.00180483, acc 1\n",
      "2018-10-26T16:24:01.820605: step 5602, loss 0.000329074, acc 1\n",
      "2018-10-26T16:24:02.595133: step 5603, loss 0.000285908, acc 1\n",
      "2018-10-26T16:24:02.960086: step 5604, loss 0.00379614, acc 1\n",
      "2018-10-26T16:24:03.277239: step 5605, loss 7.39509e-05, acc 1\n",
      "2018-10-26T16:24:03.610591: step 5606, loss 0.00179858, acc 1\n",
      "2018-10-26T16:24:04.017260: step 5607, loss 0.000263273, acc 1\n",
      "2018-10-26T16:24:04.488051: step 5608, loss 0.000605377, acc 1\n",
      "2018-10-26T16:24:04.837069: step 5609, loss 0.00180209, acc 1\n",
      "2018-10-26T16:24:05.152297: step 5610, loss 0.00110468, acc 1\n",
      "2018-10-26T16:24:05.514264: step 5611, loss 0.00998332, acc 1\n",
      "2018-10-26T16:24:05.932145: step 5612, loss 0.00721824, acc 1\n",
      "2018-10-26T16:24:06.291185: step 5613, loss 0.000185177, acc 1\n",
      "2018-10-26T16:24:06.713057: step 5614, loss 0.00191315, acc 1\n",
      "2018-10-26T16:24:07.192775: step 5615, loss 0.000914982, acc 1\n",
      "2018-10-26T16:24:07.672687: step 5616, loss 0.000796022, acc 1\n",
      "2018-10-26T16:24:08.060666: step 5617, loss 0.000761781, acc 1\n",
      "2018-10-26T16:24:08.427526: step 5618, loss 3.17658e-05, acc 1\n",
      "2018-10-26T16:24:08.793499: step 5619, loss 0.0111724, acc 0.984375\n",
      "2018-10-26T16:24:09.140572: step 5620, loss 0.00434812, acc 1\n",
      "2018-10-26T16:24:09.473681: step 5621, loss 0.00602741, acc 1\n",
      "2018-10-26T16:24:09.836712: step 5622, loss 0.00328506, acc 1\n",
      "2018-10-26T16:24:10.263607: step 5623, loss 0.00229711, acc 1\n",
      "2018-10-26T16:24:10.580754: step 5624, loss 0.000769715, acc 1\n",
      "2018-10-26T16:24:10.981742: step 5625, loss 0.00156473, acc 1\n",
      "2018-10-26T16:24:11.310774: step 5626, loss 0.00132105, acc 1\n",
      "2018-10-26T16:24:11.639894: step 5627, loss 0.000770183, acc 1\n",
      "2018-10-26T16:24:11.961035: step 5628, loss 0.000208251, acc 1\n",
      "2018-10-26T16:24:12.358973: step 5629, loss 0.00473245, acc 1\n",
      "2018-10-26T16:24:12.699134: step 5630, loss 0.000725975, acc 1\n",
      "2018-10-26T16:24:13.029182: step 5631, loss 0.00199225, acc 1\n",
      "2018-10-26T16:24:13.353318: step 5632, loss 0.00123235, acc 1\n",
      "2018-10-26T16:24:13.729312: step 5633, loss 0.000134217, acc 1\n",
      "2018-10-26T16:24:14.081427: step 5634, loss 0.000495802, acc 1\n",
      "2018-10-26T16:24:14.475319: step 5635, loss 0.000420493, acc 1\n",
      "2018-10-26T16:24:14.826380: step 5636, loss 0.000443837, acc 1\n",
      "2018-10-26T16:24:15.176444: step 5637, loss 0.00213632, acc 1\n",
      "2018-10-26T16:24:15.488611: step 5638, loss 0.000487602, acc 1\n",
      "2018-10-26T16:24:15.816773: step 5639, loss 0.00608446, acc 1\n",
      "2018-10-26T16:24:16.139872: step 5640, loss 0.000466323, acc 1\n",
      "2018-10-26T16:24:16.451043: step 5641, loss 0.024827, acc 0.984375\n",
      "2018-10-26T16:24:16.765202: step 5642, loss 0.000500612, acc 1\n",
      "2018-10-26T16:24:17.131221: step 5643, loss 0.000328149, acc 1\n",
      "2018-10-26T16:24:17.462378: step 5644, loss 0.0002091, acc 1\n",
      "2018-10-26T16:24:17.796444: step 5645, loss 7.68208e-05, acc 1\n",
      "2018-10-26T16:24:18.095645: step 5646, loss 5.76256e-05, acc 1\n",
      "2018-10-26T16:24:18.413798: step 5647, loss 0.0115782, acc 1\n",
      "2018-10-26T16:24:18.743913: step 5648, loss 0.00408052, acc 1\n",
      "2018-10-26T16:24:19.065191: step 5649, loss 0.00159204, acc 1\n",
      "2018-10-26T16:24:19.368245: step 5650, loss 0.000451646, acc 1\n",
      "2018-10-26T16:24:19.684399: step 5651, loss 0.000340988, acc 1\n",
      "2018-10-26T16:24:19.996566: step 5652, loss 0.00426204, acc 1\n",
      "2018-10-26T16:24:20.300756: step 5653, loss 0.00362806, acc 1\n",
      "2018-10-26T16:24:20.635860: step 5654, loss 0.00054261, acc 1\n",
      "2018-10-26T16:24:20.965979: step 5655, loss 0.00288187, acc 1\n",
      "2018-10-26T16:24:21.294144: step 5656, loss 0.050394, acc 0.96875\n",
      "2018-10-26T16:24:21.610257: step 5657, loss 0.00144036, acc 1\n",
      "2018-10-26T16:24:21.936385: step 5658, loss 0.00197246, acc 1\n",
      "2018-10-26T16:24:22.255529: step 5659, loss 0.00229987, acc 1\n",
      "2018-10-26T16:24:22.582657: step 5660, loss 0.000487477, acc 1\n",
      "2018-10-26T16:24:22.909786: step 5661, loss 9.32516e-05, acc 1\n",
      "2018-10-26T16:24:23.220027: step 5662, loss 0.0143317, acc 0.984375\n",
      "2018-10-26T16:24:23.536113: step 5663, loss 0.000418626, acc 1\n",
      "2018-10-26T16:24:23.863327: step 5664, loss 0.00272951, acc 1\n",
      "2018-10-26T16:24:24.176398: step 5665, loss 0.000582816, acc 1\n",
      "2018-10-26T16:24:24.500532: step 5666, loss 0.000326044, acc 1\n",
      "2018-10-26T16:24:24.802724: step 5667, loss 0.000471487, acc 1\n",
      "2018-10-26T16:24:25.131883: step 5668, loss 0.000894673, acc 1\n",
      "2018-10-26T16:24:25.450030: step 5669, loss 0.00123816, acc 1\n",
      "2018-10-26T16:24:25.766152: step 5670, loss 0.000668944, acc 1\n",
      "2018-10-26T16:24:26.103249: step 5671, loss 0.000834998, acc 1\n",
      "2018-10-26T16:24:26.425389: step 5672, loss 0.000455148, acc 1\n",
      "2018-10-26T16:24:26.752516: step 5673, loss 0.00910196, acc 1\n",
      "2018-10-26T16:24:27.077648: step 5674, loss 0.00162844, acc 1\n",
      "2018-10-26T16:24:27.415805: step 5675, loss 0.000293706, acc 1\n",
      "2018-10-26T16:24:27.730900: step 5676, loss 0.00125249, acc 1\n",
      "2018-10-26T16:24:28.059024: step 5677, loss 0.000479191, acc 1\n",
      "2018-10-26T16:24:28.394128: step 5678, loss 0.00244393, acc 1\n",
      "2018-10-26T16:24:28.715274: step 5679, loss 0.00078273, acc 1\n",
      "2018-10-26T16:24:29.028482: step 5680, loss 0.000371531, acc 1\n",
      "2018-10-26T16:24:29.355560: step 5681, loss 0.000416092, acc 1\n",
      "2018-10-26T16:24:29.677699: step 5682, loss 0.000114822, acc 1\n",
      "2018-10-26T16:24:30.011868: step 5683, loss 0.00206913, acc 1\n",
      "2018-10-26T16:24:30.328000: step 5684, loss 0.0182524, acc 0.984375\n",
      "2018-10-26T16:24:30.655091: step 5685, loss 0.00114114, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:24:30.987201: step 5686, loss 0.00172771, acc 1\n",
      "2018-10-26T16:24:31.324344: step 5687, loss 0.00136069, acc 1\n",
      "2018-10-26T16:24:31.648437: step 5688, loss 0.00105623, acc 1\n",
      "2018-10-26T16:24:31.959624: step 5689, loss 0.00261114, acc 1\n",
      "2018-10-26T16:24:32.297701: step 5690, loss 0.000712996, acc 1\n",
      "2018-10-26T16:24:32.631852: step 5691, loss 0.000178882, acc 1\n",
      "2018-10-26T16:24:32.962974: step 5692, loss 0.000907681, acc 1\n",
      "2018-10-26T16:24:33.345899: step 5693, loss 0.00329314, acc 1\n",
      "2018-10-26T16:24:33.723889: step 5694, loss 0.000150491, acc 1\n",
      "2018-10-26T16:24:34.098889: step 5695, loss 0.00650485, acc 1\n",
      "2018-10-26T16:24:34.454976: step 5696, loss 0.000382023, acc 1\n",
      "2018-10-26T16:24:34.805998: step 5697, loss 0.00425237, acc 1\n",
      "2018-10-26T16:24:35.159101: step 5698, loss 0.0101309, acc 1\n",
      "2018-10-26T16:24:35.505176: step 5699, loss 0.00208256, acc 1\n",
      "2018-10-26T16:24:35.821286: step 5700, loss 0.00313188, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:24:36.612175: step 5700, loss 1.90272, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5700\n",
      "\n",
      "2018-10-26T16:24:37.241489: step 5701, loss 0.00238776, acc 1\n",
      "2018-10-26T16:24:37.569615: step 5702, loss 0.00114183, acc 1\n",
      "2018-10-26T16:24:38.067284: step 5703, loss 0.000108821, acc 1\n",
      "2018-10-26T16:24:38.401391: step 5704, loss 0.00314328, acc 1\n",
      "2018-10-26T16:24:38.749694: step 5705, loss 0.00156274, acc 1\n",
      "2018-10-26T16:24:39.125457: step 5706, loss 0.00127761, acc 1\n",
      "2018-10-26T16:24:39.460561: step 5707, loss 0.000179185, acc 1\n",
      "2018-10-26T16:24:39.793672: step 5708, loss 0.000756677, acc 1\n",
      "2018-10-26T16:24:40.148726: step 5709, loss 0.000441361, acc 1\n",
      "2018-10-26T16:24:40.531739: step 5710, loss 0.00483059, acc 1\n",
      "2018-10-26T16:24:40.858861: step 5711, loss 0.00147957, acc 1\n",
      "2018-10-26T16:24:41.184957: step 5712, loss 0.000385576, acc 1\n",
      "2018-10-26T16:24:41.523054: step 5713, loss 0.000437761, acc 1\n",
      "2018-10-26T16:24:41.846237: step 5714, loss 0.000647277, acc 1\n",
      "2018-10-26T16:24:42.200243: step 5715, loss 4.75556e-05, acc 1\n",
      "2018-10-26T16:24:42.540335: step 5716, loss 0.014817, acc 0.984375\n",
      "2018-10-26T16:24:42.855491: step 5717, loss 0.00592951, acc 1\n",
      "2018-10-26T16:24:43.185609: step 5718, loss 0.000613766, acc 1\n",
      "2018-10-26T16:24:43.521751: step 5719, loss 0.00188156, acc 1\n",
      "2018-10-26T16:24:43.836868: step 5720, loss 0.0385524, acc 0.984375\n",
      "2018-10-26T16:24:44.158124: step 5721, loss 0.00274431, acc 1\n",
      "2018-10-26T16:24:44.491190: step 5722, loss 0.011424, acc 1\n",
      "2018-10-26T16:24:44.853180: step 5723, loss 0.000734397, acc 1\n",
      "2018-10-26T16:24:45.180279: step 5724, loss 0.000386714, acc 1\n",
      "2018-10-26T16:24:45.536327: step 5725, loss 0.00102305, acc 1\n",
      "2018-10-26T16:24:45.873428: step 5726, loss 0.00293162, acc 1\n",
      "2018-10-26T16:24:46.242441: step 5727, loss 0.000533019, acc 1\n",
      "2018-10-26T16:24:46.583532: step 5728, loss 0.00188032, acc 1\n",
      "2018-10-26T16:24:47.025349: step 5729, loss 0.00290234, acc 1\n",
      "2018-10-26T16:24:47.402342: step 5730, loss 0.00128669, acc 1\n",
      "2018-10-26T16:24:47.817235: step 5731, loss 0.00322876, acc 1\n",
      "2018-10-26T16:24:48.230131: step 5732, loss 0.00321914, acc 1\n",
      "2018-10-26T16:24:48.735779: step 5733, loss 0.000592413, acc 1\n",
      "2018-10-26T16:24:49.210512: step 5734, loss 0.0184365, acc 0.984375\n",
      "2018-10-26T16:24:49.709178: step 5735, loss 6.92563e-05, acc 1\n",
      "2018-10-26T16:24:50.170095: step 5736, loss 0.00146519, acc 1\n",
      "2018-10-26T16:24:50.641687: step 5737, loss 0.000507861, acc 1\n",
      "2018-10-26T16:24:51.134370: step 5738, loss 0.00480566, acc 1\n",
      "2018-10-26T16:24:51.522335: step 5739, loss 0.00549229, acc 1\n",
      "2018-10-26T16:24:51.884366: step 5740, loss 0.00227923, acc 1\n",
      "2018-10-26T16:24:52.302250: step 5741, loss 0.000958516, acc 1\n",
      "2018-10-26T16:24:52.742074: step 5742, loss 0.00686896, acc 1\n",
      "2018-10-26T16:24:53.102113: step 5743, loss 0.00116849, acc 1\n",
      "2018-10-26T16:24:53.528023: step 5744, loss 0.0104663, acc 1\n",
      "2018-10-26T16:24:53.894173: step 5745, loss 0.00150206, acc 1\n",
      "2018-10-26T16:24:54.231153: step 5746, loss 0.000292379, acc 1\n",
      "2018-10-26T16:24:54.580166: step 5747, loss 0.000620401, acc 1\n",
      "2018-10-26T16:24:54.915387: step 5748, loss 0.00694512, acc 1\n",
      "2018-10-26T16:24:55.243459: step 5749, loss 0.00530707, acc 1\n",
      "2018-10-26T16:24:55.558550: step 5750, loss 0.00262948, acc 1\n",
      "2018-10-26T16:24:55.875703: step 5751, loss 0.000120739, acc 1\n",
      "2018-10-26T16:24:56.197841: step 5752, loss 0.000687014, acc 1\n",
      "2018-10-26T16:24:56.508016: step 5753, loss 0.00221711, acc 1\n",
      "2018-10-26T16:24:56.879026: step 5754, loss 0.000157795, acc 1\n",
      "2018-10-26T16:24:57.250073: step 5755, loss 0.000190578, acc 1\n",
      "2018-10-26T16:24:57.650960: step 5756, loss 0.00569371, acc 1\n",
      "2018-10-26T16:24:58.055158: step 5757, loss 0.00169127, acc 1\n",
      "2018-10-26T16:24:58.391066: step 5758, loss 0.000209911, acc 1\n",
      "2018-10-26T16:24:58.710132: step 5759, loss 0.000780084, acc 1\n",
      "2018-10-26T16:24:59.030275: step 5760, loss 0.000274201, acc 1\n",
      "2018-10-26T16:24:59.357400: step 5761, loss 0.0268859, acc 0.984375\n",
      "2018-10-26T16:24:59.703358: step 5762, loss 0.00273109, acc 1\n",
      "2018-10-26T16:25:00.092436: step 5763, loss 0.000513235, acc 1\n",
      "2018-10-26T16:25:00.437517: step 5764, loss 0.00171508, acc 1\n",
      "2018-10-26T16:25:00.765641: step 5765, loss 0.000253338, acc 1\n",
      "2018-10-26T16:25:01.110762: step 5766, loss 0.000194544, acc 1\n",
      "2018-10-26T16:25:01.519687: step 5767, loss 0.0078459, acc 1\n",
      "2018-10-26T16:25:01.838839: step 5768, loss 0.00118909, acc 1\n",
      "2018-10-26T16:25:02.164898: step 5769, loss 0.00154165, acc 1\n",
      "2018-10-26T16:25:02.551864: step 5770, loss 0.00399661, acc 1\n",
      "2018-10-26T16:25:02.876997: step 5771, loss 0.000399195, acc 1\n",
      "2018-10-26T16:25:03.201173: step 5772, loss 0.00181174, acc 1\n",
      "2018-10-26T16:25:03.515293: step 5773, loss 0.00670528, acc 1\n",
      "2018-10-26T16:25:03.826463: step 5774, loss 0.000456175, acc 1\n",
      "2018-10-26T16:25:04.154583: step 5775, loss 0.000785023, acc 1\n",
      "2018-10-26T16:25:04.484731: step 5776, loss 0.000830617, acc 1\n",
      "2018-10-26T16:25:04.816838: step 5777, loss 0.000302047, acc 1\n",
      "2018-10-26T16:25:05.141945: step 5778, loss 0.000923935, acc 1\n",
      "2018-10-26T16:25:05.462092: step 5779, loss 0.000184687, acc 1\n",
      "2018-10-26T16:25:05.805383: step 5780, loss 0.000148223, acc 1\n",
      "2018-10-26T16:25:06.137285: step 5781, loss 0.00469356, acc 1\n",
      "2018-10-26T16:25:06.454438: step 5782, loss 0.00176548, acc 1\n",
      "2018-10-26T16:25:06.775272: step 5783, loss 0.00111864, acc 1\n",
      "2018-10-26T16:25:07.103703: step 5784, loss 0.00165557, acc 1\n",
      "2018-10-26T16:25:07.420857: step 5785, loss 0.0040934, acc 1\n",
      "2018-10-26T16:25:07.727041: step 5786, loss 0.000250163, acc 1\n",
      "2018-10-26T16:25:08.053165: step 5787, loss 0.000471336, acc 1\n",
      "2018-10-26T16:25:08.375305: step 5788, loss 0.00292903, acc 1\n",
      "2018-10-26T16:25:08.699439: step 5789, loss 0.00165526, acc 1\n",
      "2018-10-26T16:25:09.011606: step 5790, loss 0.000417426, acc 1\n",
      "2018-10-26T16:25:09.331758: step 5791, loss 0.0002352, acc 1\n",
      "2018-10-26T16:25:09.678854: step 5792, loss 0.00674216, acc 1\n",
      "2018-10-26T16:25:10.045842: step 5793, loss 0.00267012, acc 1\n",
      "2018-10-26T16:25:10.406877: step 5794, loss 0.000211671, acc 1\n",
      "2018-10-26T16:25:10.732009: step 5795, loss 0.000598718, acc 1\n",
      "2018-10-26T16:25:11.045172: step 5796, loss 0.000355728, acc 1\n",
      "2018-10-26T16:25:11.350357: step 5797, loss 0.00411256, acc 1\n",
      "2018-10-26T16:25:11.672498: step 5798, loss 0.000247703, acc 1\n",
      "2018-10-26T16:25:11.996766: step 5799, loss 0.000399708, acc 1\n",
      "2018-10-26T16:25:12.312787: step 5800, loss 0.00234794, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:25:13.116637: step 5800, loss 1.93487, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5800\n",
      "\n",
      "2018-10-26T16:25:13.836895: step 5801, loss 0.000177795, acc 1\n",
      "2018-10-26T16:25:14.140902: step 5802, loss 0.000441185, acc 1\n",
      "2018-10-26T16:25:14.685445: step 5803, loss 0.00147015, acc 1\n",
      "2018-10-26T16:25:15.067427: step 5804, loss 0.00148004, acc 1\n",
      "2018-10-26T16:25:15.399537: step 5805, loss 0.00223775, acc 1\n",
      "2018-10-26T16:25:15.731775: step 5806, loss 0.0010514, acc 1\n",
      "2018-10-26T16:25:16.068750: step 5807, loss 0.000826352, acc 1\n",
      "2018-10-26T16:25:16.406845: step 5808, loss 0.0104082, acc 1\n",
      "2018-10-26T16:25:16.710036: step 5809, loss 0.000347497, acc 1\n",
      "2018-10-26T16:25:17.015262: step 5810, loss 0.00058934, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:25:17.324470: step 5811, loss 0.00141128, acc 1\n",
      "2018-10-26T16:25:17.657505: step 5812, loss 0.00167598, acc 1\n",
      "2018-10-26T16:25:17.985701: step 5813, loss 0.000654475, acc 1\n",
      "2018-10-26T16:25:18.351715: step 5814, loss 0.00214278, acc 1\n",
      "2018-10-26T16:25:18.722662: step 5815, loss 0.00283241, acc 1\n",
      "2018-10-26T16:25:19.069731: step 5816, loss 0.000136857, acc 1\n",
      "2018-10-26T16:25:19.400919: step 5817, loss 0.000711546, acc 1\n",
      "2018-10-26T16:25:19.726977: step 5818, loss 0.000788564, acc 1\n",
      "2018-10-26T16:25:20.083025: step 5819, loss 0.00109111, acc 1\n",
      "2018-10-26T16:25:20.408159: step 5820, loss 0.000535266, acc 1\n",
      "2018-10-26T16:25:20.738273: step 5821, loss 0.0014946, acc 1\n",
      "2018-10-26T16:25:21.071384: step 5822, loss 0.00213312, acc 1\n",
      "2018-10-26T16:25:21.405494: step 5823, loss 0.000487424, acc 1\n",
      "2018-10-26T16:25:21.732616: step 5824, loss 0.000682157, acc 1\n",
      "2018-10-26T16:25:22.064733: step 5825, loss 0.00222598, acc 1\n",
      "2018-10-26T16:25:22.401190: step 5826, loss 0.00384682, acc 1\n",
      "2018-10-26T16:25:22.726959: step 5827, loss 8.92809e-05, acc 1\n",
      "2018-10-26T16:25:23.048103: step 5828, loss 0.0010627, acc 1\n",
      "2018-10-26T16:25:23.373234: step 5829, loss 0.000275884, acc 1\n",
      "2018-10-26T16:25:23.691383: step 5830, loss 9.23814e-05, acc 1\n",
      "2018-10-26T16:25:24.007574: step 5831, loss 0.000301082, acc 1\n",
      "2018-10-26T16:25:24.325733: step 5832, loss 0.0247045, acc 0.984375\n",
      "2018-10-26T16:25:24.640885: step 5833, loss 0.000524265, acc 1\n",
      "2018-10-26T16:25:24.971961: step 5834, loss 0.0039626, acc 1\n",
      "2018-10-26T16:25:25.302078: step 5835, loss 0.000219097, acc 1\n",
      "2018-10-26T16:25:25.614246: step 5836, loss 0.00104648, acc 1\n",
      "2018-10-26T16:25:25.946359: step 5837, loss 0.00121711, acc 1\n",
      "2018-10-26T16:25:26.267503: step 5838, loss 9.15931e-05, acc 1\n",
      "2018-10-26T16:25:26.671468: step 5839, loss 0.00378596, acc 1\n",
      "2018-10-26T16:25:26.995555: step 5840, loss 7.89836e-05, acc 1\n",
      "2018-10-26T16:25:27.350606: step 5841, loss 0.0016137, acc 1\n",
      "2018-10-26T16:25:27.786441: step 5842, loss 0.0569201, acc 0.984375\n",
      "2018-10-26T16:25:28.140550: step 5843, loss 0.00458027, acc 1\n",
      "2018-10-26T16:25:28.497542: step 5844, loss 0.000113292, acc 1\n",
      "2018-10-26T16:25:28.863564: step 5845, loss 0.00171753, acc 1\n",
      "2018-10-26T16:25:29.190748: step 5846, loss 0.000367279, acc 1\n",
      "2018-10-26T16:25:29.649466: step 5847, loss 0.00192977, acc 1\n",
      "2018-10-26T16:25:29.996536: step 5848, loss 0.00686976, acc 1\n",
      "2018-10-26T16:25:30.314687: step 5849, loss 0.000268464, acc 1\n",
      "2018-10-26T16:25:30.621958: step 5850, loss 0.00154675, acc 1\n",
      "2018-10-26T16:25:30.942012: step 5851, loss 0.000228334, acc 1\n",
      "2018-10-26T16:25:31.320041: step 5852, loss 0.00106171, acc 1\n",
      "2018-10-26T16:25:31.651170: step 5853, loss 0.00011389, acc 1\n",
      "2018-10-26T16:25:31.979239: step 5854, loss 0.000670608, acc 1\n",
      "2018-10-26T16:25:32.309357: step 5855, loss 0.000304057, acc 1\n",
      "2018-10-26T16:25:32.634490: step 5856, loss 0.00118687, acc 1\n",
      "2018-10-26T16:25:32.953662: step 5857, loss 4.05284e-05, acc 1\n",
      "2018-10-26T16:25:33.279764: step 5858, loss 0.00591492, acc 1\n",
      "2018-10-26T16:25:33.602902: step 5859, loss 0.000775301, acc 1\n",
      "2018-10-26T16:25:33.924045: step 5860, loss 0.000580341, acc 1\n",
      "2018-10-26T16:25:34.249177: step 5861, loss 0.000338082, acc 1\n",
      "2018-10-26T16:25:34.577297: step 5862, loss 0.00318742, acc 1\n",
      "2018-10-26T16:25:34.900555: step 5863, loss 0.000307085, acc 1\n",
      "2018-10-26T16:25:35.233627: step 5864, loss 0.000107539, acc 1\n",
      "2018-10-26T16:25:35.602665: step 5865, loss 0.000297556, acc 1\n",
      "2018-10-26T16:25:35.924099: step 5866, loss 0.000400114, acc 1\n",
      "2018-10-26T16:25:36.229728: step 5867, loss 0.00135753, acc 1\n",
      "2018-10-26T16:25:36.560009: step 5868, loss 0.000254723, acc 1\n",
      "2018-10-26T16:25:36.909792: step 5869, loss 0.00117988, acc 1\n",
      "2018-10-26T16:25:37.232210: step 5870, loss 0.00701066, acc 1\n",
      "2018-10-26T16:25:37.533172: step 5871, loss 0.00416384, acc 1\n",
      "2018-10-26T16:25:37.856518: step 5872, loss 0.00118122, acc 1\n",
      "2018-10-26T16:25:38.174191: step 5873, loss 0.000630642, acc 1\n",
      "2018-10-26T16:25:38.496721: step 5874, loss 0.0227739, acc 0.984375\n",
      "2018-10-26T16:25:38.811826: step 5875, loss 0.000816698, acc 1\n",
      "2018-10-26T16:25:39.178953: step 5876, loss 0.000297934, acc 1\n",
      "2018-10-26T16:25:39.552719: step 5877, loss 0.00231276, acc 1\n",
      "2018-10-26T16:25:39.925880: step 5878, loss 0.0263905, acc 0.984375\n",
      "2018-10-26T16:25:40.394835: step 5879, loss 0.000473679, acc 1\n",
      "2018-10-26T16:25:40.833692: step 5880, loss 0.0047005, acc 1\n",
      "2018-10-26T16:25:41.153608: step 5881, loss 0.00139737, acc 1\n",
      "2018-10-26T16:25:41.597861: step 5882, loss 0.00348828, acc 1\n",
      "2018-10-26T16:25:41.917265: step 5883, loss 6.15211e-05, acc 1\n",
      "2018-10-26T16:25:42.235303: step 5884, loss 0.010619, acc 1\n",
      "2018-10-26T16:25:42.579912: step 5885, loss 0.00139439, acc 1\n",
      "2018-10-26T16:25:42.914458: step 5886, loss 0.000396199, acc 1\n",
      "2018-10-26T16:25:43.236045: step 5887, loss 0.00274896, acc 1\n",
      "2018-10-26T16:25:43.569319: step 5888, loss 0.000393393, acc 1\n",
      "2018-10-26T16:25:43.900143: step 5889, loss 0.000929941, acc 1\n",
      "2018-10-26T16:25:44.238796: step 5890, loss 0.000480035, acc 1\n",
      "2018-10-26T16:25:44.574680: step 5891, loss 0.00453636, acc 1\n",
      "2018-10-26T16:25:44.895609: step 5892, loss 0.0196492, acc 0.984375\n",
      "2018-10-26T16:25:45.234190: step 5893, loss 0.00251239, acc 1\n",
      "2018-10-26T16:25:45.555021: step 5894, loss 0.00106777, acc 1\n",
      "2018-10-26T16:25:45.864186: step 5895, loss 0.000136108, acc 1\n",
      "2018-10-26T16:25:46.177362: step 5896, loss 0.00130918, acc 1\n",
      "2018-10-26T16:25:46.521371: step 5897, loss 0.00826693, acc 1\n",
      "2018-10-26T16:25:46.846894: step 5898, loss 0.0319671, acc 0.984375\n",
      "2018-10-26T16:25:47.161460: step 5899, loss 0.000633749, acc 1\n",
      "2018-10-26T16:25:47.482424: step 5900, loss 0.0554631, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:25:48.242303: step 5900, loss 1.97634, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-5900\n",
      "\n",
      "2018-10-26T16:25:48.821035: step 5901, loss 0.000763965, acc 1\n",
      "2018-10-26T16:25:49.135817: step 5902, loss 0.00198051, acc 1\n",
      "2018-10-26T16:25:49.569739: step 5903, loss 0.000301817, acc 1\n",
      "2018-10-26T16:25:49.916146: step 5904, loss 0.000295198, acc 1\n",
      "2018-10-26T16:25:50.234092: step 5905, loss 0.000216425, acc 1\n",
      "2018-10-26T16:25:50.556137: step 5906, loss 0.000925766, acc 1\n",
      "2018-10-26T16:25:50.854774: step 5907, loss 0.00102229, acc 1\n",
      "2018-10-26T16:25:51.161140: step 5908, loss 0.000406896, acc 1\n",
      "2018-10-26T16:25:51.507664: step 5909, loss 0.000645825, acc 1\n",
      "2018-10-26T16:25:51.948021: step 5910, loss 0.000226893, acc 1\n",
      "2018-10-26T16:25:52.340278: step 5911, loss 0.00333734, acc 1\n",
      "2018-10-26T16:25:52.664009: step 5912, loss 0.00340581, acc 1\n",
      "2018-10-26T16:25:53.025432: step 5913, loss 0.00311658, acc 1\n",
      "2018-10-26T16:25:53.389424: step 5914, loss 0.0010516, acc 1\n",
      "2018-10-26T16:25:53.789303: step 5915, loss 0.000915212, acc 1\n",
      "2018-10-26T16:25:54.241769: step 5916, loss 0.0012903, acc 1\n",
      "2018-10-26T16:25:54.680189: step 5917, loss 1.54936e-05, acc 1\n",
      "2018-10-26T16:25:55.060896: step 5918, loss 0.000998016, acc 1\n",
      "2018-10-26T16:25:55.478968: step 5919, loss 0.000367044, acc 1\n",
      "2018-10-26T16:25:55.879014: step 5920, loss 0.00132975, acc 1\n",
      "2018-10-26T16:25:56.260079: step 5921, loss 0.000117016, acc 1\n",
      "2018-10-26T16:25:56.617460: step 5922, loss 0.000699456, acc 1\n",
      "2018-10-26T16:25:56.975728: step 5923, loss 0.000366853, acc 1\n",
      "2018-10-26T16:25:57.377694: step 5924, loss 0.00115059, acc 1\n",
      "2018-10-26T16:25:57.784567: step 5925, loss 0.0134977, acc 0.984375\n",
      "2018-10-26T16:25:58.180541: step 5926, loss 4.2715e-05, acc 1\n",
      "2018-10-26T16:25:58.579444: step 5927, loss 0.0181512, acc 0.984375\n",
      "2018-10-26T16:25:58.934494: step 5928, loss 0.00119798, acc 1\n",
      "2018-10-26T16:25:59.312485: step 5929, loss 0.000333665, acc 1\n",
      "2018-10-26T16:25:59.610724: step 5930, loss 0.000407595, acc 1\n",
      "2018-10-26T16:25:59.913876: step 5931, loss 0.000929557, acc 1\n",
      "2018-10-26T16:26:00.247988: step 5932, loss 0.0257501, acc 0.984375\n",
      "2018-10-26T16:26:00.567131: step 5933, loss 0.00785834, acc 1\n",
      "2018-10-26T16:26:00.888312: step 5934, loss 0.000116988, acc 1\n",
      "2018-10-26T16:26:01.244324: step 5935, loss 0.00791474, acc 1\n",
      "2018-10-26T16:26:01.560480: step 5936, loss 0.000599875, acc 1\n",
      "2018-10-26T16:26:01.893587: step 5937, loss 0.000677754, acc 1\n",
      "2018-10-26T16:26:02.215727: step 5938, loss 6.01908e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:26:02.543850: step 5939, loss 0.000418617, acc 1\n",
      "2018-10-26T16:26:02.838064: step 5940, loss 0.00146749, acc 1\n",
      "2018-10-26T16:26:03.150230: step 5941, loss 0.0179069, acc 1\n",
      "2018-10-26T16:26:03.457441: step 5942, loss 0.000475685, acc 1\n",
      "2018-10-26T16:26:03.786531: step 5943, loss 0.0026806, acc 1\n",
      "2018-10-26T16:26:04.109668: step 5944, loss 0.000439717, acc 1\n",
      "2018-10-26T16:26:04.423831: step 5945, loss 0.00904893, acc 1\n",
      "2018-10-26T16:26:04.787855: step 5946, loss 0.00156743, acc 1\n",
      "2018-10-26T16:26:05.137950: step 5947, loss 0.000514365, acc 1\n",
      "2018-10-26T16:26:05.518922: step 5948, loss 0.00390093, acc 1\n",
      "2018-10-26T16:26:05.851017: step 5949, loss 0.000347278, acc 1\n",
      "2018-10-26T16:26:06.202076: step 5950, loss 0.000322683, acc 1\n",
      "2018-10-26T16:26:06.535185: step 5951, loss 0.00244905, acc 1\n",
      "2018-10-26T16:26:06.904237: step 5952, loss 0.000329371, acc 1\n",
      "2018-10-26T16:26:07.220354: step 5953, loss 5.62773e-05, acc 1\n",
      "2018-10-26T16:26:07.537512: step 5954, loss 0.000494424, acc 1\n",
      "2018-10-26T16:26:07.861686: step 5955, loss 0.00316147, acc 1\n",
      "2018-10-26T16:26:08.182784: step 5956, loss 0.000223482, acc 1\n",
      "2018-10-26T16:26:08.511905: step 5957, loss 0.0105597, acc 1\n",
      "2018-10-26T16:26:08.836041: step 5958, loss 0.000105851, acc 1\n",
      "2018-10-26T16:26:09.171144: step 5959, loss 0.0032163, acc 1\n",
      "2018-10-26T16:26:09.491327: step 5960, loss 0.000160821, acc 1\n",
      "2018-10-26T16:26:09.802514: step 5961, loss 0.00359432, acc 1\n",
      "2018-10-26T16:26:10.112629: step 5962, loss 0.00164205, acc 1\n",
      "2018-10-26T16:26:10.433770: step 5963, loss 0.00082438, acc 1\n",
      "2018-10-26T16:26:10.757906: step 5964, loss 0.0216005, acc 0.984375\n",
      "2018-10-26T16:26:11.077051: step 5965, loss 0.00233373, acc 1\n",
      "2018-10-26T16:26:11.384295: step 5966, loss 0.000293273, acc 1\n",
      "2018-10-26T16:26:11.725321: step 5967, loss 0.00383114, acc 1\n",
      "2018-10-26T16:26:12.079376: step 5968, loss 0.0371964, acc 0.984375\n",
      "2018-10-26T16:26:12.408493: step 5969, loss 0.000909499, acc 1\n",
      "2018-10-26T16:26:12.773519: step 5970, loss 0.00266217, acc 1\n",
      "2018-10-26T16:26:13.136549: step 5971, loss 0.000416997, acc 1\n",
      "2018-10-26T16:26:13.486616: step 5972, loss 0.000102624, acc 1\n",
      "2018-10-26T16:26:13.811817: step 5973, loss 0.000879318, acc 1\n",
      "2018-10-26T16:26:14.139889: step 5974, loss 0.000428726, acc 1\n",
      "2018-10-26T16:26:14.472980: step 5975, loss 0.00033122, acc 1\n",
      "2018-10-26T16:26:14.789133: step 5976, loss 0.000831531, acc 1\n",
      "2018-10-26T16:26:15.128256: step 5977, loss 0.0321653, acc 0.984375\n",
      "2018-10-26T16:26:15.439396: step 5978, loss 0.00266584, acc 1\n",
      "2018-10-26T16:26:15.760539: step 5979, loss 0.00219197, acc 1\n",
      "2018-10-26T16:26:16.074699: step 5980, loss 0.000502103, acc 1\n",
      "2018-10-26T16:26:16.400867: step 5981, loss 0.00274243, acc 1\n",
      "2018-10-26T16:26:16.720974: step 5982, loss 0.000166898, acc 1\n",
      "2018-10-26T16:26:17.085999: step 5983, loss 0.00161463, acc 1\n",
      "2018-10-26T16:26:17.411165: step 5984, loss 0.000320646, acc 1\n",
      "2018-10-26T16:26:17.730274: step 5985, loss 0.0022183, acc 1\n",
      "2018-10-26T16:26:18.031471: step 5986, loss 0.00234742, acc 1\n",
      "2018-10-26T16:26:18.348735: step 5987, loss 0.00124721, acc 1\n",
      "2018-10-26T16:26:18.689710: step 5988, loss 0.0152257, acc 0.984375\n",
      "2018-10-26T16:26:19.023819: step 5989, loss 0.000714296, acc 1\n",
      "2018-10-26T16:26:19.369909: step 5990, loss 0.00501202, acc 1\n",
      "2018-10-26T16:26:19.766844: step 5991, loss 8.84979e-05, acc 1\n",
      "2018-10-26T16:26:20.146819: step 5992, loss 0.00345579, acc 1\n",
      "2018-10-26T16:26:20.521817: step 5993, loss 0.00110088, acc 1\n",
      "2018-10-26T16:26:20.888898: step 5994, loss 0.000108156, acc 1\n",
      "2018-10-26T16:26:21.211074: step 5995, loss 0.0111907, acc 1\n",
      "2018-10-26T16:26:21.534116: step 5996, loss 0.000555674, acc 1\n",
      "2018-10-26T16:26:21.878229: step 5997, loss 0.00805498, acc 1\n",
      "2018-10-26T16:26:22.214296: step 5998, loss 0.00391227, acc 1\n",
      "2018-10-26T16:26:22.518479: step 5999, loss 0.000311604, acc 1\n",
      "2018-10-26T16:26:22.824663: step 6000, loss 0.0527595, acc 0.983333\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:26:23.583635: step 6000, loss 1.9974, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6000\n",
      "\n",
      "2018-10-26T16:26:24.151166: step 6001, loss 0.00151676, acc 1\n",
      "2018-10-26T16:26:24.479244: step 6002, loss 0.00137017, acc 1\n",
      "2018-10-26T16:26:24.919066: step 6003, loss 0.00521184, acc 1\n",
      "2018-10-26T16:26:25.296063: step 6004, loss 0.00156646, acc 1\n",
      "2018-10-26T16:26:25.611218: step 6005, loss 0.000112427, acc 1\n",
      "2018-10-26T16:26:25.951383: step 6006, loss 0.000390787, acc 1\n",
      "2018-10-26T16:26:26.271519: step 6007, loss 0.00158728, acc 1\n",
      "2018-10-26T16:26:26.578682: step 6008, loss 0.00490002, acc 1\n",
      "2018-10-26T16:26:26.901824: step 6009, loss 0.00169671, acc 1\n",
      "2018-10-26T16:26:27.217924: step 6010, loss 0.000555225, acc 1\n",
      "2018-10-26T16:26:27.530090: step 6011, loss 0.00246199, acc 1\n",
      "2018-10-26T16:26:27.865196: step 6012, loss 0.000135723, acc 1\n",
      "2018-10-26T16:26:28.170466: step 6013, loss 0.00181318, acc 1\n",
      "2018-10-26T16:26:28.529421: step 6014, loss 0.000334809, acc 1\n",
      "2018-10-26T16:26:28.833610: step 6015, loss 0.000931208, acc 1\n",
      "2018-10-26T16:26:29.171744: step 6016, loss 0.000203629, acc 1\n",
      "2018-10-26T16:26:29.497867: step 6017, loss 0.00177977, acc 1\n",
      "2018-10-26T16:26:29.804016: step 6018, loss 0.0306614, acc 0.984375\n",
      "2018-10-26T16:26:30.134133: step 6019, loss 0.00110719, acc 1\n",
      "2018-10-26T16:26:30.456276: step 6020, loss 0.000250631, acc 1\n",
      "2018-10-26T16:26:30.777536: step 6021, loss 0.0015549, acc 1\n",
      "2018-10-26T16:26:31.097659: step 6022, loss 0.00115331, acc 1\n",
      "2018-10-26T16:26:31.417703: step 6023, loss 0.000388879, acc 1\n",
      "2018-10-26T16:26:31.725884: step 6024, loss 0.00246881, acc 1\n",
      "2018-10-26T16:26:32.036051: step 6025, loss 0.0165617, acc 0.984375\n",
      "2018-10-26T16:26:32.341236: step 6026, loss 0.00107758, acc 1\n",
      "2018-10-26T16:26:32.658392: step 6027, loss 0.00204901, acc 1\n",
      "2018-10-26T16:26:32.986695: step 6028, loss 0.000103009, acc 1\n",
      "2018-10-26T16:26:33.319690: step 6029, loss 0.0108664, acc 1\n",
      "2018-10-26T16:26:33.641835: step 6030, loss 0.00787969, acc 1\n",
      "2018-10-26T16:26:33.970885: step 6031, loss 0.00560852, acc 1\n",
      "2018-10-26T16:26:34.287039: step 6032, loss 0.000597532, acc 1\n",
      "2018-10-26T16:26:34.616157: step 6033, loss 0.000974087, acc 1\n",
      "2018-10-26T16:26:34.923336: step 6034, loss 0.000116545, acc 1\n",
      "2018-10-26T16:26:35.242486: step 6035, loss 0.00547579, acc 1\n",
      "2018-10-26T16:26:35.566622: step 6036, loss 0.00674807, acc 1\n",
      "2018-10-26T16:26:35.898730: step 6037, loss 0.000269901, acc 1\n",
      "2018-10-26T16:26:36.236827: step 6038, loss 0.00642905, acc 1\n",
      "2018-10-26T16:26:36.589981: step 6039, loss 0.000645662, acc 1\n",
      "2018-10-26T16:26:36.928062: step 6040, loss 0.000491794, acc 1\n",
      "2018-10-26T16:26:37.245133: step 6041, loss 0.0108547, acc 1\n",
      "2018-10-26T16:26:37.568272: step 6042, loss 0.000538328, acc 1\n",
      "2018-10-26T16:26:37.891409: step 6043, loss 0.00155594, acc 1\n",
      "2018-10-26T16:26:38.218534: step 6044, loss 0.000475313, acc 1\n",
      "2018-10-26T16:26:38.545661: step 6045, loss 0.000296927, acc 1\n",
      "2018-10-26T16:26:38.863811: step 6046, loss 0.000401537, acc 1\n",
      "2018-10-26T16:26:39.186967: step 6047, loss 0.000146952, acc 1\n",
      "2018-10-26T16:26:39.525041: step 6048, loss 0.000137756, acc 1\n",
      "2018-10-26T16:26:39.855162: step 6049, loss 0.00489286, acc 1\n",
      "2018-10-26T16:26:40.184359: step 6050, loss 3.95257e-05, acc 1\n",
      "2018-10-26T16:26:40.481523: step 6051, loss 0.000140235, acc 1\n",
      "2018-10-26T16:26:40.807619: step 6052, loss 0.000556583, acc 1\n",
      "2018-10-26T16:26:41.140726: step 6053, loss 0.00198951, acc 1\n",
      "2018-10-26T16:26:41.463861: step 6054, loss 0.000507393, acc 1\n",
      "2018-10-26T16:26:41.818912: step 6055, loss 0.013439, acc 0.984375\n",
      "2018-10-26T16:26:42.184939: step 6056, loss 0.000454167, acc 1\n",
      "2018-10-26T16:26:42.520039: step 6057, loss 6.84261e-05, acc 1\n",
      "2018-10-26T16:26:42.902023: step 6058, loss 0.00204334, acc 1\n",
      "2018-10-26T16:26:43.212192: step 6059, loss 0.00143762, acc 1\n",
      "2018-10-26T16:26:43.541465: step 6060, loss 0.000350649, acc 1\n",
      "2018-10-26T16:26:43.896367: step 6061, loss 3.78511e-05, acc 1\n",
      "2018-10-26T16:26:44.210907: step 6062, loss 0.00809759, acc 1\n",
      "2018-10-26T16:26:44.524687: step 6063, loss 0.000653834, acc 1\n",
      "2018-10-26T16:26:44.850812: step 6064, loss 0.000109976, acc 1\n",
      "2018-10-26T16:26:45.163976: step 6065, loss 0.00261888, acc 1\n",
      "2018-10-26T16:26:45.492140: step 6066, loss 0.00111947, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:26:45.849145: step 6067, loss 0.00101867, acc 1\n",
      "2018-10-26T16:26:46.179264: step 6068, loss 0.00232982, acc 1\n",
      "2018-10-26T16:26:46.502400: step 6069, loss 0.00488982, acc 1\n",
      "2018-10-26T16:26:46.832518: step 6070, loss 0.00143705, acc 1\n",
      "2018-10-26T16:26:47.157650: step 6071, loss 0.00329647, acc 1\n",
      "2018-10-26T16:26:47.468864: step 6072, loss 0.00156295, acc 1\n",
      "2018-10-26T16:26:47.787032: step 6073, loss 0.00113825, acc 1\n",
      "2018-10-26T16:26:48.112100: step 6074, loss 0.000228525, acc 1\n",
      "2018-10-26T16:26:48.444214: step 6075, loss 0.0003161, acc 1\n",
      "2018-10-26T16:26:48.797312: step 6076, loss 0.000455776, acc 1\n",
      "2018-10-26T16:26:49.123399: step 6077, loss 0.000557647, acc 1\n",
      "2018-10-26T16:26:49.458504: step 6078, loss 0.000498297, acc 1\n",
      "2018-10-26T16:26:49.790617: step 6079, loss 5.05646e-05, acc 1\n",
      "2018-10-26T16:26:50.124721: step 6080, loss 0.000286868, acc 1\n",
      "2018-10-26T16:26:50.448077: step 6081, loss 0.000435133, acc 1\n",
      "2018-10-26T16:26:50.846025: step 6082, loss 0.00049505, acc 1\n",
      "2018-10-26T16:26:51.196859: step 6083, loss 0.00245725, acc 1\n",
      "2018-10-26T16:26:51.526975: step 6084, loss 0.000305193, acc 1\n",
      "2018-10-26T16:26:51.922916: step 6085, loss 0.00660445, acc 1\n",
      "2018-10-26T16:26:52.245056: step 6086, loss 0.0508065, acc 0.96875\n",
      "2018-10-26T16:26:52.572183: step 6087, loss 0.00917909, acc 1\n",
      "2018-10-26T16:26:52.938321: step 6088, loss 0.00268847, acc 1\n",
      "2018-10-26T16:26:53.282289: step 6089, loss 0.00107883, acc 1\n",
      "2018-10-26T16:26:53.612402: step 6090, loss 0.000569123, acc 1\n",
      "2018-10-26T16:26:53.954489: step 6091, loss 0.000238199, acc 1\n",
      "2018-10-26T16:26:54.278621: step 6092, loss 8.88581e-05, acc 1\n",
      "2018-10-26T16:26:54.633678: step 6093, loss 0.0121002, acc 0.984375\n",
      "2018-10-26T16:26:54.984737: step 6094, loss 0.00317878, acc 1\n",
      "2018-10-26T16:26:55.303884: step 6095, loss 0.0024183, acc 1\n",
      "2018-10-26T16:26:55.638989: step 6096, loss 0.00092376, acc 1\n",
      "2018-10-26T16:26:55.955144: step 6097, loss 0.00704201, acc 1\n",
      "2018-10-26T16:26:56.296234: step 6098, loss 0.00394983, acc 1\n",
      "2018-10-26T16:26:56.632425: step 6099, loss 0.00598817, acc 1\n",
      "2018-10-26T16:26:56.969432: step 6100, loss 0.000500249, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:26:57.805201: step 6100, loss 2.09427, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6100\n",
      "\n",
      "2018-10-26T16:26:58.407591: step 6101, loss 0.000454416, acc 1\n",
      "2018-10-26T16:26:58.768625: step 6102, loss 0.00067838, acc 1\n",
      "2018-10-26T16:26:59.312174: step 6103, loss 0.000447098, acc 1\n",
      "2018-10-26T16:26:59.830790: step 6104, loss 0.000180719, acc 1\n",
      "2018-10-26T16:27:00.303525: step 6105, loss 0.00822885, acc 1\n",
      "2018-10-26T16:27:00.786236: step 6106, loss 0.000201993, acc 1\n",
      "2018-10-26T16:27:01.261964: step 6107, loss 0.00171004, acc 1\n",
      "2018-10-26T16:27:01.724727: step 6108, loss 0.000651733, acc 1\n",
      "2018-10-26T16:27:02.167701: step 6109, loss 0.00255371, acc 1\n",
      "2018-10-26T16:27:02.580441: step 6110, loss 0.00271207, acc 1\n",
      "2018-10-26T16:27:02.979377: step 6111, loss 0.00146474, acc 1\n",
      "2018-10-26T16:27:03.414215: step 6112, loss 0.000323749, acc 1\n",
      "2018-10-26T16:27:03.847056: step 6113, loss 0.00209787, acc 1\n",
      "2018-10-26T16:27:04.318796: step 6114, loss 0.000320091, acc 1\n",
      "2018-10-26T16:27:04.717731: step 6115, loss 0.00135696, acc 1\n",
      "2018-10-26T16:27:05.073779: step 6116, loss 0.00249457, acc 1\n",
      "2018-10-26T16:27:05.422847: step 6117, loss 0.000686221, acc 1\n",
      "2018-10-26T16:27:05.753961: step 6118, loss 0.00085926, acc 1\n",
      "2018-10-26T16:27:06.106021: step 6119, loss 0.0275063, acc 0.984375\n",
      "2018-10-26T16:27:06.436142: step 6120, loss 0.000980352, acc 1\n",
      "2018-10-26T16:27:06.773286: step 6121, loss 0.00330713, acc 1\n",
      "2018-10-26T16:27:07.105351: step 6122, loss 0.000611121, acc 1\n",
      "2018-10-26T16:27:07.441494: step 6123, loss 0.0254799, acc 0.984375\n",
      "2018-10-26T16:27:07.760601: step 6124, loss 0.0085278, acc 1\n",
      "2018-10-26T16:27:08.083737: step 6125, loss 0.000880948, acc 1\n",
      "2018-10-26T16:27:08.399893: step 6126, loss 0.00160101, acc 1\n",
      "2018-10-26T16:27:08.753033: step 6127, loss 0.00287562, acc 1\n",
      "2018-10-26T16:27:09.069106: step 6128, loss 0.00258537, acc 1\n",
      "2018-10-26T16:27:09.416257: step 6129, loss 0.000357383, acc 1\n",
      "2018-10-26T16:27:09.808130: step 6130, loss 0.00166751, acc 1\n",
      "2018-10-26T16:27:10.173154: step 6131, loss 0.00120624, acc 1\n",
      "2018-10-26T16:27:10.514247: step 6132, loss 0.00761094, acc 1\n",
      "2018-10-26T16:27:10.843364: step 6133, loss 0.00352651, acc 1\n",
      "2018-10-26T16:27:11.192433: step 6134, loss 0.0537654, acc 0.984375\n",
      "2018-10-26T16:27:11.547487: step 6135, loss 0.0326256, acc 0.984375\n",
      "2018-10-26T16:27:11.864636: step 6136, loss 0.000497992, acc 1\n",
      "2018-10-26T16:27:12.182785: step 6137, loss 0.00344356, acc 1\n",
      "2018-10-26T16:27:12.506958: step 6138, loss 0.000942215, acc 1\n",
      "2018-10-26T16:27:12.846050: step 6139, loss 0.00158109, acc 1\n",
      "2018-10-26T16:27:13.184114: step 6140, loss 0.00528537, acc 1\n",
      "2018-10-26T16:27:13.518221: step 6141, loss 0.000468772, acc 1\n",
      "2018-10-26T16:27:13.842353: step 6142, loss 0.000361105, acc 1\n",
      "2018-10-26T16:27:14.155540: step 6143, loss 0.000362053, acc 1\n",
      "2018-10-26T16:27:14.473666: step 6144, loss 0.0190869, acc 0.984375\n",
      "2018-10-26T16:27:14.789823: step 6145, loss 0.000323881, acc 1\n",
      "2018-10-26T16:27:15.116950: step 6146, loss 0.000218586, acc 1\n",
      "2018-10-26T16:27:15.447065: step 6147, loss 0.000633214, acc 1\n",
      "2018-10-26T16:27:15.758233: step 6148, loss 0.000384234, acc 1\n",
      "2018-10-26T16:27:16.103311: step 6149, loss 0.00042006, acc 1\n",
      "2018-10-26T16:27:16.420497: step 6150, loss 0.000941379, acc 1\n",
      "2018-10-26T16:27:16.751579: step 6151, loss 0.00222374, acc 1\n",
      "2018-10-26T16:27:17.077709: step 6152, loss 3.0022e-05, acc 1\n",
      "2018-10-26T16:27:17.407828: step 6153, loss 0.00181567, acc 1\n",
      "2018-10-26T16:27:17.735958: step 6154, loss 0.00901594, acc 1\n",
      "2018-10-26T16:27:18.049111: step 6155, loss 0.00014441, acc 1\n",
      "2018-10-26T16:27:18.361278: step 6156, loss 0.00183623, acc 1\n",
      "2018-10-26T16:27:18.676436: step 6157, loss 0.00138424, acc 1\n",
      "2018-10-26T16:27:19.016529: step 6158, loss 0.000840191, acc 1\n",
      "2018-10-26T16:27:19.346647: step 6159, loss 0.0155431, acc 0.984375\n",
      "2018-10-26T16:27:19.666788: step 6160, loss 0.0030572, acc 1\n",
      "2018-10-26T16:27:19.983941: step 6161, loss 0.00175469, acc 1\n",
      "2018-10-26T16:27:20.320047: step 6162, loss 0.000150567, acc 1\n",
      "2018-10-26T16:27:20.651159: step 6163, loss 0.0311093, acc 0.984375\n",
      "2018-10-26T16:27:20.967315: step 6164, loss 0.00137062, acc 1\n",
      "2018-10-26T16:27:21.282475: step 6165, loss 0.00274031, acc 1\n",
      "2018-10-26T16:27:21.618574: step 6166, loss 0.000534865, acc 1\n",
      "2018-10-26T16:27:21.941773: step 6167, loss 0.000199968, acc 1\n",
      "2018-10-26T16:27:22.246898: step 6168, loss 0.000246243, acc 1\n",
      "2018-10-26T16:27:22.567040: step 6169, loss 0.000178785, acc 1\n",
      "2018-10-26T16:27:22.913116: step 6170, loss 0.00117562, acc 1\n",
      "2018-10-26T16:27:23.229393: step 6171, loss 0.000777888, acc 1\n",
      "2018-10-26T16:27:23.553408: step 6172, loss 0.000572344, acc 1\n",
      "2018-10-26T16:27:23.882527: step 6173, loss 0.000149954, acc 1\n",
      "2018-10-26T16:27:24.204667: step 6174, loss 0.00131052, acc 1\n",
      "2018-10-26T16:27:24.540803: step 6175, loss 0.00031237, acc 1\n",
      "2018-10-26T16:27:24.848943: step 6176, loss 0.00034767, acc 1\n",
      "2018-10-26T16:27:25.222943: step 6177, loss 0.00137159, acc 1\n",
      "2018-10-26T16:27:25.540100: step 6178, loss 0.000915671, acc 1\n",
      "2018-10-26T16:27:25.873206: step 6179, loss 0.000101562, acc 1\n",
      "2018-10-26T16:27:26.196346: step 6180, loss 0.00240932, acc 1\n",
      "2018-10-26T16:27:26.516487: step 6181, loss 0.00107993, acc 1\n",
      "2018-10-26T16:27:26.824667: step 6182, loss 0.000107879, acc 1\n",
      "2018-10-26T16:27:27.147897: step 6183, loss 0.00103253, acc 1\n",
      "2018-10-26T16:27:27.465954: step 6184, loss 0.000172763, acc 1\n",
      "2018-10-26T16:27:27.791158: step 6185, loss 0.00632194, acc 1\n",
      "2018-10-26T16:27:28.103248: step 6186, loss 0.000649578, acc 1\n",
      "2018-10-26T16:27:28.413448: step 6187, loss 0.000254238, acc 1\n",
      "2018-10-26T16:27:28.718607: step 6188, loss 0.00354485, acc 1\n",
      "2018-10-26T16:27:29.049720: step 6189, loss 0.000262678, acc 1\n",
      "2018-10-26T16:27:29.373854: step 6190, loss 0.00153683, acc 1\n",
      "2018-10-26T16:27:29.694199: step 6191, loss 0.00227415, acc 1\n",
      "2018-10-26T16:27:29.985220: step 6192, loss 0.00280202, acc 1\n",
      "2018-10-26T16:27:30.298386: step 6193, loss 0.000406919, acc 1\n",
      "2018-10-26T16:27:30.613542: step 6194, loss 0.00017502, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:27:30.932688: step 6195, loss 0.00183396, acc 1\n",
      "2018-10-26T16:27:31.262807: step 6196, loss 0.00116421, acc 1\n",
      "2018-10-26T16:27:31.580011: step 6197, loss 0.000319526, acc 1\n",
      "2018-10-26T16:27:31.890132: step 6198, loss 0.00189014, acc 1\n",
      "2018-10-26T16:27:32.207306: step 6199, loss 0.000173307, acc 1\n",
      "2018-10-26T16:27:32.535482: step 6200, loss 0.000466289, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:27:33.274465: step 6200, loss 2.09187, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6200\n",
      "\n",
      "2018-10-26T16:27:33.915734: step 6201, loss 0.0220036, acc 0.984375\n",
      "2018-10-26T16:27:34.240849: step 6202, loss 0.001865, acc 1\n",
      "2018-10-26T16:27:34.706608: step 6203, loss 0.00827182, acc 1\n",
      "2018-10-26T16:27:35.067686: step 6204, loss 0.000438189, acc 1\n",
      "2018-10-26T16:27:35.440728: step 6205, loss 0.0210732, acc 0.984375\n",
      "2018-10-26T16:27:35.755802: step 6206, loss 0.00344441, acc 1\n",
      "2018-10-26T16:27:36.086917: step 6207, loss 0.000698829, acc 1\n",
      "2018-10-26T16:27:36.407064: step 6208, loss 0.000918579, acc 1\n",
      "2018-10-26T16:27:36.771092: step 6209, loss 0.0173376, acc 0.984375\n",
      "2018-10-26T16:27:37.105197: step 6210, loss 0.00188706, acc 1\n",
      "2018-10-26T16:27:37.439333: step 6211, loss 0.000230609, acc 1\n",
      "2018-10-26T16:27:37.793356: step 6212, loss 0.00110572, acc 1\n",
      "2018-10-26T16:27:38.113502: step 6213, loss 8.40751e-05, acc 1\n",
      "2018-10-26T16:27:38.433646: step 6214, loss 0.000461823, acc 1\n",
      "2018-10-26T16:27:38.743818: step 6215, loss 0.000233753, acc 1\n",
      "2018-10-26T16:27:39.084911: step 6216, loss 0.0017081, acc 1\n",
      "2018-10-26T16:27:39.403056: step 6217, loss 0.000313363, acc 1\n",
      "2018-10-26T16:27:39.741232: step 6218, loss 0.00173858, acc 1\n",
      "2018-10-26T16:27:40.062295: step 6219, loss 0.00129932, acc 1\n",
      "2018-10-26T16:27:40.391490: step 6220, loss 0.00218879, acc 1\n",
      "2018-10-26T16:27:40.709567: step 6221, loss 3.00385e-05, acc 1\n",
      "2018-10-26T16:27:41.038689: step 6222, loss 0.000220433, acc 1\n",
      "2018-10-26T16:27:41.354915: step 6223, loss 0.000190356, acc 1\n",
      "2018-10-26T16:27:41.693936: step 6224, loss 0.00538654, acc 1\n",
      "2018-10-26T16:27:41.995131: step 6225, loss 0.000519372, acc 1\n",
      "2018-10-26T16:27:42.304387: step 6226, loss 0.000397057, acc 1\n",
      "2018-10-26T16:27:42.630437: step 6227, loss 0.0131656, acc 0.984375\n",
      "2018-10-26T16:27:42.948584: step 6228, loss 0.000477376, acc 1\n",
      "2018-10-26T16:27:43.280765: step 6229, loss 0.000460683, acc 1\n",
      "2018-10-26T16:27:43.657691: step 6230, loss 0.000414699, acc 1\n",
      "2018-10-26T16:27:43.990845: step 6231, loss 0.00443509, acc 1\n",
      "2018-10-26T16:27:44.315930: step 6232, loss 0.00180768, acc 1\n",
      "2018-10-26T16:27:44.676971: step 6233, loss 0.0112635, acc 1\n",
      "2018-10-26T16:27:45.009082: step 6234, loss 1.24544e-05, acc 1\n",
      "2018-10-26T16:27:45.343471: step 6235, loss 0.000362615, acc 1\n",
      "2018-10-26T16:27:45.675299: step 6236, loss 0.00673877, acc 1\n",
      "2018-10-26T16:27:46.000564: step 6237, loss 0.00382851, acc 1\n",
      "2018-10-26T16:27:46.329639: step 6238, loss 0.000493815, acc 1\n",
      "2018-10-26T16:27:46.649697: step 6239, loss 0.000213948, acc 1\n",
      "2018-10-26T16:27:46.954881: step 6240, loss 0.000382706, acc 1\n",
      "2018-10-26T16:27:47.263059: step 6241, loss 0.0177287, acc 0.984375\n",
      "2018-10-26T16:27:47.628080: step 6242, loss 0.00204054, acc 1\n",
      "2018-10-26T16:27:47.966177: step 6243, loss 0.000357345, acc 1\n",
      "2018-10-26T16:27:48.343173: step 6244, loss 0.000214827, acc 1\n",
      "2018-10-26T16:27:48.664380: step 6245, loss 0.0305718, acc 0.984375\n",
      "2018-10-26T16:27:48.978502: step 6246, loss 0.000362437, acc 1\n",
      "2018-10-26T16:27:49.298617: step 6247, loss 0.00313915, acc 1\n",
      "2018-10-26T16:27:49.620756: step 6248, loss 0.00011633, acc 1\n",
      "2018-10-26T16:27:49.948921: step 6249, loss 6.8077e-05, acc 1\n",
      "2018-10-26T16:27:50.279001: step 6250, loss 0.0249628, acc 0.984375\n",
      "2018-10-26T16:27:50.611161: step 6251, loss 0.000225305, acc 1\n",
      "2018-10-26T16:27:50.946214: step 6252, loss 0.00122599, acc 1\n",
      "2018-10-26T16:27:51.270453: step 6253, loss 0.0153114, acc 1\n",
      "2018-10-26T16:27:51.595485: step 6254, loss 0.0011282, acc 1\n",
      "2018-10-26T16:27:51.921610: step 6255, loss 0.000193897, acc 1\n",
      "2018-10-26T16:27:52.243749: step 6256, loss 0.00660993, acc 1\n",
      "2018-10-26T16:27:52.596806: step 6257, loss 0.00335352, acc 1\n",
      "2018-10-26T16:27:52.956843: step 6258, loss 0.0356862, acc 0.984375\n",
      "2018-10-26T16:27:53.326858: step 6259, loss 0.00919696, acc 1\n",
      "2018-10-26T16:27:53.649991: step 6260, loss 0.0120201, acc 1\n",
      "2018-10-26T16:27:53.965148: step 6261, loss 0.000500225, acc 1\n",
      "2018-10-26T16:27:54.287292: step 6262, loss 0.00284485, acc 1\n",
      "2018-10-26T16:27:54.625441: step 6263, loss 0.00417643, acc 1\n",
      "2018-10-26T16:27:54.954506: step 6264, loss 0.00710647, acc 1\n",
      "2018-10-26T16:27:55.287619: step 6265, loss 0.00140683, acc 1\n",
      "2018-10-26T16:27:55.640676: step 6266, loss 0.00100487, acc 1\n",
      "2018-10-26T16:27:55.959823: step 6267, loss 0.00523733, acc 1\n",
      "2018-10-26T16:27:56.291933: step 6268, loss 0.000443555, acc 1\n",
      "2018-10-26T16:27:56.604256: step 6269, loss 0.00104082, acc 1\n",
      "2018-10-26T16:27:56.929232: step 6270, loss 0.00131262, acc 1\n",
      "2018-10-26T16:27:57.259384: step 6271, loss 0.00169539, acc 1\n",
      "2018-10-26T16:27:57.604427: step 6272, loss 0.00033665, acc 1\n",
      "2018-10-26T16:27:57.940527: step 6273, loss 0.00446546, acc 1\n",
      "2018-10-26T16:27:58.261671: step 6274, loss 0.0148446, acc 0.984375\n",
      "2018-10-26T16:27:58.604753: step 6275, loss 0.000259253, acc 1\n",
      "2018-10-26T16:27:58.939858: step 6276, loss 0.00642647, acc 1\n",
      "2018-10-26T16:27:59.272968: step 6277, loss 0.0165411, acc 0.984375\n",
      "2018-10-26T16:27:59.595108: step 6278, loss 0.000336469, acc 1\n",
      "2018-10-26T16:27:59.910268: step 6279, loss 0.000487325, acc 1\n",
      "2018-10-26T16:28:00.258337: step 6280, loss 0.00468068, acc 1\n",
      "2018-10-26T16:28:00.579479: step 6281, loss 0.000549378, acc 1\n",
      "2018-10-26T16:28:00.903614: step 6282, loss 0.00318215, acc 1\n",
      "2018-10-26T16:28:01.221800: step 6283, loss 0.000279619, acc 1\n",
      "2018-10-26T16:28:01.543899: step 6284, loss 0.000258551, acc 1\n",
      "2018-10-26T16:28:01.835124: step 6285, loss 0.00230039, acc 1\n",
      "2018-10-26T16:28:02.156265: step 6286, loss 0.000221497, acc 1\n",
      "2018-10-26T16:28:02.488403: step 6287, loss 0.00261759, acc 1\n",
      "2018-10-26T16:28:02.850410: step 6288, loss 0.000354945, acc 1\n",
      "2018-10-26T16:28:03.210447: step 6289, loss 0.000710855, acc 1\n",
      "2018-10-26T16:28:03.592535: step 6290, loss 0.00249563, acc 1\n",
      "2018-10-26T16:28:03.962437: step 6291, loss 7.42297e-05, acc 1\n",
      "2018-10-26T16:28:04.346444: step 6292, loss 0.000351203, acc 1\n",
      "2018-10-26T16:28:04.721409: step 6293, loss 0.00240957, acc 1\n",
      "2018-10-26T16:28:05.160238: step 6294, loss 0.000854759, acc 1\n",
      "2018-10-26T16:28:05.592082: step 6295, loss 0.000262591, acc 1\n",
      "2018-10-26T16:28:06.050917: step 6296, loss 0.000611926, acc 1\n",
      "2018-10-26T16:28:06.500657: step 6297, loss 0.0172394, acc 0.984375\n",
      "2018-10-26T16:28:06.946465: step 6298, loss 0.000486475, acc 1\n",
      "2018-10-26T16:28:07.367340: step 6299, loss 0.000347113, acc 1\n",
      "2018-10-26T16:28:07.774254: step 6300, loss 0.000400114, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:28:08.762613: step 6300, loss 2.15235, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6300\n",
      "\n",
      "2018-10-26T16:28:09.444789: step 6301, loss 0.00133071, acc 1\n",
      "2018-10-26T16:28:09.844721: step 6302, loss 0.000962815, acc 1\n",
      "2018-10-26T16:28:10.308481: step 6303, loss 0.0256775, acc 0.984375\n",
      "2018-10-26T16:28:10.657577: step 6304, loss 7.7459e-05, acc 1\n",
      "2018-10-26T16:28:11.015592: step 6305, loss 0.000184621, acc 1\n",
      "2018-10-26T16:28:11.341776: step 6306, loss 0.000473379, acc 1\n",
      "2018-10-26T16:28:11.666852: step 6307, loss 2.60923e-05, acc 1\n",
      "2018-10-26T16:28:12.017129: step 6308, loss 0.0197215, acc 0.984375\n",
      "2018-10-26T16:28:12.368020: step 6309, loss 0.000345814, acc 1\n",
      "2018-10-26T16:28:12.682140: step 6310, loss 0.000673279, acc 1\n",
      "2018-10-26T16:28:12.992313: step 6311, loss 0.000729387, acc 1\n",
      "2018-10-26T16:28:13.311459: step 6312, loss 0.00215313, acc 1\n",
      "2018-10-26T16:28:13.627652: step 6313, loss 9.52148e-05, acc 1\n",
      "2018-10-26T16:28:13.946762: step 6314, loss 0.0009132, acc 1\n",
      "2018-10-26T16:28:14.241973: step 6315, loss 0.000388148, acc 1\n",
      "2018-10-26T16:28:14.548154: step 6316, loss 0.00128233, acc 1\n",
      "2018-10-26T16:28:14.875284: step 6317, loss 0.00158372, acc 1\n",
      "2018-10-26T16:28:15.201409: step 6318, loss 0.00407529, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:28:15.504598: step 6319, loss 0.00158007, acc 1\n",
      "2018-10-26T16:28:15.832723: step 6320, loss 0.00242046, acc 1\n",
      "2018-10-26T16:28:16.142959: step 6321, loss 0.000110924, acc 1\n",
      "2018-10-26T16:28:16.464035: step 6322, loss 0.000592588, acc 1\n",
      "2018-10-26T16:28:16.793191: step 6323, loss 0.000422072, acc 1\n",
      "2018-10-26T16:28:17.123303: step 6324, loss 0.00256047, acc 1\n",
      "2018-10-26T16:28:17.443475: step 6325, loss 0.00733354, acc 1\n",
      "2018-10-26T16:28:17.767588: step 6326, loss 0.000356787, acc 1\n",
      "2018-10-26T16:28:18.124599: step 6327, loss 0.000512747, acc 1\n",
      "2018-10-26T16:28:18.487670: step 6328, loss 0.0359099, acc 0.984375\n",
      "2018-10-26T16:28:18.806878: step 6329, loss 0.000934865, acc 1\n",
      "2018-10-26T16:28:19.131911: step 6330, loss 0.000449529, acc 1\n",
      "2018-10-26T16:28:19.471097: step 6331, loss 0.000162009, acc 1\n",
      "2018-10-26T16:28:19.830043: step 6332, loss 0.000475118, acc 1\n",
      "2018-10-26T16:28:20.144253: step 6333, loss 0.018824, acc 0.984375\n",
      "2018-10-26T16:28:20.469334: step 6334, loss 0.00155934, acc 1\n",
      "2018-10-26T16:28:20.792469: step 6335, loss 0.00072842, acc 1\n",
      "2018-10-26T16:28:21.117601: step 6336, loss 0.00339949, acc 1\n",
      "2018-10-26T16:28:21.448720: step 6337, loss 0.00676938, acc 1\n",
      "2018-10-26T16:28:21.764916: step 6338, loss 0.000663985, acc 1\n",
      "2018-10-26T16:28:22.076044: step 6339, loss 0.013894, acc 0.984375\n",
      "2018-10-26T16:28:22.398181: step 6340, loss 0.00097463, acc 1\n",
      "2018-10-26T16:28:22.740267: step 6341, loss 0.000344347, acc 1\n",
      "2018-10-26T16:28:23.090333: step 6342, loss 0.0205853, acc 0.984375\n",
      "2018-10-26T16:28:23.436408: step 6343, loss 0.00349208, acc 1\n",
      "2018-10-26T16:28:23.760582: step 6344, loss 0.00132057, acc 1\n",
      "2018-10-26T16:28:24.073703: step 6345, loss 0.00453118, acc 1\n",
      "2018-10-26T16:28:24.426760: step 6346, loss 0.00159716, acc 1\n",
      "2018-10-26T16:28:24.791785: step 6347, loss 0.0131186, acc 0.984375\n",
      "2018-10-26T16:28:25.119955: step 6348, loss 0.001313, acc 1\n",
      "2018-10-26T16:28:25.462994: step 6349, loss 0.00110438, acc 1\n",
      "2018-10-26T16:28:25.775279: step 6350, loss 0.000309322, acc 1\n",
      "2018-10-26T16:28:26.095303: step 6351, loss 0.00170183, acc 1\n",
      "2018-10-26T16:28:26.439420: step 6352, loss 0.00386948, acc 1\n",
      "2018-10-26T16:28:26.809428: step 6353, loss 0.00148247, acc 1\n",
      "2018-10-26T16:28:27.137544: step 6354, loss 0.00303061, acc 1\n",
      "2018-10-26T16:28:27.463646: step 6355, loss 0.000758592, acc 1\n",
      "2018-10-26T16:28:27.808822: step 6356, loss 0.00302189, acc 1\n",
      "2018-10-26T16:28:28.140837: step 6357, loss 0.000312783, acc 1\n",
      "2018-10-26T16:28:28.481010: step 6358, loss 0.000360335, acc 1\n",
      "2018-10-26T16:28:28.815166: step 6359, loss 0.000871459, acc 1\n",
      "2018-10-26T16:28:29.159115: step 6360, loss 0.00206544, acc 1\n",
      "2018-10-26T16:28:29.491274: step 6361, loss 0.0120264, acc 1\n",
      "2018-10-26T16:28:29.811373: step 6362, loss 0.00170854, acc 1\n",
      "2018-10-26T16:28:30.147477: step 6363, loss 0.000376406, acc 1\n",
      "2018-10-26T16:28:30.481647: step 6364, loss 0.00591344, acc 1\n",
      "2018-10-26T16:28:30.799736: step 6365, loss 0.00313011, acc 1\n",
      "2018-10-26T16:28:31.137832: step 6366, loss 0.00105774, acc 1\n",
      "2018-10-26T16:28:31.480992: step 6367, loss 0.000210558, acc 1\n",
      "2018-10-26T16:28:31.807042: step 6368, loss 0.000313218, acc 1\n",
      "2018-10-26T16:28:32.124194: step 6369, loss 0.000143534, acc 1\n",
      "2018-10-26T16:28:32.432371: step 6370, loss 0.000214501, acc 1\n",
      "2018-10-26T16:28:32.755508: step 6371, loss 0.000149056, acc 1\n",
      "2018-10-26T16:28:33.087709: step 6372, loss 0.00196076, acc 1\n",
      "2018-10-26T16:28:33.425784: step 6373, loss 0.00517401, acc 1\n",
      "2018-10-26T16:28:33.774836: step 6374, loss 0.00266902, acc 1\n",
      "2018-10-26T16:28:34.101910: step 6375, loss 0.00837307, acc 1\n",
      "2018-10-26T16:28:34.444992: step 6376, loss 0.00357008, acc 1\n",
      "2018-10-26T16:28:34.780105: step 6377, loss 0.039829, acc 0.984375\n",
      "2018-10-26T16:28:35.111216: step 6378, loss 0.000998295, acc 1\n",
      "2018-10-26T16:28:35.432392: step 6379, loss 0.00115629, acc 1\n",
      "2018-10-26T16:28:35.759481: step 6380, loss 0.00302512, acc 1\n",
      "2018-10-26T16:28:36.121515: step 6381, loss 0.0370423, acc 0.984375\n",
      "2018-10-26T16:28:36.447643: step 6382, loss 0.0011429, acc 1\n",
      "2018-10-26T16:28:36.768847: step 6383, loss 0.000297864, acc 1\n",
      "2018-10-26T16:28:37.108875: step 6384, loss 0.000821602, acc 1\n",
      "2018-10-26T16:28:37.441029: step 6385, loss 0.00167652, acc 1\n",
      "2018-10-26T16:28:37.758140: step 6386, loss 0.001933, acc 1\n",
      "2018-10-26T16:28:38.085267: step 6387, loss 6.8887e-05, acc 1\n",
      "2018-10-26T16:28:38.420375: step 6388, loss 0.00255249, acc 1\n",
      "2018-10-26T16:28:38.738521: step 6389, loss 0.000197156, acc 1\n",
      "2018-10-26T16:28:39.098559: step 6390, loss 0.00907804, acc 1\n",
      "2018-10-26T16:28:39.409731: step 6391, loss 0.00084837, acc 1\n",
      "2018-10-26T16:28:39.747863: step 6392, loss 0.000405802, acc 1\n",
      "2018-10-26T16:28:40.068969: step 6393, loss 0.0137368, acc 0.984375\n",
      "2018-10-26T16:28:40.403104: step 6394, loss 0.00901775, acc 1\n",
      "2018-10-26T16:28:40.734193: step 6395, loss 0.0010918, acc 1\n",
      "2018-10-26T16:28:41.090239: step 6396, loss 0.000266603, acc 1\n",
      "2018-10-26T16:28:41.428336: step 6397, loss 0.00437045, acc 1\n",
      "2018-10-26T16:28:41.742498: step 6398, loss 0.000152153, acc 1\n",
      "2018-10-26T16:28:42.053734: step 6399, loss 0.022715, acc 1\n",
      "2018-10-26T16:28:42.383843: step 6400, loss 0.000172858, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:28:43.159709: step 6400, loss 2.15193, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6400\n",
      "\n",
      "2018-10-26T16:28:43.785038: step 6401, loss 0.00153536, acc 1\n",
      "2018-10-26T16:28:44.114158: step 6402, loss 0.000146262, acc 1\n",
      "2018-10-26T16:28:44.550990: step 6403, loss 0.000436668, acc 1\n",
      "2018-10-26T16:28:44.891084: step 6404, loss 0.000650906, acc 1\n",
      "2018-10-26T16:28:45.219209: step 6405, loss 0.000362137, acc 1\n",
      "2018-10-26T16:28:45.547331: step 6406, loss 0.00219844, acc 1\n",
      "2018-10-26T16:28:45.867473: step 6407, loss 0.00234574, acc 1\n",
      "2018-10-26T16:28:46.181675: step 6408, loss 0.00280634, acc 1\n",
      "2018-10-26T16:28:46.515742: step 6409, loss 0.00481952, acc 1\n",
      "2018-10-26T16:28:46.838933: step 6410, loss 0.0110776, acc 1\n",
      "2018-10-26T16:28:47.162020: step 6411, loss 0.000497703, acc 1\n",
      "2018-10-26T16:28:47.489208: step 6412, loss 0.000106946, acc 1\n",
      "2018-10-26T16:28:47.863141: step 6413, loss 0.00458466, acc 1\n",
      "2018-10-26T16:28:48.208221: step 6414, loss 0.00177334, acc 1\n",
      "2018-10-26T16:28:48.559285: step 6415, loss 0.000778247, acc 1\n",
      "2018-10-26T16:28:48.894386: step 6416, loss 2.25198e-05, acc 1\n",
      "2018-10-26T16:28:49.228533: step 6417, loss 0.000524009, acc 1\n",
      "2018-10-26T16:28:49.597509: step 6418, loss 0.000337246, acc 1\n",
      "2018-10-26T16:28:49.951659: step 6419, loss 0.027467, acc 0.984375\n",
      "2018-10-26T16:28:50.276692: step 6420, loss 0.00193196, acc 1\n",
      "2018-10-26T16:28:50.627800: step 6421, loss 0.000536676, acc 1\n",
      "2018-10-26T16:28:50.968905: step 6422, loss 0.00019888, acc 1\n",
      "2018-10-26T16:28:51.283098: step 6423, loss 0.000552408, acc 1\n",
      "2018-10-26T16:28:51.613122: step 6424, loss 0.00522055, acc 1\n",
      "2018-10-26T16:28:51.939250: step 6425, loss 0.00107388, acc 1\n",
      "2018-10-26T16:28:52.252416: step 6426, loss 0.00989659, acc 1\n",
      "2018-10-26T16:28:52.598492: step 6427, loss 0.00066402, acc 1\n",
      "2018-10-26T16:28:53.041311: step 6428, loss 0.0267996, acc 0.984375\n",
      "2018-10-26T16:28:53.421336: step 6429, loss 0.00181054, acc 1\n",
      "2018-10-26T16:28:53.794335: step 6430, loss 0.00135003, acc 1\n",
      "2018-10-26T16:28:54.254066: step 6431, loss 0.000595584, acc 1\n",
      "2018-10-26T16:28:54.569225: step 6432, loss 0.000948313, acc 1\n",
      "2018-10-26T16:28:54.887375: step 6433, loss 0.00366585, acc 1\n",
      "2018-10-26T16:28:55.203530: step 6434, loss 0.00390406, acc 1\n",
      "2018-10-26T16:28:55.531652: step 6435, loss 4.8805e-05, acc 1\n",
      "2018-10-26T16:28:55.848807: step 6436, loss 0.00638216, acc 1\n",
      "2018-10-26T16:28:56.167994: step 6437, loss 0.0018885, acc 1\n",
      "2018-10-26T16:28:56.477130: step 6438, loss 0.000888405, acc 1\n",
      "2018-10-26T16:28:56.844525: step 6439, loss 0.0035117, acc 1\n",
      "2018-10-26T16:28:57.168283: step 6440, loss 0.0463886, acc 0.984375\n",
      "2018-10-26T16:28:57.499395: step 6441, loss 0.000152435, acc 1\n",
      "2018-10-26T16:28:57.814645: step 6442, loss 0.000378725, acc 1\n",
      "2018-10-26T16:28:58.145668: step 6443, loss 0.00245107, acc 1\n",
      "2018-10-26T16:28:58.496732: step 6444, loss 0.000426674, acc 1\n",
      "2018-10-26T16:28:58.828843: step 6445, loss 0.00267213, acc 1\n",
      "2018-10-26T16:28:59.134028: step 6446, loss 0.002996, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:28:59.458161: step 6447, loss 0.000621723, acc 1\n",
      "2018-10-26T16:28:59.782339: step 6448, loss 0.000411877, acc 1\n",
      "2018-10-26T16:29:00.130368: step 6449, loss 0.000201151, acc 1\n",
      "2018-10-26T16:29:00.438546: step 6450, loss 0.000292024, acc 1\n",
      "2018-10-26T16:29:00.768661: step 6451, loss 8.42119e-05, acc 1\n",
      "2018-10-26T16:29:01.067899: step 6452, loss 0.00393356, acc 1\n",
      "2018-10-26T16:29:01.408950: step 6453, loss 0.00845401, acc 1\n",
      "2018-10-26T16:29:01.771983: step 6454, loss 0.000206843, acc 1\n",
      "2018-10-26T16:29:02.129026: step 6455, loss 0.00055943, acc 1\n",
      "2018-10-26T16:29:02.454245: step 6456, loss 0.000315606, acc 1\n",
      "2018-10-26T16:29:02.772308: step 6457, loss 0.00214191, acc 1\n",
      "2018-10-26T16:29:03.094445: step 6458, loss 0.000650445, acc 1\n",
      "2018-10-26T16:29:03.411649: step 6459, loss 0.00649339, acc 1\n",
      "2018-10-26T16:29:03.750721: step 6460, loss 0.000329276, acc 1\n",
      "2018-10-26T16:29:04.115717: step 6461, loss 0.00101511, acc 1\n",
      "2018-10-26T16:29:04.451822: step 6462, loss 8.37798e-05, acc 1\n",
      "2018-10-26T16:29:04.793906: step 6463, loss 0.00400921, acc 1\n",
      "2018-10-26T16:29:05.132002: step 6464, loss 0.000652212, acc 1\n",
      "2018-10-26T16:29:05.463120: step 6465, loss 0.00859024, acc 1\n",
      "2018-10-26T16:29:05.791349: step 6466, loss 0.0025644, acc 1\n",
      "2018-10-26T16:29:06.131332: step 6467, loss 0.00216987, acc 1\n",
      "2018-10-26T16:29:06.463493: step 6468, loss 0.010505, acc 1\n",
      "2018-10-26T16:29:06.794183: step 6469, loss 0.0114131, acc 1\n",
      "2018-10-26T16:29:07.130661: step 6470, loss 0.000105653, acc 1\n",
      "2018-10-26T16:29:07.463773: step 6471, loss 3.50596e-05, acc 1\n",
      "2018-10-26T16:29:07.810844: step 6472, loss 0.000697599, acc 1\n",
      "2018-10-26T16:29:08.132984: step 6473, loss 9.35985e-05, acc 1\n",
      "2018-10-26T16:29:08.561898: step 6474, loss 0.00011661, acc 1\n",
      "2018-10-26T16:29:08.949801: step 6475, loss 0.000129344, acc 1\n",
      "2018-10-26T16:29:09.345744: step 6476, loss 0.000492003, acc 1\n",
      "2018-10-26T16:29:09.737746: step 6477, loss 0.00234705, acc 1\n",
      "2018-10-26T16:29:10.088757: step 6478, loss 0.000364768, acc 1\n",
      "2018-10-26T16:29:10.461815: step 6479, loss 0.00376914, acc 1\n",
      "2018-10-26T16:29:10.886627: step 6480, loss 1.72588e-05, acc 1\n",
      "2018-10-26T16:29:11.297575: step 6481, loss 0.00154525, acc 1\n",
      "2018-10-26T16:29:11.768327: step 6482, loss 0.00149169, acc 1\n",
      "2018-10-26T16:29:12.203152: step 6483, loss 0.000253662, acc 1\n",
      "2018-10-26T16:29:12.651909: step 6484, loss 0.00154272, acc 1\n",
      "2018-10-26T16:29:13.059821: step 6485, loss 0.000162322, acc 1\n",
      "2018-10-26T16:29:13.445789: step 6486, loss 0.0219985, acc 0.984375\n",
      "2018-10-26T16:29:13.819790: step 6487, loss 0.0561122, acc 0.984375\n",
      "2018-10-26T16:29:14.211795: step 6488, loss 0.00047656, acc 1\n",
      "2018-10-26T16:29:14.613669: step 6489, loss 0.00173377, acc 1\n",
      "2018-10-26T16:29:14.923887: step 6490, loss 0.000356434, acc 1\n",
      "2018-10-26T16:29:15.352693: step 6491, loss 0.0138159, acc 0.984375\n",
      "2018-10-26T16:29:15.725761: step 6492, loss 0.00167089, acc 1\n",
      "2018-10-26T16:29:16.070777: step 6493, loss 0.000560172, acc 1\n",
      "2018-10-26T16:29:16.413861: step 6494, loss 0.00279708, acc 1\n",
      "2018-10-26T16:29:16.743977: step 6495, loss 0.000346651, acc 1\n",
      "2018-10-26T16:29:17.060135: step 6496, loss 0.000958276, acc 1\n",
      "2018-10-26T16:29:17.370304: step 6497, loss 0.000500201, acc 1\n",
      "2018-10-26T16:29:17.706536: step 6498, loss 0.000243108, acc 1\n",
      "2018-10-26T16:29:18.039514: step 6499, loss 0.0193361, acc 0.984375\n",
      "2018-10-26T16:29:18.382598: step 6500, loss 0.00074292, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:29:19.130599: step 6500, loss 2.16725, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6500\n",
      "\n",
      "2018-10-26T16:29:19.762910: step 6501, loss 0.00104752, acc 1\n",
      "2018-10-26T16:29:20.071169: step 6502, loss 0.00175779, acc 1\n",
      "2018-10-26T16:29:20.452784: step 6503, loss 0.00377305, acc 1\n",
      "2018-10-26T16:29:20.816096: step 6504, loss 0.00375681, acc 1\n",
      "2018-10-26T16:29:21.171148: step 6505, loss 0.000110318, acc 1\n",
      "2018-10-26T16:29:21.487302: step 6506, loss 0.000313235, acc 1\n",
      "2018-10-26T16:29:21.830388: step 6507, loss 0.000433459, acc 1\n",
      "2018-10-26T16:29:22.152526: step 6508, loss 0.000933506, acc 1\n",
      "2018-10-26T16:29:22.483640: step 6509, loss 0.0046969, acc 1\n",
      "2018-10-26T16:29:22.833711: step 6510, loss 0.000403295, acc 1\n",
      "2018-10-26T16:29:23.190861: step 6511, loss 0.0155175, acc 0.984375\n",
      "2018-10-26T16:29:23.479979: step 6512, loss 0.000162151, acc 1\n",
      "2018-10-26T16:29:23.799126: step 6513, loss 0.000957343, acc 1\n",
      "2018-10-26T16:29:24.140215: step 6514, loss 0.00158327, acc 1\n",
      "2018-10-26T16:29:24.480309: step 6515, loss 0.00156419, acc 1\n",
      "2018-10-26T16:29:24.801527: step 6516, loss 0.000230971, acc 1\n",
      "2018-10-26T16:29:25.170493: step 6517, loss 0.000566586, acc 1\n",
      "2018-10-26T16:29:25.536483: step 6518, loss 0.0674967, acc 0.984375\n",
      "2018-10-26T16:29:25.893529: step 6519, loss 0.0447075, acc 0.984375\n",
      "2018-10-26T16:29:26.250610: step 6520, loss 0.00193936, acc 1\n",
      "2018-10-26T16:29:26.596655: step 6521, loss 0.000886572, acc 1\n",
      "2018-10-26T16:29:26.972646: step 6522, loss 0.000964003, acc 1\n",
      "2018-10-26T16:29:27.294829: step 6523, loss 0.00821308, acc 1\n",
      "2018-10-26T16:29:27.615932: step 6524, loss 0.00110548, acc 1\n",
      "2018-10-26T16:29:27.945048: step 6525, loss 0.00843289, acc 1\n",
      "2018-10-26T16:29:28.246265: step 6526, loss 0.00245805, acc 1\n",
      "2018-10-26T16:29:28.573371: step 6527, loss 8.75075e-05, acc 1\n",
      "2018-10-26T16:29:28.888617: step 6528, loss 0.00121178, acc 1\n",
      "2018-10-26T16:29:29.209669: step 6529, loss 0.00125403, acc 1\n",
      "2018-10-26T16:29:29.523832: step 6530, loss 0.000893837, acc 1\n",
      "2018-10-26T16:29:29.838989: step 6531, loss 0.00707841, acc 1\n",
      "2018-10-26T16:29:30.153148: step 6532, loss 0.0887146, acc 0.984375\n",
      "2018-10-26T16:29:30.459330: step 6533, loss 0.000288448, acc 1\n",
      "2018-10-26T16:29:30.768505: step 6534, loss 0.000741978, acc 1\n",
      "2018-10-26T16:29:31.109593: step 6535, loss 0.00151987, acc 1\n",
      "2018-10-26T16:29:31.424751: step 6536, loss 0.000893834, acc 1\n",
      "2018-10-26T16:29:31.757864: step 6537, loss 0.00375408, acc 1\n",
      "2018-10-26T16:29:32.082133: step 6538, loss 0.00142706, acc 1\n",
      "2018-10-26T16:29:32.406133: step 6539, loss 0.00695311, acc 1\n",
      "2018-10-26T16:29:32.771197: step 6540, loss 0.0232093, acc 0.984375\n",
      "2018-10-26T16:29:33.107746: step 6541, loss 0.0147372, acc 0.984375\n",
      "2018-10-26T16:29:33.423411: step 6542, loss 0.0181285, acc 0.984375\n",
      "2018-10-26T16:29:33.729718: step 6543, loss 0.000463639, acc 1\n",
      "2018-10-26T16:29:34.058235: step 6544, loss 0.0126703, acc 0.984375\n",
      "2018-10-26T16:29:34.402842: step 6545, loss 0.00655849, acc 1\n",
      "2018-10-26T16:29:34.721941: step 6546, loss 0.000190421, acc 1\n",
      "2018-10-26T16:29:35.102927: step 6547, loss 0.00290814, acc 1\n",
      "2018-10-26T16:29:35.444016: step 6548, loss 0.00123504, acc 1\n",
      "2018-10-26T16:29:35.801169: step 6549, loss 0.0100412, acc 1\n",
      "2018-10-26T16:29:36.115239: step 6550, loss 0.00170047, acc 1\n",
      "2018-10-26T16:29:36.432418: step 6551, loss 0.0266686, acc 0.984375\n",
      "2018-10-26T16:29:36.787491: step 6552, loss 0.0168, acc 0.984375\n",
      "2018-10-26T16:29:37.092610: step 6553, loss 0.00543066, acc 1\n",
      "2018-10-26T16:29:37.411829: step 6554, loss 0.000259224, acc 1\n",
      "2018-10-26T16:29:37.734893: step 6555, loss 0.000995327, acc 1\n",
      "2018-10-26T16:29:38.100970: step 6556, loss 0.00258761, acc 1\n",
      "2018-10-26T16:29:38.453971: step 6557, loss 0.000229806, acc 1\n",
      "2018-10-26T16:29:38.771201: step 6558, loss 0.000519519, acc 1\n",
      "2018-10-26T16:29:39.093264: step 6559, loss 0.00537443, acc 1\n",
      "2018-10-26T16:29:39.403435: step 6560, loss 0.00856647, acc 1\n",
      "2018-10-26T16:29:39.746520: step 6561, loss 0.00192234, acc 1\n",
      "2018-10-26T16:29:40.061676: step 6562, loss 0.000131275, acc 1\n",
      "2018-10-26T16:29:40.370852: step 6563, loss 9.70148e-05, acc 1\n",
      "2018-10-26T16:29:40.681020: step 6564, loss 0.00332272, acc 1\n",
      "2018-10-26T16:29:41.005155: step 6565, loss 0.000835123, acc 1\n",
      "2018-10-26T16:29:41.319314: step 6566, loss 0.00250823, acc 1\n",
      "2018-10-26T16:29:41.634473: step 6567, loss 0.000711126, acc 1\n",
      "2018-10-26T16:29:41.927728: step 6568, loss 0.000794971, acc 1\n",
      "2018-10-26T16:29:42.244840: step 6569, loss 0.038503, acc 0.96875\n",
      "2018-10-26T16:29:42.559003: step 6570, loss 0.00373072, acc 1\n",
      "2018-10-26T16:29:42.881140: step 6571, loss 0.000762105, acc 1\n",
      "2018-10-26T16:29:43.189318: step 6572, loss 0.000677403, acc 1\n",
      "2018-10-26T16:29:43.503548: step 6573, loss 0.00247437, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:29:43.822627: step 6574, loss 0.00037552, acc 1\n",
      "2018-10-26T16:29:44.143768: step 6575, loss 0.00278295, acc 1\n",
      "2018-10-26T16:29:44.448953: step 6576, loss 0.0244076, acc 0.984375\n",
      "2018-10-26T16:29:44.773089: step 6577, loss 0.00151679, acc 1\n",
      "2018-10-26T16:29:45.092236: step 6578, loss 0.000986157, acc 1\n",
      "2018-10-26T16:29:45.413375: step 6579, loss 0.000669869, acc 1\n",
      "2018-10-26T16:29:45.733522: step 6580, loss 0.000353567, acc 1\n",
      "2018-10-26T16:29:46.038707: step 6581, loss 0.00071625, acc 1\n",
      "2018-10-26T16:29:46.365835: step 6582, loss 0.00549939, acc 1\n",
      "2018-10-26T16:29:46.682066: step 6583, loss 0.00726847, acc 1\n",
      "2018-10-26T16:29:47.016174: step 6584, loss 0.0101365, acc 1\n",
      "2018-10-26T16:29:47.346250: step 6585, loss 0.000244302, acc 1\n",
      "2018-10-26T16:29:47.652394: step 6586, loss 0.000775884, acc 1\n",
      "2018-10-26T16:29:47.978524: step 6587, loss 0.12202, acc 0.984375\n",
      "2018-10-26T16:29:48.310637: step 6588, loss 0.000856848, acc 1\n",
      "2018-10-26T16:29:48.671672: step 6589, loss 5.64876e-05, acc 1\n",
      "2018-10-26T16:29:48.987828: step 6590, loss 7.65981e-05, acc 1\n",
      "2018-10-26T16:29:49.315950: step 6591, loss 0.000110364, acc 1\n",
      "2018-10-26T16:29:49.637091: step 6592, loss 0.00213082, acc 1\n",
      "2018-10-26T16:29:49.964219: step 6593, loss 0.000900341, acc 1\n",
      "2018-10-26T16:29:50.269404: step 6594, loss 0.000175617, acc 1\n",
      "2018-10-26T16:29:50.607531: step 6595, loss 0.0284463, acc 0.984375\n",
      "2018-10-26T16:29:50.920694: step 6596, loss 0.00108749, acc 1\n",
      "2018-10-26T16:29:51.246790: step 6597, loss 0.00280911, acc 1\n",
      "2018-10-26T16:29:51.592865: step 6598, loss 0.000589556, acc 1\n",
      "2018-10-26T16:29:51.944925: step 6599, loss 0.00101128, acc 1\n",
      "2018-10-26T16:29:52.263496: step 6600, loss 0.00326605, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:29:53.005092: step 6600, loss 2.25655, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6600\n",
      "\n",
      "2018-10-26T16:29:53.583550: step 6601, loss 0.014794, acc 1\n",
      "2018-10-26T16:29:53.909679: step 6602, loss 0.00100377, acc 1\n",
      "2018-10-26T16:29:54.435271: step 6603, loss 0.00331796, acc 1\n",
      "2018-10-26T16:29:54.892050: step 6604, loss 0.000700455, acc 1\n",
      "2018-10-26T16:29:55.288988: step 6605, loss 0.00422274, acc 1\n",
      "2018-10-26T16:29:55.691915: step 6606, loss 0.00129291, acc 1\n",
      "2018-10-26T16:29:56.007072: step 6607, loss 0.010012, acc 1\n",
      "2018-10-26T16:29:56.402111: step 6608, loss 0.000320119, acc 1\n",
      "2018-10-26T16:29:56.749133: step 6609, loss 0.00280034, acc 1\n",
      "2018-10-26T16:29:57.095168: step 6610, loss 0.00569706, acc 1\n",
      "2018-10-26T16:29:57.481155: step 6611, loss 0.000492528, acc 1\n",
      "2018-10-26T16:29:57.815240: step 6612, loss 0.000396429, acc 1\n",
      "2018-10-26T16:29:58.191280: step 6613, loss 1.61277e-05, acc 1\n",
      "2018-10-26T16:29:58.517365: step 6614, loss 0.00258635, acc 1\n",
      "2018-10-26T16:29:58.831524: step 6615, loss 0.00283648, acc 1\n",
      "2018-10-26T16:29:59.206523: step 6616, loss 0.0183379, acc 0.984375\n",
      "2018-10-26T16:29:59.617425: step 6617, loss 0.0115028, acc 1\n",
      "2018-10-26T16:30:00.020508: step 6618, loss 0.00543107, acc 1\n",
      "2018-10-26T16:30:00.438319: step 6619, loss 0.000719096, acc 1\n",
      "2018-10-26T16:30:00.802263: step 6620, loss 0.000447235, acc 1\n",
      "2018-10-26T16:30:01.158307: step 6621, loss 0.00890694, acc 1\n",
      "2018-10-26T16:30:01.495420: step 6622, loss 0.00172003, acc 1\n",
      "2018-10-26T16:30:01.858438: step 6623, loss 0.000760678, acc 1\n",
      "2018-10-26T16:30:02.196535: step 6624, loss 0.000808565, acc 1\n",
      "2018-10-26T16:30:02.514184: step 6625, loss 0.000461703, acc 1\n",
      "2018-10-26T16:30:02.962487: step 6626, loss 8.37097e-05, acc 1\n",
      "2018-10-26T16:30:03.354440: step 6627, loss 0.00272486, acc 1\n",
      "2018-10-26T16:30:03.679571: step 6628, loss 0.00140494, acc 1\n",
      "2018-10-26T16:30:04.016670: step 6629, loss 0.00245485, acc 1\n",
      "2018-10-26T16:30:04.355765: step 6630, loss 0.000845482, acc 1\n",
      "2018-10-26T16:30:04.694857: step 6631, loss 0.0120348, acc 0.984375\n",
      "2018-10-26T16:30:05.002037: step 6632, loss 3.98063e-05, acc 1\n",
      "2018-10-26T16:30:05.349117: step 6633, loss 0.00214954, acc 1\n",
      "2018-10-26T16:30:05.648717: step 6634, loss 0.00200407, acc 1\n",
      "2018-10-26T16:30:05.995383: step 6635, loss 0.000863451, acc 1\n",
      "2018-10-26T16:30:06.325501: step 6636, loss 0.020821, acc 0.984375\n",
      "2018-10-26T16:30:06.614728: step 6637, loss 0.00214471, acc 1\n",
      "2018-10-26T16:30:06.935870: step 6638, loss 0.00143389, acc 1\n",
      "2018-10-26T16:30:07.271976: step 6639, loss 0.00960009, acc 1\n",
      "2018-10-26T16:30:07.622037: step 6640, loss 0.00323948, acc 1\n",
      "2018-10-26T16:30:07.958138: step 6641, loss 0.000924519, acc 1\n",
      "2018-10-26T16:30:08.289256: step 6642, loss 0.00029531, acc 1\n",
      "2018-10-26T16:30:08.640317: step 6643, loss 0.0012521, acc 1\n",
      "2018-10-26T16:30:09.038286: step 6644, loss 0.000834492, acc 1\n",
      "2018-10-26T16:30:09.410259: step 6645, loss 0.000189676, acc 1\n",
      "2018-10-26T16:30:09.757335: step 6646, loss 0.00511789, acc 1\n",
      "2018-10-26T16:30:10.172226: step 6647, loss 0.000445995, acc 1\n",
      "2018-10-26T16:30:10.526278: step 6648, loss 0.00457117, acc 1\n",
      "2018-10-26T16:30:10.850411: step 6649, loss 0.00725575, acc 1\n",
      "2018-10-26T16:30:11.191586: step 6650, loss 0.00332356, acc 1\n",
      "2018-10-26T16:30:11.534583: step 6651, loss 0.000308913, acc 1\n",
      "2018-10-26T16:30:11.883651: step 6652, loss 0.000830523, acc 1\n",
      "2018-10-26T16:30:12.231722: step 6653, loss 0.0035543, acc 1\n",
      "2018-10-26T16:30:12.569817: step 6654, loss 0.00647403, acc 1\n",
      "2018-10-26T16:30:12.927862: step 6655, loss 0.00077068, acc 1\n",
      "2018-10-26T16:30:13.287946: step 6656, loss 0.0127624, acc 0.984375\n",
      "2018-10-26T16:30:13.664895: step 6657, loss 0.000200321, acc 1\n",
      "2018-10-26T16:30:14.034904: step 6658, loss 7.56332e-05, acc 1\n",
      "2018-10-26T16:30:14.445807: step 6659, loss 0.000227329, acc 1\n",
      "2018-10-26T16:30:14.805845: step 6660, loss 0.00166428, acc 1\n",
      "2018-10-26T16:30:15.186846: step 6661, loss 0.00279131, acc 1\n",
      "2018-10-26T16:30:15.558832: step 6662, loss 0.000161007, acc 1\n",
      "2018-10-26T16:30:15.954773: step 6663, loss 0.00272795, acc 1\n",
      "2018-10-26T16:30:16.380639: step 6664, loss 0.000639553, acc 1\n",
      "2018-10-26T16:30:16.825617: step 6665, loss 0.00174586, acc 1\n",
      "2018-10-26T16:30:17.290205: step 6666, loss 0.000335913, acc 1\n",
      "2018-10-26T16:30:17.783886: step 6667, loss 0.0121501, acc 0.984375\n",
      "2018-10-26T16:30:18.260612: step 6668, loss 0.0045657, acc 1\n",
      "2018-10-26T16:30:18.670517: step 6669, loss 0.0485799, acc 0.984375\n",
      "2018-10-26T16:30:19.112336: step 6670, loss 0.000449127, acc 1\n",
      "2018-10-26T16:30:19.510274: step 6671, loss 0.00295055, acc 1\n",
      "2018-10-26T16:30:19.891312: step 6672, loss 0.000243157, acc 1\n",
      "2018-10-26T16:30:20.277228: step 6673, loss 0.0017271, acc 1\n",
      "2018-10-26T16:30:20.660242: step 6674, loss 0.00228111, acc 1\n",
      "2018-10-26T16:30:21.124960: step 6675, loss 0.00162906, acc 1\n",
      "2018-10-26T16:30:21.546833: step 6676, loss 0.00254599, acc 1\n",
      "2018-10-26T16:30:21.942774: step 6677, loss 0.000208466, acc 1\n",
      "2018-10-26T16:30:22.379607: step 6678, loss 0.000124363, acc 1\n",
      "2018-10-26T16:30:22.716708: step 6679, loss 0.00219875, acc 1\n",
      "2018-10-26T16:30:23.118635: step 6680, loss 0.000662616, acc 1\n",
      "2018-10-26T16:30:23.443827: step 6681, loss 0.00062914, acc 1\n",
      "2018-10-26T16:30:23.788845: step 6682, loss 0.000661432, acc 1\n",
      "2018-10-26T16:30:24.134921: step 6683, loss 0.00282329, acc 1\n",
      "2018-10-26T16:30:24.477006: step 6684, loss 0.00785951, acc 1\n",
      "2018-10-26T16:30:24.841121: step 6685, loss 0.00958909, acc 1\n",
      "2018-10-26T16:30:25.258915: step 6686, loss 0.00101466, acc 1\n",
      "2018-10-26T16:30:25.596013: step 6687, loss 0.00459811, acc 1\n",
      "2018-10-26T16:30:25.923144: step 6688, loss 0.0611674, acc 0.984375\n",
      "2018-10-26T16:30:26.351049: step 6689, loss 0.00466514, acc 1\n",
      "2018-10-26T16:30:26.699171: step 6690, loss 0.00102131, acc 1\n",
      "2018-10-26T16:30:27.052178: step 6691, loss 0.00216048, acc 1\n",
      "2018-10-26T16:30:27.403190: step 6692, loss 0.000174124, acc 1\n",
      "2018-10-26T16:30:27.901853: step 6693, loss 0.000577553, acc 1\n",
      "2018-10-26T16:30:28.365617: step 6694, loss 0.000621775, acc 1\n",
      "2018-10-26T16:30:28.717674: step 6695, loss 0.000886468, acc 1\n",
      "2018-10-26T16:30:29.072724: step 6696, loss 0.0499778, acc 0.984375\n",
      "2018-10-26T16:30:29.446816: step 6697, loss 0.00113066, acc 1\n",
      "2018-10-26T16:30:29.882564: step 6698, loss 0.000749177, acc 1\n",
      "2018-10-26T16:30:30.239608: step 6699, loss 0.00190502, acc 1\n",
      "2018-10-26T16:30:30.577753: step 6700, loss 5.30671e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:30:31.465331: step 6700, loss 2.34397, acc 0.702627\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6700\n",
      "\n",
      "2018-10-26T16:30:32.084681: step 6701, loss 0.00130863, acc 1\n",
      "2018-10-26T16:30:32.453695: step 6702, loss 0.080637, acc 0.984375\n",
      "2018-10-26T16:30:32.984273: step 6703, loss 0.000240031, acc 1\n",
      "2018-10-26T16:30:33.419112: step 6704, loss 0.0106043, acc 1\n",
      "2018-10-26T16:30:33.778190: step 6705, loss 0.0919302, acc 0.984375\n",
      "2018-10-26T16:30:34.117246: step 6706, loss 0.000898507, acc 1\n",
      "2018-10-26T16:30:34.469307: step 6707, loss 0.0010329, acc 1\n",
      "2018-10-26T16:30:34.837323: step 6708, loss 0.00150051, acc 1\n",
      "2018-10-26T16:30:35.173425: step 6709, loss 0.00161887, acc 1\n",
      "2018-10-26T16:30:35.510581: step 6710, loss 0.000652329, acc 1\n",
      "2018-10-26T16:30:35.879537: step 6711, loss 0.00160476, acc 1\n",
      "2018-10-26T16:30:36.262514: step 6712, loss 0.0362428, acc 0.984375\n",
      "2018-10-26T16:30:36.610671: step 6713, loss 0.0331709, acc 0.984375\n",
      "2018-10-26T16:30:36.989575: step 6714, loss 0.000105487, acc 1\n",
      "2018-10-26T16:30:37.327669: step 6715, loss 0.000395495, acc 1\n",
      "2018-10-26T16:30:37.665768: step 6716, loss 0.00168905, acc 1\n",
      "2018-10-26T16:30:38.025060: step 6717, loss 0.00166515, acc 1\n",
      "2018-10-26T16:30:38.360908: step 6718, loss 0.000770084, acc 1\n",
      "2018-10-26T16:30:38.711054: step 6719, loss 0.00161, acc 1\n",
      "2018-10-26T16:30:39.229600: step 6720, loss 0.000178129, acc 1\n",
      "2018-10-26T16:30:39.569742: step 6721, loss 0.000644933, acc 1\n",
      "2018-10-26T16:30:39.950662: step 6722, loss 0.000137328, acc 1\n",
      "2018-10-26T16:30:40.402453: step 6723, loss 0.0115638, acc 1\n",
      "2018-10-26T16:30:40.758505: step 6724, loss 0.00525658, acc 1\n",
      "2018-10-26T16:30:41.115546: step 6725, loss 0.00112166, acc 1\n",
      "2018-10-26T16:30:41.442675: step 6726, loss 0.00165934, acc 1\n",
      "2018-10-26T16:30:41.780773: step 6727, loss 0.000113978, acc 1\n",
      "2018-10-26T16:30:42.117873: step 6728, loss 0.000410733, acc 1\n",
      "2018-10-26T16:30:42.517801: step 6729, loss 0.00141086, acc 1\n",
      "2018-10-26T16:30:42.845967: step 6730, loss 0.000423973, acc 1\n",
      "2018-10-26T16:30:43.204010: step 6731, loss 0.000746279, acc 1\n",
      "2018-10-26T16:30:43.599910: step 6732, loss 0.0018213, acc 1\n",
      "2018-10-26T16:30:43.979898: step 6733, loss 0.00167606, acc 1\n",
      "2018-10-26T16:30:44.336944: step 6734, loss 0.000453992, acc 1\n",
      "2018-10-26T16:30:44.719917: step 6735, loss 8.19914e-05, acc 1\n",
      "2018-10-26T16:30:45.064998: step 6736, loss 0.000662403, acc 1\n",
      "2018-10-26T16:30:45.455951: step 6737, loss 0.000513861, acc 1\n",
      "2018-10-26T16:30:45.792053: step 6738, loss 0.00128192, acc 1\n",
      "2018-10-26T16:30:46.162063: step 6739, loss 0.00137032, acc 1\n",
      "2018-10-26T16:30:46.515120: step 6740, loss 0.00128786, acc 1\n",
      "2018-10-26T16:30:46.864188: step 6741, loss 0.00143768, acc 1\n",
      "2018-10-26T16:30:47.281130: step 6742, loss 0.00013386, acc 1\n",
      "2018-10-26T16:30:47.635127: step 6743, loss 0.00666956, acc 1\n",
      "2018-10-26T16:30:47.965247: step 6744, loss 0.00747611, acc 1\n",
      "2018-10-26T16:30:48.281488: step 6745, loss 0.000878913, acc 1\n",
      "2018-10-26T16:30:48.608526: step 6746, loss 0.00200624, acc 1\n",
      "2018-10-26T16:30:48.939647: step 6747, loss 0.000406769, acc 1\n",
      "2018-10-26T16:30:49.279734: step 6748, loss 0.000851538, acc 1\n",
      "2018-10-26T16:30:49.583920: step 6749, loss 0.00364943, acc 1\n",
      "2018-10-26T16:30:50.068698: step 6750, loss 0.000328012, acc 1\n",
      "2018-10-26T16:30:50.434649: step 6751, loss 0.0179822, acc 0.984375\n",
      "2018-10-26T16:30:50.807852: step 6752, loss 0.00703762, acc 1\n",
      "2018-10-26T16:30:51.178830: step 6753, loss 0.000138335, acc 1\n",
      "2018-10-26T16:30:51.549673: step 6754, loss 1.722e-05, acc 1\n",
      "2018-10-26T16:30:51.924741: step 6755, loss 0.000350967, acc 1\n",
      "2018-10-26T16:30:52.265760: step 6756, loss 0.0342255, acc 0.984375\n",
      "2018-10-26T16:30:52.661698: step 6757, loss 0.000231676, acc 1\n",
      "2018-10-26T16:30:53.039769: step 6758, loss 0.00887365, acc 1\n",
      "2018-10-26T16:30:53.380777: step 6759, loss 0.0266329, acc 0.984375\n",
      "2018-10-26T16:30:53.760762: step 6760, loss 0.0164868, acc 0.984375\n",
      "2018-10-26T16:30:54.120835: step 6761, loss 0.00237521, acc 1\n",
      "2018-10-26T16:30:54.506769: step 6762, loss 0.000404555, acc 1\n",
      "2018-10-26T16:30:54.886753: step 6763, loss 0.000445215, acc 1\n",
      "2018-10-26T16:30:55.233826: step 6764, loss 0.000511127, acc 1\n",
      "2018-10-26T16:30:55.581936: step 6765, loss 0.00111256, acc 1\n",
      "2018-10-26T16:30:55.935980: step 6766, loss 0.00205587, acc 1\n",
      "2018-10-26T16:30:56.282026: step 6767, loss 0.0744519, acc 0.984375\n",
      "2018-10-26T16:30:56.624152: step 6768, loss 6.10421e-05, acc 1\n",
      "2018-10-26T16:30:56.982154: step 6769, loss 0.019737, acc 1\n",
      "2018-10-26T16:30:57.338236: step 6770, loss 0.0149792, acc 0.984375\n",
      "2018-10-26T16:30:57.672312: step 6771, loss 0.01572, acc 0.984375\n",
      "2018-10-26T16:30:58.039333: step 6772, loss 0.000385725, acc 1\n",
      "2018-10-26T16:30:58.384408: step 6773, loss 0.00469676, acc 1\n",
      "2018-10-26T16:30:58.717558: step 6774, loss 0.0022328, acc 1\n",
      "2018-10-26T16:30:59.050742: step 6775, loss 0.000479337, acc 1\n",
      "2018-10-26T16:30:59.393740: step 6776, loss 2.35642e-05, acc 1\n",
      "2018-10-26T16:30:59.726826: step 6777, loss 0.00185223, acc 1\n",
      "2018-10-26T16:31:00.100823: step 6778, loss 0.00117084, acc 1\n",
      "2018-10-26T16:31:00.454875: step 6779, loss 0.00130758, acc 1\n",
      "2018-10-26T16:31:00.816912: step 6780, loss 2.72485e-05, acc 1\n",
      "2018-10-26T16:31:01.133114: step 6781, loss 0.00227687, acc 1\n",
      "2018-10-26T16:31:01.491106: step 6782, loss 0.00267808, acc 1\n",
      "2018-10-26T16:31:01.807262: step 6783, loss 0.000217887, acc 1\n",
      "2018-10-26T16:31:02.128404: step 6784, loss 0.000926613, acc 1\n",
      "2018-10-26T16:31:02.455533: step 6785, loss 0.000822875, acc 1\n",
      "2018-10-26T16:31:02.788641: step 6786, loss 0.000301355, acc 1\n",
      "2018-10-26T16:31:03.126737: step 6787, loss 0.00123317, acc 1\n",
      "2018-10-26T16:31:03.453952: step 6788, loss 0.00179319, acc 1\n",
      "2018-10-26T16:31:03.826866: step 6789, loss 0.00378728, acc 1\n",
      "2018-10-26T16:31:04.156984: step 6790, loss 0.00818186, acc 1\n",
      "2018-10-26T16:31:04.470194: step 6791, loss 0.0763725, acc 0.96875\n",
      "2018-10-26T16:31:04.812233: step 6792, loss 0.00616588, acc 1\n",
      "2018-10-26T16:31:05.118417: step 6793, loss 0.00279128, acc 1\n",
      "2018-10-26T16:31:05.434570: step 6794, loss 0.000195211, acc 1\n",
      "2018-10-26T16:31:05.820587: step 6795, loss 0.0231716, acc 0.984375\n",
      "2018-10-26T16:31:06.198531: step 6796, loss 0.00572929, acc 1\n",
      "2018-10-26T16:31:06.570570: step 6797, loss 0.000490191, acc 1\n",
      "2018-10-26T16:31:06.912621: step 6798, loss 0.00049635, acc 1\n",
      "2018-10-26T16:31:07.260692: step 6799, loss 0.000813075, acc 1\n",
      "2018-10-26T16:31:07.741408: step 6800, loss 0.00318973, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:31:08.562215: step 6800, loss 2.31531, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6800\n",
      "\n",
      "2018-10-26T16:31:09.193528: step 6801, loss 0.000336026, acc 1\n",
      "2018-10-26T16:31:09.515668: step 6802, loss 0.00039326, acc 1\n",
      "2018-10-26T16:31:09.976435: step 6803, loss 0.00302539, acc 1\n",
      "2018-10-26T16:31:10.296581: step 6804, loss 0.000960079, acc 1\n",
      "2018-10-26T16:31:10.652632: step 6805, loss 0.00065187, acc 1\n",
      "2018-10-26T16:31:10.972819: step 6806, loss 0.000137786, acc 1\n",
      "2018-10-26T16:31:11.296908: step 6807, loss 0.000314936, acc 1\n",
      "2018-10-26T16:31:11.598104: step 6808, loss 0.00129114, acc 1\n",
      "2018-10-26T16:31:11.922237: step 6809, loss 0.000209093, acc 1\n",
      "2018-10-26T16:31:12.274297: step 6810, loss 0.000511881, acc 1\n",
      "2018-10-26T16:31:12.615383: step 6811, loss 0.0342298, acc 0.984375\n",
      "2018-10-26T16:31:12.944004: step 6812, loss 0.0118031, acc 0.984375\n",
      "2018-10-26T16:31:13.253680: step 6813, loss 0.00143316, acc 1\n",
      "2018-10-26T16:31:13.578812: step 6814, loss 0.000227074, acc 1\n",
      "2018-10-26T16:31:13.887988: step 6815, loss 0.00189926, acc 1\n",
      "2018-10-26T16:31:14.213115: step 6816, loss 0.000203102, acc 1\n",
      "2018-10-26T16:31:14.528487: step 6817, loss 0.00647168, acc 1\n",
      "2018-10-26T16:31:14.856399: step 6818, loss 0.000748569, acc 1\n",
      "2018-10-26T16:31:15.171557: step 6819, loss 0.0126184, acc 0.984375\n",
      "2018-10-26T16:31:15.493694: step 6820, loss 0.000211467, acc 1\n",
      "2018-10-26T16:31:15.823812: step 6821, loss 0.001328, acc 1\n",
      "2018-10-26T16:31:16.140968: step 6822, loss 0.000271243, acc 1\n",
      "2018-10-26T16:31:16.502997: step 6823, loss 0.000680412, acc 1\n",
      "2018-10-26T16:31:16.809869: step 6824, loss 0.000204921, acc 1\n",
      "2018-10-26T16:31:17.133364: step 6825, loss 0.00424164, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:31:17.446528: step 6826, loss 0.000891933, acc 1\n",
      "2018-10-26T16:31:17.765626: step 6827, loss 0.00139616, acc 1\n",
      "2018-10-26T16:31:18.117686: step 6828, loss 0.000369087, acc 1\n",
      "2018-10-26T16:31:18.465056: step 6829, loss 0.00036189, acc 1\n",
      "2018-10-26T16:31:18.831775: step 6830, loss 0.00194821, acc 1\n",
      "2018-10-26T16:31:19.158904: step 6831, loss 0.000461167, acc 1\n",
      "2018-10-26T16:31:19.502164: step 6832, loss 0.00125699, acc 1\n",
      "2018-10-26T16:31:19.892979: step 6833, loss 0.00147079, acc 1\n",
      "2018-10-26T16:31:20.278910: step 6834, loss 0.000632272, acc 1\n",
      "2018-10-26T16:31:20.624985: step 6835, loss 0.0117203, acc 0.984375\n",
      "2018-10-26T16:31:21.063813: step 6836, loss 0.0338238, acc 0.96875\n",
      "2018-10-26T16:31:21.435883: step 6837, loss 0.000214907, acc 1\n",
      "2018-10-26T16:31:21.785888: step 6838, loss 0.000136448, acc 1\n",
      "2018-10-26T16:31:22.177836: step 6839, loss 0.000444519, acc 1\n",
      "2018-10-26T16:31:22.605692: step 6840, loss 0.00017473, acc 1\n",
      "2018-10-26T16:31:23.041527: step 6841, loss 0.0216915, acc 0.984375\n",
      "2018-10-26T16:31:23.511450: step 6842, loss 0.0142698, acc 0.984375\n",
      "2018-10-26T16:31:23.978026: step 6843, loss 0.00212598, acc 1\n",
      "2018-10-26T16:31:24.476694: step 6844, loss 0.00060021, acc 1\n",
      "2018-10-26T16:31:24.907589: step 6845, loss 0.000668773, acc 1\n",
      "2018-10-26T16:31:25.286692: step 6846, loss 0.000620871, acc 1\n",
      "2018-10-26T16:31:25.699429: step 6847, loss 0.000304242, acc 1\n",
      "2018-10-26T16:31:26.102349: step 6848, loss 0.000115887, acc 1\n",
      "2018-10-26T16:31:26.496297: step 6849, loss 0.00113033, acc 1\n",
      "2018-10-26T16:31:26.878276: step 6850, loss 0.00179647, acc 1\n",
      "2018-10-26T16:31:27.249289: step 6851, loss 0.0078156, acc 1\n",
      "2018-10-26T16:31:27.626279: step 6852, loss 0.0497468, acc 0.984375\n",
      "2018-10-26T16:31:27.958393: step 6853, loss 0.028889, acc 0.984375\n",
      "2018-10-26T16:31:28.268563: step 6854, loss 0.00046811, acc 1\n",
      "2018-10-26T16:31:28.603666: step 6855, loss 0.0009536, acc 1\n",
      "2018-10-26T16:31:29.029529: step 6856, loss 0.00185539, acc 1\n",
      "2018-10-26T16:31:29.369619: step 6857, loss 0.000224064, acc 1\n",
      "2018-10-26T16:31:29.698744: step 6858, loss 0.000915875, acc 1\n",
      "2018-10-26T16:31:30.058782: step 6859, loss 0.000130201, acc 1\n",
      "2018-10-26T16:31:30.380964: step 6860, loss 0.00282184, acc 1\n",
      "2018-10-26T16:31:30.750929: step 6861, loss 0.00125453, acc 1\n",
      "2018-10-26T16:31:31.228702: step 6862, loss 0.00107895, acc 1\n",
      "2018-10-26T16:31:31.607640: step 6863, loss 0.00128902, acc 1\n",
      "2018-10-26T16:31:31.986630: step 6864, loss 0.00143906, acc 1\n",
      "2018-10-26T16:31:32.302827: step 6865, loss 0.0033998, acc 1\n",
      "2018-10-26T16:31:32.611017: step 6866, loss 0.00182659, acc 1\n",
      "2018-10-26T16:31:32.974028: step 6867, loss 0.0188204, acc 0.984375\n",
      "2018-10-26T16:31:33.391923: step 6868, loss 0.00010006, acc 1\n",
      "2018-10-26T16:31:33.732962: step 6869, loss 0.00794608, acc 1\n",
      "2018-10-26T16:31:34.155834: step 6870, loss 7.28778e-05, acc 1\n",
      "2018-10-26T16:31:34.538849: step 6871, loss 0.00321305, acc 1\n",
      "2018-10-26T16:31:34.864939: step 6872, loss 0.00094229, acc 1\n",
      "2018-10-26T16:31:35.192133: step 6873, loss 0.00128155, acc 1\n",
      "2018-10-26T16:31:35.516198: step 6874, loss 0.000650681, acc 1\n",
      "2018-10-26T16:31:35.831356: step 6875, loss 0.00645735, acc 1\n",
      "2018-10-26T16:31:36.153498: step 6876, loss 0.000747625, acc 1\n",
      "2018-10-26T16:31:36.476632: step 6877, loss 0.000808353, acc 1\n",
      "2018-10-26T16:31:36.800768: step 6878, loss 0.00309179, acc 1\n",
      "2018-10-26T16:31:37.127892: step 6879, loss 0.000414736, acc 1\n",
      "2018-10-26T16:31:37.464990: step 6880, loss 3.40691e-05, acc 1\n",
      "2018-10-26T16:31:37.789124: step 6881, loss 0.000653113, acc 1\n",
      "2018-10-26T16:31:38.106277: step 6882, loss 0.000464875, acc 1\n",
      "2018-10-26T16:31:38.458337: step 6883, loss 5.75741e-05, acc 1\n",
      "2018-10-26T16:31:38.809398: step 6884, loss 0.00111109, acc 1\n",
      "2018-10-26T16:31:39.133534: step 6885, loss 0.000265662, acc 1\n",
      "2018-10-26T16:31:39.454678: step 6886, loss 0.00949078, acc 1\n",
      "2018-10-26T16:31:39.803742: step 6887, loss 0.000294441, acc 1\n",
      "2018-10-26T16:31:40.129871: step 6888, loss 0.00849008, acc 1\n",
      "2018-10-26T16:31:40.449020: step 6889, loss 0.00113121, acc 1\n",
      "2018-10-26T16:31:40.778168: step 6890, loss 0.00219521, acc 1\n",
      "2018-10-26T16:31:41.103269: step 6891, loss 0.00187946, acc 1\n",
      "2018-10-26T16:31:41.444359: step 6892, loss 0.00100719, acc 1\n",
      "2018-10-26T16:31:41.780460: step 6893, loss 0.0222114, acc 0.984375\n",
      "2018-10-26T16:31:42.113571: step 6894, loss 0.000145898, acc 1\n",
      "2018-10-26T16:31:42.437704: step 6895, loss 0.00134485, acc 1\n",
      "2018-10-26T16:31:42.768820: step 6896, loss 0.00115048, acc 1\n",
      "2018-10-26T16:31:43.081036: step 6897, loss 0.000120264, acc 1\n",
      "2018-10-26T16:31:43.415097: step 6898, loss 0.0147736, acc 0.984375\n",
      "2018-10-26T16:31:43.729255: step 6899, loss 0.00186745, acc 1\n",
      "2018-10-26T16:31:44.043413: step 6900, loss 0.0010849, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:31:44.821441: step 6900, loss 2.3853, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-6900\n",
      "\n",
      "2018-10-26T16:31:45.446664: step 6901, loss 0.000455368, acc 1\n",
      "2018-10-26T16:31:45.771799: step 6902, loss 0.0271413, acc 0.984375\n",
      "2018-10-26T16:31:46.248521: step 6903, loss 0.0020194, acc 1\n",
      "2018-10-26T16:31:46.750184: step 6904, loss 0.000189618, acc 1\n",
      "2018-10-26T16:31:47.124182: step 6905, loss 0.0102397, acc 1\n",
      "2018-10-26T16:31:47.481272: step 6906, loss 0.00414828, acc 1\n",
      "2018-10-26T16:31:47.879168: step 6907, loss 0.00115935, acc 1\n",
      "2018-10-26T16:31:48.219257: step 6908, loss 0.00149201, acc 1\n",
      "2018-10-26T16:31:48.527433: step 6909, loss 0.00794263, acc 1\n",
      "2018-10-26T16:31:48.856553: step 6910, loss 0.0284139, acc 0.984375\n",
      "2018-10-26T16:31:49.196691: step 6911, loss 0.000942525, acc 1\n",
      "2018-10-26T16:31:49.520818: step 6912, loss 0.000774495, acc 1\n",
      "2018-10-26T16:31:49.859874: step 6913, loss 0.000344798, acc 1\n",
      "2018-10-26T16:31:50.227927: step 6914, loss 0.000689666, acc 1\n",
      "2018-10-26T16:31:50.569980: step 6915, loss 0.00108577, acc 1\n",
      "2018-10-26T16:31:50.957967: step 6916, loss 0.00334978, acc 1\n",
      "2018-10-26T16:31:51.304045: step 6917, loss 0.000152758, acc 1\n",
      "2018-10-26T16:31:51.706941: step 6918, loss 0.00228515, acc 1\n",
      "2018-10-26T16:31:52.079941: step 6919, loss 0.00245201, acc 1\n",
      "2018-10-26T16:31:52.429008: step 6920, loss 0.00434131, acc 1\n",
      "2018-10-26T16:31:52.762118: step 6921, loss 0.000240463, acc 1\n",
      "2018-10-26T16:31:53.103206: step 6922, loss 0.029114, acc 0.984375\n",
      "2018-10-26T16:31:53.429337: step 6923, loss 0.000171139, acc 1\n",
      "2018-10-26T16:31:53.790374: step 6924, loss 0.00012934, acc 1\n",
      "2018-10-26T16:31:54.122486: step 6925, loss 0.000975226, acc 1\n",
      "2018-10-26T16:31:54.578267: step 6926, loss 0.0175317, acc 0.984375\n",
      "2018-10-26T16:31:54.930324: step 6927, loss 0.00077518, acc 1\n",
      "2018-10-26T16:31:55.267428: step 6928, loss 0.00311282, acc 1\n",
      "2018-10-26T16:31:55.620481: step 6929, loss 0.000101416, acc 1\n",
      "2018-10-26T16:31:55.963566: step 6930, loss 0.0127909, acc 0.984375\n",
      "2018-10-26T16:31:56.300711: step 6931, loss 0.0268498, acc 0.984375\n",
      "2018-10-26T16:31:56.642753: step 6932, loss 0.00151739, acc 1\n",
      "2018-10-26T16:31:56.977857: step 6933, loss 0.00294311, acc 1\n",
      "2018-10-26T16:31:57.311002: step 6934, loss 0.000226866, acc 1\n",
      "2018-10-26T16:31:57.629114: step 6935, loss 0.00472404, acc 1\n",
      "2018-10-26T16:31:57.984167: step 6936, loss 0.000310008, acc 1\n",
      "2018-10-26T16:31:58.328287: step 6937, loss 0.00113481, acc 1\n",
      "2018-10-26T16:31:58.670335: step 6938, loss 0.000772447, acc 1\n",
      "2018-10-26T16:31:58.990478: step 6939, loss 0.00196502, acc 1\n",
      "2018-10-26T16:31:59.378488: step 6940, loss 0.00045002, acc 1\n",
      "2018-10-26T16:31:59.736606: step 6941, loss 0.000273287, acc 1\n",
      "2018-10-26T16:32:00.169329: step 6942, loss 0.000501942, acc 1\n",
      "2018-10-26T16:32:00.505433: step 6943, loss 0.000505642, acc 1\n",
      "2018-10-26T16:32:00.875751: step 6944, loss 0.000857963, acc 1\n",
      "2018-10-26T16:32:01.254428: step 6945, loss 2.78999e-05, acc 1\n",
      "2018-10-26T16:32:01.688315: step 6946, loss 0.000919872, acc 1\n",
      "2018-10-26T16:32:02.057284: step 6947, loss 0.000681075, acc 1\n",
      "2018-10-26T16:32:02.414329: step 6948, loss 0.0276483, acc 0.984375\n",
      "2018-10-26T16:32:02.830218: step 6949, loss 6.35536e-05, acc 1\n",
      "2018-10-26T16:32:03.201291: step 6950, loss 0.000317574, acc 1\n",
      "2018-10-26T16:32:03.633074: step 6951, loss 0.0397344, acc 0.984375\n",
      "2018-10-26T16:32:04.009069: step 6952, loss 0.000497758, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:32:04.378083: step 6953, loss 0.00115596, acc 1\n",
      "2018-10-26T16:32:04.714280: step 6954, loss 0.000204767, acc 1\n",
      "2018-10-26T16:32:05.084197: step 6955, loss 0.000230537, acc 1\n",
      "2018-10-26T16:32:05.441306: step 6956, loss 0.000124344, acc 1\n",
      "2018-10-26T16:32:05.852144: step 6957, loss 0.0070323, acc 1\n",
      "2018-10-26T16:32:06.222156: step 6958, loss 5.63606e-05, acc 1\n",
      "2018-10-26T16:32:06.583192: step 6959, loss 0.000491489, acc 1\n",
      "2018-10-26T16:32:06.947292: step 6960, loss 0.000968316, acc 1\n",
      "2018-10-26T16:32:07.311245: step 6961, loss 0.000879682, acc 1\n",
      "2018-10-26T16:32:07.683253: step 6962, loss 0.00114141, acc 1\n",
      "2018-10-26T16:32:08.058251: step 6963, loss 0.000126132, acc 1\n",
      "2018-10-26T16:32:08.503178: step 6964, loss 0.000843342, acc 1\n",
      "2018-10-26T16:32:08.880056: step 6965, loss 0.00191341, acc 1\n",
      "2018-10-26T16:32:09.269014: step 6966, loss 0.00038451, acc 1\n",
      "2018-10-26T16:32:09.610164: step 6967, loss 0.000157698, acc 1\n",
      "2018-10-26T16:32:09.929251: step 6968, loss 0.000170964, acc 1\n",
      "2018-10-26T16:32:10.402984: step 6969, loss 0.00354354, acc 1\n",
      "2018-10-26T16:32:10.796936: step 6970, loss 0.000520131, acc 1\n",
      "2018-10-26T16:32:11.147994: step 6971, loss 5.76441e-05, acc 1\n",
      "2018-10-26T16:32:11.489194: step 6972, loss 0.000290503, acc 1\n",
      "2018-10-26T16:32:11.820199: step 6973, loss 0.00066212, acc 1\n",
      "2018-10-26T16:32:12.162283: step 6974, loss 0.000217015, acc 1\n",
      "2018-10-26T16:32:12.522324: step 6975, loss 0.00103754, acc 1\n",
      "2018-10-26T16:32:12.848492: step 6976, loss 0.000274744, acc 1\n",
      "2018-10-26T16:32:13.191722: step 6977, loss 0.0046604, acc 1\n",
      "2018-10-26T16:32:13.549797: step 6978, loss 0.00028688, acc 1\n",
      "2018-10-26T16:32:13.929564: step 6979, loss 8.5491e-05, acc 1\n",
      "2018-10-26T16:32:14.303565: step 6980, loss 0.00948245, acc 1\n",
      "2018-10-26T16:32:14.645652: step 6981, loss 0.000348106, acc 1\n",
      "2018-10-26T16:32:15.023643: step 6982, loss 0.000270301, acc 1\n",
      "2018-10-26T16:32:15.373704: step 6983, loss 0.000453147, acc 1\n",
      "2018-10-26T16:32:15.769646: step 6984, loss 0.000311751, acc 1\n",
      "2018-10-26T16:32:16.112780: step 6985, loss 0.00103578, acc 1\n",
      "2018-10-26T16:32:16.536604: step 6986, loss 0.000933091, acc 1\n",
      "2018-10-26T16:32:16.910065: step 6987, loss 0.00409552, acc 1\n",
      "2018-10-26T16:32:17.239719: step 6988, loss 0.00037981, acc 1\n",
      "2018-10-26T16:32:17.554032: step 6989, loss 0.000515727, acc 1\n",
      "2018-10-26T16:32:17.882061: step 6990, loss 0.000268114, acc 1\n",
      "2018-10-26T16:32:18.214116: step 6991, loss 0.00739783, acc 1\n",
      "2018-10-26T16:32:18.646034: step 6992, loss 0.000726327, acc 1\n",
      "2018-10-26T16:32:18.998022: step 6993, loss 0.000705916, acc 1\n",
      "2018-10-26T16:32:19.400979: step 6994, loss 0.000477819, acc 1\n",
      "2018-10-26T16:32:19.776939: step 6995, loss 0.00225385, acc 1\n",
      "2018-10-26T16:32:20.189837: step 6996, loss 0.000920165, acc 1\n",
      "2018-10-26T16:32:20.574808: step 6997, loss 0.000249343, acc 1\n",
      "2018-10-26T16:32:20.947811: step 6998, loss 0.00139614, acc 1\n",
      "2018-10-26T16:32:21.294883: step 6999, loss 0.000211078, acc 1\n",
      "2018-10-26T16:32:21.685840: step 7000, loss 0.00123074, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:32:22.693148: step 7000, loss 2.39038, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7000\n",
      "\n",
      "2018-10-26T16:32:23.365354: step 7001, loss 0.00300489, acc 1\n",
      "2018-10-26T16:32:23.698462: step 7002, loss 0.000359646, acc 1\n",
      "2018-10-26T16:32:24.141277: step 7003, loss 0.051058, acc 0.984375\n",
      "2018-10-26T16:32:24.505306: step 7004, loss 0.000345785, acc 1\n",
      "2018-10-26T16:32:24.853375: step 7005, loss 0.000473074, acc 1\n",
      "2018-10-26T16:32:25.228412: step 7006, loss 0.0172922, acc 0.984375\n",
      "2018-10-26T16:32:25.618335: step 7007, loss 0.000515336, acc 1\n",
      "2018-10-26T16:32:25.983357: step 7008, loss 0.00026879, acc 1\n",
      "2018-10-26T16:32:26.339405: step 7009, loss 8.3928e-06, acc 1\n",
      "2018-10-26T16:32:26.767263: step 7010, loss 0.0133681, acc 0.984375\n",
      "2018-10-26T16:32:27.154230: step 7011, loss 3.28732e-05, acc 1\n",
      "2018-10-26T16:32:27.560146: step 7012, loss 0.00201429, acc 1\n",
      "2018-10-26T16:32:27.950101: step 7013, loss 0.000122363, acc 1\n",
      "2018-10-26T16:32:28.270294: step 7014, loss 0.000213641, acc 1\n",
      "2018-10-26T16:32:28.742983: step 7015, loss 0.027621, acc 0.984375\n",
      "2018-10-26T16:32:29.217885: step 7016, loss 3.53707e-06, acc 1\n",
      "2018-10-26T16:32:29.676489: step 7017, loss 0.000156485, acc 1\n",
      "2018-10-26T16:32:30.111326: step 7018, loss 0.000128341, acc 1\n",
      "2018-10-26T16:32:30.544170: step 7019, loss 0.00205486, acc 1\n",
      "2018-10-26T16:32:30.949089: step 7020, loss 0.0291975, acc 0.984375\n",
      "2018-10-26T16:32:31.384924: step 7021, loss 0.00015669, acc 1\n",
      "2018-10-26T16:32:31.828739: step 7022, loss 0.000144343, acc 1\n",
      "2018-10-26T16:32:32.267566: step 7023, loss 0.000698068, acc 1\n",
      "2018-10-26T16:32:32.644568: step 7024, loss 0.00141297, acc 1\n",
      "2018-10-26T16:32:33.051470: step 7025, loss 0.000544277, acc 1\n",
      "2018-10-26T16:32:33.507321: step 7026, loss 0.0019319, acc 1\n",
      "2018-10-26T16:32:33.887238: step 7027, loss 0.000216056, acc 1\n",
      "2018-10-26T16:32:34.273208: step 7028, loss 0.000241986, acc 1\n",
      "2018-10-26T16:32:34.661398: step 7029, loss 0.000713481, acc 1\n",
      "2018-10-26T16:32:35.059107: step 7030, loss 0.000112747, acc 1\n",
      "2018-10-26T16:32:35.456047: step 7031, loss 0.000433172, acc 1\n",
      "2018-10-26T16:32:35.851036: step 7032, loss 0.0167997, acc 1\n",
      "2018-10-26T16:32:36.251921: step 7033, loss 0.0789386, acc 0.984375\n",
      "2018-10-26T16:32:36.627917: step 7034, loss 0.000810616, acc 1\n",
      "2018-10-26T16:32:36.965019: step 7035, loss 0.0023188, acc 1\n",
      "2018-10-26T16:32:37.303336: step 7036, loss 0.000826982, acc 1\n",
      "2018-10-26T16:32:37.718003: step 7037, loss 0.00353683, acc 1\n",
      "2018-10-26T16:32:38.061132: step 7038, loss 0.000705619, acc 1\n",
      "2018-10-26T16:32:38.390207: step 7039, loss 0.00787558, acc 1\n",
      "2018-10-26T16:32:38.700379: step 7040, loss 0.000527457, acc 1\n",
      "2018-10-26T16:32:39.057424: step 7041, loss 0.00554002, acc 1\n",
      "2018-10-26T16:32:39.368593: step 7042, loss 0.0117554, acc 0.984375\n",
      "2018-10-26T16:32:39.692727: step 7043, loss 0.00416888, acc 1\n",
      "2018-10-26T16:32:40.007887: step 7044, loss 0.0006234, acc 1\n",
      "2018-10-26T16:32:40.335010: step 7045, loss 0.000236203, acc 1\n",
      "2018-10-26T16:32:40.651246: step 7046, loss 0.0103822, acc 1\n",
      "2018-10-26T16:32:40.976298: step 7047, loss 0.0146016, acc 0.984375\n",
      "2018-10-26T16:32:41.308413: step 7048, loss 0.00241065, acc 1\n",
      "2018-10-26T16:32:41.628557: step 7049, loss 0.00019172, acc 1\n",
      "2018-10-26T16:32:41.942714: step 7050, loss 0.0682776, acc 0.983333\n",
      "2018-10-26T16:32:42.268901: step 7051, loss 0.00258437, acc 1\n",
      "2018-10-26T16:32:42.603028: step 7052, loss 0.000595115, acc 1\n",
      "2018-10-26T16:32:42.969970: step 7053, loss 0.000372228, acc 1\n",
      "2018-10-26T16:32:43.281139: step 7054, loss 0.000107558, acc 1\n",
      "2018-10-26T16:32:43.644170: step 7055, loss 0.0569176, acc 0.984375\n",
      "2018-10-26T16:32:43.955339: step 7056, loss 0.00885915, acc 1\n",
      "2018-10-26T16:32:44.276480: step 7057, loss 0.00298219, acc 1\n",
      "2018-10-26T16:32:44.591688: step 7058, loss 1.19405e-05, acc 1\n",
      "2018-10-26T16:32:44.940935: step 7059, loss 3.2167e-05, acc 1\n",
      "2018-10-26T16:32:45.246888: step 7060, loss 0.000194006, acc 1\n",
      "2018-10-26T16:32:45.563115: step 7061, loss 0.000559318, acc 1\n",
      "2018-10-26T16:32:45.883192: step 7062, loss 0.0092413, acc 1\n",
      "2018-10-26T16:32:46.332027: step 7063, loss 0.00237011, acc 1\n",
      "2018-10-26T16:32:46.671170: step 7064, loss 0.000433289, acc 1\n",
      "2018-10-26T16:32:47.003196: step 7065, loss 0.00125277, acc 1\n",
      "2018-10-26T16:32:47.336368: step 7066, loss 0.00033678, acc 1\n",
      "2018-10-26T16:32:47.665426: step 7067, loss 0.000415402, acc 1\n",
      "2018-10-26T16:32:48.032445: step 7068, loss 0.000535014, acc 1\n",
      "2018-10-26T16:32:48.379521: step 7069, loss 0.00169418, acc 1\n",
      "2018-10-26T16:32:48.726590: step 7070, loss 0.000115347, acc 1\n",
      "2018-10-26T16:32:49.075657: step 7071, loss 0.00015719, acc 1\n",
      "2018-10-26T16:32:49.425723: step 7072, loss 0.0106895, acc 1\n",
      "2018-10-26T16:32:49.780774: step 7073, loss 0.00191404, acc 1\n",
      "2018-10-26T16:32:50.095933: step 7074, loss 0.046872, acc 0.984375\n",
      "2018-10-26T16:32:50.439014: step 7075, loss 0.002875, acc 1\n",
      "2018-10-26T16:32:50.862881: step 7076, loss 0.00247556, acc 1\n",
      "2018-10-26T16:32:51.202974: step 7077, loss 0.000211193, acc 1\n",
      "2018-10-26T16:32:51.550045: step 7078, loss 0.0115568, acc 0.984375\n",
      "2018-10-26T16:32:51.946985: step 7079, loss 2.0502e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:32:52.331956: step 7080, loss 0.000206011, acc 1\n",
      "2018-10-26T16:32:52.689006: step 7081, loss 0.00409251, acc 1\n",
      "2018-10-26T16:32:53.050112: step 7082, loss 0.00217786, acc 1\n",
      "2018-10-26T16:32:53.367191: step 7083, loss 7.94015e-05, acc 1\n",
      "2018-10-26T16:32:53.701377: step 7084, loss 0.0121436, acc 0.984375\n",
      "2018-10-26T16:32:54.181110: step 7085, loss 0.000279448, acc 1\n",
      "2018-10-26T16:32:54.581046: step 7086, loss 0.0115326, acc 0.984375\n",
      "2018-10-26T16:32:54.903088: step 7087, loss 0.00121161, acc 1\n",
      "2018-10-26T16:32:55.237197: step 7088, loss 0.00132561, acc 1\n",
      "2018-10-26T16:32:55.556343: step 7089, loss 0.000393934, acc 1\n",
      "2018-10-26T16:32:55.949291: step 7090, loss 0.000399666, acc 1\n",
      "2018-10-26T16:32:56.322294: step 7091, loss 0.0005894, acc 1\n",
      "2018-10-26T16:32:56.605570: step 7092, loss 0.000260613, acc 1\n",
      "2018-10-26T16:32:56.888785: step 7093, loss 0.0017412, acc 1\n",
      "2018-10-26T16:32:57.196958: step 7094, loss 0.0230089, acc 0.984375\n",
      "2018-10-26T16:32:57.522089: step 7095, loss 0.00233671, acc 1\n",
      "2018-10-26T16:32:57.827273: step 7096, loss 0.012611, acc 0.984375\n",
      "2018-10-26T16:32:58.127471: step 7097, loss 0.00836552, acc 1\n",
      "2018-10-26T16:32:58.402841: step 7098, loss 0.00241247, acc 1\n",
      "2018-10-26T16:32:58.737841: step 7099, loss 0.0260754, acc 0.984375\n",
      "2018-10-26T16:32:59.014103: step 7100, loss 4.42648e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:32:59.715259: step 7100, loss 2.45123, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7100\n",
      "\n",
      "2018-10-26T16:33:00.371506: step 7101, loss 0.00734705, acc 1\n",
      "2018-10-26T16:33:00.681686: step 7102, loss 0.000160309, acc 1\n",
      "2018-10-26T16:33:01.056645: step 7103, loss 0.00131974, acc 1\n",
      "2018-10-26T16:33:01.365839: step 7104, loss 0.00165542, acc 1\n",
      "2018-10-26T16:33:01.698970: step 7105, loss 0.00243835, acc 1\n",
      "2018-10-26T16:33:02.009145: step 7106, loss 0.000278401, acc 1\n",
      "2018-10-26T16:33:02.327250: step 7107, loss 0.00171666, acc 1\n",
      "2018-10-26T16:33:02.618499: step 7108, loss 0.00154564, acc 1\n",
      "2018-10-26T16:33:02.952581: step 7109, loss 0.000918333, acc 1\n",
      "2018-10-26T16:33:03.260807: step 7110, loss 0.000431894, acc 1\n",
      "2018-10-26T16:33:03.597857: step 7111, loss 0.000109928, acc 1\n",
      "2018-10-26T16:33:03.935992: step 7112, loss 0.00265854, acc 1\n",
      "2018-10-26T16:33:04.280073: step 7113, loss 0.00579778, acc 1\n",
      "2018-10-26T16:33:04.589212: step 7114, loss 0.00762254, acc 1\n",
      "2018-10-26T16:33:04.911439: step 7115, loss 0.00215023, acc 1\n",
      "2018-10-26T16:33:05.209630: step 7116, loss 0.00174179, acc 1\n",
      "2018-10-26T16:33:05.519720: step 7117, loss 0.00167107, acc 1\n",
      "2018-10-26T16:33:05.822943: step 7118, loss 0.00145016, acc 1\n",
      "2018-10-26T16:33:06.150036: step 7119, loss 0.000170289, acc 1\n",
      "2018-10-26T16:33:06.422309: step 7120, loss 0.00255567, acc 1\n",
      "2018-10-26T16:33:06.712535: step 7121, loss 0.0453859, acc 0.984375\n",
      "2018-10-26T16:33:07.055617: step 7122, loss 0.0053606, acc 1\n",
      "2018-10-26T16:33:07.395728: step 7123, loss 4.93057e-05, acc 1\n",
      "2018-10-26T16:33:07.692004: step 7124, loss 0.0897724, acc 0.984375\n",
      "2018-10-26T16:33:08.001119: step 7125, loss 0.000519862, acc 1\n",
      "2018-10-26T16:33:08.305389: step 7126, loss 0.00505204, acc 1\n",
      "2018-10-26T16:33:08.639385: step 7127, loss 0.0382436, acc 0.984375\n",
      "2018-10-26T16:33:09.013389: step 7128, loss 0.000440508, acc 1\n",
      "2018-10-26T16:33:09.360459: step 7129, loss 0.000597208, acc 1\n",
      "2018-10-26T16:33:09.841270: step 7130, loss 0.00248019, acc 1\n",
      "2018-10-26T16:33:10.167345: step 7131, loss 0.000820693, acc 1\n",
      "2018-10-26T16:33:10.462515: step 7132, loss 0.000135368, acc 1\n",
      "2018-10-26T16:33:10.770691: step 7133, loss 0.00030136, acc 1\n",
      "2018-10-26T16:33:11.154666: step 7134, loss 0.000285283, acc 1\n",
      "2018-10-26T16:33:11.583518: step 7135, loss 0.0136378, acc 0.984375\n",
      "2018-10-26T16:33:11.930107: step 7136, loss 0.000836376, acc 1\n",
      "2018-10-26T16:33:12.270209: step 7137, loss 0.000825766, acc 1\n",
      "2018-10-26T16:33:12.681629: step 7138, loss 0.00026579, acc 1\n",
      "2018-10-26T16:33:13.063564: step 7139, loss 0.00470285, acc 1\n",
      "2018-10-26T16:33:13.410929: step 7140, loss 0.000231799, acc 1\n",
      "2018-10-26T16:33:13.796835: step 7141, loss 0.00288824, acc 1\n",
      "2018-10-26T16:33:14.191553: step 7142, loss 0.000334672, acc 1\n",
      "2018-10-26T16:33:14.557695: step 7143, loss 0.000957683, acc 1\n",
      "2018-10-26T16:33:14.907641: step 7144, loss 0.000354463, acc 1\n",
      "2018-10-26T16:33:15.278648: step 7145, loss 6.3906e-05, acc 1\n",
      "2018-10-26T16:33:15.648660: step 7146, loss 0.0059143, acc 1\n",
      "2018-10-26T16:33:15.987756: step 7147, loss 0.0138719, acc 0.984375\n",
      "2018-10-26T16:33:16.389713: step 7148, loss 0.0027401, acc 1\n",
      "2018-10-26T16:33:16.733760: step 7149, loss 0.000592455, acc 1\n",
      "2018-10-26T16:33:17.083823: step 7150, loss 0.000309238, acc 1\n",
      "2018-10-26T16:33:17.418931: step 7151, loss 5.43763e-05, acc 1\n",
      "2018-10-26T16:33:17.794927: step 7152, loss 5.10889e-05, acc 1\n",
      "2018-10-26T16:33:18.131026: step 7153, loss 0.000466963, acc 1\n",
      "2018-10-26T16:33:18.516000: step 7154, loss 0.000703741, acc 1\n",
      "2018-10-26T16:33:18.868087: step 7155, loss 0.000348674, acc 1\n",
      "2018-10-26T16:33:19.198208: step 7156, loss 0.00287788, acc 1\n",
      "2018-10-26T16:33:19.536274: step 7157, loss 6.49309e-05, acc 1\n",
      "2018-10-26T16:33:19.866393: step 7158, loss 0.00175512, acc 1\n",
      "2018-10-26T16:33:20.191522: step 7159, loss 0.000217549, acc 1\n",
      "2018-10-26T16:33:20.551609: step 7160, loss 4.12155e-05, acc 1\n",
      "2018-10-26T16:33:20.921606: step 7161, loss 0.0430637, acc 0.984375\n",
      "2018-10-26T16:33:21.289587: step 7162, loss 0.00893907, acc 1\n",
      "2018-10-26T16:33:21.632672: step 7163, loss 6.68971e-05, acc 1\n",
      "2018-10-26T16:33:21.984758: step 7164, loss 0.000518766, acc 1\n",
      "2018-10-26T16:33:22.370699: step 7165, loss 0.00302303, acc 1\n",
      "2018-10-26T16:33:22.753798: step 7166, loss 0.00212097, acc 1\n",
      "2018-10-26T16:33:23.098753: step 7167, loss 0.000390161, acc 1\n",
      "2018-10-26T16:33:23.457078: step 7168, loss 0.000477114, acc 1\n",
      "2018-10-26T16:33:23.867698: step 7169, loss 1.94656e-05, acc 1\n",
      "2018-10-26T16:33:24.194826: step 7170, loss 0.00925382, acc 1\n",
      "2018-10-26T16:33:24.520046: step 7171, loss 0.00848252, acc 1\n",
      "2018-10-26T16:33:24.868095: step 7172, loss 0.000102308, acc 1\n",
      "2018-10-26T16:33:25.194155: step 7173, loss 0.0465885, acc 0.96875\n",
      "2018-10-26T16:33:25.523277: step 7174, loss 0.0113499, acc 0.984375\n",
      "2018-10-26T16:33:25.841479: step 7175, loss 0.000218913, acc 1\n",
      "2018-10-26T16:33:26.179568: step 7176, loss 0.0231353, acc 0.984375\n",
      "2018-10-26T16:33:26.511635: step 7177, loss 0.00268442, acc 1\n",
      "2018-10-26T16:33:26.832775: step 7178, loss 7.5943e-05, acc 1\n",
      "2018-10-26T16:33:27.162896: step 7179, loss 0.000777506, acc 1\n",
      "2018-10-26T16:33:27.483038: step 7180, loss 0.00133249, acc 1\n",
      "2018-10-26T16:33:27.824251: step 7181, loss 0.000899455, acc 1\n",
      "2018-10-26T16:33:28.149260: step 7182, loss 0.0102475, acc 1\n",
      "2018-10-26T16:33:28.462423: step 7183, loss 0.000236393, acc 1\n",
      "2018-10-26T16:33:28.785563: step 7184, loss 0.000402012, acc 1\n",
      "2018-10-26T16:33:29.104732: step 7185, loss 4.05478e-05, acc 1\n",
      "2018-10-26T16:33:29.417899: step 7186, loss 0.00102427, acc 1\n",
      "2018-10-26T16:33:29.748984: step 7187, loss 0.00284702, acc 1\n",
      "2018-10-26T16:33:30.067133: step 7188, loss 0.026946, acc 0.984375\n",
      "2018-10-26T16:33:30.384287: step 7189, loss 0.0537022, acc 0.984375\n",
      "2018-10-26T16:33:30.687477: step 7190, loss 0.00247085, acc 1\n",
      "2018-10-26T16:33:31.005629: step 7191, loss 0.00599495, acc 1\n",
      "2018-10-26T16:33:31.354695: step 7192, loss 0.00899405, acc 1\n",
      "2018-10-26T16:33:31.735679: step 7193, loss 0.00173418, acc 1\n",
      "2018-10-26T16:33:32.081791: step 7194, loss 0.000296754, acc 1\n",
      "2018-10-26T16:33:32.442788: step 7195, loss 0.000108471, acc 1\n",
      "2018-10-26T16:33:32.816790: step 7196, loss 0.000130578, acc 1\n",
      "2018-10-26T16:33:33.198772: step 7197, loss 0.0278118, acc 0.984375\n",
      "2018-10-26T16:33:33.494098: step 7198, loss 0.00175707, acc 1\n",
      "2018-10-26T16:33:33.853019: step 7199, loss 0.0115963, acc 1\n",
      "2018-10-26T16:33:34.253950: step 7200, loss 6.3371e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:33:35.300152: step 7200, loss 2.48176, acc 0.707317\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7200\n",
      "\n",
      "2018-10-26T16:33:36.015242: step 7201, loss 0.0172163, acc 0.984375\n",
      "2018-10-26T16:33:36.426144: step 7202, loss 0.00292659, acc 1\n",
      "2018-10-26T16:33:36.916832: step 7203, loss 0.00021612, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:33:37.331754: step 7204, loss 0.00116957, acc 1\n",
      "2018-10-26T16:33:37.755592: step 7205, loss 0.0156204, acc 0.984375\n",
      "2018-10-26T16:33:38.198412: step 7206, loss 9.55028e-05, acc 1\n",
      "2018-10-26T16:33:38.625268: step 7207, loss 0.000189525, acc 1\n",
      "2018-10-26T16:33:39.035172: step 7208, loss 0.00222862, acc 1\n",
      "2018-10-26T16:33:39.429121: step 7209, loss 0.000276977, acc 1\n",
      "2018-10-26T16:33:39.773204: step 7210, loss 0.00207728, acc 1\n",
      "2018-10-26T16:33:40.104315: step 7211, loss 6.63332e-05, acc 1\n",
      "2018-10-26T16:33:40.425461: step 7212, loss 0.00102116, acc 1\n",
      "2018-10-26T16:33:40.764556: step 7213, loss 0.000726669, acc 1\n",
      "2018-10-26T16:33:41.103645: step 7214, loss 2.91943e-05, acc 1\n",
      "2018-10-26T16:33:41.432770: step 7215, loss 0.00891984, acc 1\n",
      "2018-10-26T16:33:41.788818: step 7216, loss 0.00918796, acc 1\n",
      "2018-10-26T16:33:42.163815: step 7217, loss 0.00548641, acc 1\n",
      "2018-10-26T16:33:42.503904: step 7218, loss 0.000353435, acc 1\n",
      "2018-10-26T16:33:42.825051: step 7219, loss 0.00050441, acc 1\n",
      "2018-10-26T16:33:43.152172: step 7220, loss 0.00032935, acc 1\n",
      "2018-10-26T16:33:43.473314: step 7221, loss 0.000563607, acc 1\n",
      "2018-10-26T16:33:43.796454: step 7222, loss 0.00022647, acc 1\n",
      "2018-10-26T16:33:44.133552: step 7223, loss 0.00771834, acc 1\n",
      "2018-10-26T16:33:44.472647: step 7224, loss 0.00101408, acc 1\n",
      "2018-10-26T16:33:44.807749: step 7225, loss 0.00118412, acc 1\n",
      "2018-10-26T16:33:45.140860: step 7226, loss 0.0228782, acc 0.984375\n",
      "2018-10-26T16:33:45.471974: step 7227, loss 0.00233039, acc 1\n",
      "2018-10-26T16:33:45.832013: step 7228, loss 0.00342928, acc 1\n",
      "2018-10-26T16:33:46.167120: step 7229, loss 0.00061715, acc 1\n",
      "2018-10-26T16:33:46.520218: step 7230, loss 0.00097439, acc 1\n",
      "2018-10-26T16:33:46.869245: step 7231, loss 0.000252686, acc 1\n",
      "2018-10-26T16:33:47.204350: step 7232, loss 0.000114431, acc 1\n",
      "2018-10-26T16:33:47.564423: step 7233, loss 1.24491e-05, acc 1\n",
      "2018-10-26T16:33:47.892508: step 7234, loss 0.000557876, acc 1\n",
      "2018-10-26T16:33:48.230604: step 7235, loss 0.000206123, acc 1\n",
      "2018-10-26T16:33:48.547760: step 7236, loss 0.0004519, acc 1\n",
      "2018-10-26T16:33:48.872887: step 7237, loss 0.000390799, acc 1\n",
      "2018-10-26T16:33:49.192038: step 7238, loss 0.000147858, acc 1\n",
      "2018-10-26T16:33:49.524149: step 7239, loss 0.000574939, acc 1\n",
      "2018-10-26T16:33:49.844296: step 7240, loss 0.000291882, acc 1\n",
      "2018-10-26T16:33:50.203333: step 7241, loss 0.000365358, acc 1\n",
      "2018-10-26T16:33:50.545488: step 7242, loss 0.000113626, acc 1\n",
      "2018-10-26T16:33:50.869553: step 7243, loss 0.000525559, acc 1\n",
      "2018-10-26T16:33:51.182748: step 7244, loss 0.00103025, acc 1\n",
      "2018-10-26T16:33:51.508846: step 7245, loss 0.00120965, acc 1\n",
      "2018-10-26T16:33:51.838997: step 7246, loss 0.0139357, acc 0.984375\n",
      "2018-10-26T16:33:52.199001: step 7247, loss 0.00137836, acc 1\n",
      "2018-10-26T16:33:52.537098: step 7248, loss 0.00166693, acc 1\n",
      "2018-10-26T16:33:52.881182: step 7249, loss 1.72813e-05, acc 1\n",
      "2018-10-26T16:33:53.199423: step 7250, loss 0.000809476, acc 1\n",
      "2018-10-26T16:33:53.542413: step 7251, loss 0.000947614, acc 1\n",
      "2018-10-26T16:33:53.862557: step 7252, loss 0.00030928, acc 1\n",
      "2018-10-26T16:33:54.189713: step 7253, loss 0.00164529, acc 1\n",
      "2018-10-26T16:33:54.524787: step 7254, loss 0.0109184, acc 1\n",
      "2018-10-26T16:33:54.851912: step 7255, loss 0.000497425, acc 1\n",
      "2018-10-26T16:33:55.169065: step 7256, loss 0.0127242, acc 0.984375\n",
      "2018-10-26T16:33:55.497235: step 7257, loss 0.000479645, acc 1\n",
      "2018-10-26T16:33:55.842270: step 7258, loss 0.000124901, acc 1\n",
      "2018-10-26T16:33:56.178402: step 7259, loss 0.00163978, acc 1\n",
      "2018-10-26T16:33:56.490573: step 7260, loss 0.00120961, acc 1\n",
      "2018-10-26T16:33:56.816665: step 7261, loss 0.00103697, acc 1\n",
      "2018-10-26T16:33:57.126838: step 7262, loss 0.00123619, acc 1\n",
      "2018-10-26T16:33:57.428032: step 7263, loss 0.0141701, acc 0.984375\n",
      "2018-10-26T16:33:57.754158: step 7264, loss 0.00331521, acc 1\n",
      "2018-10-26T16:33:58.064329: step 7265, loss 9.62795e-05, acc 1\n",
      "2018-10-26T16:33:58.402431: step 7266, loss 0.00367708, acc 1\n",
      "2018-10-26T16:33:58.739526: step 7267, loss 0.0107023, acc 1\n",
      "2018-10-26T16:33:59.052690: step 7268, loss 0.000303667, acc 1\n",
      "2018-10-26T16:33:59.375826: step 7269, loss 0.000407545, acc 1\n",
      "2018-10-26T16:33:59.696971: step 7270, loss 6.39243e-05, acc 1\n",
      "2018-10-26T16:34:00.072013: step 7271, loss 0.000351169, acc 1\n",
      "2018-10-26T16:34:00.406074: step 7272, loss 0.00220472, acc 1\n",
      "2018-10-26T16:34:00.753147: step 7273, loss 0.000555853, acc 1\n",
      "2018-10-26T16:34:01.074287: step 7274, loss 0.000873758, acc 1\n",
      "2018-10-26T16:34:01.395432: step 7275, loss 0.00122222, acc 1\n",
      "2018-10-26T16:34:01.723556: step 7276, loss 0.00211912, acc 1\n",
      "2018-10-26T16:34:02.065640: step 7277, loss 0.000468344, acc 1\n",
      "2018-10-26T16:34:02.444123: step 7278, loss 0.00156299, acc 1\n",
      "2018-10-26T16:34:02.816633: step 7279, loss 0.000671189, acc 1\n",
      "2018-10-26T16:34:03.192628: step 7280, loss 0.000117634, acc 1\n",
      "2018-10-26T16:34:03.608516: step 7281, loss 0.000225751, acc 1\n",
      "2018-10-26T16:34:04.022410: step 7282, loss 0.000109523, acc 1\n",
      "2018-10-26T16:34:04.443286: step 7283, loss 0.000404771, acc 1\n",
      "2018-10-26T16:34:04.890166: step 7284, loss 0.000177481, acc 1\n",
      "2018-10-26T16:34:05.319944: step 7285, loss 0.00603724, acc 1\n",
      "2018-10-26T16:34:05.728851: step 7286, loss 0.00106089, acc 1\n",
      "2018-10-26T16:34:06.067945: step 7287, loss 0.000150849, acc 1\n",
      "2018-10-26T16:34:06.436958: step 7288, loss 0.124863, acc 0.984375\n",
      "2018-10-26T16:34:06.797995: step 7289, loss 0.00300631, acc 1\n",
      "2018-10-26T16:34:07.248830: step 7290, loss 0.000900822, acc 1\n",
      "2018-10-26T16:34:07.634760: step 7291, loss 0.0020489, acc 1\n",
      "2018-10-26T16:34:08.004773: step 7292, loss 3.98641e-05, acc 1\n",
      "2018-10-26T16:34:08.527375: step 7293, loss 0.00113045, acc 1\n",
      "2018-10-26T16:34:08.877438: step 7294, loss 0.000552979, acc 1\n",
      "2018-10-26T16:34:09.272383: step 7295, loss 0.00602652, acc 1\n",
      "2018-10-26T16:34:09.685280: step 7296, loss 0.00112572, acc 1\n",
      "2018-10-26T16:34:10.093192: step 7297, loss 0.00819706, acc 1\n",
      "2018-10-26T16:34:10.498109: step 7298, loss 0.000819532, acc 1\n",
      "2018-10-26T16:34:10.838200: step 7299, loss 0.00196274, acc 1\n",
      "2018-10-26T16:34:11.170312: step 7300, loss 0.000141704, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:34:11.944246: step 7300, loss 2.46825, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7300\n",
      "\n",
      "2018-10-26T16:34:12.529680: step 7301, loss 0.000139902, acc 1\n",
      "2018-10-26T16:34:12.852819: step 7302, loss 0.000239198, acc 1\n",
      "2018-10-26T16:34:13.304609: step 7303, loss 0.000192529, acc 1\n",
      "2018-10-26T16:34:13.637722: step 7304, loss 6.06766e-05, acc 1\n",
      "2018-10-26T16:34:14.002745: step 7305, loss 0.000160581, acc 1\n",
      "2018-10-26T16:34:14.316905: step 7306, loss 0.000156417, acc 1\n",
      "2018-10-26T16:34:14.654005: step 7307, loss 0.000536925, acc 1\n",
      "2018-10-26T16:34:14.984122: step 7308, loss 0.00376486, acc 1\n",
      "2018-10-26T16:34:15.341170: step 7309, loss 0.00198283, acc 1\n",
      "2018-10-26T16:34:15.705195: step 7310, loss 0.000185006, acc 1\n",
      "2018-10-26T16:34:16.082189: step 7311, loss 0.000410753, acc 1\n",
      "2018-10-26T16:34:16.472149: step 7312, loss 0.000172967, acc 1\n",
      "2018-10-26T16:34:16.902996: step 7313, loss 0.000232898, acc 1\n",
      "2018-10-26T16:34:17.249071: step 7314, loss 3.67106e-05, acc 1\n",
      "2018-10-26T16:34:17.569215: step 7315, loss 0.00155132, acc 1\n",
      "2018-10-26T16:34:17.911342: step 7316, loss 0.00115357, acc 1\n",
      "2018-10-26T16:34:18.342150: step 7317, loss 0.0036705, acc 1\n",
      "2018-10-26T16:34:18.702192: step 7318, loss 0.000195734, acc 1\n",
      "2018-10-26T16:34:19.079180: step 7319, loss 0.000101306, acc 1\n",
      "2018-10-26T16:34:19.395400: step 7320, loss 0.00244643, acc 1\n",
      "2018-10-26T16:34:19.695534: step 7321, loss 0.00176074, acc 1\n",
      "2018-10-26T16:34:20.005867: step 7322, loss 6.66403e-05, acc 1\n",
      "2018-10-26T16:34:20.413618: step 7323, loss 0.0162675, acc 1\n",
      "2018-10-26T16:34:20.891342: step 7324, loss 0.0017764, acc 1\n",
      "2018-10-26T16:34:21.270330: step 7325, loss 0.000813084, acc 1\n",
      "2018-10-26T16:34:21.611516: step 7326, loss 0.001171, acc 1\n",
      "2018-10-26T16:34:21.975444: step 7327, loss 0.000121841, acc 1\n",
      "2018-10-26T16:34:22.389341: step 7328, loss 0.00035018, acc 1\n",
      "2018-10-26T16:34:22.700506: step 7329, loss 0.00731804, acc 1\n",
      "2018-10-26T16:34:23.032618: step 7330, loss 0.000569067, acc 1\n",
      "2018-10-26T16:34:23.357749: step 7331, loss 0.000200785, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:34:23.659941: step 7332, loss 0.000257273, acc 1\n",
      "2018-10-26T16:34:24.062865: step 7333, loss 0.0153751, acc 0.984375\n",
      "2018-10-26T16:34:24.421907: step 7334, loss 0.00205781, acc 1\n",
      "2018-10-26T16:34:24.748081: step 7335, loss 0.000214185, acc 1\n",
      "2018-10-26T16:34:25.128022: step 7336, loss 0.00016066, acc 1\n",
      "2018-10-26T16:34:25.460132: step 7337, loss 0.000292405, acc 1\n",
      "2018-10-26T16:34:25.775293: step 7338, loss 0.00011429, acc 1\n",
      "2018-10-26T16:34:26.087459: step 7339, loss 0.00129547, acc 1\n",
      "2018-10-26T16:34:26.402616: step 7340, loss 0.00323144, acc 1\n",
      "2018-10-26T16:34:26.720766: step 7341, loss 0.000129177, acc 1\n",
      "2018-10-26T16:34:27.050883: step 7342, loss 0.00131079, acc 1\n",
      "2018-10-26T16:34:27.381069: step 7343, loss 0.000870899, acc 1\n",
      "2018-10-26T16:34:27.745031: step 7344, loss 0.000165487, acc 1\n",
      "2018-10-26T16:34:28.070275: step 7345, loss 0.00378555, acc 1\n",
      "2018-10-26T16:34:28.392334: step 7346, loss 0.00217483, acc 1\n",
      "2018-10-26T16:34:28.740370: step 7347, loss 0.00123818, acc 1\n",
      "2018-10-26T16:34:29.075472: step 7348, loss 0.000236657, acc 1\n",
      "2018-10-26T16:34:29.404755: step 7349, loss 0.000255392, acc 1\n",
      "2018-10-26T16:34:29.742693: step 7350, loss 0.0030629, acc 1\n",
      "2018-10-26T16:34:30.076834: step 7351, loss 0.0299874, acc 0.984375\n",
      "2018-10-26T16:34:30.407012: step 7352, loss 5.48698e-05, acc 1\n",
      "2018-10-26T16:34:30.733073: step 7353, loss 0.000605622, acc 1\n",
      "2018-10-26T16:34:31.115026: step 7354, loss 0.000330309, acc 1\n",
      "2018-10-26T16:34:31.584768: step 7355, loss 0.000392112, acc 1\n",
      "2018-10-26T16:34:31.975724: step 7356, loss 0.00405265, acc 1\n",
      "2018-10-26T16:34:32.316812: step 7357, loss 0.000161529, acc 1\n",
      "2018-10-26T16:34:32.677878: step 7358, loss 0.000124685, acc 1\n",
      "2018-10-26T16:34:33.050098: step 7359, loss 0.00126739, acc 1\n",
      "2018-10-26T16:34:33.505925: step 7360, loss 0.00508689, acc 1\n",
      "2018-10-26T16:34:33.858694: step 7361, loss 0.00501081, acc 1\n",
      "2018-10-26T16:34:34.281697: step 7362, loss 0.000874637, acc 1\n",
      "2018-10-26T16:34:34.615672: step 7363, loss 0.000456696, acc 1\n",
      "2018-10-26T16:34:34.942798: step 7364, loss 0.0002214, acc 1\n",
      "2018-10-26T16:34:35.314803: step 7365, loss 0.000246578, acc 1\n",
      "2018-10-26T16:34:35.650990: step 7366, loss 6.76452e-05, acc 1\n",
      "2018-10-26T16:34:36.004962: step 7367, loss 0.000145608, acc 1\n",
      "2018-10-26T16:34:36.420848: step 7368, loss 0.000138188, acc 1\n",
      "2018-10-26T16:34:36.799983: step 7369, loss 4.49628e-05, acc 1\n",
      "2018-10-26T16:34:37.194782: step 7370, loss 0.00260197, acc 1\n",
      "2018-10-26T16:34:37.638594: step 7371, loss 0.000238022, acc 1\n",
      "2018-10-26T16:34:38.099363: step 7372, loss 0.00394572, acc 1\n",
      "2018-10-26T16:34:38.480344: step 7373, loss 5.06288e-05, acc 1\n",
      "2018-10-26T16:34:38.934132: step 7374, loss 0.000141902, acc 1\n",
      "2018-10-26T16:34:39.384285: step 7375, loss 0.000381172, acc 1\n",
      "2018-10-26T16:34:39.823755: step 7376, loss 0.000895695, acc 1\n",
      "2018-10-26T16:34:40.326411: step 7377, loss 6.04312e-05, acc 1\n",
      "2018-10-26T16:34:40.810118: step 7378, loss 0.000122929, acc 1\n",
      "2018-10-26T16:34:41.261912: step 7379, loss 0.000487094, acc 1\n",
      "2018-10-26T16:34:41.689770: step 7380, loss 0.00110699, acc 1\n",
      "2018-10-26T16:34:42.175471: step 7381, loss 0.00064038, acc 1\n",
      "2018-10-26T16:34:42.581472: step 7382, loss 5.20485e-05, acc 1\n",
      "2018-10-26T16:34:42.970348: step 7383, loss 0.00414513, acc 1\n",
      "2018-10-26T16:34:43.347340: step 7384, loss 0.000221741, acc 1\n",
      "2018-10-26T16:34:43.752356: step 7385, loss 0.00166545, acc 1\n",
      "2018-10-26T16:34:44.109304: step 7386, loss 0.000248608, acc 1\n",
      "2018-10-26T16:34:44.477321: step 7387, loss 0.000301916, acc 1\n",
      "2018-10-26T16:34:44.806587: step 7388, loss 0.000386424, acc 1\n",
      "2018-10-26T16:34:45.189417: step 7389, loss 0.00944341, acc 1\n",
      "2018-10-26T16:34:45.531546: step 7390, loss 0.000395688, acc 1\n",
      "2018-10-26T16:34:45.871658: step 7391, loss 9.519e-05, acc 1\n",
      "2018-10-26T16:34:46.198721: step 7392, loss 0.0591016, acc 0.984375\n",
      "2018-10-26T16:34:46.526972: step 7393, loss 0.0002642, acc 1\n",
      "2018-10-26T16:34:46.834027: step 7394, loss 0.00212408, acc 1\n",
      "2018-10-26T16:34:47.149181: step 7395, loss 0.00150117, acc 1\n",
      "2018-10-26T16:34:47.466335: step 7396, loss 0.000368309, acc 1\n",
      "2018-10-26T16:34:47.796604: step 7397, loss 0.000707315, acc 1\n",
      "2018-10-26T16:34:48.117624: step 7398, loss 0.104649, acc 0.984375\n",
      "2018-10-26T16:34:48.452820: step 7399, loss 0.00196417, acc 1\n",
      "2018-10-26T16:34:48.788804: step 7400, loss 0.000970018, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:34:49.538797: step 7400, loss 2.47104, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7400\n",
      "\n",
      "2018-10-26T16:34:50.170110: step 7401, loss 0.00778449, acc 1\n",
      "2018-10-26T16:34:50.538253: step 7402, loss 0.0021987, acc 1\n",
      "2018-10-26T16:34:51.037792: step 7403, loss 0.000256318, acc 1\n",
      "2018-10-26T16:34:51.373894: step 7404, loss 0.000407807, acc 1\n",
      "2018-10-26T16:34:51.680282: step 7405, loss 9.34775e-05, acc 1\n",
      "2018-10-26T16:34:52.017176: step 7406, loss 0.000214716, acc 1\n",
      "2018-10-26T16:34:52.364247: step 7407, loss 0.00584028, acc 1\n",
      "2018-10-26T16:34:52.718302: step 7408, loss 8.97447e-05, acc 1\n",
      "2018-10-26T16:34:53.135253: step 7409, loss 0.00342946, acc 1\n",
      "2018-10-26T16:34:53.467305: step 7410, loss 0.000783383, acc 1\n",
      "2018-10-26T16:34:53.793430: step 7411, loss 0.000133369, acc 1\n",
      "2018-10-26T16:34:54.113576: step 7412, loss 0.00128811, acc 1\n",
      "2018-10-26T16:34:54.434716: step 7413, loss 0.000106458, acc 1\n",
      "2018-10-26T16:34:54.767829: step 7414, loss 0.0214589, acc 0.984375\n",
      "2018-10-26T16:34:55.086974: step 7415, loss 0.000166521, acc 1\n",
      "2018-10-26T16:34:55.424076: step 7416, loss 0.00222787, acc 1\n",
      "2018-10-26T16:34:55.777133: step 7417, loss 0.00407172, acc 1\n",
      "2018-10-26T16:34:56.101309: step 7418, loss 0.000718701, acc 1\n",
      "2018-10-26T16:34:56.436369: step 7419, loss 0.00246262, acc 1\n",
      "2018-10-26T16:34:56.741597: step 7420, loss 7.13793e-05, acc 1\n",
      "2018-10-26T16:34:57.059769: step 7421, loss 0.0584352, acc 0.984375\n",
      "2018-10-26T16:34:57.394851: step 7422, loss 0.000179771, acc 1\n",
      "2018-10-26T16:34:57.717945: step 7423, loss 0.00139729, acc 1\n",
      "2018-10-26T16:34:58.056042: step 7424, loss 0.000685591, acc 1\n",
      "2018-10-26T16:34:58.398127: step 7425, loss 0.00196776, acc 1\n",
      "2018-10-26T16:34:58.718271: step 7426, loss 0.00519603, acc 1\n",
      "2018-10-26T16:34:59.044403: step 7427, loss 0.000617311, acc 1\n",
      "2018-10-26T16:34:59.342664: step 7428, loss 0.00818294, acc 1\n",
      "2018-10-26T16:34:59.666740: step 7429, loss 0.000173419, acc 1\n",
      "2018-10-26T16:34:59.971921: step 7430, loss 0.000100369, acc 1\n",
      "2018-10-26T16:35:00.314011: step 7431, loss 0.000709542, acc 1\n",
      "2018-10-26T16:35:00.645303: step 7432, loss 0.000114521, acc 1\n",
      "2018-10-26T16:35:00.977273: step 7433, loss 0.000932432, acc 1\n",
      "2018-10-26T16:35:01.290400: step 7434, loss 0.000252669, acc 1\n",
      "2018-10-26T16:35:01.613609: step 7435, loss 0.000363871, acc 1\n",
      "2018-10-26T16:35:02.114329: step 7436, loss 6.02358e-05, acc 1\n",
      "2018-10-26T16:35:02.490258: step 7437, loss 0.0163529, acc 0.984375\n",
      "2018-10-26T16:35:02.830337: step 7438, loss 0.00158855, acc 1\n",
      "2018-10-26T16:35:03.164393: step 7439, loss 0.000587013, acc 1\n",
      "2018-10-26T16:35:03.501535: step 7440, loss 0.00595223, acc 1\n",
      "2018-10-26T16:35:03.830650: step 7441, loss 0.0013154, acc 1\n",
      "2018-10-26T16:35:04.157737: step 7442, loss 0.00187498, acc 1\n",
      "2018-10-26T16:35:04.502822: step 7443, loss 0.00134476, acc 1\n",
      "2018-10-26T16:35:04.870832: step 7444, loss 6.54822e-05, acc 1\n",
      "2018-10-26T16:35:05.189094: step 7445, loss 0.00861638, acc 1\n",
      "2018-10-26T16:35:05.563980: step 7446, loss 0.00513484, acc 1\n",
      "2018-10-26T16:35:05.926016: step 7447, loss 9.82991e-05, acc 1\n",
      "2018-10-26T16:35:06.256134: step 7448, loss 0.00290748, acc 1\n",
      "2018-10-26T16:35:06.566305: step 7449, loss 0.000149817, acc 1\n",
      "2018-10-26T16:35:06.902404: step 7450, loss 0.00105244, acc 1\n",
      "2018-10-26T16:35:07.285384: step 7451, loss 0.000472614, acc 1\n",
      "2018-10-26T16:35:07.684317: step 7452, loss 0.00299125, acc 1\n",
      "2018-10-26T16:35:08.040363: step 7453, loss 0.000937709, acc 1\n",
      "2018-10-26T16:35:08.408384: step 7454, loss 0.000228621, acc 1\n",
      "2018-10-26T16:35:08.757452: step 7455, loss 0.00311768, acc 1\n",
      "2018-10-26T16:35:09.170345: step 7456, loss 0.133256, acc 0.984375\n",
      "2018-10-26T16:35:09.536371: step 7457, loss 0.000438664, acc 1\n",
      "2018-10-26T16:35:09.888538: step 7458, loss 4.41878e-05, acc 1\n",
      "2018-10-26T16:35:10.274395: step 7459, loss 0.000362778, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:35:10.658370: step 7460, loss 0.000970985, acc 1\n",
      "2018-10-26T16:35:11.004444: step 7461, loss 0.001808, acc 1\n",
      "2018-10-26T16:35:11.352516: step 7462, loss 0.000156465, acc 1\n",
      "2018-10-26T16:35:11.702580: step 7463, loss 2.64899e-05, acc 1\n",
      "2018-10-26T16:35:12.071668: step 7464, loss 0.000249006, acc 1\n",
      "2018-10-26T16:35:12.390741: step 7465, loss 0.000311698, acc 1\n",
      "2018-10-26T16:35:12.745792: step 7466, loss 6.85708e-05, acc 1\n",
      "2018-10-26T16:35:13.080966: step 7467, loss 0.000101552, acc 1\n",
      "2018-10-26T16:35:13.484480: step 7468, loss 0.00147179, acc 1\n",
      "2018-10-26T16:35:13.832128: step 7469, loss 0.000485774, acc 1\n",
      "2018-10-26T16:35:14.231821: step 7470, loss 0.00373232, acc 1\n",
      "2018-10-26T16:35:14.590863: step 7471, loss 0.00378609, acc 1\n",
      "2018-10-26T16:35:14.963866: step 7472, loss 0.000496287, acc 1\n",
      "2018-10-26T16:35:15.364795: step 7473, loss 9.04283e-05, acc 1\n",
      "2018-10-26T16:35:15.741833: step 7474, loss 7.38843e-05, acc 1\n",
      "2018-10-26T16:35:16.123767: step 7475, loss 0.00158384, acc 1\n",
      "2018-10-26T16:35:16.518711: step 7476, loss 0.000101747, acc 1\n",
      "2018-10-26T16:35:16.877752: step 7477, loss 0.00500005, acc 1\n",
      "2018-10-26T16:35:17.219848: step 7478, loss 0.00011473, acc 1\n",
      "2018-10-26T16:35:17.577883: step 7479, loss 0.000478747, acc 1\n",
      "2018-10-26T16:35:17.968163: step 7480, loss 0.00159049, acc 1\n",
      "2018-10-26T16:35:18.305162: step 7481, loss 0.00922676, acc 1\n",
      "2018-10-26T16:35:18.638048: step 7482, loss 7.30845e-05, acc 1\n",
      "2018-10-26T16:35:18.991106: step 7483, loss 0.000103684, acc 1\n",
      "2018-10-26T16:35:19.403005: step 7484, loss 0.00049457, acc 1\n",
      "2018-10-26T16:35:19.746091: step 7485, loss 0.00029233, acc 1\n",
      "2018-10-26T16:35:20.086182: step 7486, loss 0.000273892, acc 1\n",
      "2018-10-26T16:35:20.419289: step 7487, loss 0.000218132, acc 1\n",
      "2018-10-26T16:35:20.745656: step 7488, loss 0.0100345, acc 1\n",
      "2018-10-26T16:35:21.076535: step 7489, loss 0.00215151, acc 1\n",
      "2018-10-26T16:35:21.424603: step 7490, loss 0.000316598, acc 1\n",
      "2018-10-26T16:35:21.745746: step 7491, loss 0.000204413, acc 1\n",
      "2018-10-26T16:35:22.131814: step 7492, loss 7.06365e-05, acc 1\n",
      "2018-10-26T16:35:22.462829: step 7493, loss 0.00234636, acc 1\n",
      "2018-10-26T16:35:22.805912: step 7494, loss 0.0169991, acc 0.984375\n",
      "2018-10-26T16:35:23.141017: step 7495, loss 0.00028102, acc 1\n",
      "2018-10-26T16:35:23.478116: step 7496, loss 0.000453899, acc 1\n",
      "2018-10-26T16:35:23.814219: step 7497, loss 0.000509737, acc 1\n",
      "2018-10-26T16:35:24.197362: step 7498, loss 0.000352599, acc 1\n",
      "2018-10-26T16:35:24.560304: step 7499, loss 0.0119386, acc 0.984375\n",
      "2018-10-26T16:35:24.890343: step 7500, loss 0.0010448, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:35:25.711406: step 7500, loss 2.61752, acc 0.702627\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7500\n",
      "\n",
      "2018-10-26T16:35:26.405298: step 7501, loss 0.0205631, acc 0.984375\n",
      "2018-10-26T16:35:26.771318: step 7502, loss 0.0204947, acc 0.984375\n",
      "2018-10-26T16:35:27.232085: step 7503, loss 0.0036944, acc 1\n",
      "2018-10-26T16:35:27.530290: step 7504, loss 0.000573219, acc 1\n",
      "2018-10-26T16:35:27.867427: step 7505, loss 0.0398858, acc 0.984375\n",
      "2018-10-26T16:35:28.268445: step 7506, loss 0.00338114, acc 1\n",
      "2018-10-26T16:35:28.594479: step 7507, loss 0.000184513, acc 1\n",
      "2018-10-26T16:35:28.935534: step 7508, loss 0.00112191, acc 1\n",
      "2018-10-26T16:35:29.296571: step 7509, loss 0.00459307, acc 1\n",
      "2018-10-26T16:35:29.647663: step 7510, loss 0.000324632, acc 1\n",
      "2018-10-26T16:35:30.011661: step 7511, loss 4.46258e-05, acc 1\n",
      "2018-10-26T16:35:30.396631: step 7512, loss 0.000234275, acc 1\n",
      "2018-10-26T16:35:30.849424: step 7513, loss 0.000127743, acc 1\n",
      "2018-10-26T16:35:31.231435: step 7514, loss 3.93763e-05, acc 1\n",
      "2018-10-26T16:35:31.596431: step 7515, loss 0.00401886, acc 1\n",
      "2018-10-26T16:35:31.940506: step 7516, loss 9.37696e-05, acc 1\n",
      "2018-10-26T16:35:32.283174: step 7517, loss 0.000107501, acc 1\n",
      "2018-10-26T16:35:32.631659: step 7518, loss 0.0120896, acc 1\n",
      "2018-10-26T16:35:32.996686: step 7519, loss 0.00173674, acc 1\n",
      "2018-10-26T16:35:33.367693: step 7520, loss 0.00241832, acc 1\n",
      "2018-10-26T16:35:33.724740: step 7521, loss 0.00233152, acc 1\n",
      "2018-10-26T16:35:34.051954: step 7522, loss 0.000625314, acc 1\n",
      "2018-10-26T16:35:34.406921: step 7523, loss 0.00065185, acc 1\n",
      "2018-10-26T16:35:34.748075: step 7524, loss 0.00108555, acc 1\n",
      "2018-10-26T16:35:35.135974: step 7525, loss 0.000431636, acc 1\n",
      "2018-10-26T16:35:35.486034: step 7526, loss 0.000792308, acc 1\n",
      "2018-10-26T16:35:35.858097: step 7527, loss 7.97503e-05, acc 1\n",
      "2018-10-26T16:35:36.258996: step 7528, loss 0.0640495, acc 0.984375\n",
      "2018-10-26T16:35:36.661891: step 7529, loss 0.000995655, acc 1\n",
      "2018-10-26T16:35:37.042874: step 7530, loss 4.39293e-05, acc 1\n",
      "2018-10-26T16:35:37.403911: step 7531, loss 4.90644e-05, acc 1\n",
      "2018-10-26T16:35:37.785889: step 7532, loss 0.000536075, acc 1\n",
      "2018-10-26T16:35:38.133958: step 7533, loss 0.000164298, acc 1\n",
      "2018-10-26T16:35:38.538130: step 7534, loss 0.000631249, acc 1\n",
      "2018-10-26T16:35:38.879004: step 7535, loss 0.000189459, acc 1\n",
      "2018-10-26T16:35:39.242998: step 7536, loss 0.000194387, acc 1\n",
      "2018-10-26T16:35:39.613006: step 7537, loss 0.055054, acc 0.984375\n",
      "2018-10-26T16:35:39.950110: step 7538, loss 0.00201236, acc 1\n",
      "2018-10-26T16:35:40.353125: step 7539, loss 0.000472855, acc 1\n",
      "2018-10-26T16:35:40.717058: step 7540, loss 0.000290845, acc 1\n",
      "2018-10-26T16:35:41.088093: step 7541, loss 0.000479018, acc 1\n",
      "2018-10-26T16:35:41.430155: step 7542, loss 0.000553646, acc 1\n",
      "2018-10-26T16:35:41.796177: step 7543, loss 0.0012739, acc 1\n",
      "2018-10-26T16:35:42.167183: step 7544, loss 0.000970928, acc 1\n",
      "2018-10-26T16:35:42.532207: step 7545, loss 0.000101951, acc 1\n",
      "2018-10-26T16:35:42.934133: step 7546, loss 0.000359064, acc 1\n",
      "2018-10-26T16:35:43.337056: step 7547, loss 0.0284825, acc 0.984375\n",
      "2018-10-26T16:35:43.825770: step 7548, loss 0.000148994, acc 1\n",
      "2018-10-26T16:35:44.245629: step 7549, loss 6.65468e-06, acc 1\n",
      "2018-10-26T16:35:44.615641: step 7550, loss 0.000218438, acc 1\n",
      "2018-10-26T16:35:45.036517: step 7551, loss 1.32062e-05, acc 1\n",
      "2018-10-26T16:35:45.458507: step 7552, loss 0.00027751, acc 1\n",
      "2018-10-26T16:35:45.902441: step 7553, loss 6.27184e-05, acc 1\n",
      "2018-10-26T16:35:46.357985: step 7554, loss 0.00190949, acc 1\n",
      "2018-10-26T16:35:46.867624: step 7555, loss 1.87119e-05, acc 1\n",
      "2018-10-26T16:35:47.444082: step 7556, loss 6.62228e-05, acc 1\n",
      "2018-10-26T16:35:47.946740: step 7557, loss 0.000194639, acc 1\n",
      "2018-10-26T16:35:48.394640: step 7558, loss 0.00250689, acc 1\n",
      "2018-10-26T16:35:48.857308: step 7559, loss 0.000105545, acc 1\n",
      "2018-10-26T16:35:49.268265: step 7560, loss 6.45001e-05, acc 1\n",
      "2018-10-26T16:35:49.702050: step 7561, loss 0.00826674, acc 1\n",
      "2018-10-26T16:35:50.111115: step 7562, loss 0.00187863, acc 1\n",
      "2018-10-26T16:35:50.485956: step 7563, loss 8.46888e-05, acc 1\n",
      "2018-10-26T16:35:50.867934: step 7564, loss 0.000298103, acc 1\n",
      "2018-10-26T16:35:51.218999: step 7565, loss 0.0063404, acc 1\n",
      "2018-10-26T16:35:51.555103: step 7566, loss 0.0127533, acc 0.984375\n",
      "2018-10-26T16:35:51.957027: step 7567, loss 0.000441807, acc 1\n",
      "2018-10-26T16:35:52.364936: step 7568, loss 0.00338584, acc 1\n",
      "2018-10-26T16:35:52.717993: step 7569, loss 0.0188214, acc 0.984375\n",
      "2018-10-26T16:35:53.069053: step 7570, loss 0.00015546, acc 1\n",
      "2018-10-26T16:35:53.409191: step 7571, loss 7.09148e-05, acc 1\n",
      "2018-10-26T16:35:53.763202: step 7572, loss 0.000276591, acc 1\n",
      "2018-10-26T16:35:54.086336: step 7573, loss 0.000675058, acc 1\n",
      "2018-10-26T16:35:54.453355: step 7574, loss 0.00889461, acc 1\n",
      "2018-10-26T16:35:54.841374: step 7575, loss 0.0234634, acc 0.984375\n",
      "2018-10-26T16:35:55.167449: step 7576, loss 0.00075356, acc 1\n",
      "2018-10-26T16:35:55.491950: step 7577, loss 0.00445702, acc 1\n",
      "2018-10-26T16:35:55.853720: step 7578, loss 0.000113779, acc 1\n",
      "2018-10-26T16:35:56.206672: step 7579, loss 0.00406635, acc 1\n",
      "2018-10-26T16:35:56.579675: step 7580, loss 0.000382935, acc 1\n",
      "2018-10-26T16:35:56.957824: step 7581, loss 0.000123236, acc 1\n",
      "2018-10-26T16:35:57.332664: step 7582, loss 0.00762799, acc 1\n",
      "2018-10-26T16:35:57.714645: step 7583, loss 0.000233211, acc 1\n",
      "2018-10-26T16:35:58.067699: step 7584, loss 0.000864166, acc 1\n",
      "2018-10-26T16:35:58.443698: step 7585, loss 0.000343631, acc 1\n",
      "2018-10-26T16:35:58.791800: step 7586, loss 0.00102922, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:35:59.142826: step 7587, loss 0.00181739, acc 1\n",
      "2018-10-26T16:35:59.492891: step 7588, loss 0.000113581, acc 1\n",
      "2018-10-26T16:35:59.885841: step 7589, loss 0.000262245, acc 1\n",
      "2018-10-26T16:36:00.260842: step 7590, loss 0.00413276, acc 1\n",
      "2018-10-26T16:36:00.601930: step 7591, loss 3.82883e-05, acc 1\n",
      "2018-10-26T16:36:00.942517: step 7592, loss 0.000217481, acc 1\n",
      "2018-10-26T16:36:01.309110: step 7593, loss 0.000345973, acc 1\n",
      "2018-10-26T16:36:01.671135: step 7594, loss 0.00021932, acc 1\n",
      "2018-10-26T16:36:02.150789: step 7595, loss 0.000138064, acc 1\n",
      "2018-10-26T16:36:02.475939: step 7596, loss 1.86323e-05, acc 1\n",
      "2018-10-26T16:36:02.879851: step 7597, loss 0.00186362, acc 1\n",
      "2018-10-26T16:36:03.278776: step 7598, loss 0.00183616, acc 1\n",
      "2018-10-26T16:36:03.625849: step 7599, loss 0.000852513, acc 1\n",
      "2018-10-26T16:36:03.952973: step 7600, loss 0.000409911, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:36:04.765802: step 7600, loss 2.54182, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7600\n",
      "\n",
      "2018-10-26T16:36:05.402101: step 7601, loss 0.00020964, acc 1\n",
      "2018-10-26T16:36:05.714269: step 7602, loss 0.00701048, acc 1\n",
      "2018-10-26T16:36:06.128186: step 7603, loss 0.000122936, acc 1\n",
      "2018-10-26T16:36:06.484212: step 7604, loss 0.00286043, acc 1\n",
      "2018-10-26T16:36:06.829290: step 7605, loss 0.00601755, acc 1\n",
      "2018-10-26T16:36:07.155492: step 7606, loss 2.13151e-05, acc 1\n",
      "2018-10-26T16:36:07.484541: step 7607, loss 0.000903816, acc 1\n",
      "2018-10-26T16:36:07.800752: step 7608, loss 0.000894364, acc 1\n",
      "2018-10-26T16:36:08.124828: step 7609, loss 0.000130461, acc 1\n",
      "2018-10-26T16:36:08.463924: step 7610, loss 0.000251326, acc 1\n",
      "2018-10-26T16:36:08.791158: step 7611, loss 0.000155559, acc 1\n",
      "2018-10-26T16:36:09.117346: step 7612, loss 0.000855125, acc 1\n",
      "2018-10-26T16:36:09.448294: step 7613, loss 0.00223852, acc 1\n",
      "2018-10-26T16:36:09.818306: step 7614, loss 0.000968839, acc 1\n",
      "2018-10-26T16:36:10.168408: step 7615, loss 9.38567e-05, acc 1\n",
      "2018-10-26T16:36:10.506464: step 7616, loss 0.0014059, acc 1\n",
      "2018-10-26T16:36:10.853539: step 7617, loss 4.70802e-05, acc 1\n",
      "2018-10-26T16:36:11.199616: step 7618, loss 0.00285257, acc 1\n",
      "2018-10-26T16:36:11.541740: step 7619, loss 9.18416e-05, acc 1\n",
      "2018-10-26T16:36:11.901738: step 7620, loss 0.00228287, acc 1\n",
      "2018-10-26T16:36:12.248809: step 7621, loss 0.0039762, acc 1\n",
      "2018-10-26T16:36:12.618820: step 7622, loss 0.000362408, acc 1\n",
      "2018-10-26T16:36:12.963899: step 7623, loss 0.000186048, acc 1\n",
      "2018-10-26T16:36:13.301130: step 7624, loss 0.00127946, acc 1\n",
      "2018-10-26T16:36:13.642150: step 7625, loss 0.00274548, acc 1\n",
      "2018-10-26T16:36:14.002196: step 7626, loss 0.00220814, acc 1\n",
      "2018-10-26T16:36:14.362162: step 7627, loss 0.00122501, acc 1\n",
      "2018-10-26T16:36:14.705249: step 7628, loss 0.000803108, acc 1\n",
      "2018-10-26T16:36:15.049329: step 7629, loss 0.000130579, acc 1\n",
      "2018-10-26T16:36:15.393407: step 7630, loss 9.53436e-06, acc 1\n",
      "2018-10-26T16:36:15.735493: step 7631, loss 0.000255882, acc 1\n",
      "2018-10-26T16:36:16.083563: step 7632, loss 0.000471855, acc 1\n",
      "2018-10-26T16:36:16.471527: step 7633, loss 0.000451309, acc 1\n",
      "2018-10-26T16:36:16.861483: step 7634, loss 8.22233e-05, acc 1\n",
      "2018-10-26T16:36:17.155750: step 7635, loss 0.00727147, acc 1\n",
      "2018-10-26T16:36:17.422983: step 7636, loss 3.54547e-05, acc 1\n",
      "2018-10-26T16:36:17.718223: step 7637, loss 3.39213e-05, acc 1\n",
      "2018-10-26T16:36:18.018392: step 7638, loss 0.000100174, acc 1\n",
      "2018-10-26T16:36:18.304627: step 7639, loss 0.00174872, acc 1\n",
      "2018-10-26T16:36:18.593855: step 7640, loss 9.82717e-05, acc 1\n",
      "2018-10-26T16:36:18.898062: step 7641, loss 0.000444335, acc 1\n",
      "2018-10-26T16:36:19.198302: step 7642, loss 7.35243e-05, acc 1\n",
      "2018-10-26T16:36:19.510636: step 7643, loss 0.000594509, acc 1\n",
      "2018-10-26T16:36:19.796694: step 7644, loss 0.0101724, acc 1\n",
      "2018-10-26T16:36:20.102823: step 7645, loss 7.77234e-05, acc 1\n",
      "2018-10-26T16:36:20.393047: step 7646, loss 0.000585025, acc 1\n",
      "2018-10-26T16:36:20.725160: step 7647, loss 0.000468424, acc 1\n",
      "2018-10-26T16:36:21.143046: step 7648, loss 0.00550543, acc 1\n",
      "2018-10-26T16:36:21.466310: step 7649, loss 0.000136707, acc 1\n",
      "2018-10-26T16:36:21.750421: step 7650, loss 0.000529675, acc 1\n",
      "2018-10-26T16:36:22.047627: step 7651, loss 0.00146976, acc 1\n",
      "2018-10-26T16:36:22.340863: step 7652, loss 0.000205583, acc 1\n",
      "2018-10-26T16:36:22.638070: step 7653, loss 0.0283665, acc 0.984375\n",
      "2018-10-26T16:36:22.951212: step 7654, loss 1.82019e-05, acc 1\n",
      "2018-10-26T16:36:23.269362: step 7655, loss 0.000115921, acc 1\n",
      "2018-10-26T16:36:23.567565: step 7656, loss 0.000721889, acc 1\n",
      "2018-10-26T16:36:23.876739: step 7657, loss 6.03767e-05, acc 1\n",
      "2018-10-26T16:36:24.177954: step 7658, loss 0.000107992, acc 1\n",
      "2018-10-26T16:36:24.461237: step 7659, loss 0.00050828, acc 1\n",
      "2018-10-26T16:36:24.741480: step 7660, loss 0.00272455, acc 1\n",
      "2018-10-26T16:36:25.049859: step 7661, loss 0.00117585, acc 1\n",
      "2018-10-26T16:36:25.353793: step 7662, loss 0.00229555, acc 1\n",
      "2018-10-26T16:36:25.660973: step 7663, loss 0.00568327, acc 1\n",
      "2018-10-26T16:36:25.951198: step 7664, loss 0.00365997, acc 1\n",
      "2018-10-26T16:36:26.255384: step 7665, loss 0.00399947, acc 1\n",
      "2018-10-26T16:36:26.537655: step 7666, loss 1.71834e-05, acc 1\n",
      "2018-10-26T16:36:26.830872: step 7667, loss 7.75306e-05, acc 1\n",
      "2018-10-26T16:36:27.126059: step 7668, loss 0.000442731, acc 1\n",
      "2018-10-26T16:36:27.416283: step 7669, loss 0.0232064, acc 0.984375\n",
      "2018-10-26T16:36:27.731477: step 7670, loss 0.000100718, acc 1\n",
      "2018-10-26T16:36:28.046602: step 7671, loss 0.000683052, acc 1\n",
      "2018-10-26T16:36:28.359762: step 7672, loss 0.00042022, acc 1\n",
      "2018-10-26T16:36:28.695003: step 7673, loss 1.46367e-05, acc 1\n",
      "2018-10-26T16:36:29.022017: step 7674, loss 0.00280669, acc 1\n",
      "2018-10-26T16:36:29.342270: step 7675, loss 0.00186902, acc 1\n",
      "2018-10-26T16:36:29.642334: step 7676, loss 6.45042e-05, acc 1\n",
      "2018-10-26T16:36:29.964479: step 7677, loss 0.00697868, acc 1\n",
      "2018-10-26T16:36:30.261683: step 7678, loss 8.6378e-05, acc 1\n",
      "2018-10-26T16:36:30.587847: step 7679, loss 0.05366, acc 0.984375\n",
      "2018-10-26T16:36:30.925907: step 7680, loss 0.0013027, acc 1\n",
      "2018-10-26T16:36:31.254063: step 7681, loss 0.00040777, acc 1\n",
      "2018-10-26T16:36:31.580158: step 7682, loss 0.00117676, acc 1\n",
      "2018-10-26T16:36:31.923242: step 7683, loss 0.00215895, acc 1\n",
      "2018-10-26T16:36:32.375045: step 7684, loss 1.59653e-05, acc 1\n",
      "2018-10-26T16:36:32.812304: step 7685, loss 0.00473944, acc 1\n",
      "2018-10-26T16:36:33.279621: step 7686, loss 1.26982e-05, acc 1\n",
      "2018-10-26T16:36:33.697501: step 7687, loss 4.00792e-05, acc 1\n",
      "2018-10-26T16:36:34.159377: step 7688, loss 0.000942474, acc 1\n",
      "2018-10-26T16:36:34.591113: step 7689, loss 0.000215001, acc 1\n",
      "2018-10-26T16:36:34.893307: step 7690, loss 0.00584046, acc 1\n",
      "2018-10-26T16:36:35.218483: step 7691, loss 0.000187825, acc 1\n",
      "2018-10-26T16:36:35.560624: step 7692, loss 0.000456021, acc 1\n",
      "2018-10-26T16:36:35.887690: step 7693, loss 7.5992e-05, acc 1\n",
      "2018-10-26T16:36:36.325483: step 7694, loss 0.00100789, acc 1\n",
      "2018-10-26T16:36:36.670684: step 7695, loss 0.00214901, acc 1\n",
      "2018-10-26T16:36:37.022617: step 7696, loss 0.00484281, acc 1\n",
      "2018-10-26T16:36:37.387863: step 7697, loss 0.00416726, acc 1\n",
      "2018-10-26T16:36:37.728729: step 7698, loss 0.00193568, acc 1\n",
      "2018-10-26T16:36:38.059846: step 7699, loss 0.000188818, acc 1\n",
      "2018-10-26T16:36:38.406918: step 7700, loss 0.000297023, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:36:39.267660: step 7700, loss 2.60117, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7700\n",
      "\n",
      "2018-10-26T16:36:39.978750: step 7701, loss 7.08199e-05, acc 1\n",
      "2018-10-26T16:36:40.294873: step 7702, loss 0.000246187, acc 1\n",
      "2018-10-26T16:36:40.721732: step 7703, loss 0.00318162, acc 1\n",
      "2018-10-26T16:36:41.048860: step 7704, loss 0.000303969, acc 1\n",
      "2018-10-26T16:36:41.383965: step 7705, loss 2.92786e-05, acc 1\n",
      "2018-10-26T16:36:41.733031: step 7706, loss 0.000927188, acc 1\n",
      "2018-10-26T16:36:42.019295: step 7707, loss 5.23514e-05, acc 1\n",
      "2018-10-26T16:36:42.321566: step 7708, loss 0.0014403, acc 1\n",
      "2018-10-26T16:36:42.616669: step 7709, loss 3.8283e-05, acc 1\n",
      "2018-10-26T16:36:42.908888: step 7710, loss 0.00109348, acc 1\n",
      "2018-10-26T16:36:43.201107: step 7711, loss 0.000451005, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:36:43.488341: step 7712, loss 9.24949e-05, acc 1\n",
      "2018-10-26T16:36:43.758618: step 7713, loss 1.64293e-05, acc 1\n",
      "2018-10-26T16:36:44.063803: step 7714, loss 6.18775e-05, acc 1\n",
      "2018-10-26T16:36:44.401071: step 7715, loss 0.000128935, acc 1\n",
      "2018-10-26T16:36:44.729027: step 7716, loss 0.0166056, acc 0.984375\n",
      "2018-10-26T16:36:45.047176: step 7717, loss 0.00100564, acc 1\n",
      "2018-10-26T16:36:45.335405: step 7718, loss 0.00191782, acc 1\n",
      "2018-10-26T16:36:45.623636: step 7719, loss 5.04845e-05, acc 1\n",
      "2018-10-26T16:36:45.923834: step 7720, loss 0.0193861, acc 0.984375\n",
      "2018-10-26T16:36:46.233007: step 7721, loss 0.00849093, acc 1\n",
      "2018-10-26T16:36:46.560133: step 7722, loss 0.00178143, acc 1\n",
      "2018-10-26T16:36:46.926157: step 7723, loss 0.000186053, acc 1\n",
      "2018-10-26T16:36:47.257270: step 7724, loss 0.000524739, acc 1\n",
      "2018-10-26T16:36:47.619302: step 7725, loss 0.00304366, acc 1\n",
      "2018-10-26T16:36:48.004274: step 7726, loss 3.93922e-05, acc 1\n",
      "2018-10-26T16:36:48.321482: step 7727, loss 4.49396e-05, acc 1\n",
      "2018-10-26T16:36:48.749284: step 7728, loss 0.000938867, acc 1\n",
      "2018-10-26T16:36:49.078476: step 7729, loss 0.000720385, acc 1\n",
      "2018-10-26T16:36:49.486314: step 7730, loss 0.000110478, acc 1\n",
      "2018-10-26T16:36:49.884252: step 7731, loss 0.000428136, acc 1\n",
      "2018-10-26T16:36:50.240378: step 7732, loss 0.00129325, acc 1\n",
      "2018-10-26T16:36:50.647214: step 7733, loss 0.00189918, acc 1\n",
      "2018-10-26T16:36:51.082052: step 7734, loss 0.000227272, acc 1\n",
      "2018-10-26T16:36:51.481983: step 7735, loss 0.000338227, acc 1\n",
      "2018-10-26T16:36:51.871328: step 7736, loss 5.66844e-05, acc 1\n",
      "2018-10-26T16:36:52.301912: step 7737, loss 6.16837e-05, acc 1\n",
      "2018-10-26T16:36:52.706710: step 7738, loss 7.7297e-06, acc 1\n",
      "2018-10-26T16:36:53.135564: step 7739, loss 7.63918e-05, acc 1\n",
      "2018-10-26T16:36:53.532505: step 7740, loss 0.00248732, acc 1\n",
      "2018-10-26T16:36:53.925454: step 7741, loss 0.000180177, acc 1\n",
      "2018-10-26T16:36:54.274606: step 7742, loss 0.00106718, acc 1\n",
      "2018-10-26T16:36:54.641541: step 7743, loss 0.000381557, acc 1\n",
      "2018-10-26T16:36:54.995595: step 7744, loss 0.00519215, acc 1\n",
      "2018-10-26T16:36:55.377574: step 7745, loss 0.000202891, acc 1\n",
      "2018-10-26T16:36:55.809421: step 7746, loss 8.90959e-05, acc 1\n",
      "2018-10-26T16:36:56.164472: step 7747, loss 0.0113707, acc 0.984375\n",
      "2018-10-26T16:36:56.533485: step 7748, loss 0.015941, acc 0.984375\n",
      "2018-10-26T16:36:56.849643: step 7749, loss 0.00203095, acc 1\n",
      "2018-10-26T16:36:57.196713: step 7750, loss 0.000382445, acc 1\n",
      "2018-10-26T16:36:57.494917: step 7751, loss 6.07503e-05, acc 1\n",
      "2018-10-26T16:36:57.771223: step 7752, loss 0.000506955, acc 1\n",
      "2018-10-26T16:36:58.061466: step 7753, loss 0.00132018, acc 1\n",
      "2018-10-26T16:36:58.335670: step 7754, loss 0.00404567, acc 1\n",
      "2018-10-26T16:36:58.649859: step 7755, loss 0.000717182, acc 1\n",
      "2018-10-26T16:36:58.964988: step 7756, loss 0.000514296, acc 1\n",
      "2018-10-26T16:36:59.264190: step 7757, loss 0.000131895, acc 1\n",
      "2018-10-26T16:36:59.556408: step 7758, loss 0.000201703, acc 1\n",
      "2018-10-26T16:36:59.905481: step 7759, loss 0.000153373, acc 1\n",
      "2018-10-26T16:37:00.236594: step 7760, loss 0.0010477, acc 1\n",
      "2018-10-26T16:37:00.545767: step 7761, loss 0.000767351, acc 1\n",
      "2018-10-26T16:37:00.872891: step 7762, loss 9.83153e-05, acc 1\n",
      "2018-10-26T16:37:01.166202: step 7763, loss 0.00119547, acc 1\n",
      "2018-10-26T16:37:01.495281: step 7764, loss 0.000241683, acc 1\n",
      "2018-10-26T16:37:01.852276: step 7765, loss 0.00341901, acc 1\n",
      "2018-10-26T16:37:02.163503: step 7766, loss 0.00525938, acc 1\n",
      "2018-10-26T16:37:02.571695: step 7767, loss 7.82886e-05, acc 1\n",
      "2018-10-26T16:37:03.013173: step 7768, loss 2.45679e-05, acc 1\n",
      "2018-10-26T16:37:03.315365: step 7769, loss 0.00529573, acc 1\n",
      "2018-10-26T16:37:03.600604: step 7770, loss 0.000711342, acc 1\n",
      "2018-10-26T16:37:03.885841: step 7771, loss 0.00147814, acc 1\n",
      "2018-10-26T16:37:04.190030: step 7772, loss 0.000247937, acc 1\n",
      "2018-10-26T16:37:04.520146: step 7773, loss 0.000416644, acc 1\n",
      "2018-10-26T16:37:04.875199: step 7774, loss 0.00616023, acc 1\n",
      "2018-10-26T16:37:05.193348: step 7775, loss 0.000111925, acc 1\n",
      "2018-10-26T16:37:05.504517: step 7776, loss 0.000787261, acc 1\n",
      "2018-10-26T16:37:05.852761: step 7777, loss 6.18431e-05, acc 1\n",
      "2018-10-26T16:37:06.224592: step 7778, loss 0.00121158, acc 1\n",
      "2018-10-26T16:37:06.593788: step 7779, loss 0.00460163, acc 1\n",
      "2018-10-26T16:37:06.937709: step 7780, loss 0.0121347, acc 0.984375\n",
      "2018-10-26T16:37:07.287772: step 7781, loss 0.00289371, acc 1\n",
      "2018-10-26T16:37:07.579973: step 7782, loss 0.00014975, acc 1\n",
      "2018-10-26T16:37:07.883161: step 7783, loss 0.0172651, acc 0.984375\n",
      "2018-10-26T16:37:08.172390: step 7784, loss 9.76135e-05, acc 1\n",
      "2018-10-26T16:37:08.474581: step 7785, loss 0.00093168, acc 1\n",
      "2018-10-26T16:37:08.778966: step 7786, loss 0.000359701, acc 1\n",
      "2018-10-26T16:37:09.057103: step 7787, loss 0.000933749, acc 1\n",
      "2018-10-26T16:37:09.389159: step 7788, loss 0.000325887, acc 1\n",
      "2018-10-26T16:37:09.717261: step 7789, loss 0.000720651, acc 1\n",
      "2018-10-26T16:37:10.008503: step 7790, loss 0.0017699, acc 1\n",
      "2018-10-26T16:37:10.304691: step 7791, loss 0.000252257, acc 1\n",
      "2018-10-26T16:37:10.655753: step 7792, loss 5.84485e-05, acc 1\n",
      "2018-10-26T16:37:10.972905: step 7793, loss 9.89343e-06, acc 1\n",
      "2018-10-26T16:37:11.276219: step 7794, loss 0.00017527, acc 1\n",
      "2018-10-26T16:37:11.591540: step 7795, loss 0.000130829, acc 1\n",
      "2018-10-26T16:37:11.915473: step 7796, loss 5.60991e-06, acc 1\n",
      "2018-10-26T16:37:12.335266: step 7797, loss 0.000282363, acc 1\n",
      "2018-10-26T16:37:12.753151: step 7798, loss 5.72097e-05, acc 1\n",
      "2018-10-26T16:37:13.079558: step 7799, loss 0.000122567, acc 1\n",
      "2018-10-26T16:37:13.371574: step 7800, loss 0.000104348, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:37:14.136454: step 7800, loss 2.6428, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7800\n",
      "\n",
      "2018-10-26T16:37:14.653073: step 7801, loss 4.22668e-05, acc 1\n",
      "2018-10-26T16:37:15.068961: step 7802, loss 0.000202623, acc 1\n",
      "2018-10-26T16:37:15.441965: step 7803, loss 9.36153e-05, acc 1\n",
      "2018-10-26T16:37:15.727221: step 7804, loss 2.8882e-05, acc 1\n",
      "2018-10-26T16:37:15.999478: step 7805, loss 0.000221959, acc 1\n",
      "2018-10-26T16:37:16.268757: step 7806, loss 0.000684111, acc 1\n",
      "2018-10-26T16:37:16.531166: step 7807, loss 0.000167106, acc 1\n",
      "2018-10-26T16:37:16.982850: step 7808, loss 0.000460233, acc 1\n",
      "2018-10-26T16:37:17.329946: step 7809, loss 0.000464862, acc 1\n",
      "2018-10-26T16:37:17.604189: step 7810, loss 0.000263554, acc 1\n",
      "2018-10-26T16:37:17.940338: step 7811, loss 7.68554e-05, acc 1\n",
      "2018-10-26T16:37:18.338321: step 7812, loss 0.000178472, acc 1\n",
      "2018-10-26T16:37:18.687296: step 7813, loss 0.0019401, acc 1\n",
      "2018-10-26T16:37:18.987493: step 7814, loss 0.000721305, acc 1\n",
      "2018-10-26T16:37:19.284698: step 7815, loss 0.000128151, acc 1\n",
      "2018-10-26T16:37:19.577951: step 7816, loss 9.45682e-05, acc 1\n",
      "2018-10-26T16:37:19.916153: step 7817, loss 0.0452278, acc 0.984375\n",
      "2018-10-26T16:37:20.325917: step 7818, loss 0.000211408, acc 1\n",
      "2018-10-26T16:37:20.694931: step 7819, loss 0.0879069, acc 0.984375\n",
      "2018-10-26T16:37:21.021188: step 7820, loss 0.00072772, acc 1\n",
      "2018-10-26T16:37:21.428970: step 7821, loss 4.32009e-05, acc 1\n",
      "2018-10-26T16:37:21.936907: step 7822, loss 0.00207138, acc 1\n",
      "2018-10-26T16:37:22.272715: step 7823, loss 0.00941953, acc 1\n",
      "2018-10-26T16:37:22.591942: step 7824, loss 0.000191992, acc 1\n",
      "2018-10-26T16:37:22.923975: step 7825, loss 0.00754012, acc 1\n",
      "2018-10-26T16:37:23.258084: step 7826, loss 0.00344688, acc 1\n",
      "2018-10-26T16:37:23.582216: step 7827, loss 0.000331643, acc 1\n",
      "2018-10-26T16:37:23.941300: step 7828, loss 0.000309225, acc 1\n",
      "2018-10-26T16:37:24.263398: step 7829, loss 0.000153192, acc 1\n",
      "2018-10-26T16:37:24.587531: step 7830, loss 0.000572926, acc 1\n",
      "2018-10-26T16:37:24.950562: step 7831, loss 0.00260783, acc 1\n",
      "2018-10-26T16:37:25.319577: step 7832, loss 0.000898033, acc 1\n",
      "2018-10-26T16:37:25.632798: step 7833, loss 0.000651802, acc 1\n",
      "2018-10-26T16:37:25.936925: step 7834, loss 0.000184854, acc 1\n",
      "2018-10-26T16:37:26.251180: step 7835, loss 0.00039736, acc 1\n",
      "2018-10-26T16:37:26.584195: step 7836, loss 0.00027836, acc 1\n",
      "2018-10-26T16:37:26.925285: step 7837, loss 0.000407504, acc 1\n",
      "2018-10-26T16:37:27.289311: step 7838, loss 3.95156e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:37:27.647354: step 7839, loss 0.000492167, acc 1\n",
      "2018-10-26T16:37:28.001466: step 7840, loss 0.00534804, acc 1\n",
      "2018-10-26T16:37:28.334537: step 7841, loss 0.000550229, acc 1\n",
      "2018-10-26T16:37:28.676606: step 7842, loss 7.84821e-05, acc 1\n",
      "2018-10-26T16:37:29.030711: step 7843, loss 6.91098e-05, acc 1\n",
      "2018-10-26T16:37:29.371829: step 7844, loss 3.62656e-05, acc 1\n",
      "2018-10-26T16:37:29.762703: step 7845, loss 0.00181719, acc 1\n",
      "2018-10-26T16:37:30.099805: step 7846, loss 0.00130371, acc 1\n",
      "2018-10-26T16:37:30.403988: step 7847, loss 1.11905e-05, acc 1\n",
      "2018-10-26T16:37:30.735108: step 7848, loss 0.00264568, acc 1\n",
      "2018-10-26T16:37:31.075199: step 7849, loss 0.0142016, acc 0.984375\n",
      "2018-10-26T16:37:31.398363: step 7850, loss 0.00234766, acc 1\n",
      "2018-10-26T16:37:31.747401: step 7851, loss 1.77414e-05, acc 1\n",
      "2018-10-26T16:37:32.074580: step 7852, loss 0.0003369, acc 1\n",
      "2018-10-26T16:37:32.400656: step 7853, loss 0.000505233, acc 1\n",
      "2018-10-26T16:37:32.712822: step 7854, loss 0.000197924, acc 1\n",
      "2018-10-26T16:37:33.056901: step 7855, loss 0.00143843, acc 1\n",
      "2018-10-26T16:37:33.402977: step 7856, loss 2.64142e-05, acc 1\n",
      "2018-10-26T16:37:33.755073: step 7857, loss 0.000926142, acc 1\n",
      "2018-10-26T16:37:34.072190: step 7858, loss 0.000409013, acc 1\n",
      "2018-10-26T16:37:34.390385: step 7859, loss 0.000632638, acc 1\n",
      "2018-10-26T16:37:34.891999: step 7860, loss 0.0389655, acc 0.984375\n",
      "2018-10-26T16:37:35.227102: step 7861, loss 0.00329578, acc 1\n",
      "2018-10-26T16:37:35.540265: step 7862, loss 0.00459504, acc 1\n",
      "2018-10-26T16:37:35.882354: step 7863, loss 0.000602794, acc 1\n",
      "2018-10-26T16:37:36.226435: step 7864, loss 0.000536205, acc 1\n",
      "2018-10-26T16:37:36.544582: step 7865, loss 0.00113879, acc 1\n",
      "2018-10-26T16:37:36.864727: step 7866, loss 0.00155661, acc 1\n",
      "2018-10-26T16:37:37.155977: step 7867, loss 0.000484246, acc 1\n",
      "2018-10-26T16:37:37.487067: step 7868, loss 0.0014922, acc 1\n",
      "2018-10-26T16:37:37.822245: step 7869, loss 0.00102766, acc 1\n",
      "2018-10-26T16:37:38.134336: step 7870, loss 1.85709e-05, acc 1\n",
      "2018-10-26T16:37:38.448497: step 7871, loss 0.000750786, acc 1\n",
      "2018-10-26T16:37:38.767643: step 7872, loss 0.000180635, acc 1\n",
      "2018-10-26T16:37:39.117709: step 7873, loss 0.00476642, acc 1\n",
      "2018-10-26T16:37:39.426958: step 7874, loss 0.00453596, acc 1\n",
      "2018-10-26T16:37:39.823822: step 7875, loss 0.000181696, acc 1\n",
      "2018-10-26T16:37:40.160966: step 7876, loss 0.000220023, acc 1\n",
      "2018-10-26T16:37:40.511982: step 7877, loss 2.86675e-05, acc 1\n",
      "2018-10-26T16:37:40.859056: step 7878, loss 0.00995509, acc 1\n",
      "2018-10-26T16:37:41.218179: step 7879, loss 0.000974227, acc 1\n",
      "2018-10-26T16:37:41.582124: step 7880, loss 0.000111342, acc 1\n",
      "2018-10-26T16:37:41.907256: step 7881, loss 0.00252079, acc 1\n",
      "2018-10-26T16:37:42.263432: step 7882, loss 0.000987097, acc 1\n",
      "2018-10-26T16:37:42.598634: step 7883, loss 0.00705143, acc 1\n",
      "2018-10-26T16:37:42.955460: step 7884, loss 0.000122585, acc 1\n",
      "2018-10-26T16:37:43.293587: step 7885, loss 0.000437697, acc 1\n",
      "2018-10-26T16:37:43.623668: step 7886, loss 0.000267388, acc 1\n",
      "2018-10-26T16:37:43.956783: step 7887, loss 0.00021878, acc 1\n",
      "2018-10-26T16:37:44.294876: step 7888, loss 0.000553397, acc 1\n",
      "2018-10-26T16:37:44.655912: step 7889, loss 0.0026574, acc 1\n",
      "2018-10-26T16:37:45.025925: step 7890, loss 8.0871e-05, acc 1\n",
      "2018-10-26T16:37:45.387957: step 7891, loss 4.43294e-05, acc 1\n",
      "2018-10-26T16:37:45.741135: step 7892, loss 4.24438e-05, acc 1\n",
      "2018-10-26T16:37:46.104045: step 7893, loss 0.00146544, acc 1\n",
      "2018-10-26T16:37:46.453144: step 7894, loss 0.00781584, acc 1\n",
      "2018-10-26T16:37:46.794198: step 7895, loss 0.000200372, acc 1\n",
      "2018-10-26T16:37:47.155486: step 7896, loss 8.49153e-05, acc 1\n",
      "2018-10-26T16:37:47.503302: step 7897, loss 0.00417964, acc 1\n",
      "2018-10-26T16:37:47.868329: step 7898, loss 0.029759, acc 0.984375\n",
      "2018-10-26T16:37:48.199443: step 7899, loss 0.00394127, acc 1\n",
      "2018-10-26T16:37:48.589407: step 7900, loss 0.000434665, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:37:49.466058: step 7900, loss 2.68961, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-7900\n",
      "\n",
      "2018-10-26T16:37:50.105350: step 7901, loss 0.000301736, acc 1\n",
      "2018-10-26T16:37:50.422505: step 7902, loss 0.0122179, acc 0.984375\n",
      "2018-10-26T16:37:50.868345: step 7903, loss 0.000317695, acc 1\n",
      "2018-10-26T16:37:51.227356: step 7904, loss 4.46424e-05, acc 1\n",
      "2018-10-26T16:37:51.569551: step 7905, loss 0.00104274, acc 1\n",
      "2018-10-26T16:37:51.943440: step 7906, loss 0.00191058, acc 1\n",
      "2018-10-26T16:37:52.314451: step 7907, loss 0.000388789, acc 1\n",
      "2018-10-26T16:37:52.669500: step 7908, loss 0.004972, acc 1\n",
      "2018-10-26T16:37:52.997770: step 7909, loss 0.000469812, acc 1\n",
      "2018-10-26T16:37:53.334823: step 7910, loss 0.000776984, acc 1\n",
      "2018-10-26T16:37:53.696754: step 7911, loss 0.0117713, acc 0.984375\n",
      "2018-10-26T16:37:54.050840: step 7912, loss 3.31261e-05, acc 1\n",
      "2018-10-26T16:37:54.448747: step 7913, loss 0.000284707, acc 1\n",
      "2018-10-26T16:37:54.849674: step 7914, loss 6.88565e-05, acc 1\n",
      "2018-10-26T16:37:55.249606: step 7915, loss 0.00386403, acc 1\n",
      "2018-10-26T16:37:55.657553: step 7916, loss 0.00645387, acc 1\n",
      "2018-10-26T16:37:56.051507: step 7917, loss 0.000954081, acc 1\n",
      "2018-10-26T16:37:56.440507: step 7918, loss 0.000957355, acc 1\n",
      "2018-10-26T16:37:56.838361: step 7919, loss 0.000389409, acc 1\n",
      "2018-10-26T16:37:57.193544: step 7920, loss 0.000102699, acc 1\n",
      "2018-10-26T16:37:57.624717: step 7921, loss 0.000580496, acc 1\n",
      "2018-10-26T16:37:58.049124: step 7922, loss 1.7948e-05, acc 1\n",
      "2018-10-26T16:37:58.521864: step 7923, loss 3.95547e-05, acc 1\n",
      "2018-10-26T16:37:58.984627: step 7924, loss 0.000279964, acc 1\n",
      "2018-10-26T16:37:59.419465: step 7925, loss 0.00101822, acc 1\n",
      "2018-10-26T16:37:59.760553: step 7926, loss 0.00301564, acc 1\n",
      "2018-10-26T16:38:00.131561: step 7927, loss 0.000414224, acc 1\n",
      "2018-10-26T16:38:00.544459: step 7928, loss 0.000576286, acc 1\n",
      "2018-10-26T16:38:00.947383: step 7929, loss 0.00791004, acc 1\n",
      "2018-10-26T16:38:01.345318: step 7930, loss 5.29762e-05, acc 1\n",
      "2018-10-26T16:38:01.729293: step 7931, loss 0.00265862, acc 1\n",
      "2018-10-26T16:38:02.074370: step 7932, loss 6.15928e-05, acc 1\n",
      "2018-10-26T16:38:02.481438: step 7933, loss 0.000471754, acc 1\n",
      "2018-10-26T16:38:02.927096: step 7934, loss 0.00024516, acc 1\n",
      "2018-10-26T16:38:03.267183: step 7935, loss 2.60575e-05, acc 1\n",
      "2018-10-26T16:38:03.613258: step 7936, loss 0.00161063, acc 1\n",
      "2018-10-26T16:38:03.956346: step 7937, loss 0.00107483, acc 1\n",
      "2018-10-26T16:38:04.332339: step 7938, loss 0.00174133, acc 1\n",
      "2018-10-26T16:38:04.674427: step 7939, loss 0.000651284, acc 1\n",
      "2018-10-26T16:38:05.007534: step 7940, loss 0.00528947, acc 1\n",
      "2018-10-26T16:38:05.372558: step 7941, loss 0.000507285, acc 1\n",
      "2018-10-26T16:38:05.794433: step 7942, loss 0.000254915, acc 1\n",
      "2018-10-26T16:38:06.136520: step 7943, loss 0.000291798, acc 1\n",
      "2018-10-26T16:38:06.488627: step 7944, loss 3.0089e-05, acc 1\n",
      "2018-10-26T16:38:06.822685: step 7945, loss 0.00313495, acc 1\n",
      "2018-10-26T16:38:07.177736: step 7946, loss 0.000512972, acc 1\n",
      "2018-10-26T16:38:07.522814: step 7947, loss 0.000889869, acc 1\n",
      "2018-10-26T16:38:07.835067: step 7948, loss 1.03797e-05, acc 1\n",
      "2018-10-26T16:38:08.198010: step 7949, loss 4.80922e-06, acc 1\n",
      "2018-10-26T16:38:08.507187: step 7950, loss 0.000919797, acc 1\n",
      "2018-10-26T16:38:08.958005: step 7951, loss 0.0008378, acc 1\n",
      "2018-10-26T16:38:09.395811: step 7952, loss 0.0172512, acc 0.984375\n",
      "2018-10-26T16:38:09.742883: step 7953, loss 0.000421934, acc 1\n",
      "2018-10-26T16:38:10.095275: step 7954, loss 0.0009538, acc 1\n",
      "2018-10-26T16:38:10.453022: step 7955, loss 0.0390483, acc 0.984375\n",
      "2018-10-26T16:38:10.780114: step 7956, loss 0.000323582, acc 1\n",
      "2018-10-26T16:38:11.167086: step 7957, loss 0.000313744, acc 1\n",
      "2018-10-26T16:38:11.548059: step 7958, loss 0.00129727, acc 1\n",
      "2018-10-26T16:38:11.882167: step 7959, loss 0.000214987, acc 1\n",
      "2018-10-26T16:38:12.245196: step 7960, loss 0.00207558, acc 1\n",
      "2018-10-26T16:38:12.569330: step 7961, loss 0.000199283, acc 1\n",
      "2018-10-26T16:38:12.914415: step 7962, loss 0.000113151, acc 1\n",
      "2018-10-26T16:38:13.252526: step 7963, loss 6.81501e-05, acc 1\n",
      "2018-10-26T16:38:13.587609: step 7964, loss 0.005, acc 1\n",
      "2018-10-26T16:38:13.948644: step 7965, loss 9.4376e-05, acc 1\n",
      "2018-10-26T16:38:14.306690: step 7966, loss 0.000474873, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:38:14.648775: step 7967, loss 0.000380232, acc 1\n",
      "2018-10-26T16:38:14.988866: step 7968, loss 0.00317772, acc 1\n",
      "2018-10-26T16:38:15.367854: step 7969, loss 0.000148543, acc 1\n",
      "2018-10-26T16:38:15.742851: step 7970, loss 0.000101257, acc 1\n",
      "2018-10-26T16:38:16.150761: step 7971, loss 0.00261072, acc 1\n",
      "2018-10-26T16:38:16.505812: step 7972, loss 0.000555545, acc 1\n",
      "2018-10-26T16:38:16.822964: step 7973, loss 0.0161913, acc 0.984375\n",
      "2018-10-26T16:38:17.166050: step 7974, loss 1.45746e-05, acc 1\n",
      "2018-10-26T16:38:17.483857: step 7975, loss 7.16818e-05, acc 1\n",
      "2018-10-26T16:38:17.811340: step 7976, loss 0.000864935, acc 1\n",
      "2018-10-26T16:38:18.175353: step 7977, loss 0.00471832, acc 1\n",
      "2018-10-26T16:38:18.550350: step 7978, loss 9.24921e-05, acc 1\n",
      "2018-10-26T16:38:18.875484: step 7979, loss 0.00415934, acc 1\n",
      "2018-10-26T16:38:19.219562: step 7980, loss 0.000944669, acc 1\n",
      "2018-10-26T16:38:19.604535: step 7981, loss 4.64102e-05, acc 1\n",
      "2018-10-26T16:38:19.942630: step 7982, loss 0.000537224, acc 1\n",
      "2018-10-26T16:38:20.310745: step 7983, loss 0.000189394, acc 1\n",
      "2018-10-26T16:38:20.667692: step 7984, loss 0.000318896, acc 1\n",
      "2018-10-26T16:38:20.981932: step 7985, loss 9.28113e-05, acc 1\n",
      "2018-10-26T16:38:21.346877: step 7986, loss 7.37252e-05, acc 1\n",
      "2018-10-26T16:38:21.692957: step 7987, loss 0.000246057, acc 1\n",
      "2018-10-26T16:38:22.036036: step 7988, loss 0.000298085, acc 1\n",
      "2018-10-26T16:38:22.383109: step 7989, loss 0.000269978, acc 1\n",
      "2018-10-26T16:38:22.739161: step 7990, loss 9.15129e-05, acc 1\n",
      "2018-10-26T16:38:23.093211: step 7991, loss 0.00025117, acc 1\n",
      "2018-10-26T16:38:23.469208: step 7992, loss 2.13373e-05, acc 1\n",
      "2018-10-26T16:38:23.839220: step 7993, loss 0.02048, acc 0.984375\n",
      "2018-10-26T16:38:24.209332: step 7994, loss 0.00517427, acc 1\n",
      "2018-10-26T16:38:24.608166: step 7995, loss 0.000673937, acc 1\n",
      "2018-10-26T16:38:24.964241: step 7996, loss 1.25117e-05, acc 1\n",
      "2018-10-26T16:38:25.310288: step 7997, loss 3.10584e-05, acc 1\n",
      "2018-10-26T16:38:25.698362: step 7998, loss 0.000212241, acc 1\n",
      "2018-10-26T16:38:26.083256: step 7999, loss 0.000771642, acc 1\n",
      "2018-10-26T16:38:26.413378: step 8000, loss 8.21731e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:38:27.214200: step 8000, loss 2.7007, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8000\n",
      "\n",
      "2018-10-26T16:38:27.875438: step 8001, loss 0.00323029, acc 1\n",
      "2018-10-26T16:38:28.239557: step 8002, loss 0.000230953, acc 1\n",
      "2018-10-26T16:38:28.710204: step 8003, loss 4.90864e-05, acc 1\n",
      "2018-10-26T16:38:29.080217: step 8004, loss 1.20205e-05, acc 1\n",
      "2018-10-26T16:38:29.452222: step 8005, loss 0.000236282, acc 1\n",
      "2018-10-26T16:38:29.817349: step 8006, loss 0.000281325, acc 1\n",
      "2018-10-26T16:38:30.158375: step 8007, loss 8.51256e-05, acc 1\n",
      "2018-10-26T16:38:30.507406: step 8008, loss 0.00235084, acc 1\n",
      "2018-10-26T16:38:30.889382: step 8009, loss 0.000142918, acc 1\n",
      "2018-10-26T16:38:31.277349: step 8010, loss 0.00112685, acc 1\n",
      "2018-10-26T16:38:31.624418: step 8011, loss 0.000220059, acc 1\n",
      "2018-10-26T16:38:31.984456: step 8012, loss 0.000101597, acc 1\n",
      "2018-10-26T16:38:32.317569: step 8013, loss 0.00124056, acc 1\n",
      "2018-10-26T16:38:32.658751: step 8014, loss 0.000198026, acc 1\n",
      "2018-10-26T16:38:33.010716: step 8015, loss 0.0017206, acc 1\n",
      "2018-10-26T16:38:33.380727: step 8016, loss 0.00100553, acc 1\n",
      "2018-10-26T16:38:33.717825: step 8017, loss 0.00149136, acc 1\n",
      "2018-10-26T16:38:34.038970: step 8018, loss 0.000330603, acc 1\n",
      "2018-10-26T16:38:34.368758: step 8019, loss 0.000163727, acc 1\n",
      "2018-10-26T16:38:34.716159: step 8020, loss 0.000113688, acc 1\n",
      "2018-10-26T16:38:35.087166: step 8021, loss 0.00507203, acc 1\n",
      "2018-10-26T16:38:35.424304: step 8022, loss 3.99882e-05, acc 1\n",
      "2018-10-26T16:38:35.788393: step 8023, loss 0.00553102, acc 1\n",
      "2018-10-26T16:38:36.155314: step 8024, loss 0.000145625, acc 1\n",
      "2018-10-26T16:38:36.505377: step 8025, loss 0.000131928, acc 1\n",
      "2018-10-26T16:38:36.902352: step 8026, loss 0.000447734, acc 1\n",
      "2018-10-26T16:38:37.257367: step 8027, loss 0.0199409, acc 0.984375\n",
      "2018-10-26T16:38:37.645331: step 8028, loss 0.000149798, acc 1\n",
      "2018-10-26T16:38:38.006833: step 8029, loss 0.107512, acc 0.984375\n",
      "2018-10-26T16:38:38.352441: step 8030, loss 0.000256304, acc 1\n",
      "2018-10-26T16:38:38.710486: step 8031, loss 0.000302352, acc 1\n",
      "2018-10-26T16:38:39.058555: step 8032, loss 0.00108035, acc 1\n",
      "2018-10-26T16:38:39.390671: step 8033, loss 0.00212241, acc 1\n",
      "2018-10-26T16:38:39.752700: step 8034, loss 0.00122028, acc 1\n",
      "2018-10-26T16:38:40.090796: step 8035, loss 3.34472e-05, acc 1\n",
      "2018-10-26T16:38:40.458874: step 8036, loss 9.18976e-05, acc 1\n",
      "2018-10-26T16:38:40.817890: step 8037, loss 0.0332995, acc 0.984375\n",
      "2018-10-26T16:38:41.164929: step 8038, loss 0.000267018, acc 1\n",
      "2018-10-26T16:38:41.505018: step 8039, loss 0.00136994, acc 1\n",
      "2018-10-26T16:38:41.851131: step 8040, loss 0.0135451, acc 0.984375\n",
      "2018-10-26T16:38:42.189191: step 8041, loss 0.0208024, acc 1\n",
      "2018-10-26T16:38:42.542246: step 8042, loss 0.0268655, acc 0.984375\n",
      "2018-10-26T16:38:42.914253: step 8043, loss 4.2083e-05, acc 1\n",
      "2018-10-26T16:38:43.241378: step 8044, loss 0.000312238, acc 1\n",
      "2018-10-26T16:38:43.553544: step 8045, loss 0.000749487, acc 1\n",
      "2018-10-26T16:38:43.917670: step 8046, loss 3.01089e-05, acc 1\n",
      "2018-10-26T16:38:44.364378: step 8047, loss 5.38383e-05, acc 1\n",
      "2018-10-26T16:38:44.767302: step 8048, loss 0.0096869, acc 1\n",
      "2018-10-26T16:38:45.167237: step 8049, loss 7.54061e-05, acc 1\n",
      "2018-10-26T16:38:45.604066: step 8050, loss 0.00440396, acc 1\n",
      "2018-10-26T16:38:46.008012: step 8051, loss 3.87413e-06, acc 1\n",
      "2018-10-26T16:38:46.344089: step 8052, loss 0.0115424, acc 1\n",
      "2018-10-26T16:38:46.705125: step 8053, loss 0.000463729, acc 1\n",
      "2018-10-26T16:38:47.121013: step 8054, loss 0.00230719, acc 1\n",
      "2018-10-26T16:38:47.466091: step 8055, loss 0.000100464, acc 1\n",
      "2018-10-26T16:38:47.810190: step 8056, loss 0.00604513, acc 1\n",
      "2018-10-26T16:38:48.149266: step 8057, loss 0.000261156, acc 1\n",
      "2018-10-26T16:38:48.532379: step 8058, loss 2.30195e-05, acc 1\n",
      "2018-10-26T16:38:48.895273: step 8059, loss 0.00149491, acc 1\n",
      "2018-10-26T16:38:49.248328: step 8060, loss 1.82275e-05, acc 1\n",
      "2018-10-26T16:38:49.606386: step 8061, loss 0.00450397, acc 1\n",
      "2018-10-26T16:38:50.035227: step 8062, loss 6.60607e-05, acc 1\n",
      "2018-10-26T16:38:50.412220: step 8063, loss 0.000247733, acc 1\n",
      "2018-10-26T16:38:50.777243: step 8064, loss 0.0116437, acc 0.984375\n",
      "2018-10-26T16:38:51.106383: step 8065, loss 7.54489e-05, acc 1\n",
      "2018-10-26T16:38:51.469396: step 8066, loss 0.00438428, acc 1\n",
      "2018-10-26T16:38:51.825445: step 8067, loss 0.000940596, acc 1\n",
      "2018-10-26T16:38:52.172515: step 8068, loss 0.000581966, acc 1\n",
      "2018-10-26T16:38:52.520586: step 8069, loss 6.54304e-05, acc 1\n",
      "2018-10-26T16:38:52.926549: step 8070, loss 0.000127723, acc 1\n",
      "2018-10-26T16:38:53.309480: step 8071, loss 0.00624351, acc 1\n",
      "2018-10-26T16:38:53.675504: step 8072, loss 1.21992e-05, acc 1\n",
      "2018-10-26T16:38:54.093384: step 8073, loss 0.00120561, acc 1\n",
      "2018-10-26T16:38:54.449569: step 8074, loss 0.000248825, acc 1\n",
      "2018-10-26T16:38:54.801493: step 8075, loss 0.000208848, acc 1\n",
      "2018-10-26T16:38:55.188240: step 8076, loss 0.000295562, acc 1\n",
      "2018-10-26T16:38:55.551537: step 8077, loss 0.000554912, acc 1\n",
      "2018-10-26T16:38:55.919508: step 8078, loss 0.000470207, acc 1\n",
      "2018-10-26T16:38:56.264582: step 8079, loss 0.177534, acc 0.96875\n",
      "2018-10-26T16:38:56.602679: step 8080, loss 0.000579533, acc 1\n",
      "2018-10-26T16:38:56.960857: step 8081, loss 0.000240612, acc 1\n",
      "2018-10-26T16:38:57.300817: step 8082, loss 0.000712045, acc 1\n",
      "2018-10-26T16:38:57.651920: step 8083, loss 0.0344131, acc 0.96875\n",
      "2018-10-26T16:38:57.999946: step 8084, loss 0.000357524, acc 1\n",
      "2018-10-26T16:38:58.342116: step 8085, loss 0.000852509, acc 1\n",
      "2018-10-26T16:38:58.699079: step 8086, loss 0.0104943, acc 1\n",
      "2018-10-26T16:38:59.077463: step 8087, loss 0.0141467, acc 0.984375\n",
      "2018-10-26T16:38:59.449075: step 8088, loss 0.00503638, acc 1\n",
      "2018-10-26T16:38:59.857984: step 8089, loss 0.0159048, acc 0.984375\n",
      "2018-10-26T16:39:00.283843: step 8090, loss 0.00107841, acc 1\n",
      "2018-10-26T16:39:00.670811: step 8091, loss 0.00734553, acc 1\n",
      "2018-10-26T16:39:01.089693: step 8092, loss 0.000163949, acc 1\n",
      "2018-10-26T16:39:01.552454: step 8093, loss 0.0027732, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:39:01.952387: step 8094, loss 0.00171246, acc 1\n",
      "2018-10-26T16:39:02.312634: step 8095, loss 0.00100105, acc 1\n",
      "2018-10-26T16:39:02.698393: step 8096, loss 0.0017675, acc 1\n",
      "2018-10-26T16:39:03.145198: step 8097, loss 0.000346188, acc 1\n",
      "2018-10-26T16:39:03.611952: step 8098, loss 0.000207212, acc 1\n",
      "2018-10-26T16:39:04.113611: step 8099, loss 0.000206894, acc 1\n",
      "2018-10-26T16:39:04.551441: step 8100, loss 0.000718569, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:39:05.605626: step 8100, loss 2.82078, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8100\n",
      "\n",
      "2018-10-26T16:39:06.326698: step 8101, loss 0.00326337, acc 1\n",
      "2018-10-26T16:39:06.755681: step 8102, loss 0.0168012, acc 0.984375\n",
      "2018-10-26T16:39:07.291122: step 8103, loss 0.000395689, acc 1\n",
      "2018-10-26T16:39:07.686067: step 8104, loss 0.0650423, acc 0.984375\n",
      "2018-10-26T16:39:08.047102: step 8105, loss 0.00266591, acc 1\n",
      "2018-10-26T16:39:08.425091: step 8106, loss 3.55017e-05, acc 1\n",
      "2018-10-26T16:39:08.757203: step 8107, loss 0.00161868, acc 1\n",
      "2018-10-26T16:39:09.088320: step 8108, loss 0.000132194, acc 1\n",
      "2018-10-26T16:39:09.406473: step 8109, loss 0.000566789, acc 1\n",
      "2018-10-26T16:39:09.736586: step 8110, loss 0.000237092, acc 1\n",
      "2018-10-26T16:39:10.062715: step 8111, loss 0.000312059, acc 1\n",
      "2018-10-26T16:39:10.388847: step 8112, loss 0.025166, acc 0.984375\n",
      "2018-10-26T16:39:10.722989: step 8113, loss 0.000191939, acc 1\n",
      "2018-10-26T16:39:11.063043: step 8114, loss 0.000170459, acc 1\n",
      "2018-10-26T16:39:11.413111: step 8115, loss 1.43964e-05, acc 1\n",
      "2018-10-26T16:39:11.740238: step 8116, loss 9.94865e-05, acc 1\n",
      "2018-10-26T16:39:12.064386: step 8117, loss 0.000254721, acc 1\n",
      "2018-10-26T16:39:12.397478: step 8118, loss 0.00037367, acc 1\n",
      "2018-10-26T16:39:12.710644: step 8119, loss 0.000666579, acc 1\n",
      "2018-10-26T16:39:13.052773: step 8120, loss 0.000458976, acc 1\n",
      "2018-10-26T16:39:13.385842: step 8121, loss 0.00199222, acc 1\n",
      "2018-10-26T16:39:13.712963: step 8122, loss 0.0577321, acc 0.984375\n",
      "2018-10-26T16:39:14.025128: step 8123, loss 0.000517177, acc 1\n",
      "2018-10-26T16:39:14.377190: step 8124, loss 0.00686154, acc 1\n",
      "2018-10-26T16:39:14.693343: step 8125, loss 0.0460748, acc 0.984375\n",
      "2018-10-26T16:39:15.056375: step 8126, loss 0.00287547, acc 1\n",
      "2018-10-26T16:39:15.407437: step 8127, loss 5.40452e-05, acc 1\n",
      "2018-10-26T16:39:15.724589: step 8128, loss 0.000281573, acc 1\n",
      "2018-10-26T16:39:16.063682: step 8129, loss 0.000491788, acc 1\n",
      "2018-10-26T16:39:16.367873: step 8130, loss 0.000209033, acc 1\n",
      "2018-10-26T16:39:16.674053: step 8131, loss 9.47825e-05, acc 1\n",
      "2018-10-26T16:39:16.991277: step 8132, loss 0.000482963, acc 1\n",
      "2018-10-26T16:39:17.289408: step 8133, loss 0.000142378, acc 1\n",
      "2018-10-26T16:39:17.606561: step 8134, loss 0.000473893, acc 1\n",
      "2018-10-26T16:39:17.942663: step 8135, loss 0.000115584, acc 1\n",
      "2018-10-26T16:39:18.285745: step 8136, loss 0.000852608, acc 1\n",
      "2018-10-26T16:39:18.596914: step 8137, loss 0.0330277, acc 0.984375\n",
      "2018-10-26T16:39:18.950972: step 8138, loss 0.000308223, acc 1\n",
      "2018-10-26T16:39:19.311208: step 8139, loss 0.0016039, acc 1\n",
      "2018-10-26T16:39:19.635144: step 8140, loss 0.000186316, acc 1\n",
      "2018-10-26T16:39:19.983212: step 8141, loss 0.0163346, acc 0.984375\n",
      "2018-10-26T16:39:20.320382: step 8142, loss 0.00091251, acc 1\n",
      "2018-10-26T16:39:20.632504: step 8143, loss 1.8841e-05, acc 1\n",
      "2018-10-26T16:39:20.964590: step 8144, loss 0.00162072, acc 1\n",
      "2018-10-26T16:39:21.289771: step 8145, loss 0.000349276, acc 1\n",
      "2018-10-26T16:39:21.649758: step 8146, loss 0.000507913, acc 1\n",
      "2018-10-26T16:39:21.994836: step 8147, loss 0.000639707, acc 1\n",
      "2018-10-26T16:39:22.342314: step 8148, loss 0.00516909, acc 1\n",
      "2018-10-26T16:39:22.661091: step 8149, loss 6.84294e-05, acc 1\n",
      "2018-10-26T16:39:22.970229: step 8150, loss 7.25258e-05, acc 1\n",
      "2018-10-26T16:39:23.339243: step 8151, loss 2.41635e-05, acc 1\n",
      "2018-10-26T16:39:23.689310: step 8152, loss 0.00135792, acc 1\n",
      "2018-10-26T16:39:24.060316: step 8153, loss 0.000254549, acc 1\n",
      "2018-10-26T16:39:24.384457: step 8154, loss 0.00156228, acc 1\n",
      "2018-10-26T16:39:24.697615: step 8155, loss 0.0036591, acc 1\n",
      "2018-10-26T16:39:25.048676: step 8156, loss 0.0245293, acc 0.984375\n",
      "2018-10-26T16:39:25.382782: step 8157, loss 0.00367475, acc 1\n",
      "2018-10-26T16:39:25.712901: step 8158, loss 0.00148588, acc 1\n",
      "2018-10-26T16:39:26.051996: step 8159, loss 0.00742906, acc 1\n",
      "2018-10-26T16:39:26.383113: step 8160, loss 0.00100876, acc 1\n",
      "2018-10-26T16:39:26.699265: step 8161, loss 6.13556e-05, acc 1\n",
      "2018-10-26T16:39:27.030385: step 8162, loss 0.000249282, acc 1\n",
      "2018-10-26T16:39:27.355516: step 8163, loss 0.00065208, acc 1\n",
      "2018-10-26T16:39:27.707573: step 8164, loss 0.0104341, acc 1\n",
      "2018-10-26T16:39:28.057638: step 8165, loss 0.000689422, acc 1\n",
      "2018-10-26T16:39:28.369805: step 8166, loss 0.000962422, acc 1\n",
      "2018-10-26T16:39:28.689950: step 8167, loss 0.0662534, acc 0.984375\n",
      "2018-10-26T16:39:29.107829: step 8168, loss 0.0413657, acc 0.984375\n",
      "2018-10-26T16:39:29.435956: step 8169, loss 0.000110171, acc 1\n",
      "2018-10-26T16:39:29.771058: step 8170, loss 0.000442907, acc 1\n",
      "2018-10-26T16:39:30.134135: step 8171, loss 0.00975447, acc 1\n",
      "2018-10-26T16:39:30.465347: step 8172, loss 0.00339918, acc 1\n",
      "2018-10-26T16:39:30.778366: step 8173, loss 4.67131e-06, acc 1\n",
      "2018-10-26T16:39:31.114468: step 8174, loss 0.000379774, acc 1\n",
      "2018-10-26T16:39:31.453563: step 8175, loss 0.00489777, acc 1\n",
      "2018-10-26T16:39:31.773710: step 8176, loss 3.88329e-05, acc 1\n",
      "2018-10-26T16:39:32.156687: step 8177, loss 0.0381267, acc 0.984375\n",
      "2018-10-26T16:39:32.521762: step 8178, loss 0.00211689, acc 1\n",
      "2018-10-26T16:39:32.842058: step 8179, loss 0.000951967, acc 1\n",
      "2018-10-26T16:39:33.204884: step 8180, loss 0.00138743, acc 1\n",
      "2018-10-26T16:39:33.571903: step 8181, loss 0.000153586, acc 1\n",
      "2018-10-26T16:39:33.904018: step 8182, loss 0.0111534, acc 1\n",
      "2018-10-26T16:39:34.228206: step 8183, loss 0.000141263, acc 1\n",
      "2018-10-26T16:39:34.608135: step 8184, loss 2.22921e-05, acc 1\n",
      "2018-10-26T16:39:34.913320: step 8185, loss 0.000382095, acc 1\n",
      "2018-10-26T16:39:35.236493: step 8186, loss 0.00621203, acc 1\n",
      "2018-10-26T16:39:35.550620: step 8187, loss 0.00116353, acc 1\n",
      "2018-10-26T16:39:35.860789: step 8188, loss 7.04732e-05, acc 1\n",
      "2018-10-26T16:39:36.176942: step 8189, loss 0.00067538, acc 1\n",
      "2018-10-26T16:39:36.518033: step 8190, loss 0.000948953, acc 1\n",
      "2018-10-26T16:39:36.826207: step 8191, loss 2.93525e-05, acc 1\n",
      "2018-10-26T16:39:37.149396: step 8192, loss 0.00142817, acc 1\n",
      "2018-10-26T16:39:37.469536: step 8193, loss 0.00274864, acc 1\n",
      "2018-10-26T16:39:37.799607: step 8194, loss 0.00021368, acc 1\n",
      "2018-10-26T16:39:38.127768: step 8195, loss 0.000708932, acc 1\n",
      "2018-10-26T16:39:38.450870: step 8196, loss 7.32127e-05, acc 1\n",
      "2018-10-26T16:39:38.786969: step 8197, loss 0.0348342, acc 0.984375\n",
      "2018-10-26T16:39:39.116090: step 8198, loss 0.000759737, acc 1\n",
      "2018-10-26T16:39:39.437369: step 8199, loss 0.000148884, acc 1\n",
      "2018-10-26T16:39:39.776326: step 8200, loss 1.57467e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:39:40.555244: step 8200, loss 2.82805, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8200\n",
      "\n",
      "2018-10-26T16:39:41.160695: step 8201, loss 0.000519053, acc 1\n",
      "2018-10-26T16:39:41.484761: step 8202, loss 0.000333069, acc 1\n",
      "2018-10-26T16:39:41.905637: step 8203, loss 0.000898475, acc 1\n",
      "2018-10-26T16:39:42.271660: step 8204, loss 0.0638053, acc 0.96875\n",
      "2018-10-26T16:39:42.614805: step 8205, loss 0.000370458, acc 1\n",
      "2018-10-26T16:39:42.927905: step 8206, loss 0.000941958, acc 1\n",
      "2018-10-26T16:39:43.262067: step 8207, loss 0.00646697, acc 1\n",
      "2018-10-26T16:39:43.574178: step 8208, loss 0.00264451, acc 1\n",
      "2018-10-26T16:39:43.886344: step 8209, loss 0.000783549, acc 1\n",
      "2018-10-26T16:39:44.260346: step 8210, loss 0.000122047, acc 1\n",
      "2018-10-26T16:39:44.618391: step 8211, loss 0.000993618, acc 1\n",
      "2018-10-26T16:39:44.995387: step 8212, loss 5.78413e-05, acc 1\n",
      "2018-10-26T16:39:45.390328: step 8213, loss 0.000793533, acc 1\n",
      "2018-10-26T16:39:45.738396: step 8214, loss 7.86516e-05, acc 1\n",
      "2018-10-26T16:39:46.065524: step 8215, loss 0.000852892, acc 1\n",
      "2018-10-26T16:39:46.425562: step 8216, loss 0.000249164, acc 1\n",
      "2018-10-26T16:39:46.821565: step 8217, loss 0.00724043, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:39:47.139652: step 8218, loss 0.000363686, acc 1\n",
      "2018-10-26T16:39:47.436862: step 8219, loss 0.00572393, acc 1\n",
      "2018-10-26T16:39:47.777950: step 8220, loss 0.000167842, acc 1\n",
      "2018-10-26T16:39:48.107069: step 8221, loss 0.00518808, acc 1\n",
      "2018-10-26T16:39:48.451149: step 8222, loss 0.000346977, acc 1\n",
      "2018-10-26T16:39:48.819165: step 8223, loss 0.000576333, acc 1\n",
      "2018-10-26T16:39:49.148343: step 8224, loss 3.67842e-05, acc 1\n",
      "2018-10-26T16:39:49.482393: step 8225, loss 0.00162897, acc 1\n",
      "2018-10-26T16:39:49.806527: step 8226, loss 0.000694473, acc 1\n",
      "2018-10-26T16:39:50.106753: step 8227, loss 0.00148256, acc 1\n",
      "2018-10-26T16:39:50.418937: step 8228, loss 0.01582, acc 0.984375\n",
      "2018-10-26T16:39:50.744049: step 8229, loss 0.000203041, acc 1\n",
      "2018-10-26T16:39:51.081124: step 8230, loss 2.98047e-05, acc 1\n",
      "2018-10-26T16:39:51.445152: step 8231, loss 0.00173329, acc 1\n",
      "2018-10-26T16:39:51.758566: step 8232, loss 0.00550079, acc 1\n",
      "2018-10-26T16:39:52.080523: step 8233, loss 0.00260686, acc 1\n",
      "2018-10-26T16:39:52.390622: step 8234, loss 0.00087831, acc 1\n",
      "2018-10-26T16:39:52.725727: step 8235, loss 0.000319351, acc 1\n",
      "2018-10-26T16:39:53.054915: step 8236, loss 0.0021155, acc 1\n",
      "2018-10-26T16:39:53.358037: step 8237, loss 0.000179197, acc 1\n",
      "2018-10-26T16:39:53.675361: step 8238, loss 0.00744192, acc 1\n",
      "2018-10-26T16:39:53.981372: step 8239, loss 0.014618, acc 0.984375\n",
      "2018-10-26T16:39:54.305506: step 8240, loss 0.000380454, acc 1\n",
      "2018-10-26T16:39:54.636621: step 8241, loss 0.000725858, acc 1\n",
      "2018-10-26T16:39:54.968734: step 8242, loss 7.60138e-05, acc 1\n",
      "2018-10-26T16:39:55.305834: step 8243, loss 0.03294, acc 0.984375\n",
      "2018-10-26T16:39:55.628004: step 8244, loss 0.00108623, acc 1\n",
      "2018-10-26T16:39:55.961128: step 8245, loss 3.10783e-05, acc 1\n",
      "2018-10-26T16:39:56.288211: step 8246, loss 0.0166192, acc 0.984375\n",
      "2018-10-26T16:39:56.631292: step 8247, loss 0.0049914, acc 1\n",
      "2018-10-26T16:39:56.964405: step 8248, loss 0.000186707, acc 1\n",
      "2018-10-26T16:39:57.280589: step 8249, loss 0.000178499, acc 1\n",
      "2018-10-26T16:39:57.588734: step 8250, loss 0.000777682, acc 1\n",
      "2018-10-26T16:39:57.904889: step 8251, loss 0.000168325, acc 1\n",
      "2018-10-26T16:39:58.228026: step 8252, loss 0.016746, acc 0.984375\n",
      "2018-10-26T16:39:58.521291: step 8253, loss 0.00211766, acc 1\n",
      "2018-10-26T16:39:58.848503: step 8254, loss 2.70919e-05, acc 1\n",
      "2018-10-26T16:39:59.178546: step 8255, loss 1.82538e-05, acc 1\n",
      "2018-10-26T16:39:59.499628: step 8256, loss 1.55865e-05, acc 1\n",
      "2018-10-26T16:39:59.834797: step 8257, loss 9.48929e-05, acc 1\n",
      "2018-10-26T16:40:00.170904: step 8258, loss 0.000614138, acc 1\n",
      "2018-10-26T16:40:00.502007: step 8259, loss 0.000224425, acc 1\n",
      "2018-10-26T16:40:00.848061: step 8260, loss 0.000535131, acc 1\n",
      "2018-10-26T16:40:01.171232: step 8261, loss 0.000447351, acc 1\n",
      "2018-10-26T16:40:01.486320: step 8262, loss 0.000294859, acc 1\n",
      "2018-10-26T16:40:01.810455: step 8263, loss 0.000168693, acc 1\n",
      "2018-10-26T16:40:02.135585: step 8264, loss 0.000101922, acc 1\n",
      "2018-10-26T16:40:02.457728: step 8265, loss 0.000179842, acc 1\n",
      "2018-10-26T16:40:02.773911: step 8266, loss 0.000997928, acc 1\n",
      "2018-10-26T16:40:03.095022: step 8267, loss 9.15978e-06, acc 1\n",
      "2018-10-26T16:40:03.398244: step 8268, loss 0.000211605, acc 1\n",
      "2018-10-26T16:40:03.724341: step 8269, loss 0.000861576, acc 1\n",
      "2018-10-26T16:40:04.032517: step 8270, loss 5.25572e-05, acc 1\n",
      "2018-10-26T16:40:04.344727: step 8271, loss 0.00030839, acc 1\n",
      "2018-10-26T16:40:04.650904: step 8272, loss 1.34734e-05, acc 1\n",
      "2018-10-26T16:40:04.975025: step 8273, loss 0.000174316, acc 1\n",
      "2018-10-26T16:40:05.308146: step 8274, loss 0.0427647, acc 0.984375\n",
      "2018-10-26T16:40:05.639224: step 8275, loss 4.09519e-05, acc 1\n",
      "2018-10-26T16:40:06.020368: step 8276, loss 2.03047e-05, acc 1\n",
      "2018-10-26T16:40:06.388226: step 8277, loss 0.000268422, acc 1\n",
      "2018-10-26T16:40:06.753248: step 8278, loss 0.000679836, acc 1\n",
      "2018-10-26T16:40:07.065414: step 8279, loss 1.25515e-05, acc 1\n",
      "2018-10-26T16:40:07.450385: step 8280, loss 0.000150852, acc 1\n",
      "2018-10-26T16:40:07.785516: step 8281, loss 8.34748e-05, acc 1\n",
      "2018-10-26T16:40:08.125581: step 8282, loss 0.00157026, acc 1\n",
      "2018-10-26T16:40:08.499582: step 8283, loss 0.000497132, acc 1\n",
      "2018-10-26T16:40:08.821847: step 8284, loss 3.13644e-05, acc 1\n",
      "2018-10-26T16:40:09.254564: step 8285, loss 0.00156941, acc 1\n",
      "2018-10-26T16:40:09.652614: step 8286, loss 3.83056e-05, acc 1\n",
      "2018-10-26T16:40:10.075371: step 8287, loss 1.02009e-05, acc 1\n",
      "2018-10-26T16:40:10.523174: step 8288, loss 4.01795e-05, acc 1\n",
      "2018-10-26T16:40:10.914136: step 8289, loss 0.00151808, acc 1\n",
      "2018-10-26T16:40:11.271236: step 8290, loss 0.00136249, acc 1\n",
      "2018-10-26T16:40:11.649167: step 8291, loss 0.000165106, acc 1\n",
      "2018-10-26T16:40:12.032356: step 8292, loss 2.1211e-05, acc 1\n",
      "2018-10-26T16:40:12.418112: step 8293, loss 0.000156158, acc 1\n",
      "2018-10-26T16:40:12.805104: step 8294, loss 0.000685257, acc 1\n",
      "2018-10-26T16:40:13.251884: step 8295, loss 0.00216341, acc 1\n",
      "2018-10-26T16:40:13.598982: step 8296, loss 0.000876436, acc 1\n",
      "2018-10-26T16:40:13.953010: step 8297, loss 1.26796e-05, acc 1\n",
      "2018-10-26T16:40:14.245230: step 8298, loss 4.34055e-05, acc 1\n",
      "2018-10-26T16:40:14.556437: step 8299, loss 0.00381652, acc 1\n",
      "2018-10-26T16:40:14.903471: step 8300, loss 2.07775e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:40:15.673433: step 8300, loss 2.83448, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8300\n",
      "\n",
      "2018-10-26T16:40:16.186043: step 8301, loss 0.000109597, acc 1\n",
      "2018-10-26T16:40:16.494220: step 8302, loss 0.00127902, acc 1\n",
      "2018-10-26T16:40:16.803438: step 8303, loss 0.0132882, acc 0.984375\n",
      "2018-10-26T16:40:17.212302: step 8304, loss 0.00196204, acc 1\n",
      "2018-10-26T16:40:17.545411: step 8305, loss 0.000880089, acc 1\n",
      "2018-10-26T16:40:17.854612: step 8306, loss 0.00436641, acc 1\n",
      "2018-10-26T16:40:18.161766: step 8307, loss 0.000134488, acc 1\n",
      "2018-10-26T16:40:18.487895: step 8308, loss 0.0081719, acc 1\n",
      "2018-10-26T16:40:18.823083: step 8309, loss 5.04147e-05, acc 1\n",
      "2018-10-26T16:40:19.118251: step 8310, loss 0.000428971, acc 1\n",
      "2018-10-26T16:40:19.434366: step 8311, loss 0.000170782, acc 1\n",
      "2018-10-26T16:40:19.738552: step 8312, loss 5.24592e-05, acc 1\n",
      "2018-10-26T16:40:20.030773: step 8313, loss 0.000880341, acc 1\n",
      "2018-10-26T16:40:20.346927: step 8314, loss 0.00155221, acc 1\n",
      "2018-10-26T16:40:20.732989: step 8315, loss 0.00628448, acc 1\n",
      "2018-10-26T16:40:21.023119: step 8316, loss 0.00109056, acc 1\n",
      "2018-10-26T16:40:21.320359: step 8317, loss 0.000635238, acc 1\n",
      "2018-10-26T16:40:21.636481: step 8318, loss 0.000463365, acc 1\n",
      "2018-10-26T16:40:21.997619: step 8319, loss 0.0265456, acc 0.984375\n",
      "2018-10-26T16:40:22.348579: step 8320, loss 7.8133e-05, acc 1\n",
      "2018-10-26T16:40:22.680691: step 8321, loss 0.000177005, acc 1\n",
      "2018-10-26T16:40:22.996848: step 8322, loss 9.73354e-05, acc 1\n",
      "2018-10-26T16:40:23.308047: step 8323, loss 0.000525823, acc 1\n",
      "2018-10-26T16:40:23.605223: step 8324, loss 0.00048758, acc 1\n",
      "2018-10-26T16:40:23.911403: step 8325, loss 0.0372549, acc 0.984375\n",
      "2018-10-26T16:40:24.200630: step 8326, loss 0.000654699, acc 1\n",
      "2018-10-26T16:40:24.535076: step 8327, loss 2.55295e-05, acc 1\n",
      "2018-10-26T16:40:24.845945: step 8328, loss 0.00015719, acc 1\n",
      "2018-10-26T16:40:25.123191: step 8329, loss 0.00179109, acc 1\n",
      "2018-10-26T16:40:25.435394: step 8330, loss 0.00146884, acc 1\n",
      "2018-10-26T16:40:25.756476: step 8331, loss 0.00238863, acc 1\n",
      "2018-10-26T16:40:26.076619: step 8332, loss 0.0338843, acc 0.984375\n",
      "2018-10-26T16:40:26.389781: step 8333, loss 2.77095e-05, acc 1\n",
      "2018-10-26T16:40:26.702945: step 8334, loss 0.00266856, acc 1\n",
      "2018-10-26T16:40:27.014185: step 8335, loss 0.000236023, acc 1\n",
      "2018-10-26T16:40:27.306332: step 8336, loss 0.00233491, acc 1\n",
      "2018-10-26T16:40:27.612516: step 8337, loss 0.000993398, acc 1\n",
      "2018-10-26T16:40:27.936648: step 8338, loss 0.0757088, acc 0.984375\n",
      "2018-10-26T16:40:28.264802: step 8339, loss 0.000383069, acc 1\n",
      "2018-10-26T16:40:28.584917: step 8340, loss 5.12153e-05, acc 1\n",
      "2018-10-26T16:40:28.914040: step 8341, loss 0.000718828, acc 1\n",
      "2018-10-26T16:40:29.233188: step 8342, loss 0.00925068, acc 1\n",
      "2018-10-26T16:40:29.539367: step 8343, loss 0.00057862, acc 1\n",
      "2018-10-26T16:40:29.847542: step 8344, loss 0.0067788, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:40:30.158712: step 8345, loss 0.000179919, acc 1\n",
      "2018-10-26T16:40:30.497911: step 8346, loss 0.00305254, acc 1\n",
      "2018-10-26T16:40:30.836903: step 8347, loss 0.000218548, acc 1\n",
      "2018-10-26T16:40:31.155049: step 8348, loss 0.0804665, acc 0.984375\n",
      "2018-10-26T16:40:31.483173: step 8349, loss 0.000185944, acc 1\n",
      "2018-10-26T16:40:31.820273: step 8350, loss 0.000352131, acc 1\n",
      "2018-10-26T16:40:32.159370: step 8351, loss 0.00115704, acc 1\n",
      "2018-10-26T16:40:32.500547: step 8352, loss 0.000989085, acc 1\n",
      "2018-10-26T16:40:32.823595: step 8353, loss 8.85284e-05, acc 1\n",
      "2018-10-26T16:40:33.126780: step 8354, loss 0.000116872, acc 1\n",
      "2018-10-26T16:40:33.455905: step 8355, loss 5.28288e-05, acc 1\n",
      "2018-10-26T16:40:33.819928: step 8356, loss 0.000338642, acc 1\n",
      "2018-10-26T16:40:34.141116: step 8357, loss 0.00147543, acc 1\n",
      "2018-10-26T16:40:34.485152: step 8358, loss 0.0159212, acc 0.984375\n",
      "2018-10-26T16:40:34.806297: step 8359, loss 0.0246269, acc 0.984375\n",
      "2018-10-26T16:40:35.122495: step 8360, loss 0.00233901, acc 1\n",
      "2018-10-26T16:40:35.469521: step 8361, loss 0.00148934, acc 1\n",
      "2018-10-26T16:40:35.778771: step 8362, loss 0.000352478, acc 1\n",
      "2018-10-26T16:40:36.115795: step 8363, loss 0.000254265, acc 1\n",
      "2018-10-26T16:40:36.494913: step 8364, loss 0.000237735, acc 1\n",
      "2018-10-26T16:40:36.815925: step 8365, loss 0.000109541, acc 1\n",
      "2018-10-26T16:40:37.137069: step 8366, loss 0.00249565, acc 1\n",
      "2018-10-26T16:40:37.462329: step 8367, loss 0.00308331, acc 1\n",
      "2018-10-26T16:40:37.769379: step 8368, loss 5.98842e-05, acc 1\n",
      "2018-10-26T16:40:38.092513: step 8369, loss 0.000656712, acc 1\n",
      "2018-10-26T16:40:38.401686: step 8370, loss 0.00050361, acc 1\n",
      "2018-10-26T16:40:38.728816: step 8371, loss 7.77797e-05, acc 1\n",
      "2018-10-26T16:40:39.067911: step 8372, loss 0.000903367, acc 1\n",
      "2018-10-26T16:40:39.395033: step 8373, loss 0.125425, acc 0.984375\n",
      "2018-10-26T16:40:39.726149: step 8374, loss 0.000283835, acc 1\n",
      "2018-10-26T16:40:40.046293: step 8375, loss 0.000366195, acc 1\n",
      "2018-10-26T16:40:40.367436: step 8376, loss 9.8086e-05, acc 1\n",
      "2018-10-26T16:40:40.696555: step 8377, loss 0.0340526, acc 0.984375\n",
      "2018-10-26T16:40:41.009718: step 8378, loss 0.000263424, acc 1\n",
      "2018-10-26T16:40:41.335847: step 8379, loss 0.0134821, acc 0.984375\n",
      "2018-10-26T16:40:41.652056: step 8380, loss 0.000951214, acc 1\n",
      "2018-10-26T16:40:41.985121: step 8381, loss 0.00585921, acc 1\n",
      "2018-10-26T16:40:42.313239: step 8382, loss 7.16977e-05, acc 1\n",
      "2018-10-26T16:40:42.635377: step 8383, loss 0.000798747, acc 1\n",
      "2018-10-26T16:40:42.998476: step 8384, loss 0.000266761, acc 1\n",
      "2018-10-26T16:40:43.325534: step 8385, loss 0.0013557, acc 1\n",
      "2018-10-26T16:40:43.658645: step 8386, loss 6.74735e-05, acc 1\n",
      "2018-10-26T16:40:43.993809: step 8387, loss 0.000602126, acc 1\n",
      "2018-10-26T16:40:44.327853: step 8388, loss 0.00194894, acc 1\n",
      "2018-10-26T16:40:44.658970: step 8389, loss 0.000163031, acc 1\n",
      "2018-10-26T16:40:44.985096: step 8390, loss 0.000174585, acc 1\n",
      "2018-10-26T16:40:45.306239: step 8391, loss 0.0181844, acc 0.984375\n",
      "2018-10-26T16:40:45.625386: step 8392, loss 0.000733341, acc 1\n",
      "2018-10-26T16:40:45.959498: step 8393, loss 0.000929961, acc 1\n",
      "2018-10-26T16:40:46.333506: step 8394, loss 0.000578503, acc 1\n",
      "2018-10-26T16:40:46.657669: step 8395, loss 0.000797583, acc 1\n",
      "2018-10-26T16:40:46.965805: step 8396, loss 0.00320315, acc 1\n",
      "2018-10-26T16:40:47.302960: step 8397, loss 0.00011388, acc 1\n",
      "2018-10-26T16:40:47.626045: step 8398, loss 0.000373064, acc 1\n",
      "2018-10-26T16:40:47.955165: step 8399, loss 3.52591e-06, acc 1\n",
      "2018-10-26T16:40:48.276307: step 8400, loss 2.46965e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:40:49.022406: step 8400, loss 3.0624, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8400\n",
      "\n",
      "2018-10-26T16:40:49.666589: step 8401, loss 0.00132993, acc 1\n",
      "2018-10-26T16:40:49.994712: step 8402, loss 0.00190809, acc 1\n",
      "2018-10-26T16:40:50.386666: step 8403, loss 5.0158e-06, acc 1\n",
      "2018-10-26T16:40:50.736731: step 8404, loss 0.00103679, acc 1\n",
      "2018-10-26T16:40:51.059925: step 8405, loss 0.00158455, acc 1\n",
      "2018-10-26T16:40:51.390074: step 8406, loss 0.000295202, acc 1\n",
      "2018-10-26T16:40:51.763986: step 8407, loss 0.00242955, acc 1\n",
      "2018-10-26T16:40:52.067176: step 8408, loss 0.00122664, acc 1\n",
      "2018-10-26T16:40:52.394301: step 8409, loss 3.77133e-05, acc 1\n",
      "2018-10-26T16:40:52.725416: step 8410, loss 0.0241484, acc 0.984375\n",
      "2018-10-26T16:40:53.055534: step 8411, loss 0.000181947, acc 1\n",
      "2018-10-26T16:40:53.368698: step 8412, loss 0.000647812, acc 1\n",
      "2018-10-26T16:40:53.664907: step 8413, loss 0.000213804, acc 1\n",
      "2018-10-26T16:40:53.986047: step 8414, loss 0.000962125, acc 1\n",
      "2018-10-26T16:40:54.301279: step 8415, loss 0.000878384, acc 1\n",
      "2018-10-26T16:40:54.635349: step 8416, loss 0.000174315, acc 1\n",
      "2018-10-26T16:40:54.955461: step 8417, loss 0.000677743, acc 1\n",
      "2018-10-26T16:40:55.269622: step 8418, loss 7.61488e-05, acc 1\n",
      "2018-10-26T16:40:55.597745: step 8419, loss 0.000691129, acc 1\n",
      "2018-10-26T16:40:55.921931: step 8420, loss 0.0022321, acc 1\n",
      "2018-10-26T16:40:56.243018: step 8421, loss 0.000588161, acc 1\n",
      "2018-10-26T16:40:56.565161: step 8422, loss 2.84034e-05, acc 1\n",
      "2018-10-26T16:40:56.891286: step 8423, loss 0.000199124, acc 1\n",
      "2018-10-26T16:40:57.200507: step 8424, loss 0.000719732, acc 1\n",
      "2018-10-26T16:40:57.526627: step 8425, loss 4.50551e-05, acc 1\n",
      "2018-10-26T16:40:57.848730: step 8426, loss 0.000172915, acc 1\n",
      "2018-10-26T16:40:58.162892: step 8427, loss 0.000383681, acc 1\n",
      "2018-10-26T16:40:58.470071: step 8428, loss 2.07833e-05, acc 1\n",
      "2018-10-26T16:40:58.778284: step 8429, loss 0.00135639, acc 1\n",
      "2018-10-26T16:40:59.090410: step 8430, loss 0.000207397, acc 1\n",
      "2018-10-26T16:40:59.389615: step 8431, loss 7.20249e-05, acc 1\n",
      "2018-10-26T16:40:59.704769: step 8432, loss 2.02796e-05, acc 1\n",
      "2018-10-26T16:41:00.036885: step 8433, loss 8.16572e-05, acc 1\n",
      "2018-10-26T16:41:00.345062: step 8434, loss 0.00011171, acc 1\n",
      "2018-10-26T16:41:00.667201: step 8435, loss 0.000154008, acc 1\n",
      "2018-10-26T16:41:00.990334: step 8436, loss 0.00135728, acc 1\n",
      "2018-10-26T16:41:01.296519: step 8437, loss 0.000182619, acc 1\n",
      "2018-10-26T16:41:01.592724: step 8438, loss 0.0010687, acc 1\n",
      "2018-10-26T16:41:01.908884: step 8439, loss 0.00099862, acc 1\n",
      "2018-10-26T16:41:02.253958: step 8440, loss 0.0176271, acc 0.984375\n",
      "2018-10-26T16:41:02.568121: step 8441, loss 0.00466727, acc 1\n",
      "2018-10-26T16:41:02.895291: step 8442, loss 5.89711e-05, acc 1\n",
      "2018-10-26T16:41:03.218412: step 8443, loss 6.12808e-07, acc 1\n",
      "2018-10-26T16:41:03.533540: step 8444, loss 0.000862009, acc 1\n",
      "2018-10-26T16:41:03.847699: step 8445, loss 0.000378709, acc 1\n",
      "2018-10-26T16:41:04.157954: step 8446, loss 0.00032569, acc 1\n",
      "2018-10-26T16:41:04.475025: step 8447, loss 7.60567e-06, acc 1\n",
      "2018-10-26T16:41:04.796165: step 8448, loss 8.97894e-05, acc 1\n",
      "2018-10-26T16:41:05.148257: step 8449, loss 2.76412e-06, acc 1\n",
      "2018-10-26T16:41:05.468370: step 8450, loss 0.000130336, acc 1\n",
      "2018-10-26T16:41:05.797494: step 8451, loss 8.19994e-05, acc 1\n",
      "2018-10-26T16:41:06.117638: step 8452, loss 0.0275909, acc 0.984375\n",
      "2018-10-26T16:41:06.445786: step 8453, loss 0.00349355, acc 1\n",
      "2018-10-26T16:41:06.771886: step 8454, loss 0.0184185, acc 0.984375\n",
      "2018-10-26T16:41:07.094026: step 8455, loss 0.000332569, acc 1\n",
      "2018-10-26T16:41:07.416169: step 8456, loss 0.00334108, acc 1\n",
      "2018-10-26T16:41:07.745286: step 8457, loss 3.0291e-05, acc 1\n",
      "2018-10-26T16:41:08.085380: step 8458, loss 1.67065e-05, acc 1\n",
      "2018-10-26T16:41:08.398544: step 8459, loss 0.000181962, acc 1\n",
      "2018-10-26T16:41:08.710706: step 8460, loss 0.00123516, acc 1\n",
      "2018-10-26T16:41:09.035839: step 8461, loss 0.00050398, acc 1\n",
      "2018-10-26T16:41:09.330116: step 8462, loss 0.00010105, acc 1\n",
      "2018-10-26T16:41:09.648201: step 8463, loss 2.74689e-05, acc 1\n",
      "2018-10-26T16:41:09.950395: step 8464, loss 0.0175941, acc 0.984375\n",
      "2018-10-26T16:41:10.262754: step 8465, loss 0.021002, acc 0.984375\n",
      "2018-10-26T16:41:10.570740: step 8466, loss 0.00239646, acc 1\n",
      "2018-10-26T16:41:10.897862: step 8467, loss 0.000554535, acc 1\n",
      "2018-10-26T16:41:11.208034: step 8468, loss 9.54214e-05, acc 1\n",
      "2018-10-26T16:41:11.560093: step 8469, loss 0.000362165, acc 1\n",
      "2018-10-26T16:41:11.929106: step 8470, loss 0.00112575, acc 1\n",
      "2018-10-26T16:41:12.296125: step 8471, loss 4.08406e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:41:12.640207: step 8472, loss 0.000626394, acc 1\n",
      "2018-10-26T16:41:12.996258: step 8473, loss 0.00194601, acc 1\n",
      "2018-10-26T16:41:13.346495: step 8474, loss 6.7713e-05, acc 1\n",
      "2018-10-26T16:41:13.715334: step 8475, loss 0.000733966, acc 1\n",
      "2018-10-26T16:41:14.074376: step 8476, loss 0.000809661, acc 1\n",
      "2018-10-26T16:41:14.385544: step 8477, loss 0.00380521, acc 1\n",
      "2018-10-26T16:41:14.815395: step 8478, loss 6.39212e-05, acc 1\n",
      "2018-10-26T16:41:15.268222: step 8479, loss 0.00661901, acc 1\n",
      "2018-10-26T16:41:15.663130: step 8480, loss 0.000128178, acc 1\n",
      "2018-10-26T16:41:16.105947: step 8481, loss 0.000275899, acc 1\n",
      "2018-10-26T16:41:16.510042: step 8482, loss 0.000524739, acc 1\n",
      "2018-10-26T16:41:16.912790: step 8483, loss 0.00091226, acc 1\n",
      "2018-10-26T16:41:17.324692: step 8484, loss 0.0145672, acc 0.984375\n",
      "2018-10-26T16:41:17.732601: step 8485, loss 0.00160489, acc 1\n",
      "2018-10-26T16:41:18.098820: step 8486, loss 0.000168027, acc 1\n",
      "2018-10-26T16:41:18.501591: step 8487, loss 1.59213e-05, acc 1\n",
      "2018-10-26T16:41:18.904511: step 8488, loss 0.000325832, acc 1\n",
      "2018-10-26T16:41:19.283459: step 8489, loss 0.000200209, acc 1\n",
      "2018-10-26T16:41:19.702338: step 8490, loss 0.0046857, acc 1\n",
      "2018-10-26T16:41:20.064370: step 8491, loss 8.65845e-06, acc 1\n",
      "2018-10-26T16:41:20.401473: step 8492, loss 0.000834573, acc 1\n",
      "2018-10-26T16:41:20.738570: step 8493, loss 9.23432e-05, acc 1\n",
      "2018-10-26T16:41:21.056722: step 8494, loss 0.00367319, acc 1\n",
      "2018-10-26T16:41:21.384845: step 8495, loss 0.000487713, acc 1\n",
      "2018-10-26T16:41:21.702993: step 8496, loss 6.16264e-05, acc 1\n",
      "2018-10-26T16:41:22.081980: step 8497, loss 0.0293974, acc 0.984375\n",
      "2018-10-26T16:41:22.443015: step 8498, loss 0.000969954, acc 1\n",
      "2018-10-26T16:41:22.817015: step 8499, loss 0.000110815, acc 1\n",
      "2018-10-26T16:41:23.136315: step 8500, loss 4.14641e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:41:23.900122: step 8500, loss 2.94244, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8500\n",
      "\n",
      "2018-10-26T16:41:24.536424: step 8501, loss 7.78008e-05, acc 1\n",
      "2018-10-26T16:41:24.845638: step 8502, loss 0.00368863, acc 1\n",
      "2018-10-26T16:41:25.288413: step 8503, loss 0.00282457, acc 1\n",
      "2018-10-26T16:41:25.671389: step 8504, loss 1.68353e-05, acc 1\n",
      "2018-10-26T16:41:26.003504: step 8505, loss 0.00045393, acc 1\n",
      "2018-10-26T16:41:26.321771: step 8506, loss 0.000396127, acc 1\n",
      "2018-10-26T16:41:26.644792: step 8507, loss 0.0533939, acc 0.984375\n",
      "2018-10-26T16:41:26.974907: step 8508, loss 0.00215719, acc 1\n",
      "2018-10-26T16:41:27.344972: step 8509, loss 0.000344437, acc 1\n",
      "2018-10-26T16:41:27.688005: step 8510, loss 0.00453882, acc 1\n",
      "2018-10-26T16:41:28.024107: step 8511, loss 5.88958e-05, acc 1\n",
      "2018-10-26T16:41:28.351277: step 8512, loss 0.0308079, acc 0.984375\n",
      "2018-10-26T16:41:28.699379: step 8513, loss 0.00431917, acc 1\n",
      "2018-10-26T16:41:29.033411: step 8514, loss 0.00365156, acc 1\n",
      "2018-10-26T16:41:29.361529: step 8515, loss 0.0691238, acc 0.984375\n",
      "2018-10-26T16:41:29.693646: step 8516, loss 0.000111574, acc 1\n",
      "2018-10-26T16:41:30.034818: step 8517, loss 0.000275274, acc 1\n",
      "2018-10-26T16:41:30.382881: step 8518, loss 0.0584259, acc 0.984375\n",
      "2018-10-26T16:41:30.731870: step 8519, loss 0.000303738, acc 1\n",
      "2018-10-26T16:41:31.048076: step 8520, loss 0.000944966, acc 1\n",
      "2018-10-26T16:41:31.382131: step 8521, loss 0.00622135, acc 1\n",
      "2018-10-26T16:41:31.715241: step 8522, loss 0.00352599, acc 1\n",
      "2018-10-26T16:41:32.053525: step 8523, loss 0.104389, acc 0.984375\n",
      "2018-10-26T16:41:32.377472: step 8524, loss 0.000260364, acc 1\n",
      "2018-10-26T16:41:32.697616: step 8525, loss 0.000646466, acc 1\n",
      "2018-10-26T16:41:33.011813: step 8526, loss 0.000102554, acc 1\n",
      "2018-10-26T16:41:33.338906: step 8527, loss 0.00348729, acc 1\n",
      "2018-10-26T16:41:33.656056: step 8528, loss 3.70241e-05, acc 1\n",
      "2018-10-26T16:41:33.972213: step 8529, loss 1.34818e-05, acc 1\n",
      "2018-10-26T16:41:34.292358: step 8530, loss 0.000712712, acc 1\n",
      "2018-10-26T16:41:34.612564: step 8531, loss 0.0232409, acc 0.984375\n",
      "2018-10-26T16:41:34.917850: step 8532, loss 0.000450367, acc 1\n",
      "2018-10-26T16:41:35.238890: step 8533, loss 0.000120155, acc 1\n",
      "2018-10-26T16:41:35.563958: step 8534, loss 0.00280539, acc 1\n",
      "2018-10-26T16:41:35.888122: step 8535, loss 0.00039717, acc 1\n",
      "2018-10-26T16:41:36.199260: step 8536, loss 0.00588185, acc 1\n",
      "2018-10-26T16:41:36.528461: step 8537, loss 0.000545998, acc 1\n",
      "2018-10-26T16:41:36.859500: step 8538, loss 0.000472924, acc 1\n",
      "2018-10-26T16:41:37.184627: step 8539, loss 0.0401897, acc 0.984375\n",
      "2018-10-26T16:41:37.495871: step 8540, loss 0.000844017, acc 1\n",
      "2018-10-26T16:41:37.823923: step 8541, loss 0.000640722, acc 1\n",
      "2018-10-26T16:41:38.151136: step 8542, loss 0.000290011, acc 1\n",
      "2018-10-26T16:41:38.448255: step 8543, loss 0.000383535, acc 1\n",
      "2018-10-26T16:41:38.776375: step 8544, loss 0.000564799, acc 1\n",
      "2018-10-26T16:41:39.092533: step 8545, loss 0.00742991, acc 1\n",
      "2018-10-26T16:41:39.418659: step 8546, loss 0.00135984, acc 1\n",
      "2018-10-26T16:41:39.749775: step 8547, loss 0.000202714, acc 1\n",
      "2018-10-26T16:41:40.081887: step 8548, loss 5.54526e-05, acc 1\n",
      "2018-10-26T16:41:40.403030: step 8549, loss 0.000135726, acc 1\n",
      "2018-10-26T16:41:40.711286: step 8550, loss 0.0338903, acc 0.983333\n",
      "2018-10-26T16:41:41.058278: step 8551, loss 0.000375355, acc 1\n",
      "2018-10-26T16:41:41.381522: step 8552, loss 0.0199154, acc 0.984375\n",
      "2018-10-26T16:41:41.717520: step 8553, loss 0.00134565, acc 1\n",
      "2018-10-26T16:41:42.037714: step 8554, loss 0.00117457, acc 1\n",
      "2018-10-26T16:41:42.358806: step 8555, loss 0.00244663, acc 1\n",
      "2018-10-26T16:41:42.684933: step 8556, loss 0.00414815, acc 1\n",
      "2018-10-26T16:41:43.029060: step 8557, loss 0.000580616, acc 1\n",
      "2018-10-26T16:41:43.349158: step 8558, loss 0.0245568, acc 0.984375\n",
      "2018-10-26T16:41:43.662320: step 8559, loss 0.00358944, acc 1\n",
      "2018-10-26T16:41:43.968504: step 8560, loss 0.000450873, acc 1\n",
      "2018-10-26T16:41:44.294631: step 8561, loss 7.41646e-05, acc 1\n",
      "2018-10-26T16:41:44.605800: step 8562, loss 0.0799128, acc 0.984375\n",
      "2018-10-26T16:41:44.922952: step 8563, loss 0.00180245, acc 1\n",
      "2018-10-26T16:41:45.243101: step 8564, loss 0.000126758, acc 1\n",
      "2018-10-26T16:41:45.583237: step 8565, loss 0.000701464, acc 1\n",
      "2018-10-26T16:41:45.898384: step 8566, loss 0.000100408, acc 1\n",
      "2018-10-26T16:41:46.215602: step 8567, loss 0.00568552, acc 1\n",
      "2018-10-26T16:41:46.538636: step 8568, loss 0.000368711, acc 1\n",
      "2018-10-26T16:41:46.901722: step 8569, loss 0.000219115, acc 1\n",
      "2018-10-26T16:41:47.234776: step 8570, loss 2.0116e-06, acc 1\n",
      "2018-10-26T16:41:47.602794: step 8571, loss 0.0116609, acc 0.984375\n",
      "2018-10-26T16:41:47.922937: step 8572, loss 0.00014645, acc 1\n",
      "2018-10-26T16:41:48.248104: step 8573, loss 0.00342874, acc 1\n",
      "2018-10-26T16:41:48.577265: step 8574, loss 0.000167545, acc 1\n",
      "2018-10-26T16:41:48.962161: step 8575, loss 0.000475128, acc 1\n",
      "2018-10-26T16:41:49.321202: step 8576, loss 4.64653e-05, acc 1\n",
      "2018-10-26T16:41:49.643398: step 8577, loss 3.87928e-05, acc 1\n",
      "2018-10-26T16:41:49.984433: step 8578, loss 0.000135269, acc 1\n",
      "2018-10-26T16:41:50.308564: step 8579, loss 0.000423242, acc 1\n",
      "2018-10-26T16:41:50.641742: step 8580, loss 0.061703, acc 0.984375\n",
      "2018-10-26T16:41:50.959823: step 8581, loss 0.000323096, acc 1\n",
      "2018-10-26T16:41:51.315875: step 8582, loss 0.00156297, acc 1\n",
      "2018-10-26T16:41:51.635019: step 8583, loss 0.000378561, acc 1\n",
      "2018-10-26T16:41:51.949183: step 8584, loss 2.76351e-05, acc 1\n",
      "2018-10-26T16:41:52.273314: step 8585, loss 0.00302782, acc 1\n",
      "2018-10-26T16:41:52.593457: step 8586, loss 9.4238e-06, acc 1\n",
      "2018-10-26T16:41:52.904628: step 8587, loss 0.000379281, acc 1\n",
      "2018-10-26T16:41:53.231830: step 8588, loss 6.62605e-05, acc 1\n",
      "2018-10-26T16:41:53.557883: step 8589, loss 0.00166678, acc 1\n",
      "2018-10-26T16:41:53.893987: step 8590, loss 0.000961719, acc 1\n",
      "2018-10-26T16:41:54.210138: step 8591, loss 0.00596651, acc 1\n",
      "2018-10-26T16:41:54.529285: step 8592, loss 0.00080845, acc 1\n",
      "2018-10-26T16:41:54.858406: step 8593, loss 0.00519678, acc 1\n",
      "2018-10-26T16:41:55.161596: step 8594, loss 0.00803316, acc 1\n",
      "2018-10-26T16:41:55.491812: step 8595, loss 0.0316882, acc 0.984375\n",
      "2018-10-26T16:41:55.814854: step 8596, loss 5.67356e-05, acc 1\n",
      "2018-10-26T16:41:56.136990: step 8597, loss 0.00157012, acc 1\n",
      "2018-10-26T16:41:56.456136: step 8598, loss 0.00405805, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:41:56.833129: step 8599, loss 0.000647084, acc 1\n",
      "2018-10-26T16:41:57.167238: step 8600, loss 0.000568982, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:41:57.906262: step 8600, loss 2.92715, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8600\n",
      "\n",
      "2018-10-26T16:41:58.545556: step 8601, loss 0.0366999, acc 0.984375\n",
      "2018-10-26T16:41:58.879666: step 8602, loss 0.00135345, acc 1\n",
      "2018-10-26T16:41:59.333577: step 8603, loss 0.00023785, acc 1\n",
      "2018-10-26T16:41:59.770283: step 8604, loss 3.95276e-05, acc 1\n",
      "2018-10-26T16:42:00.316821: step 8605, loss 0.000113128, acc 1\n",
      "2018-10-26T16:42:00.662897: step 8606, loss 0.000599524, acc 1\n",
      "2018-10-26T16:42:01.027968: step 8607, loss 0.0013343, acc 1\n",
      "2018-10-26T16:42:01.391949: step 8608, loss 0.000325922, acc 1\n",
      "2018-10-26T16:42:01.757972: step 8609, loss 0.00114683, acc 1\n",
      "2018-10-26T16:42:02.114021: step 8610, loss 8.98357e-05, acc 1\n",
      "2018-10-26T16:42:02.485059: step 8611, loss 0.000303723, acc 1\n",
      "2018-10-26T16:42:02.821206: step 8612, loss 0.00117854, acc 1\n",
      "2018-10-26T16:42:03.145265: step 8613, loss 0.000783561, acc 1\n",
      "2018-10-26T16:42:03.469399: step 8614, loss 3.95918e-05, acc 1\n",
      "2018-10-26T16:42:03.787549: step 8615, loss 0.000254285, acc 1\n",
      "2018-10-26T16:42:04.111685: step 8616, loss 0.000412004, acc 1\n",
      "2018-10-26T16:42:04.433825: step 8617, loss 0.068872, acc 0.984375\n",
      "2018-10-26T16:42:04.764940: step 8618, loss 0.000490313, acc 1\n",
      "2018-10-26T16:42:05.108296: step 8619, loss 0.000620551, acc 1\n",
      "2018-10-26T16:42:05.444126: step 8620, loss 8.9351e-05, acc 1\n",
      "2018-10-26T16:42:05.845052: step 8621, loss 0.0827711, acc 0.984375\n",
      "2018-10-26T16:42:06.261028: step 8622, loss 0.000694227, acc 1\n",
      "2018-10-26T16:42:06.643917: step 8623, loss 8.84194e-05, acc 1\n",
      "2018-10-26T16:42:07.003956: step 8624, loss 0.00146613, acc 1\n",
      "2018-10-26T16:42:07.336071: step 8625, loss 0.000496361, acc 1\n",
      "2018-10-26T16:42:07.714058: step 8626, loss 0.00636478, acc 1\n",
      "2018-10-26T16:42:08.057143: step 8627, loss 3.38964e-05, acc 1\n",
      "2018-10-26T16:42:08.436128: step 8628, loss 0.000562386, acc 1\n",
      "2018-10-26T16:42:08.822098: step 8629, loss 0.00174241, acc 1\n",
      "2018-10-26T16:42:09.203079: step 8630, loss 0.0162706, acc 0.984375\n",
      "2018-10-26T16:42:09.560125: step 8631, loss 0.000122051, acc 1\n",
      "2018-10-26T16:42:09.957099: step 8632, loss 0.000403836, acc 1\n",
      "2018-10-26T16:42:10.292401: step 8633, loss 0.000835729, acc 1\n",
      "2018-10-26T16:42:10.630270: step 8634, loss 0.00419301, acc 1\n",
      "2018-10-26T16:42:10.994524: step 8635, loss 0.000880858, acc 1\n",
      "2018-10-26T16:42:11.364305: step 8636, loss 0.00014307, acc 1\n",
      "2018-10-26T16:42:11.708423: step 8637, loss 0.000423469, acc 1\n",
      "2018-10-26T16:42:12.047482: step 8638, loss 0.000121022, acc 1\n",
      "2018-10-26T16:42:12.391561: step 8639, loss 0.00193978, acc 1\n",
      "2018-10-26T16:42:12.763566: step 8640, loss 3.63573e-05, acc 1\n",
      "2018-10-26T16:42:13.136570: step 8641, loss 0.00533403, acc 1\n",
      "2018-10-26T16:42:13.528523: step 8642, loss 0.000100296, acc 1\n",
      "2018-10-26T16:42:13.859637: step 8643, loss 0.0362312, acc 0.984375\n",
      "2018-10-26T16:42:14.201723: step 8644, loss 0.000447687, acc 1\n",
      "2018-10-26T16:42:14.510913: step 8645, loss 0.0330422, acc 0.984375\n",
      "2018-10-26T16:42:14.847997: step 8646, loss 0.00161158, acc 1\n",
      "2018-10-26T16:42:15.192189: step 8647, loss 1.67692e-05, acc 1\n",
      "2018-10-26T16:42:15.570071: step 8648, loss 0.000425445, acc 1\n",
      "2018-10-26T16:42:15.919136: step 8649, loss 0.000145448, acc 1\n",
      "2018-10-26T16:42:16.315445: step 8650, loss 0.000511959, acc 1\n",
      "2018-10-26T16:42:16.696060: step 8651, loss 5.26279e-05, acc 1\n",
      "2018-10-26T16:42:17.035155: step 8652, loss 0.000140849, acc 1\n",
      "2018-10-26T16:42:17.434088: step 8653, loss 0.0498421, acc 0.984375\n",
      "2018-10-26T16:42:17.855959: step 8654, loss 0.00277383, acc 1\n",
      "2018-10-26T16:42:18.225979: step 8655, loss 0.00109691, acc 1\n",
      "2018-10-26T16:42:18.662804: step 8656, loss 3.48607e-05, acc 1\n",
      "2018-10-26T16:42:19.027831: step 8657, loss 0.000140051, acc 1\n",
      "2018-10-26T16:42:19.409812: step 8658, loss 0.000215739, acc 1\n",
      "2018-10-26T16:42:19.849634: step 8659, loss 0.00295165, acc 1\n",
      "2018-10-26T16:42:20.313396: step 8660, loss 0.00179163, acc 1\n",
      "2018-10-26T16:42:20.769176: step 8661, loss 0.00324432, acc 1\n",
      "2018-10-26T16:42:21.258914: step 8662, loss 0.000266475, acc 1\n",
      "2018-10-26T16:42:21.734597: step 8663, loss 0.00336597, acc 1\n",
      "2018-10-26T16:42:22.189382: step 8664, loss 0.00176353, acc 1\n",
      "2018-10-26T16:42:22.661157: step 8665, loss 0.00163227, acc 1\n",
      "2018-10-26T16:42:23.219630: step 8666, loss 0.00518539, acc 1\n",
      "2018-10-26T16:42:23.672550: step 8667, loss 0.0233369, acc 0.984375\n",
      "2018-10-26T16:42:24.141207: step 8668, loss 0.0226394, acc 0.984375\n",
      "2018-10-26T16:42:24.598141: step 8669, loss 0.000751083, acc 1\n",
      "2018-10-26T16:42:25.000900: step 8670, loss 0.00261439, acc 1\n",
      "2018-10-26T16:42:25.444696: step 8671, loss 0.00135126, acc 1\n",
      "2018-10-26T16:42:25.853591: step 8672, loss 0.000240981, acc 1\n",
      "2018-10-26T16:42:26.247572: step 8673, loss 0.000542505, acc 1\n",
      "2018-10-26T16:42:26.648468: step 8674, loss 3.57078e-05, acc 1\n",
      "2018-10-26T16:42:26.991626: step 8675, loss 0.0013871, acc 1\n",
      "2018-10-26T16:42:27.351590: step 8676, loss 0.000279402, acc 1\n",
      "2018-10-26T16:42:27.698661: step 8677, loss 0.000940242, acc 1\n",
      "2018-10-26T16:42:28.035761: step 8678, loss 0.000280441, acc 1\n",
      "2018-10-26T16:42:28.387821: step 8679, loss 0.0180465, acc 0.984375\n",
      "2018-10-26T16:42:28.733044: step 8680, loss 3.56657e-05, acc 1\n",
      "2018-10-26T16:42:29.109891: step 8681, loss 0.0392768, acc 0.96875\n",
      "2018-10-26T16:42:29.464943: step 8682, loss 0.000842168, acc 1\n",
      "2018-10-26T16:42:29.823291: step 8683, loss 0.000163377, acc 1\n",
      "2018-10-26T16:42:30.158140: step 8684, loss 1.91042e-05, acc 1\n",
      "2018-10-26T16:42:30.520124: step 8685, loss 0.000559815, acc 1\n",
      "2018-10-26T16:42:30.856229: step 8686, loss 0.000515371, acc 1\n",
      "2018-10-26T16:42:31.210279: step 8687, loss 0.00105144, acc 1\n",
      "2018-10-26T16:42:31.607223: step 8688, loss 0.0201644, acc 0.984375\n",
      "2018-10-26T16:42:32.004289: step 8689, loss 0.000332433, acc 1\n",
      "2018-10-26T16:42:32.435007: step 8690, loss 0.000506212, acc 1\n",
      "2018-10-26T16:42:32.753156: step 8691, loss 0.0181377, acc 0.984375\n",
      "2018-10-26T16:42:33.079392: step 8692, loss 0.00172154, acc 1\n",
      "2018-10-26T16:42:33.399915: step 8693, loss 1.86972e-05, acc 1\n",
      "2018-10-26T16:42:33.728631: step 8694, loss 0.15463, acc 0.984375\n",
      "2018-10-26T16:42:34.064654: step 8695, loss 0.00428629, acc 1\n",
      "2018-10-26T16:42:34.376818: step 8696, loss 0.000230464, acc 1\n",
      "2018-10-26T16:42:34.698028: step 8697, loss 0.000233496, acc 1\n",
      "2018-10-26T16:42:35.013120: step 8698, loss 4.17089e-05, acc 1\n",
      "2018-10-26T16:42:35.319386: step 8699, loss 5.97795e-05, acc 1\n",
      "2018-10-26T16:42:35.625482: step 8700, loss 4.68851e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:42:36.387449: step 8700, loss 3.01854, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8700\n",
      "\n",
      "2018-10-26T16:42:36.970887: step 8701, loss 0.00043225, acc 1\n",
      "2018-10-26T16:42:37.353867: step 8702, loss 0.0003766, acc 1\n",
      "2018-10-26T16:42:37.796685: step 8703, loss 0.000654711, acc 1\n",
      "2018-10-26T16:42:38.131847: step 8704, loss 0.000453372, acc 1\n",
      "2018-10-26T16:42:38.462971: step 8705, loss 0.000633793, acc 1\n",
      "2018-10-26T16:42:38.800055: step 8706, loss 0.00151103, acc 1\n",
      "2018-10-26T16:42:39.137139: step 8707, loss 0.000376351, acc 1\n",
      "2018-10-26T16:42:39.443282: step 8708, loss 1.88245e-05, acc 1\n",
      "2018-10-26T16:42:39.780384: step 8709, loss 0.000142044, acc 1\n",
      "2018-10-26T16:42:40.103518: step 8710, loss 0.000337862, acc 1\n",
      "2018-10-26T16:42:40.418748: step 8711, loss 2.77161e-05, acc 1\n",
      "2018-10-26T16:42:40.742830: step 8712, loss 0.000160642, acc 1\n",
      "2018-10-26T16:42:41.049990: step 8713, loss 0.00207891, acc 1\n",
      "2018-10-26T16:42:41.370134: step 8714, loss 0.115498, acc 0.984375\n",
      "2018-10-26T16:42:41.678312: step 8715, loss 0.00075696, acc 1\n",
      "2018-10-26T16:42:41.975515: step 8716, loss 8.23214e-05, acc 1\n",
      "2018-10-26T16:42:42.304674: step 8717, loss 0.00118328, acc 1\n",
      "2018-10-26T16:42:42.622871: step 8718, loss 0.000105978, acc 1\n",
      "2018-10-26T16:42:42.981828: step 8719, loss 3.9636e-05, acc 1\n",
      "2018-10-26T16:42:43.351898: step 8720, loss 0.000112406, acc 1\n",
      "2018-10-26T16:42:43.681205: step 8721, loss 4.59266e-05, acc 1\n",
      "2018-10-26T16:42:44.135869: step 8722, loss 0.00212098, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:42:44.433951: step 8723, loss 0.000974171, acc 1\n",
      "2018-10-26T16:42:44.761142: step 8724, loss 0.00012628, acc 1\n",
      "2018-10-26T16:42:45.077228: step 8725, loss 0.00205189, acc 1\n",
      "2018-10-26T16:42:45.474168: step 8726, loss 0.000193552, acc 1\n",
      "2018-10-26T16:42:45.882078: step 8727, loss 0.000234732, acc 1\n",
      "2018-10-26T16:42:46.222170: step 8728, loss 4.56799e-05, acc 1\n",
      "2018-10-26T16:42:46.579218: step 8729, loss 0.000102471, acc 1\n",
      "2018-10-26T16:42:47.038987: step 8730, loss 0.00119214, acc 1\n",
      "2018-10-26T16:42:47.399025: step 8731, loss 0.000317284, acc 1\n",
      "2018-10-26T16:42:47.731139: step 8732, loss 0.00011729, acc 1\n",
      "2018-10-26T16:42:48.119102: step 8733, loss 4.82433e-05, acc 1\n",
      "2018-10-26T16:42:48.457202: step 8734, loss 0.00333442, acc 1\n",
      "2018-10-26T16:42:48.783328: step 8735, loss 3.94215e-05, acc 1\n",
      "2018-10-26T16:42:49.145360: step 8736, loss 0.000858465, acc 1\n",
      "2018-10-26T16:42:49.537559: step 8737, loss 0.00698436, acc 1\n",
      "2018-10-26T16:42:49.924279: step 8738, loss 0.00148925, acc 1\n",
      "2018-10-26T16:42:50.291410: step 8739, loss 4.21188e-05, acc 1\n",
      "2018-10-26T16:42:50.657343: step 8740, loss 9.81949e-05, acc 1\n",
      "2018-10-26T16:42:51.008412: step 8741, loss 6.50515e-05, acc 1\n",
      "2018-10-26T16:42:51.365428: step 8742, loss 1.59592e-05, acc 1\n",
      "2018-10-26T16:42:51.706518: step 8743, loss 0.0055705, acc 1\n",
      "2018-10-26T16:42:52.067589: step 8744, loss 0.000124732, acc 1\n",
      "2018-10-26T16:42:52.433575: step 8745, loss 2.05512e-05, acc 1\n",
      "2018-10-26T16:42:52.792615: step 8746, loss 0.000289234, acc 1\n",
      "2018-10-26T16:42:53.166617: step 8747, loss 0.000688735, acc 1\n",
      "2018-10-26T16:42:53.530646: step 8748, loss 0.00148997, acc 1\n",
      "2018-10-26T16:42:53.948528: step 8749, loss 0.00088513, acc 1\n",
      "2018-10-26T16:42:54.340483: step 8750, loss 0.0172072, acc 0.984375\n",
      "2018-10-26T16:42:54.645664: step 8751, loss 0.0001126, acc 1\n",
      "2018-10-26T16:42:55.010687: step 8752, loss 9.55841e-06, acc 1\n",
      "2018-10-26T16:42:55.381844: step 8753, loss 0.00144692, acc 1\n",
      "2018-10-26T16:42:55.805654: step 8754, loss 0.00416003, acc 1\n",
      "2018-10-26T16:42:56.136730: step 8755, loss 0.000155089, acc 1\n",
      "2018-10-26T16:42:56.469800: step 8756, loss 6.83019e-05, acc 1\n",
      "2018-10-26T16:42:56.858757: step 8757, loss 0.00020733, acc 1\n",
      "2018-10-26T16:42:57.219789: step 8758, loss 0.000180291, acc 1\n",
      "2018-10-26T16:42:57.577895: step 8759, loss 0.0182843, acc 1\n",
      "2018-10-26T16:42:57.969783: step 8760, loss 0.0334717, acc 0.984375\n",
      "2018-10-26T16:42:58.309912: step 8761, loss 0.00829742, acc 1\n",
      "2018-10-26T16:42:58.687868: step 8762, loss 0.00542654, acc 1\n",
      "2018-10-26T16:42:59.040968: step 8763, loss 9.2872e-05, acc 1\n",
      "2018-10-26T16:42:59.429884: step 8764, loss 0.000241863, acc 1\n",
      "2018-10-26T16:42:59.774961: step 8765, loss 0.000609006, acc 1\n",
      "2018-10-26T16:43:00.134000: step 8766, loss 9.07473e-05, acc 1\n",
      "2018-10-26T16:43:00.496107: step 8767, loss 0.00128172, acc 1\n",
      "2018-10-26T16:43:00.842222: step 8768, loss 0.000383259, acc 1\n",
      "2018-10-26T16:43:01.198160: step 8769, loss 0.000160375, acc 1\n",
      "2018-10-26T16:43:01.632994: step 8770, loss 0.0031568, acc 1\n",
      "2018-10-26T16:43:02.009990: step 8771, loss 0.00018434, acc 1\n",
      "2018-10-26T16:43:02.365041: step 8772, loss 0.000130235, acc 1\n",
      "2018-10-26T16:43:02.694276: step 8773, loss 0.000643585, acc 1\n",
      "2018-10-26T16:43:03.138971: step 8774, loss 0.00205018, acc 1\n",
      "2018-10-26T16:43:03.472192: step 8775, loss 0.00455809, acc 1\n",
      "2018-10-26T16:43:03.809181: step 8776, loss 0.00178044, acc 1\n",
      "2018-10-26T16:43:04.128329: step 8777, loss 2.36661e-05, acc 1\n",
      "2018-10-26T16:43:04.503325: step 8778, loss 0.0123667, acc 0.984375\n",
      "2018-10-26T16:43:04.830451: step 8779, loss 0.00887792, acc 1\n",
      "2018-10-26T16:43:05.167583: step 8780, loss 0.006298, acc 1\n",
      "2018-10-26T16:43:05.488736: step 8781, loss 0.00312087, acc 1\n",
      "2018-10-26T16:43:05.817816: step 8782, loss 0.00281314, acc 1\n",
      "2018-10-26T16:43:06.141949: step 8783, loss 0.0258741, acc 0.984375\n",
      "2018-10-26T16:43:06.480044: step 8784, loss 0.000448893, acc 1\n",
      "2018-10-26T16:43:06.797196: step 8785, loss 0.000141734, acc 1\n",
      "2018-10-26T16:43:07.125320: step 8786, loss 0.000148854, acc 1\n",
      "2018-10-26T16:43:07.415543: step 8787, loss 0.0500624, acc 0.984375\n",
      "2018-10-26T16:43:07.718738: step 8788, loss 0.0047607, acc 1\n",
      "2018-10-26T16:43:08.019930: step 8789, loss 0.000228773, acc 1\n",
      "2018-10-26T16:43:08.351090: step 8790, loss 0.000583822, acc 1\n",
      "2018-10-26T16:43:08.677177: step 8791, loss 0.000788489, acc 1\n",
      "2018-10-26T16:43:09.020257: step 8792, loss 2.94784e-05, acc 1\n",
      "2018-10-26T16:43:09.355361: step 8793, loss 0.000718, acc 1\n",
      "2018-10-26T16:43:09.696451: step 8794, loss 4.04497e-05, acc 1\n",
      "2018-10-26T16:43:10.027566: step 8795, loss 0.000329628, acc 1\n",
      "2018-10-26T16:43:10.344718: step 8796, loss 6.64873e-06, acc 1\n",
      "2018-10-26T16:43:10.653892: step 8797, loss 0.000692964, acc 1\n",
      "2018-10-26T16:43:11.047883: step 8798, loss 0.000640674, acc 1\n",
      "2018-10-26T16:43:11.366989: step 8799, loss 3.66634e-05, acc 1\n",
      "2018-10-26T16:43:11.700096: step 8800, loss 0.00908835, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:43:12.558802: step 8800, loss 3.10627, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8800\n",
      "\n",
      "2018-10-26T16:43:13.230008: step 8801, loss 0.00361512, acc 1\n",
      "2018-10-26T16:43:13.558133: step 8802, loss 0.00904316, acc 1\n",
      "2018-10-26T16:43:14.069766: step 8803, loss 0.000366983, acc 1\n",
      "2018-10-26T16:43:14.419830: step 8804, loss 0.000124228, acc 1\n",
      "2018-10-26T16:43:14.764907: step 8805, loss 0.000465927, acc 1\n",
      "2018-10-26T16:43:15.138909: step 8806, loss 0.000627114, acc 1\n",
      "2018-10-26T16:43:15.495956: step 8807, loss 0.00016114, acc 1\n",
      "2018-10-26T16:43:15.842082: step 8808, loss 0.000133291, acc 1\n",
      "2018-10-26T16:43:16.179132: step 8809, loss 0.000223402, acc 1\n",
      "2018-10-26T16:43:16.555126: step 8810, loss 0.068006, acc 0.96875\n",
      "2018-10-26T16:43:16.868321: step 8811, loss 0.0012788, acc 1\n",
      "2018-10-26T16:43:17.184444: step 8812, loss 0.0343493, acc 0.984375\n",
      "2018-10-26T16:43:17.509575: step 8813, loss 0.00120289, acc 1\n",
      "2018-10-26T16:43:17.853687: step 8814, loss 0.00396216, acc 1\n",
      "2018-10-26T16:43:18.215688: step 8815, loss 0.00529697, acc 1\n",
      "2018-10-26T16:43:18.557805: step 8816, loss 0.000200854, acc 1\n",
      "2018-10-26T16:43:18.887892: step 8817, loss 3.33331e-05, acc 1\n",
      "2018-10-26T16:43:19.246932: step 8818, loss 0.0326135, acc 0.984375\n",
      "2018-10-26T16:43:19.610961: step 8819, loss 0.000146345, acc 1\n",
      "2018-10-26T16:43:19.973990: step 8820, loss 0.000141781, acc 1\n",
      "2018-10-26T16:43:20.351034: step 8821, loss 0.0026498, acc 1\n",
      "2018-10-26T16:43:20.691075: step 8822, loss 0.000232817, acc 1\n",
      "2018-10-26T16:43:21.043132: step 8823, loss 8.03139e-05, acc 1\n",
      "2018-10-26T16:43:21.410151: step 8824, loss 8.13685e-05, acc 1\n",
      "2018-10-26T16:43:21.780164: step 8825, loss 5.06421e-05, acc 1\n",
      "2018-10-26T16:43:22.183091: step 8826, loss 0.00084903, acc 1\n",
      "2018-10-26T16:43:22.639901: step 8827, loss 0.00268497, acc 1\n",
      "2018-10-26T16:43:23.003897: step 8828, loss 0.000172235, acc 1\n",
      "2018-10-26T16:43:23.476631: step 8829, loss 0.000774474, acc 1\n",
      "2018-10-26T16:43:23.870581: step 8830, loss 0.000138081, acc 1\n",
      "2018-10-26T16:43:24.231619: step 8831, loss 0.000169167, acc 1\n",
      "2018-10-26T16:43:24.659471: step 8832, loss 0.0127179, acc 0.984375\n",
      "2018-10-26T16:43:25.061434: step 8833, loss 5.0964e-05, acc 1\n",
      "2018-10-26T16:43:25.433403: step 8834, loss 0.000129933, acc 1\n",
      "2018-10-26T16:43:25.827351: step 8835, loss 3.51356e-05, acc 1\n",
      "2018-10-26T16:43:26.272161: step 8836, loss 0.000387435, acc 1\n",
      "2018-10-26T16:43:26.757865: step 8837, loss 7.8142e-05, acc 1\n",
      "2018-10-26T16:43:27.199684: step 8838, loss 0.000458005, acc 1\n",
      "2018-10-26T16:43:27.665539: step 8839, loss 9.30659e-05, acc 1\n",
      "2018-10-26T16:43:28.131260: step 8840, loss 4.78931e-05, acc 1\n",
      "2018-10-26T16:43:28.548080: step 8841, loss 0.00781779, acc 1\n",
      "2018-10-26T16:43:28.981922: step 8842, loss 0.000101489, acc 1\n",
      "2018-10-26T16:43:29.378863: step 8843, loss 0.000324616, acc 1\n",
      "2018-10-26T16:43:29.799811: step 8844, loss 8.26702e-05, acc 1\n",
      "2018-10-26T16:43:30.227647: step 8845, loss 0.000160141, acc 1\n",
      "2018-10-26T16:43:30.639590: step 8846, loss 0.00215108, acc 1\n",
      "2018-10-26T16:43:31.068385: step 8847, loss 0.00712852, acc 1\n",
      "2018-10-26T16:43:31.466287: step 8848, loss 0.003023, acc 1\n",
      "2018-10-26T16:43:31.787429: step 8849, loss 0.00241531, acc 1\n",
      "2018-10-26T16:43:32.150456: step 8850, loss 0.00137142, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:43:32.558366: step 8851, loss 0.000161799, acc 1\n",
      "2018-10-26T16:43:32.898460: step 8852, loss 0.000149739, acc 1\n",
      "2018-10-26T16:43:33.260491: step 8853, loss 0.000500088, acc 1\n",
      "2018-10-26T16:43:33.653440: step 8854, loss 0.00392506, acc 1\n",
      "2018-10-26T16:43:34.000667: step 8855, loss 0.000717894, acc 1\n",
      "2018-10-26T16:43:34.376509: step 8856, loss 3.85575e-05, acc 1\n",
      "2018-10-26T16:43:34.759485: step 8857, loss 2.63145e-05, acc 1\n",
      "2018-10-26T16:43:35.094594: step 8858, loss 5.12017e-05, acc 1\n",
      "2018-10-26T16:43:35.447647: step 8859, loss 0.133323, acc 0.984375\n",
      "2018-10-26T16:43:35.786998: step 8860, loss 0.00144466, acc 1\n",
      "2018-10-26T16:43:36.135808: step 8861, loss 0.000107812, acc 1\n",
      "2018-10-26T16:43:36.484875: step 8862, loss 3.41429e-05, acc 1\n",
      "2018-10-26T16:43:36.821975: step 8863, loss 2.25197e-05, acc 1\n",
      "2018-10-26T16:43:37.148140: step 8864, loss 6.33341e-05, acc 1\n",
      "2018-10-26T16:43:37.497170: step 8865, loss 0.00535517, acc 1\n",
      "2018-10-26T16:43:37.851268: step 8866, loss 0.000142223, acc 1\n",
      "2018-10-26T16:43:38.199412: step 8867, loss 0.0302452, acc 0.984375\n",
      "2018-10-26T16:43:38.548586: step 8868, loss 0.000225531, acc 1\n",
      "2018-10-26T16:43:38.910394: step 8869, loss 0.000668073, acc 1\n",
      "2018-10-26T16:43:39.263538: step 8870, loss 0.00887543, acc 1\n",
      "2018-10-26T16:43:39.596565: step 8871, loss 0.00750953, acc 1\n",
      "2018-10-26T16:43:39.943636: step 8872, loss 0.0020981, acc 1\n",
      "2018-10-26T16:43:40.283727: step 8873, loss 6.56412e-05, acc 1\n",
      "2018-10-26T16:43:40.625858: step 8874, loss 0.00155242, acc 1\n",
      "2018-10-26T16:43:40.977938: step 8875, loss 0.000234801, acc 1\n",
      "2018-10-26T16:43:41.303998: step 8876, loss 0.00046143, acc 1\n",
      "2018-10-26T16:43:41.640105: step 8877, loss 0.0011183, acc 1\n",
      "2018-10-26T16:43:41.991163: step 8878, loss 0.000268892, acc 1\n",
      "2018-10-26T16:43:42.330258: step 8879, loss 5.26712e-05, acc 1\n",
      "2018-10-26T16:43:42.665412: step 8880, loss 0.000136572, acc 1\n",
      "2018-10-26T16:43:42.993489: step 8881, loss 0.000679472, acc 1\n",
      "2018-10-26T16:43:43.331581: step 8882, loss 0.000290476, acc 1\n",
      "2018-10-26T16:43:43.660748: step 8883, loss 0.0481416, acc 0.984375\n",
      "2018-10-26T16:43:44.014873: step 8884, loss 0.00030983, acc 1\n",
      "2018-10-26T16:43:44.360874: step 8885, loss 0.000125614, acc 1\n",
      "2018-10-26T16:43:44.730873: step 8886, loss 6.08335e-05, acc 1\n",
      "2018-10-26T16:43:45.085898: step 8887, loss 0.000348852, acc 1\n",
      "2018-10-26T16:43:45.431974: step 8888, loss 4.53932e-05, acc 1\n",
      "2018-10-26T16:43:45.781040: step 8889, loss 0.00982988, acc 1\n",
      "2018-10-26T16:43:46.160028: step 8890, loss 4.29551e-05, acc 1\n",
      "2018-10-26T16:43:46.519066: step 8891, loss 0.000237359, acc 1\n",
      "2018-10-26T16:43:46.877112: step 8892, loss 8.05148e-05, acc 1\n",
      "2018-10-26T16:43:47.236151: step 8893, loss 0.000924189, acc 1\n",
      "2018-10-26T16:43:47.629100: step 8894, loss 0.00191573, acc 1\n",
      "2018-10-26T16:43:47.980233: step 8895, loss 5.44821e-05, acc 1\n",
      "2018-10-26T16:43:48.361317: step 8896, loss 0.00126517, acc 1\n",
      "2018-10-26T16:43:48.698316: step 8897, loss 8.59365e-05, acc 1\n",
      "2018-10-26T16:43:49.039332: step 8898, loss 0.0485501, acc 0.984375\n",
      "2018-10-26T16:43:49.387403: step 8899, loss 0.000201421, acc 1\n",
      "2018-10-26T16:43:49.732481: step 8900, loss 0.00171352, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:43:50.571319: step 8900, loss 3.05573, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-8900\n",
      "\n",
      "2018-10-26T16:43:51.210530: step 8901, loss 0.000367577, acc 1\n",
      "2018-10-26T16:43:51.570572: step 8902, loss 4.83406e-05, acc 1\n",
      "2018-10-26T16:43:52.053280: step 8903, loss 0.00265771, acc 1\n",
      "2018-10-26T16:43:52.409328: step 8904, loss 0.00453407, acc 1\n",
      "2018-10-26T16:43:52.751533: step 8905, loss 2.94457e-05, acc 1\n",
      "2018-10-26T16:43:53.103850: step 8906, loss 0.00259249, acc 1\n",
      "2018-10-26T16:43:53.430603: step 8907, loss 6.37073e-05, acc 1\n",
      "2018-10-26T16:43:53.778670: step 8908, loss 0.000515395, acc 1\n",
      "2018-10-26T16:43:54.130728: step 8909, loss 0.00251544, acc 1\n",
      "2018-10-26T16:43:54.471857: step 8910, loss 0.00180558, acc 1\n",
      "2018-10-26T16:43:54.803929: step 8911, loss 0.0153181, acc 0.984375\n",
      "2018-10-26T16:43:55.145019: step 8912, loss 0.000116535, acc 1\n",
      "2018-10-26T16:43:55.481167: step 8913, loss 1.51423e-05, acc 1\n",
      "2018-10-26T16:43:55.825204: step 8914, loss 0.000172532, acc 1\n",
      "2018-10-26T16:43:56.213291: step 8915, loss 0.000269506, acc 1\n",
      "2018-10-26T16:43:56.557469: step 8916, loss 0.00175621, acc 1\n",
      "2018-10-26T16:43:56.898334: step 8917, loss 0.00195035, acc 1\n",
      "2018-10-26T16:43:57.253384: step 8918, loss 0.00115918, acc 1\n",
      "2018-10-26T16:43:57.615421: step 8919, loss 0.0026735, acc 1\n",
      "2018-10-26T16:43:57.962491: step 8920, loss 0.000745434, acc 1\n",
      "2018-10-26T16:43:58.281674: step 8921, loss 0.000189417, acc 1\n",
      "2018-10-26T16:43:58.617834: step 8922, loss 0.000331495, acc 1\n",
      "2018-10-26T16:43:58.988751: step 8923, loss 0.000631926, acc 1\n",
      "2018-10-26T16:43:59.333827: step 8924, loss 0.00262287, acc 1\n",
      "2018-10-26T16:43:59.676915: step 8925, loss 0.0283209, acc 0.984375\n",
      "2018-10-26T16:44:00.041936: step 8926, loss 0.000667912, acc 1\n",
      "2018-10-26T16:44:00.375115: step 8927, loss 0.000245372, acc 1\n",
      "2018-10-26T16:44:00.710149: step 8928, loss 0.00193528, acc 1\n",
      "2018-10-26T16:44:01.056271: step 8929, loss 1.07715e-05, acc 1\n",
      "2018-10-26T16:44:01.415268: step 8930, loss 0.000783138, acc 1\n",
      "2018-10-26T16:44:01.782336: step 8931, loss 0.000712815, acc 1\n",
      "2018-10-26T16:44:02.129359: step 8932, loss 0.000479892, acc 1\n",
      "2018-10-26T16:44:02.479422: step 8933, loss 0.00133373, acc 1\n",
      "2018-10-26T16:44:02.858438: step 8934, loss 0.0306414, acc 0.984375\n",
      "2018-10-26T16:44:03.202489: step 8935, loss 7.03535e-05, acc 1\n",
      "2018-10-26T16:44:03.556548: step 8936, loss 0.000513469, acc 1\n",
      "2018-10-26T16:44:03.908603: step 8937, loss 0.00124891, acc 1\n",
      "2018-10-26T16:44:04.284658: step 8938, loss 7.40156e-05, acc 1\n",
      "2018-10-26T16:44:04.629680: step 8939, loss 0.0309616, acc 0.984375\n",
      "2018-10-26T16:44:04.957801: step 8940, loss 6.82346e-05, acc 1\n",
      "2018-10-26T16:44:05.314854: step 8941, loss 7.38398e-05, acc 1\n",
      "2018-10-26T16:44:05.735722: step 8942, loss 0.00742785, acc 1\n",
      "2018-10-26T16:44:06.159588: step 8943, loss 0.0226958, acc 0.984375\n",
      "2018-10-26T16:44:06.526610: step 8944, loss 0.0263454, acc 0.984375\n",
      "2018-10-26T16:44:06.894732: step 8945, loss 0.000777481, acc 1\n",
      "2018-10-26T16:44:07.293560: step 8946, loss 3.80969e-05, acc 1\n",
      "2018-10-26T16:44:07.694670: step 8947, loss 0.000312714, acc 1\n",
      "2018-10-26T16:44:08.096414: step 8948, loss 0.000292497, acc 1\n",
      "2018-10-26T16:44:08.467421: step 8949, loss 2.45389e-05, acc 1\n",
      "2018-10-26T16:44:08.900270: step 8950, loss 0.00113498, acc 1\n",
      "2018-10-26T16:44:09.290228: step 8951, loss 8.10663e-05, acc 1\n",
      "2018-10-26T16:44:09.715090: step 8952, loss 0.000271954, acc 1\n",
      "2018-10-26T16:44:10.195808: step 8953, loss 0.000176787, acc 1\n",
      "2018-10-26T16:44:10.597771: step 8954, loss 7.31578e-06, acc 1\n",
      "2018-10-26T16:44:10.954776: step 8955, loss 0.00105086, acc 1\n",
      "2018-10-26T16:44:11.317811: step 8956, loss 0.000582097, acc 1\n",
      "2018-10-26T16:44:11.664884: step 8957, loss 0.000235083, acc 1\n",
      "2018-10-26T16:44:12.007053: step 8958, loss 0.00124717, acc 1\n",
      "2018-10-26T16:44:12.354038: step 8959, loss 0.00330722, acc 1\n",
      "2018-10-26T16:44:12.704106: step 8960, loss 4.03239e-05, acc 1\n",
      "2018-10-26T16:44:13.060152: step 8961, loss 0.00188295, acc 1\n",
      "2018-10-26T16:44:13.412215: step 8962, loss 0.000261188, acc 1\n",
      "2018-10-26T16:44:13.776394: step 8963, loss 0.000130268, acc 1\n",
      "2018-10-26T16:44:14.136279: step 8964, loss 0.000310193, acc 1\n",
      "2018-10-26T16:44:14.491382: step 8965, loss 0.000167375, acc 1\n",
      "2018-10-26T16:44:14.851608: step 8966, loss 1.77949e-05, acc 1\n",
      "2018-10-26T16:44:15.239331: step 8967, loss 0.000719418, acc 1\n",
      "2018-10-26T16:44:15.621370: step 8968, loss 0.00797792, acc 1\n",
      "2018-10-26T16:44:16.022238: step 8969, loss 6.86894e-05, acc 1\n",
      "2018-10-26T16:44:16.386265: step 8970, loss 0.00293767, acc 1\n",
      "2018-10-26T16:44:16.790190: step 8971, loss 3.67218e-05, acc 1\n",
      "2018-10-26T16:44:17.160197: step 8972, loss 0.00730353, acc 1\n",
      "2018-10-26T16:44:17.509426: step 8973, loss 0.000659513, acc 1\n",
      "2018-10-26T16:44:17.865375: step 8974, loss 0.000115116, acc 1\n",
      "2018-10-26T16:44:18.236325: step 8975, loss 3.39254e-05, acc 1\n",
      "2018-10-26T16:44:18.592370: step 8976, loss 0.00239921, acc 1\n",
      "2018-10-26T16:44:18.891572: step 8977, loss 7.65775e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:44:19.213831: step 8978, loss 0.000727832, acc 1\n",
      "2018-10-26T16:44:19.501983: step 8979, loss 0.0015125, acc 1\n",
      "2018-10-26T16:44:19.821149: step 8980, loss 1.0094e-05, acc 1\n",
      "2018-10-26T16:44:20.133485: step 8981, loss 0.000403604, acc 1\n",
      "2018-10-26T16:44:20.471397: step 8982, loss 1.19408e-05, acc 1\n",
      "2018-10-26T16:44:20.811509: step 8983, loss 6.48712e-05, acc 1\n",
      "2018-10-26T16:44:21.119618: step 8984, loss 0.00109176, acc 1\n",
      "2018-10-26T16:44:21.447786: step 8985, loss 9.77677e-05, acc 1\n",
      "2018-10-26T16:44:21.790824: step 8986, loss 2.09069e-05, acc 1\n",
      "2018-10-26T16:44:22.190807: step 8987, loss 0.000579907, acc 1\n",
      "2018-10-26T16:44:22.505918: step 8988, loss 0.0015345, acc 1\n",
      "2018-10-26T16:44:22.823068: step 8989, loss 5.15177e-05, acc 1\n",
      "2018-10-26T16:44:23.146204: step 8990, loss 0.00261056, acc 1\n",
      "2018-10-26T16:44:23.509237: step 8991, loss 1.50302e-05, acc 1\n",
      "2018-10-26T16:44:23.868273: step 8992, loss 0.000118798, acc 1\n",
      "2018-10-26T16:44:24.219337: step 8993, loss 0.000325623, acc 1\n",
      "2018-10-26T16:44:24.557432: step 8994, loss 0.00342618, acc 1\n",
      "2018-10-26T16:44:24.890658: step 8995, loss 0.00582963, acc 1\n",
      "2018-10-26T16:44:25.232630: step 8996, loss 0.000867028, acc 1\n",
      "2018-10-26T16:44:25.606630: step 8997, loss 5.25894e-05, acc 1\n",
      "2018-10-26T16:44:25.934755: step 8998, loss 0.00692022, acc 1\n",
      "2018-10-26T16:44:26.275845: step 8999, loss 0.000989288, acc 1\n",
      "2018-10-26T16:44:26.616934: step 9000, loss 0.0474022, acc 0.966667\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:44:27.411808: step 9000, loss 3.07133, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9000\n",
      "\n",
      "2018-10-26T16:44:28.114936: step 9001, loss 6.17034e-06, acc 1\n",
      "2018-10-26T16:44:28.446043: step 9002, loss 0.000400795, acc 1\n",
      "2018-10-26T16:44:28.978620: step 9003, loss 0.000254913, acc 1\n",
      "2018-10-26T16:44:29.350666: step 9004, loss 0.000171894, acc 1\n",
      "2018-10-26T16:44:29.774496: step 9005, loss 0.0014218, acc 1\n",
      "2018-10-26T16:44:30.151486: step 9006, loss 1.92095e-05, acc 1\n",
      "2018-10-26T16:44:30.531533: step 9007, loss 0.00169211, acc 1\n",
      "2018-10-26T16:44:30.908463: step 9008, loss 0.00059162, acc 1\n",
      "2018-10-26T16:44:31.311386: step 9009, loss 0.000126524, acc 1\n",
      "2018-10-26T16:44:31.778139: step 9010, loss 0.000744936, acc 1\n",
      "2018-10-26T16:44:32.251874: step 9011, loss 0.000857077, acc 1\n",
      "2018-10-26T16:44:32.700675: step 9012, loss 1.48525e-05, acc 1\n",
      "2018-10-26T16:44:33.162627: step 9013, loss 0.000932557, acc 1\n",
      "2018-10-26T16:44:33.678064: step 9014, loss 0.000204469, acc 1\n",
      "2018-10-26T16:44:34.136838: step 9015, loss 3.87761e-05, acc 1\n",
      "2018-10-26T16:44:34.520855: step 9016, loss 1.30124e-05, acc 1\n",
      "2018-10-26T16:44:34.926727: step 9017, loss 0.000208721, acc 1\n",
      "2018-10-26T16:44:35.351593: step 9018, loss 2.48395e-05, acc 1\n",
      "2018-10-26T16:44:35.725592: step 9019, loss 7.90316e-05, acc 1\n",
      "2018-10-26T16:44:36.072666: step 9020, loss 0.000417468, acc 1\n",
      "2018-10-26T16:44:36.488555: step 9021, loss 0.00193105, acc 1\n",
      "2018-10-26T16:44:36.888487: step 9022, loss 0.00187515, acc 1\n",
      "2018-10-26T16:44:37.236557: step 9023, loss 9.2988e-05, acc 1\n",
      "2018-10-26T16:44:37.638483: step 9024, loss 9.15053e-06, acc 1\n",
      "2018-10-26T16:44:38.013484: step 9025, loss 0.00127942, acc 1\n",
      "2018-10-26T16:44:38.426605: step 9026, loss 0.000422012, acc 1\n",
      "2018-10-26T16:44:38.789408: step 9027, loss 0.000173737, acc 1\n",
      "2018-10-26T16:44:39.158497: step 9028, loss 0.000101187, acc 1\n",
      "2018-10-26T16:44:39.589317: step 9029, loss 0.00137818, acc 1\n",
      "2018-10-26T16:44:39.963317: step 9030, loss 9.29313e-06, acc 1\n",
      "2018-10-26T16:44:40.306353: step 9031, loss 0.000170746, acc 1\n",
      "2018-10-26T16:44:40.640464: step 9032, loss 0.000550304, acc 1\n",
      "2018-10-26T16:44:40.998559: step 9033, loss 3.73822e-06, acc 1\n",
      "2018-10-26T16:44:41.387472: step 9034, loss 0.00566092, acc 1\n",
      "2018-10-26T16:44:41.770442: step 9035, loss 7.6662e-06, acc 1\n",
      "2018-10-26T16:44:42.113528: step 9036, loss 5.29075e-05, acc 1\n",
      "2018-10-26T16:44:42.473566: step 9037, loss 0.0253531, acc 0.984375\n",
      "2018-10-26T16:44:42.813655: step 9038, loss 2.64266e-05, acc 1\n",
      "2018-10-26T16:44:43.157735: step 9039, loss 9.62685e-06, acc 1\n",
      "2018-10-26T16:44:43.502815: step 9040, loss 9.37311e-05, acc 1\n",
      "2018-10-26T16:44:43.843903: step 9041, loss 2.76992e-05, acc 1\n",
      "2018-10-26T16:44:44.208929: step 9042, loss 0.000130698, acc 1\n",
      "2018-10-26T16:44:44.576945: step 9043, loss 0.00773094, acc 1\n",
      "2018-10-26T16:44:44.938978: step 9044, loss 0.000699172, acc 1\n",
      "2018-10-26T16:44:45.325945: step 9045, loss 0.000293759, acc 1\n",
      "2018-10-26T16:44:45.683079: step 9046, loss 0.00200278, acc 1\n",
      "2018-10-26T16:44:46.040035: step 9047, loss 0.00032934, acc 1\n",
      "2018-10-26T16:44:46.415090: step 9048, loss 1.00211e-05, acc 1\n",
      "2018-10-26T16:44:46.768125: step 9049, loss 0.000220955, acc 1\n",
      "2018-10-26T16:44:47.130126: step 9050, loss 0.00855554, acc 1\n",
      "2018-10-26T16:44:47.514098: step 9051, loss 0.00141169, acc 1\n",
      "2018-10-26T16:44:47.834241: step 9052, loss 1.87103e-05, acc 1\n",
      "2018-10-26T16:44:48.178321: step 9053, loss 1.75505e-05, acc 1\n",
      "2018-10-26T16:44:48.545340: step 9054, loss 0.000712776, acc 1\n",
      "2018-10-26T16:44:48.904380: step 9055, loss 0.000925984, acc 1\n",
      "2018-10-26T16:44:49.289363: step 9056, loss 0.000830259, acc 1\n",
      "2018-10-26T16:44:49.663353: step 9057, loss 0.000350862, acc 1\n",
      "2018-10-26T16:44:50.052315: step 9058, loss 3.61291e-05, acc 1\n",
      "2018-10-26T16:44:50.387687: step 9059, loss 0.00014313, acc 1\n",
      "2018-10-26T16:44:50.745462: step 9060, loss 1.8761e-05, acc 1\n",
      "2018-10-26T16:44:51.109599: step 9061, loss 4.43314e-05, acc 1\n",
      "2018-10-26T16:44:51.454567: step 9062, loss 0.000311988, acc 1\n",
      "2018-10-26T16:44:51.793662: step 9063, loss 6.11293e-06, acc 1\n",
      "2018-10-26T16:44:52.168662: step 9064, loss 1.06886e-05, acc 1\n",
      "2018-10-26T16:44:52.511744: step 9065, loss 6.35661e-05, acc 1\n",
      "2018-10-26T16:44:52.913717: step 9066, loss 0.00164152, acc 1\n",
      "2018-10-26T16:44:53.340568: step 9067, loss 0.000146162, acc 1\n",
      "2018-10-26T16:44:53.664661: step 9068, loss 0.0158585, acc 0.984375\n",
      "2018-10-26T16:44:54.009742: step 9069, loss 0.000168242, acc 1\n",
      "2018-10-26T16:44:54.332876: step 9070, loss 2.61128e-06, acc 1\n",
      "2018-10-26T16:44:54.687969: step 9071, loss 9.79439e-05, acc 1\n",
      "2018-10-26T16:44:55.043978: step 9072, loss 0.000630796, acc 1\n",
      "2018-10-26T16:44:55.408006: step 9073, loss 3.45131e-06, acc 1\n",
      "2018-10-26T16:44:55.781048: step 9074, loss 0.000453648, acc 1\n",
      "2018-10-26T16:44:56.136070: step 9075, loss 0.00195855, acc 1\n",
      "2018-10-26T16:44:56.462188: step 9076, loss 7.34475e-05, acc 1\n",
      "2018-10-26T16:44:56.936923: step 9077, loss 9.31937e-05, acc 1\n",
      "2018-10-26T16:44:57.271087: step 9078, loss 0.00086736, acc 1\n",
      "2018-10-26T16:44:57.600149: step 9079, loss 0.000505816, acc 1\n",
      "2018-10-26T16:44:57.951297: step 9080, loss 0.000194516, acc 1\n",
      "2018-10-26T16:44:58.320224: step 9081, loss 1.95103e-05, acc 1\n",
      "2018-10-26T16:44:58.695223: step 9082, loss 0.000735384, acc 1\n",
      "2018-10-26T16:44:59.074209: step 9083, loss 0.00101155, acc 1\n",
      "2018-10-26T16:44:59.466161: step 9084, loss 0.000644997, acc 1\n",
      "2018-10-26T16:44:59.841240: step 9085, loss 0.000882215, acc 1\n",
      "2018-10-26T16:45:00.250113: step 9086, loss 0.000115202, acc 1\n",
      "2018-10-26T16:45:00.634042: step 9087, loss 0.000398074, acc 1\n",
      "2018-10-26T16:45:00.994165: step 9088, loss 2.21639e-05, acc 1\n",
      "2018-10-26T16:45:01.353266: step 9089, loss 0.00466263, acc 1\n",
      "2018-10-26T16:45:01.775135: step 9090, loss 3.75234e-05, acc 1\n",
      "2018-10-26T16:45:02.155974: step 9091, loss 0.0332281, acc 0.984375\n",
      "2018-10-26T16:45:02.532967: step 9092, loss 0.000427001, acc 1\n",
      "2018-10-26T16:45:02.909293: step 9093, loss 0.00667524, acc 1\n",
      "2018-10-26T16:45:03.338922: step 9094, loss 0.000171647, acc 1\n",
      "2018-10-26T16:45:03.713812: step 9095, loss 3.26031e-05, acc 1\n",
      "2018-10-26T16:45:04.081861: step 9096, loss 0.000278036, acc 1\n",
      "2018-10-26T16:45:04.423923: step 9097, loss 0.000491867, acc 1\n",
      "2018-10-26T16:45:04.765005: step 9098, loss 0.00155407, acc 1\n",
      "2018-10-26T16:45:05.156957: step 9099, loss 8.58261e-05, acc 1\n",
      "2018-10-26T16:45:05.535948: step 9100, loss 0.00025919, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:45:06.420620: step 9100, loss 3.09887, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9100\n",
      "\n",
      "2018-10-26T16:45:07.137665: step 9101, loss 0.00284355, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:45:07.488726: step 9102, loss 0.00344152, acc 1\n",
      "2018-10-26T16:45:07.924562: step 9103, loss 0.000166694, acc 1\n",
      "2018-10-26T16:45:08.268643: step 9104, loss 0.00788454, acc 1\n",
      "2018-10-26T16:45:08.612723: step 9105, loss 0.00179252, acc 1\n",
      "2018-10-26T16:45:08.923931: step 9106, loss 1.56997e-05, acc 1\n",
      "2018-10-26T16:45:09.246031: step 9107, loss 0.00171452, acc 1\n",
      "2018-10-26T16:45:09.592120: step 9108, loss 0.00119739, acc 1\n",
      "2018-10-26T16:45:09.961194: step 9109, loss 1.2653e-05, acc 1\n",
      "2018-10-26T16:45:10.278273: step 9110, loss 0.000291506, acc 1\n",
      "2018-10-26T16:45:10.606398: step 9111, loss 2.74677e-05, acc 1\n",
      "2018-10-26T16:45:10.959500: step 9112, loss 0.000169368, acc 1\n",
      "2018-10-26T16:45:11.306643: step 9113, loss 7.54461e-05, acc 1\n",
      "2018-10-26T16:45:11.657619: step 9114, loss 0.000164084, acc 1\n",
      "2018-10-26T16:45:11.985715: step 9115, loss 0.000428768, acc 1\n",
      "2018-10-26T16:45:12.307850: step 9116, loss 5.76835e-05, acc 1\n",
      "2018-10-26T16:45:12.663900: step 9117, loss 0.000426979, acc 1\n",
      "2018-10-26T16:45:13.060842: step 9118, loss 6.35104e-05, acc 1\n",
      "2018-10-26T16:45:13.477727: step 9119, loss 0.00614787, acc 1\n",
      "2018-10-26T16:45:13.853769: step 9120, loss 0.000125396, acc 1\n",
      "2018-10-26T16:45:14.220852: step 9121, loss 0.000943361, acc 1\n",
      "2018-10-26T16:45:14.550861: step 9122, loss 5.81312e-05, acc 1\n",
      "2018-10-26T16:45:14.872996: step 9123, loss 0.00360424, acc 1\n",
      "2018-10-26T16:45:15.194138: step 9124, loss 0.00440286, acc 1\n",
      "2018-10-26T16:45:15.514285: step 9125, loss 0.000420697, acc 1\n",
      "2018-10-26T16:45:15.846396: step 9126, loss 0.00126181, acc 1\n",
      "2018-10-26T16:45:16.191474: step 9127, loss 3.95335e-05, acc 1\n",
      "2018-10-26T16:45:16.514615: step 9128, loss 8.4533e-05, acc 1\n",
      "2018-10-26T16:45:16.901576: step 9129, loss 0.00227575, acc 1\n",
      "2018-10-26T16:45:17.276905: step 9130, loss 0.000329749, acc 1\n",
      "2018-10-26T16:45:17.641647: step 9131, loss 0.000110742, acc 1\n",
      "2018-10-26T16:45:18.007625: step 9132, loss 9.25407e-06, acc 1\n",
      "2018-10-26T16:45:18.365666: step 9133, loss 0.000245512, acc 1\n",
      "2018-10-26T16:45:18.698777: step 9134, loss 0.0915212, acc 0.984375\n",
      "2018-10-26T16:45:19.047002: step 9135, loss 0.000911924, acc 1\n",
      "2018-10-26T16:45:19.398906: step 9136, loss 0.000142005, acc 1\n",
      "2018-10-26T16:45:19.747976: step 9137, loss 0.00128582, acc 1\n",
      "2018-10-26T16:45:20.132990: step 9138, loss 0.00117904, acc 1\n",
      "2018-10-26T16:45:20.458183: step 9139, loss 6.15423e-05, acc 1\n",
      "2018-10-26T16:45:20.819113: step 9140, loss 0.000324369, acc 1\n",
      "2018-10-26T16:45:21.170171: step 9141, loss 0.000875697, acc 1\n",
      "2018-10-26T16:45:21.507272: step 9142, loss 0.00444492, acc 1\n",
      "2018-10-26T16:45:21.890248: step 9143, loss 0.0161576, acc 0.984375\n",
      "2018-10-26T16:45:22.252280: step 9144, loss 0.000195794, acc 1\n",
      "2018-10-26T16:45:22.603342: step 9145, loss 0.00053659, acc 1\n",
      "2018-10-26T16:45:22.970362: step 9146, loss 0.00121981, acc 1\n",
      "2018-10-26T16:45:23.332396: step 9147, loss 0.0307959, acc 0.984375\n",
      "2018-10-26T16:45:23.675478: step 9148, loss 1.31589e-05, acc 1\n",
      "2018-10-26T16:45:24.036514: step 9149, loss 0.00911683, acc 1\n",
      "2018-10-26T16:45:24.392719: step 9150, loss 0.000825663, acc 1\n",
      "2018-10-26T16:45:24.737648: step 9151, loss 0.000816319, acc 1\n",
      "2018-10-26T16:45:25.052800: step 9152, loss 0.00020302, acc 1\n",
      "2018-10-26T16:45:25.441758: step 9153, loss 0.000237091, acc 1\n",
      "2018-10-26T16:45:25.807815: step 9154, loss 4.36236e-05, acc 1\n",
      "2018-10-26T16:45:26.141889: step 9155, loss 8.66122e-07, acc 1\n",
      "2018-10-26T16:45:26.503922: step 9156, loss 0.0073415, acc 1\n",
      "2018-10-26T16:45:26.847004: step 9157, loss 0.000881222, acc 1\n",
      "2018-10-26T16:45:27.222001: step 9158, loss 0.000450483, acc 1\n",
      "2018-10-26T16:45:27.580045: step 9159, loss 1.65123e-05, acc 1\n",
      "2018-10-26T16:45:27.901542: step 9160, loss 3.88249e-05, acc 1\n",
      "2018-10-26T16:45:28.267209: step 9161, loss 0.0003764, acc 1\n",
      "2018-10-26T16:45:28.620265: step 9162, loss 0.000580048, acc 1\n",
      "2018-10-26T16:45:28.977313: step 9163, loss 2.59209e-05, acc 1\n",
      "2018-10-26T16:45:29.295535: step 9164, loss 3.50349e-05, acc 1\n",
      "2018-10-26T16:45:29.637552: step 9165, loss 0.000244312, acc 1\n",
      "2018-10-26T16:45:29.993600: step 9166, loss 3.2147e-06, acc 1\n",
      "2018-10-26T16:45:30.356630: step 9167, loss 0.00281563, acc 1\n",
      "2018-10-26T16:45:30.777502: step 9168, loss 0.00010193, acc 1\n",
      "2018-10-26T16:45:31.086676: step 9169, loss 0.000567809, acc 1\n",
      "2018-10-26T16:45:31.430791: step 9170, loss 0.000225018, acc 1\n",
      "2018-10-26T16:45:31.741926: step 9171, loss 3.71941e-05, acc 1\n",
      "2018-10-26T16:45:32.068100: step 9172, loss 9.17217e-05, acc 1\n",
      "2018-10-26T16:45:32.393334: step 9173, loss 8.09604e-05, acc 1\n",
      "2018-10-26T16:45:32.744247: step 9174, loss 7.95224e-05, acc 1\n",
      "2018-10-26T16:45:33.074365: step 9175, loss 0.000338782, acc 1\n",
      "2018-10-26T16:45:33.399496: step 9176, loss 8.97406e-05, acc 1\n",
      "2018-10-26T16:45:33.768512: step 9177, loss 7.60937e-05, acc 1\n",
      "2018-10-26T16:45:34.101621: step 9178, loss 2.96026e-05, acc 1\n",
      "2018-10-26T16:45:34.449691: step 9179, loss 0.000203109, acc 1\n",
      "2018-10-26T16:45:34.821739: step 9180, loss 0.00103944, acc 1\n",
      "2018-10-26T16:45:35.233643: step 9181, loss 0.000330111, acc 1\n",
      "2018-10-26T16:45:35.577677: step 9182, loss 0.000131667, acc 1\n",
      "2018-10-26T16:45:35.948894: step 9183, loss 0.000144385, acc 1\n",
      "2018-10-26T16:45:36.355600: step 9184, loss 2.50848e-05, acc 1\n",
      "2018-10-26T16:45:36.706663: step 9185, loss 1.45669e-05, acc 1\n",
      "2018-10-26T16:45:37.079665: step 9186, loss 0.00178512, acc 1\n",
      "2018-10-26T16:45:37.449675: step 9187, loss 7.25874e-05, acc 1\n",
      "2018-10-26T16:45:37.860578: step 9188, loss 1.59239e-05, acc 1\n",
      "2018-10-26T16:45:38.267491: step 9189, loss 0.00508314, acc 1\n",
      "2018-10-26T16:45:38.700333: step 9190, loss 0.00397206, acc 1\n",
      "2018-10-26T16:45:39.159756: step 9191, loss 0.00773806, acc 1\n",
      "2018-10-26T16:45:39.616885: step 9192, loss 4.54594e-05, acc 1\n",
      "2018-10-26T16:45:40.033771: step 9193, loss 0.000104532, acc 1\n",
      "2018-10-26T16:45:40.431796: step 9194, loss 0.0143852, acc 0.984375\n",
      "2018-10-26T16:45:40.863673: step 9195, loss 0.000277683, acc 1\n",
      "2018-10-26T16:45:41.262488: step 9196, loss 0.000112548, acc 1\n",
      "2018-10-26T16:45:41.671472: step 9197, loss 0.0044025, acc 1\n",
      "2018-10-26T16:45:42.052379: step 9198, loss 0.000157022, acc 1\n",
      "2018-10-26T16:45:42.416405: step 9199, loss 5.03624e-06, acc 1\n",
      "2018-10-26T16:45:42.805368: step 9200, loss 4.14094e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:45:43.657126: step 9200, loss 3.15745, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9200\n",
      "\n",
      "2018-10-26T16:45:44.237539: step 9201, loss 0.000256944, acc 1\n",
      "2018-10-26T16:45:44.562717: step 9202, loss 0.000333748, acc 1\n",
      "2018-10-26T16:45:44.904758: step 9203, loss 4.97069e-05, acc 1\n",
      "2018-10-26T16:45:45.312666: step 9204, loss 3.29704e-05, acc 1\n",
      "2018-10-26T16:45:45.651762: step 9205, loss 0.00184529, acc 1\n",
      "2018-10-26T16:45:45.975898: step 9206, loss 0.000104739, acc 1\n",
      "2018-10-26T16:45:46.300028: step 9207, loss 0.00227977, acc 1\n",
      "2018-10-26T16:45:46.624240: step 9208, loss 7.88755e-05, acc 1\n",
      "2018-10-26T16:45:46.951292: step 9209, loss 0.00213863, acc 1\n",
      "2018-10-26T16:45:47.255477: step 9210, loss 7.17428e-05, acc 1\n",
      "2018-10-26T16:45:47.587771: step 9211, loss 0.000379376, acc 1\n",
      "2018-10-26T16:45:47.902746: step 9212, loss 0.000500503, acc 1\n",
      "2018-10-26T16:45:48.235855: step 9213, loss 6.76599e-05, acc 1\n",
      "2018-10-26T16:45:48.537052: step 9214, loss 0.00103222, acc 1\n",
      "2018-10-26T16:45:48.857197: step 9215, loss 0.000224858, acc 1\n",
      "2018-10-26T16:45:49.163382: step 9216, loss 0.00027153, acc 1\n",
      "2018-10-26T16:45:49.517515: step 9217, loss 0.000134329, acc 1\n",
      "2018-10-26T16:45:49.827646: step 9218, loss 0.000135894, acc 1\n",
      "2018-10-26T16:45:50.149745: step 9219, loss 9.11152e-05, acc 1\n",
      "2018-10-26T16:45:50.464903: step 9220, loss 6.64493e-05, acc 1\n",
      "2018-10-26T16:45:50.787041: step 9221, loss 5.5634e-05, acc 1\n",
      "2018-10-26T16:45:51.110176: step 9222, loss 1.30207e-05, acc 1\n",
      "2018-10-26T16:45:51.406385: step 9223, loss 0.00166633, acc 1\n",
      "2018-10-26T16:45:51.728524: step 9224, loss 0.00174676, acc 1\n",
      "2018-10-26T16:45:52.064626: step 9225, loss 3.89544e-05, acc 1\n",
      "2018-10-26T16:45:52.403260: step 9226, loss 1.9868e-05, acc 1\n",
      "2018-10-26T16:45:52.710900: step 9227, loss 0.000423579, acc 1\n",
      "2018-10-26T16:45:53.040024: step 9228, loss 0.000308321, acc 1\n",
      "2018-10-26T16:45:53.366151: step 9229, loss 1.22252e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:45:53.677318: step 9230, loss 0.00346053, acc 1\n",
      "2018-10-26T16:45:53.992476: step 9231, loss 0.000484893, acc 1\n",
      "2018-10-26T16:45:54.318606: step 9232, loss 6.78466e-05, acc 1\n",
      "2018-10-26T16:45:54.623790: step 9233, loss 4.30248e-05, acc 1\n",
      "2018-10-26T16:45:54.942972: step 9234, loss 0.000141615, acc 1\n",
      "2018-10-26T16:45:55.278043: step 9235, loss 0.000308003, acc 1\n",
      "2018-10-26T16:45:55.608160: step 9236, loss 0.000388891, acc 1\n",
      "2018-10-26T16:45:55.932294: step 9237, loss 6.33833e-05, acc 1\n",
      "2018-10-26T16:45:56.248450: step 9238, loss 4.45352e-05, acc 1\n",
      "2018-10-26T16:45:56.609484: step 9239, loss 0.000182844, acc 1\n",
      "2018-10-26T16:45:56.943591: step 9240, loss 0.000340915, acc 1\n",
      "2018-10-26T16:45:57.308867: step 9241, loss 0.000754942, acc 1\n",
      "2018-10-26T16:45:57.657855: step 9242, loss 0.00293118, acc 1\n",
      "2018-10-26T16:45:57.993788: step 9243, loss 0.00144671, acc 1\n",
      "2018-10-26T16:45:58.303957: step 9244, loss 0.000435331, acc 1\n",
      "2018-10-26T16:45:58.725831: step 9245, loss 0.00151557, acc 1\n",
      "2018-10-26T16:45:59.071905: step 9246, loss 0.00018196, acc 1\n",
      "2018-10-26T16:45:59.398033: step 9247, loss 6.50365e-06, acc 1\n",
      "2018-10-26T16:45:59.730164: step 9248, loss 1.14561e-05, acc 1\n",
      "2018-10-26T16:46:00.083207: step 9249, loss 0.000181872, acc 1\n",
      "2018-10-26T16:46:00.390382: step 9250, loss 0.000157676, acc 1\n",
      "2018-10-26T16:46:00.716510: step 9251, loss 0.000310282, acc 1\n",
      "2018-10-26T16:46:01.037682: step 9252, loss 0.000100499, acc 1\n",
      "2018-10-26T16:46:01.362789: step 9253, loss 0.000116152, acc 1\n",
      "2018-10-26T16:46:01.690906: step 9254, loss 4.60237e-05, acc 1\n",
      "2018-10-26T16:46:02.024018: step 9255, loss 9.10223e-06, acc 1\n",
      "2018-10-26T16:46:02.342168: step 9256, loss 0.000118716, acc 1\n",
      "2018-10-26T16:46:02.667408: step 9257, loss 0.000100128, acc 1\n",
      "2018-10-26T16:46:02.990470: step 9258, loss 7.99865e-05, acc 1\n",
      "2018-10-26T16:46:03.314607: step 9259, loss 0.0011077, acc 1\n",
      "2018-10-26T16:46:03.632800: step 9260, loss 4.68898e-05, acc 1\n",
      "2018-10-26T16:46:03.958849: step 9261, loss 0.000116426, acc 1\n",
      "2018-10-26T16:46:04.281032: step 9262, loss 0.000529392, acc 1\n",
      "2018-10-26T16:46:04.580209: step 9263, loss 0.000864423, acc 1\n",
      "2018-10-26T16:46:04.906316: step 9264, loss 0.00450437, acc 1\n",
      "2018-10-26T16:46:05.242418: step 9265, loss 0.00459041, acc 1\n",
      "2018-10-26T16:46:05.537629: step 9266, loss 9.30489e-06, acc 1\n",
      "2018-10-26T16:46:05.861878: step 9267, loss 4.14853e-05, acc 1\n",
      "2018-10-26T16:46:06.187890: step 9268, loss 0.0019079, acc 1\n",
      "2018-10-26T16:46:06.510078: step 9269, loss 0.00251133, acc 1\n",
      "2018-10-26T16:46:06.860096: step 9270, loss 5.8563e-05, acc 1\n",
      "2018-10-26T16:46:07.165279: step 9271, loss 0.000648557, acc 1\n",
      "2018-10-26T16:46:07.491411: step 9272, loss 0.000161826, acc 1\n",
      "2018-10-26T16:46:07.828510: step 9273, loss 2.90611e-05, acc 1\n",
      "2018-10-26T16:46:08.140748: step 9274, loss 2.25962e-05, acc 1\n",
      "2018-10-26T16:46:08.471975: step 9275, loss 8.08542e-05, acc 1\n",
      "2018-10-26T16:46:08.798918: step 9276, loss 0.00017939, acc 1\n",
      "2018-10-26T16:46:09.117065: step 9277, loss 9.59346e-06, acc 1\n",
      "2018-10-26T16:46:09.438207: step 9278, loss 8.53874e-05, acc 1\n",
      "2018-10-26T16:46:09.738408: step 9279, loss 0.00358722, acc 1\n",
      "2018-10-26T16:46:10.124863: step 9280, loss 0.00460022, acc 1\n",
      "2018-10-26T16:46:10.437703: step 9281, loss 0.000121513, acc 1\n",
      "2018-10-26T16:46:10.785606: step 9282, loss 0.000118377, acc 1\n",
      "2018-10-26T16:46:11.155621: step 9283, loss 4.62589e-05, acc 1\n",
      "2018-10-26T16:46:11.485739: step 9284, loss 8.89132e-05, acc 1\n",
      "2018-10-26T16:46:11.899630: step 9285, loss 0.000270429, acc 1\n",
      "2018-10-26T16:46:12.285602: step 9286, loss 0.000125345, acc 1\n",
      "2018-10-26T16:46:12.646635: step 9287, loss 6.47241e-05, acc 1\n",
      "2018-10-26T16:46:13.027617: step 9288, loss 0.000713891, acc 1\n",
      "2018-10-26T16:46:13.352750: step 9289, loss 6.59343e-05, acc 1\n",
      "2018-10-26T16:46:13.684550: step 9290, loss 0.00191862, acc 1\n",
      "2018-10-26T16:46:14.072824: step 9291, loss 3.80339e-06, acc 1\n",
      "2018-10-26T16:46:14.393968: step 9292, loss 0.000105218, acc 1\n",
      "2018-10-26T16:46:14.754003: step 9293, loss 0.000903305, acc 1\n",
      "2018-10-26T16:46:15.166904: step 9294, loss 0.000154327, acc 1\n",
      "2018-10-26T16:46:15.495025: step 9295, loss 0.00021935, acc 1\n",
      "2018-10-26T16:46:15.874014: step 9296, loss 3.67728e-05, acc 1\n",
      "2018-10-26T16:46:16.218143: step 9297, loss 0.000315276, acc 1\n",
      "2018-10-26T16:46:16.580126: step 9298, loss 0.000343449, acc 1\n",
      "2018-10-26T16:46:16.917229: step 9299, loss 0.0194911, acc 0.984375\n",
      "2018-10-26T16:46:17.274271: step 9300, loss 0.000230854, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:46:18.052191: step 9300, loss 3.20027, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9300\n",
      "\n",
      "2018-10-26T16:46:18.709434: step 9301, loss 9.76851e-05, acc 1\n",
      "2018-10-26T16:46:19.030579: step 9302, loss 0.00461446, acc 1\n",
      "2018-10-26T16:46:19.426728: step 9303, loss 1.69889e-05, acc 1\n",
      "2018-10-26T16:46:19.770665: step 9304, loss 0.000605744, acc 1\n",
      "2018-10-26T16:46:20.094733: step 9305, loss 2.65612e-05, acc 1\n",
      "2018-10-26T16:46:20.385960: step 9306, loss 0.000104001, acc 1\n",
      "2018-10-26T16:46:20.731109: step 9307, loss 3.75974e-05, acc 1\n",
      "2018-10-26T16:46:21.087086: step 9308, loss 0.000801465, acc 1\n",
      "2018-10-26T16:46:21.404236: step 9309, loss 0.000857849, acc 1\n",
      "2018-10-26T16:46:21.737347: step 9310, loss 0.000227709, acc 1\n",
      "2018-10-26T16:46:22.079509: step 9311, loss 2.31199e-05, acc 1\n",
      "2018-10-26T16:46:22.401571: step 9312, loss 2.428e-05, acc 1\n",
      "2018-10-26T16:46:22.752668: step 9313, loss 0.00047074, acc 1\n",
      "2018-10-26T16:46:23.066795: step 9314, loss 9.51939e-05, acc 1\n",
      "2018-10-26T16:46:23.387934: step 9315, loss 0.000679183, acc 1\n",
      "2018-10-26T16:46:23.704091: step 9316, loss 0.00067878, acc 1\n",
      "2018-10-26T16:46:24.014264: step 9317, loss 0.00020375, acc 1\n",
      "2018-10-26T16:46:24.319450: step 9318, loss 0.000462239, acc 1\n",
      "2018-10-26T16:46:24.636603: step 9319, loss 1.56774e-05, acc 1\n",
      "2018-10-26T16:46:24.936926: step 9320, loss 1.52886e-05, acc 1\n",
      "2018-10-26T16:46:25.262067: step 9321, loss 2.37489e-05, acc 1\n",
      "2018-10-26T16:46:25.582075: step 9322, loss 9.32112e-05, acc 1\n",
      "2018-10-26T16:46:25.916207: step 9323, loss 1.11992e-05, acc 1\n",
      "2018-10-26T16:46:26.231383: step 9324, loss 6.88552e-06, acc 1\n",
      "2018-10-26T16:46:26.553481: step 9325, loss 0.000257049, acc 1\n",
      "2018-10-26T16:46:26.863648: step 9326, loss 0.000579208, acc 1\n",
      "2018-10-26T16:46:27.179805: step 9327, loss 0.000118447, acc 1\n",
      "2018-10-26T16:46:27.496956: step 9328, loss 0.000211646, acc 1\n",
      "2018-10-26T16:46:27.822090: step 9329, loss 0.000124648, acc 1\n",
      "2018-10-26T16:46:28.120295: step 9330, loss 0.00102809, acc 1\n",
      "2018-10-26T16:46:28.429617: step 9331, loss 0.000470103, acc 1\n",
      "2018-10-26T16:46:28.735647: step 9332, loss 0.000241414, acc 1\n",
      "2018-10-26T16:46:29.105659: step 9333, loss 2.59453e-05, acc 1\n",
      "2018-10-26T16:46:29.405860: step 9334, loss 0.00010376, acc 1\n",
      "2018-10-26T16:46:29.720018: step 9335, loss 0.00071481, acc 1\n",
      "2018-10-26T16:46:30.073073: step 9336, loss 7.65307e-05, acc 1\n",
      "2018-10-26T16:46:30.372274: step 9337, loss 5.12729e-05, acc 1\n",
      "2018-10-26T16:46:30.681499: step 9338, loss 0.107673, acc 0.96875\n",
      "2018-10-26T16:46:30.999599: step 9339, loss 0.000516631, acc 1\n",
      "2018-10-26T16:46:31.333705: step 9340, loss 6.053e-06, acc 1\n",
      "2018-10-26T16:46:31.683770: step 9341, loss 0.00107666, acc 1\n",
      "2018-10-26T16:46:32.037858: step 9342, loss 9.87453e-06, acc 1\n",
      "2018-10-26T16:46:32.391878: step 9343, loss 6.58865e-05, acc 1\n",
      "2018-10-26T16:46:32.766877: step 9344, loss 0.00215405, acc 1\n",
      "2018-10-26T16:46:33.147858: step 9345, loss 0.00440315, acc 1\n",
      "2018-10-26T16:46:33.453044: step 9346, loss 0.000295102, acc 1\n",
      "2018-10-26T16:46:33.786223: step 9347, loss 4.54083e-06, acc 1\n",
      "2018-10-26T16:46:34.067403: step 9348, loss 7.91122e-05, acc 1\n",
      "2018-10-26T16:46:34.362767: step 9349, loss 1.88862e-05, acc 1\n",
      "2018-10-26T16:46:34.676849: step 9350, loss 1.86085e-05, acc 1\n",
      "2018-10-26T16:46:35.002904: step 9351, loss 0.000224819, acc 1\n",
      "2018-10-26T16:46:35.303101: step 9352, loss 9.90079e-05, acc 1\n",
      "2018-10-26T16:46:35.623246: step 9353, loss 0.00131524, acc 1\n",
      "2018-10-26T16:46:35.940397: step 9354, loss 9.68084e-05, acc 1\n",
      "2018-10-26T16:46:36.271512: step 9355, loss 0.0183623, acc 0.984375\n",
      "2018-10-26T16:46:36.574746: step 9356, loss 1.36804e-05, acc 1\n",
      "2018-10-26T16:46:36.914797: step 9357, loss 4.31915e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:46:37.234940: step 9358, loss 0.000167116, acc 1\n",
      "2018-10-26T16:46:37.553087: step 9359, loss 0.000150456, acc 1\n",
      "2018-10-26T16:46:37.873234: step 9360, loss 0.0010423, acc 1\n",
      "2018-10-26T16:46:38.193378: step 9361, loss 0.0725842, acc 0.984375\n",
      "2018-10-26T16:46:38.520502: step 9362, loss 3.95605e-06, acc 1\n",
      "2018-10-26T16:46:38.828679: step 9363, loss 5.50463e-05, acc 1\n",
      "2018-10-26T16:46:39.127884: step 9364, loss 0.000193862, acc 1\n",
      "2018-10-26T16:46:39.449022: step 9365, loss 1.25691e-05, acc 1\n",
      "2018-10-26T16:46:39.767173: step 9366, loss 1.08963e-06, acc 1\n",
      "2018-10-26T16:46:40.167103: step 9367, loss 0.000556379, acc 1\n",
      "2018-10-26T16:46:40.536118: step 9368, loss 2.44642e-05, acc 1\n",
      "2018-10-26T16:46:40.902142: step 9369, loss 0.00169139, acc 1\n",
      "2018-10-26T16:46:41.276140: step 9370, loss 6.30983e-05, acc 1\n",
      "2018-10-26T16:46:41.625257: step 9371, loss 7.34664e-05, acc 1\n",
      "2018-10-26T16:46:41.938411: step 9372, loss 2.69654e-05, acc 1\n",
      "2018-10-26T16:46:42.271480: step 9373, loss 0.00298959, acc 1\n",
      "2018-10-26T16:46:42.624543: step 9374, loss 0.00136582, acc 1\n",
      "2018-10-26T16:46:43.019482: step 9375, loss 0.000131534, acc 1\n",
      "2018-10-26T16:46:43.388496: step 9376, loss 5.67753e-05, acc 1\n",
      "2018-10-26T16:46:43.811411: step 9377, loss 0.00038432, acc 1\n",
      "2018-10-26T16:46:44.234238: step 9378, loss 0.000378064, acc 1\n",
      "2018-10-26T16:46:44.638202: step 9379, loss 0.000208821, acc 1\n",
      "2018-10-26T16:46:45.052124: step 9380, loss 0.00106623, acc 1\n",
      "2018-10-26T16:46:45.457969: step 9381, loss 6.60306e-05, acc 1\n",
      "2018-10-26T16:46:45.850961: step 9382, loss 3.72859e-05, acc 1\n",
      "2018-10-26T16:46:46.253841: step 9383, loss 0.000477978, acc 1\n",
      "2018-10-26T16:46:46.588944: step 9384, loss 0.00233115, acc 1\n",
      "2018-10-26T16:46:47.007826: step 9385, loss 0.00146888, acc 1\n",
      "2018-10-26T16:46:47.418728: step 9386, loss 0.00592084, acc 1\n",
      "2018-10-26T16:46:47.808708: step 9387, loss 5.54795e-05, acc 1\n",
      "2018-10-26T16:46:48.187674: step 9388, loss 0.000438866, acc 1\n",
      "2018-10-26T16:46:48.619519: step 9389, loss 0.000259237, acc 1\n",
      "2018-10-26T16:46:48.967796: step 9390, loss 0.000186186, acc 1\n",
      "2018-10-26T16:46:49.302696: step 9391, loss 5.48264e-05, acc 1\n",
      "2018-10-26T16:46:49.620895: step 9392, loss 3.98722e-05, acc 1\n",
      "2018-10-26T16:46:49.949122: step 9393, loss 2.21661e-05, acc 1\n",
      "2018-10-26T16:46:50.274098: step 9394, loss 0.050243, acc 0.984375\n",
      "2018-10-26T16:46:50.609300: step 9395, loss 1.18462e-06, acc 1\n",
      "2018-10-26T16:46:50.930391: step 9396, loss 1.16235e-05, acc 1\n",
      "2018-10-26T16:46:51.246554: step 9397, loss 0.00011764, acc 1\n",
      "2018-10-26T16:46:51.582604: step 9398, loss 0.000227421, acc 1\n",
      "2018-10-26T16:46:51.906741: step 9399, loss 0.000145478, acc 1\n",
      "2018-10-26T16:46:52.238850: step 9400, loss 0.000536616, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:46:52.986851: step 9400, loss 3.26061, acc 0.707317\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9400\n",
      "\n",
      "2018-10-26T16:46:53.592240: step 9401, loss 0.00212949, acc 1\n",
      "2018-10-26T16:46:53.914373: step 9402, loss 0.0001256, acc 1\n",
      "2018-10-26T16:46:54.284384: step 9403, loss 4.57757e-05, acc 1\n",
      "2018-10-26T16:46:54.660383: step 9404, loss 0.000193817, acc 1\n",
      "2018-10-26T16:46:54.998478: step 9405, loss 0.00106444, acc 1\n",
      "2018-10-26T16:46:55.281769: step 9406, loss 0.000571665, acc 1\n",
      "2018-10-26T16:46:55.596946: step 9407, loss 8.46694e-06, acc 1\n",
      "2018-10-26T16:46:55.919019: step 9408, loss 1.4131e-05, acc 1\n",
      "2018-10-26T16:46:56.253232: step 9409, loss 0.00242246, acc 1\n",
      "2018-10-26T16:46:56.606183: step 9410, loss 0.0109844, acc 0.984375\n",
      "2018-10-26T16:46:56.928320: step 9411, loss 0.00114763, acc 1\n",
      "2018-10-26T16:46:57.248466: step 9412, loss 0.000207657, acc 1\n",
      "2018-10-26T16:46:57.573596: step 9413, loss 8.70583e-05, acc 1\n",
      "2018-10-26T16:46:57.902718: step 9414, loss 0.00662899, acc 1\n",
      "2018-10-26T16:46:58.233898: step 9415, loss 0.00140576, acc 1\n",
      "2018-10-26T16:46:58.556967: step 9416, loss 0.00181664, acc 1\n",
      "2018-10-26T16:46:58.873132: step 9417, loss 0.000187922, acc 1\n",
      "2018-10-26T16:46:59.204402: step 9418, loss 0.000211685, acc 1\n",
      "2018-10-26T16:46:59.533360: step 9419, loss 1.61301e-06, acc 1\n",
      "2018-10-26T16:46:59.841578: step 9420, loss 0.000222192, acc 1\n",
      "2018-10-26T16:47:00.169660: step 9421, loss 0.000137944, acc 1\n",
      "2018-10-26T16:47:00.489854: step 9422, loss 0.000216308, acc 1\n",
      "2018-10-26T16:47:00.815012: step 9423, loss 0.000700408, acc 1\n",
      "2018-10-26T16:47:01.146053: step 9424, loss 0.000770515, acc 1\n",
      "2018-10-26T16:47:01.485145: step 9425, loss 0.000122495, acc 1\n",
      "2018-10-26T16:47:01.854159: step 9426, loss 0.000350287, acc 1\n",
      "2018-10-26T16:47:02.183282: step 9427, loss 0.000630916, acc 1\n",
      "2018-10-26T16:47:02.540326: step 9428, loss 0.000313627, acc 1\n",
      "2018-10-26T16:47:02.874480: step 9429, loss 0.000185956, acc 1\n",
      "2018-10-26T16:47:03.182611: step 9430, loss 0.0287671, acc 0.984375\n",
      "2018-10-26T16:47:03.503819: step 9431, loss 0.00518655, acc 1\n",
      "2018-10-26T16:47:03.826889: step 9432, loss 0.000250405, acc 1\n",
      "2018-10-26T16:47:04.188922: step 9433, loss 0.0600004, acc 0.984375\n",
      "2018-10-26T16:47:04.488151: step 9434, loss 2.6745e-05, acc 1\n",
      "2018-10-26T16:47:04.805303: step 9435, loss 1.91027e-05, acc 1\n",
      "2018-10-26T16:47:05.118440: step 9436, loss 0.000568719, acc 1\n",
      "2018-10-26T16:47:05.437616: step 9437, loss 0.0059081, acc 1\n",
      "2018-10-26T16:47:05.749750: step 9438, loss 0.000101005, acc 1\n",
      "2018-10-26T16:47:06.071889: step 9439, loss 0.000115161, acc 1\n",
      "2018-10-26T16:47:06.383059: step 9440, loss 4.93155e-05, acc 1\n",
      "2018-10-26T16:47:06.707195: step 9441, loss 1.55481e-05, acc 1\n",
      "2018-10-26T16:47:07.031384: step 9442, loss 0.00016047, acc 1\n",
      "2018-10-26T16:47:07.431306: step 9443, loss 0.001308, acc 1\n",
      "2018-10-26T16:47:07.755393: step 9444, loss 0.000899178, acc 1\n",
      "2018-10-26T16:47:08.099473: step 9445, loss 0.000212517, acc 1\n",
      "2018-10-26T16:47:08.409643: step 9446, loss 7.61604e-06, acc 1\n",
      "2018-10-26T16:47:08.767688: step 9447, loss 0.000865934, acc 1\n",
      "2018-10-26T16:47:09.123763: step 9448, loss 0.000171549, acc 1\n",
      "2018-10-26T16:47:09.451860: step 9449, loss 0.000135705, acc 1\n",
      "2018-10-26T16:47:09.763059: step 9450, loss 1.16239e-05, acc 1\n",
      "2018-10-26T16:47:10.081178: step 9451, loss 0.01846, acc 1\n",
      "2018-10-26T16:47:10.400328: step 9452, loss 0.000295076, acc 1\n",
      "2018-10-26T16:47:10.714486: step 9453, loss 0.0572795, acc 0.984375\n",
      "2018-10-26T16:47:11.037625: step 9454, loss 0.000306555, acc 1\n",
      "2018-10-26T16:47:11.353838: step 9455, loss 0.00026663, acc 1\n",
      "2018-10-26T16:47:11.678908: step 9456, loss 1.49795e-05, acc 1\n",
      "2018-10-26T16:47:12.006034: step 9457, loss 6.61887e-05, acc 1\n",
      "2018-10-26T16:47:12.323189: step 9458, loss 5.86092e-06, acc 1\n",
      "2018-10-26T16:47:12.689211: step 9459, loss 0.00203786, acc 1\n",
      "2018-10-26T16:47:13.018372: step 9460, loss 0.000479096, acc 1\n",
      "2018-10-26T16:47:13.331557: step 9461, loss 1.58944e-05, acc 1\n",
      "2018-10-26T16:47:13.653758: step 9462, loss 0.00130402, acc 1\n",
      "2018-10-26T16:47:13.947847: step 9463, loss 6.97898e-05, acc 1\n",
      "2018-10-26T16:47:14.331881: step 9464, loss 0.000146409, acc 1\n",
      "2018-10-26T16:47:14.694851: step 9465, loss 8.67906e-05, acc 1\n",
      "2018-10-26T16:47:15.015996: step 9466, loss 8.20833e-05, acc 1\n",
      "2018-10-26T16:47:15.349103: step 9467, loss 2.49959e-06, acc 1\n",
      "2018-10-26T16:47:15.655288: step 9468, loss 6.31757e-05, acc 1\n",
      "2018-10-26T16:47:15.983407: step 9469, loss 0.0063614, acc 1\n",
      "2018-10-26T16:47:16.313526: step 9470, loss 1.35997e-05, acc 1\n",
      "2018-10-26T16:47:16.627725: step 9471, loss 1.31077e-05, acc 1\n",
      "2018-10-26T16:47:16.950826: step 9472, loss 0.00010865, acc 1\n",
      "2018-10-26T16:47:17.279945: step 9473, loss 0.00256585, acc 1\n",
      "2018-10-26T16:47:17.605076: step 9474, loss 0.00022427, acc 1\n",
      "2018-10-26T16:47:17.934325: step 9475, loss 0.000394647, acc 1\n",
      "2018-10-26T16:47:18.252346: step 9476, loss 7.16288e-05, acc 1\n",
      "2018-10-26T16:47:18.573487: step 9477, loss 0.000723269, acc 1\n",
      "2018-10-26T16:47:18.871736: step 9478, loss 7.74694e-05, acc 1\n",
      "2018-10-26T16:47:19.188845: step 9479, loss 0.000726226, acc 1\n",
      "2018-10-26T16:47:19.500014: step 9480, loss 0.0190357, acc 0.984375\n",
      "2018-10-26T16:47:19.823150: step 9481, loss 1.80702e-05, acc 1\n",
      "2018-10-26T16:47:20.140364: step 9482, loss 2.51731e-05, acc 1\n",
      "2018-10-26T16:47:20.471417: step 9483, loss 0.000272274, acc 1\n",
      "2018-10-26T16:47:20.799539: step 9484, loss 0.00461247, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:47:21.113700: step 9485, loss 1.6997e-05, acc 1\n",
      "2018-10-26T16:47:21.440826: step 9486, loss 2.94894e-05, acc 1\n",
      "2018-10-26T16:47:21.761967: step 9487, loss 7.695e-06, acc 1\n",
      "2018-10-26T16:47:22.078126: step 9488, loss 0.0174436, acc 0.984375\n",
      "2018-10-26T16:47:22.402317: step 9489, loss 0.000309437, acc 1\n",
      "2018-10-26T16:47:22.711431: step 9490, loss 2.3341e-05, acc 1\n",
      "2018-10-26T16:47:23.048534: step 9491, loss 0.00482115, acc 1\n",
      "2018-10-26T16:47:23.364735: step 9492, loss 7.02463e-05, acc 1\n",
      "2018-10-26T16:47:23.697799: step 9493, loss 0.000133127, acc 1\n",
      "2018-10-26T16:47:24.024923: step 9494, loss 0.000114017, acc 1\n",
      "2018-10-26T16:47:24.344103: step 9495, loss 0.000157233, acc 1\n",
      "2018-10-26T16:47:24.667247: step 9496, loss 0.000160692, acc 1\n",
      "2018-10-26T16:47:25.026398: step 9497, loss 0.00422632, acc 1\n",
      "2018-10-26T16:47:25.348426: step 9498, loss 0.000119953, acc 1\n",
      "2018-10-26T16:47:25.686484: step 9499, loss 0.00143255, acc 1\n",
      "2018-10-26T16:47:26.017631: step 9500, loss 2.92399e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:47:26.799509: step 9500, loss 3.30631, acc 0.703565\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9500\n",
      "\n",
      "2018-10-26T16:47:27.419853: step 9501, loss 0.00201266, acc 1\n",
      "2018-10-26T16:47:27.753962: step 9502, loss 3.05001e-05, acc 1\n",
      "2018-10-26T16:47:28.230703: step 9503, loss 0.00234875, acc 1\n",
      "2018-10-26T16:47:28.572771: step 9504, loss 0.000754804, acc 1\n",
      "2018-10-26T16:47:28.906878: step 9505, loss 4.11462e-05, acc 1\n",
      "2018-10-26T16:47:29.218086: step 9506, loss 0.000948361, acc 1\n",
      "2018-10-26T16:47:29.537194: step 9507, loss 0.000275988, acc 1\n",
      "2018-10-26T16:47:29.859333: step 9508, loss 0.000809621, acc 1\n",
      "2018-10-26T16:47:30.199425: step 9509, loss 0.00315383, acc 1\n",
      "2018-10-26T16:47:30.511646: step 9510, loss 1.78806e-05, acc 1\n",
      "2018-10-26T16:47:30.851954: step 9511, loss 0.000697937, acc 1\n",
      "2018-10-26T16:47:31.171830: step 9512, loss 4.96888e-05, acc 1\n",
      "2018-10-26T16:47:31.498953: step 9513, loss 0.000223064, acc 1\n",
      "2018-10-26T16:47:31.839044: step 9514, loss 0.000441702, acc 1\n",
      "2018-10-26T16:47:32.166170: step 9515, loss 0.000181913, acc 1\n",
      "2018-10-26T16:47:32.481328: step 9516, loss 0.00188397, acc 1\n",
      "2018-10-26T16:47:32.854331: step 9517, loss 0.00127826, acc 1\n",
      "2018-10-26T16:47:33.168495: step 9518, loss 6.00147e-05, acc 1\n",
      "2018-10-26T16:47:33.537597: step 9519, loss 0.025951, acc 0.984375\n",
      "2018-10-26T16:47:33.880106: step 9520, loss 0.000772984, acc 1\n",
      "2018-10-26T16:47:34.210742: step 9521, loss 0.00241552, acc 1\n",
      "2018-10-26T16:47:34.517886: step 9522, loss 6.4293e-06, acc 1\n",
      "2018-10-26T16:47:34.841062: step 9523, loss 0.000227143, acc 1\n",
      "2018-10-26T16:47:35.152193: step 9524, loss 6.47704e-05, acc 1\n",
      "2018-10-26T16:47:35.471424: step 9525, loss 0.000592417, acc 1\n",
      "2018-10-26T16:47:35.803454: step 9526, loss 0.0852011, acc 0.984375\n",
      "2018-10-26T16:47:36.117613: step 9527, loss 8.4035e-05, acc 1\n",
      "2018-10-26T16:47:36.446733: step 9528, loss 9.0498e-05, acc 1\n",
      "2018-10-26T16:47:36.777927: step 9529, loss 9.5991e-06, acc 1\n",
      "2018-10-26T16:47:37.108966: step 9530, loss 0.00156871, acc 1\n",
      "2018-10-26T16:47:37.463018: step 9531, loss 1.47519e-06, acc 1\n",
      "2018-10-26T16:47:37.795130: step 9532, loss 0.000351313, acc 1\n",
      "2018-10-26T16:47:38.131315: step 9533, loss 0.000149048, acc 1\n",
      "2018-10-26T16:47:38.442428: step 9534, loss 0.000869293, acc 1\n",
      "2018-10-26T16:47:38.779504: step 9535, loss 1.83241e-05, acc 1\n",
      "2018-10-26T16:47:39.097650: step 9536, loss 0.0020928, acc 1\n",
      "2018-10-26T16:47:39.421838: step 9537, loss 0.000110609, acc 1\n",
      "2018-10-26T16:47:39.742928: step 9538, loss 0.000168937, acc 1\n",
      "2018-10-26T16:47:40.064070: step 9539, loss 4.41601e-06, acc 1\n",
      "2018-10-26T16:47:40.380226: step 9540, loss 0.000100995, acc 1\n",
      "2018-10-26T16:47:40.693408: step 9541, loss 0.000524738, acc 1\n",
      "2018-10-26T16:47:41.011536: step 9542, loss 1.08134e-05, acc 1\n",
      "2018-10-26T16:47:41.335671: step 9543, loss 0.000971431, acc 1\n",
      "2018-10-26T16:47:41.674765: step 9544, loss 0.00329575, acc 1\n",
      "2018-10-26T16:47:41.993913: step 9545, loss 0.00149531, acc 1\n",
      "2018-10-26T16:47:42.323034: step 9546, loss 0.114502, acc 0.984375\n",
      "2018-10-26T16:47:42.631262: step 9547, loss 6.1682e-06, acc 1\n",
      "2018-10-26T16:47:42.941421: step 9548, loss 0.000887489, acc 1\n",
      "2018-10-26T16:47:43.259531: step 9549, loss 3.15888e-06, acc 1\n",
      "2018-10-26T16:47:43.563719: step 9550, loss 0.0119697, acc 0.984375\n",
      "2018-10-26T16:47:43.893836: step 9551, loss 9.43775e-05, acc 1\n",
      "2018-10-26T16:47:44.218966: step 9552, loss 4.03632e-05, acc 1\n",
      "2018-10-26T16:47:44.553076: step 9553, loss 6.2987e-06, acc 1\n",
      "2018-10-26T16:47:44.869229: step 9554, loss 0.000168031, acc 1\n",
      "2018-10-26T16:47:45.199373: step 9555, loss 7.16287e-05, acc 1\n",
      "2018-10-26T16:47:45.519560: step 9556, loss 0.000416327, acc 1\n",
      "2018-10-26T16:47:45.853600: step 9557, loss 0.00569987, acc 1\n",
      "2018-10-26T16:47:46.215677: step 9558, loss 1.37788e-05, acc 1\n",
      "2018-10-26T16:47:46.573677: step 9559, loss 0.000894613, acc 1\n",
      "2018-10-26T16:47:46.945682: step 9560, loss 0.000125576, acc 1\n",
      "2018-10-26T16:47:47.330704: step 9561, loss 4.90309e-05, acc 1\n",
      "2018-10-26T16:47:47.671783: step 9562, loss 1.2866e-05, acc 1\n",
      "2018-10-26T16:47:48.040757: step 9563, loss 0.00216469, acc 1\n",
      "2018-10-26T16:47:48.392816: step 9564, loss 4.89112e-06, acc 1\n",
      "2018-10-26T16:47:48.762016: step 9565, loss 0.000224275, acc 1\n",
      "2018-10-26T16:47:49.116881: step 9566, loss 5.02282e-05, acc 1\n",
      "2018-10-26T16:47:49.501851: step 9567, loss 0.000198315, acc 1\n",
      "2018-10-26T16:47:49.963625: step 9568, loss 0.000367659, acc 1\n",
      "2018-10-26T16:47:50.422391: step 9569, loss 0.000165845, acc 1\n",
      "2018-10-26T16:47:50.832297: step 9570, loss 0.000648307, acc 1\n",
      "2018-10-26T16:47:51.283128: step 9571, loss 0.000234017, acc 1\n",
      "2018-10-26T16:47:51.710052: step 9572, loss 5.70837e-05, acc 1\n",
      "2018-10-26T16:47:52.059020: step 9573, loss 0.000192586, acc 1\n",
      "2018-10-26T16:47:52.406091: step 9574, loss 0.00304804, acc 1\n",
      "2018-10-26T16:47:52.815178: step 9575, loss 0.000170193, acc 1\n",
      "2018-10-26T16:47:53.208221: step 9576, loss 8.57163e-06, acc 1\n",
      "2018-10-26T16:47:53.653758: step 9577, loss 8.73314e-05, acc 1\n",
      "2018-10-26T16:47:54.026762: step 9578, loss 0.00100239, acc 1\n",
      "2018-10-26T16:47:54.452634: step 9579, loss 0.000385424, acc 1\n",
      "2018-10-26T16:47:54.790722: step 9580, loss 0.00302054, acc 1\n",
      "2018-10-26T16:47:55.158737: step 9581, loss 0.00124336, acc 1\n",
      "2018-10-26T16:47:55.495837: step 9582, loss 0.00034824, acc 1\n",
      "2018-10-26T16:47:55.824956: step 9583, loss 0.00109971, acc 1\n",
      "2018-10-26T16:47:56.155078: step 9584, loss 3.92968e-05, acc 1\n",
      "2018-10-26T16:47:56.486219: step 9585, loss 4.30695e-05, acc 1\n",
      "2018-10-26T16:47:56.822292: step 9586, loss 0.00365458, acc 1\n",
      "2018-10-26T16:47:57.136454: step 9587, loss 0.000344847, acc 1\n",
      "2018-10-26T16:47:57.469563: step 9588, loss 0.000267723, acc 1\n",
      "2018-10-26T16:47:57.784720: step 9589, loss 0.00223209, acc 1\n",
      "2018-10-26T16:47:58.119871: step 9590, loss 1.32563e-05, acc 1\n",
      "2018-10-26T16:47:58.440968: step 9591, loss 8.60791e-06, acc 1\n",
      "2018-10-26T16:47:58.764104: step 9592, loss 0.00349181, acc 1\n",
      "2018-10-26T16:47:59.118271: step 9593, loss 7.40328e-05, acc 1\n",
      "2018-10-26T16:47:59.442291: step 9594, loss 8.00911e-05, acc 1\n",
      "2018-10-26T16:47:59.778395: step 9595, loss 0.00166192, acc 1\n",
      "2018-10-26T16:48:00.154389: step 9596, loss 0.000354146, acc 1\n",
      "2018-10-26T16:48:00.547527: step 9597, loss 0.00274706, acc 1\n",
      "2018-10-26T16:48:00.927340: step 9598, loss 0.00143091, acc 1\n",
      "2018-10-26T16:48:01.239490: step 9599, loss 0.000664878, acc 1\n",
      "2018-10-26T16:48:01.545672: step 9600, loss 0.00127657, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:48:02.577915: step 9600, loss 3.31154, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9600\n",
      "\n",
      "2018-10-26T16:48:03.165344: step 9601, loss 8.00669e-06, acc 1\n",
      "2018-10-26T16:48:03.530369: step 9602, loss 5.43744e-05, acc 1\n",
      "2018-10-26T16:48:04.045995: step 9603, loss 6.81572e-05, acc 1\n",
      "2018-10-26T16:48:04.477993: step 9604, loss 2.9536e-05, acc 1\n",
      "2018-10-26T16:48:04.815936: step 9605, loss 4.21462e-05, acc 1\n",
      "2018-10-26T16:48:05.164109: step 9606, loss 0.000892652, acc 1\n",
      "2018-10-26T16:48:05.496118: step 9607, loss 0.00556716, acc 1\n",
      "2018-10-26T16:48:06.071123: step 9608, loss 0.000380468, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:48:06.401411: step 9609, loss 0.000844809, acc 1\n",
      "2018-10-26T16:48:06.718853: step 9610, loss 4.40848e-05, acc 1\n",
      "2018-10-26T16:48:07.071949: step 9611, loss 1.95962e-05, acc 1\n",
      "2018-10-26T16:48:07.479865: step 9612, loss 0.00212458, acc 1\n",
      "2018-10-26T16:48:07.833871: step 9613, loss 0.000296598, acc 1\n",
      "2018-10-26T16:48:08.208870: step 9614, loss 0.00213938, acc 1\n",
      "2018-10-26T16:48:08.523045: step 9615, loss 0.000924622, acc 1\n",
      "2018-10-26T16:48:08.856217: step 9616, loss 0.000167226, acc 1\n",
      "2018-10-26T16:48:09.208204: step 9617, loss 7.15642e-05, acc 1\n",
      "2018-10-26T16:48:09.628078: step 9618, loss 0.000272932, acc 1\n",
      "2018-10-26T16:48:09.979139: step 9619, loss 0.00357714, acc 1\n",
      "2018-10-26T16:48:10.375382: step 9620, loss 9.98658e-06, acc 1\n",
      "2018-10-26T16:48:10.783991: step 9621, loss 0.00291105, acc 1\n",
      "2018-10-26T16:48:11.208900: step 9622, loss 3.53317e-05, acc 1\n",
      "2018-10-26T16:48:11.585851: step 9623, loss 3.81524e-05, acc 1\n",
      "2018-10-26T16:48:11.922096: step 9624, loss 0.00198927, acc 1\n",
      "2018-10-26T16:48:12.256059: step 9625, loss 0.000437631, acc 1\n",
      "2018-10-26T16:48:12.605124: step 9626, loss 0.00304335, acc 1\n",
      "2018-10-26T16:48:12.961171: step 9627, loss 2.15685e-05, acc 1\n",
      "2018-10-26T16:48:13.317221: step 9628, loss 0.000275143, acc 1\n",
      "2018-10-26T16:48:13.745077: step 9629, loss 2.20418e-05, acc 1\n",
      "2018-10-26T16:48:14.132043: step 9630, loss 0.000252939, acc 1\n",
      "2018-10-26T16:48:14.527985: step 9631, loss 0.000378561, acc 1\n",
      "2018-10-26T16:48:14.874454: step 9632, loss 0.000300144, acc 1\n",
      "2018-10-26T16:48:15.231204: step 9633, loss 0.000289351, acc 1\n",
      "2018-10-26T16:48:15.589153: step 9634, loss 0.0138011, acc 0.984375\n",
      "2018-10-26T16:48:15.956170: step 9635, loss 0.00079018, acc 1\n",
      "2018-10-26T16:48:16.300252: step 9636, loss 0.000163312, acc 1\n",
      "2018-10-26T16:48:16.660290: step 9637, loss 0.000170174, acc 1\n",
      "2018-10-26T16:48:17.092138: step 9638, loss 0.000205946, acc 1\n",
      "2018-10-26T16:48:17.543927: step 9639, loss 5.71403e-05, acc 1\n",
      "2018-10-26T16:48:17.913942: step 9640, loss 2.90208e-05, acc 1\n",
      "2018-10-26T16:48:18.357753: step 9641, loss 1.9911e-06, acc 1\n",
      "2018-10-26T16:48:18.709813: step 9642, loss 0.0283298, acc 0.984375\n",
      "2018-10-26T16:48:19.055887: step 9643, loss 1.22131e-05, acc 1\n",
      "2018-10-26T16:48:19.405955: step 9644, loss 1.65956e-05, acc 1\n",
      "2018-10-26T16:48:19.747045: step 9645, loss 0.00312652, acc 1\n",
      "2018-10-26T16:48:20.103088: step 9646, loss 2.4389e-05, acc 1\n",
      "2018-10-26T16:48:20.446175: step 9647, loss 0.0179896, acc 0.984375\n",
      "2018-10-26T16:48:20.794244: step 9648, loss 0.00205512, acc 1\n",
      "2018-10-26T16:48:21.139321: step 9649, loss 7.33655e-06, acc 1\n",
      "2018-10-26T16:48:21.523820: step 9650, loss 0.00137044, acc 1\n",
      "2018-10-26T16:48:21.869370: step 9651, loss 0.00354135, acc 1\n",
      "2018-10-26T16:48:22.307202: step 9652, loss 0.00302008, acc 1\n",
      "2018-10-26T16:48:22.662301: step 9653, loss 0.000118416, acc 1\n",
      "2018-10-26T16:48:23.040242: step 9654, loss 0.000424611, acc 1\n",
      "2018-10-26T16:48:23.432344: step 9655, loss 0.000140657, acc 1\n",
      "2018-10-26T16:48:23.810185: step 9656, loss 3.9294e-05, acc 1\n",
      "2018-10-26T16:48:24.180197: step 9657, loss 1.16548e-05, acc 1\n",
      "2018-10-26T16:48:24.576139: step 9658, loss 0.00161625, acc 1\n",
      "2018-10-26T16:48:24.923211: step 9659, loss 0.000163682, acc 1\n",
      "2018-10-26T16:48:25.272282: step 9660, loss 9.8545e-05, acc 1\n",
      "2018-10-26T16:48:25.615366: step 9661, loss 5.30597e-05, acc 1\n",
      "2018-10-26T16:48:25.961437: step 9662, loss 0.000164675, acc 1\n",
      "2018-10-26T16:48:26.318486: step 9663, loss 2.80213e-05, acc 1\n",
      "2018-10-26T16:48:26.649600: step 9664, loss 9.77668e-05, acc 1\n",
      "2018-10-26T16:48:27.021605: step 9665, loss 0.000613576, acc 1\n",
      "2018-10-26T16:48:27.360779: step 9666, loss 0.00168945, acc 1\n",
      "2018-10-26T16:48:27.750694: step 9667, loss 0.000529097, acc 1\n",
      "2018-10-26T16:48:28.089752: step 9668, loss 0.0188705, acc 0.984375\n",
      "2018-10-26T16:48:28.438902: step 9669, loss 0.00072607, acc 1\n",
      "2018-10-26T16:48:28.789882: step 9670, loss 0.00259461, acc 1\n",
      "2018-10-26T16:48:29.192807: step 9671, loss 0.000629011, acc 1\n",
      "2018-10-26T16:48:29.554839: step 9672, loss 1.85145e-05, acc 1\n",
      "2018-10-26T16:48:29.906899: step 9673, loss 1.59629e-05, acc 1\n",
      "2018-10-26T16:48:30.241109: step 9674, loss 0.0381143, acc 0.984375\n",
      "2018-10-26T16:48:30.596057: step 9675, loss 0.00107561, acc 1\n",
      "2018-10-26T16:48:30.939140: step 9676, loss 9.97801e-05, acc 1\n",
      "2018-10-26T16:48:31.278233: step 9677, loss 8.03069e-06, acc 1\n",
      "2018-10-26T16:48:31.613337: step 9678, loss 8.90433e-05, acc 1\n",
      "2018-10-26T16:48:31.936926: step 9679, loss 0.000144146, acc 1\n",
      "2018-10-26T16:48:32.289530: step 9680, loss 0.00045791, acc 1\n",
      "2018-10-26T16:48:32.643584: step 9681, loss 0.00172214, acc 1\n",
      "2018-10-26T16:48:32.977691: step 9682, loss 1.90277e-05, acc 1\n",
      "2018-10-26T16:48:33.309803: step 9683, loss 4.97022e-05, acc 1\n",
      "2018-10-26T16:48:33.655881: step 9684, loss 0.00150274, acc 1\n",
      "2018-10-26T16:48:34.009933: step 9685, loss 0.000779369, acc 1\n",
      "2018-10-26T16:48:34.400891: step 9686, loss 7.97456e-05, acc 1\n",
      "2018-10-26T16:48:34.784862: step 9687, loss 0.0022729, acc 1\n",
      "2018-10-26T16:48:35.127948: step 9688, loss 0.000253064, acc 1\n",
      "2018-10-26T16:48:35.477016: step 9689, loss 5.13869e-05, acc 1\n",
      "2018-10-26T16:48:35.859993: step 9690, loss 0.0349827, acc 0.984375\n",
      "2018-10-26T16:48:36.199085: step 9691, loss 0.00172447, acc 1\n",
      "2018-10-26T16:48:36.544197: step 9692, loss 4.37613e-05, acc 1\n",
      "2018-10-26T16:48:36.899213: step 9693, loss 5.96201e-05, acc 1\n",
      "2018-10-26T16:48:37.244293: step 9694, loss 0.00395276, acc 1\n",
      "2018-10-26T16:48:37.579399: step 9695, loss 0.000908658, acc 1\n",
      "2018-10-26T16:48:37.929469: step 9696, loss 5.6948e-05, acc 1\n",
      "2018-10-26T16:48:38.265565: step 9697, loss 0.00137467, acc 1\n",
      "2018-10-26T16:48:38.625602: step 9698, loss 5.27459e-05, acc 1\n",
      "2018-10-26T16:48:39.026530: step 9699, loss 6.5292e-05, acc 1\n",
      "2018-10-26T16:48:39.377593: step 9700, loss 0.0117004, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:48:40.209369: step 9700, loss 3.40332, acc 0.705441\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9700\n",
      "\n",
      "2018-10-26T16:48:40.881620: step 9701, loss 2.22487e-05, acc 1\n",
      "2018-10-26T16:48:41.247596: step 9702, loss 0.00105765, acc 1\n",
      "2018-10-26T16:48:41.669535: step 9703, loss 0.000513901, acc 1\n",
      "2018-10-26T16:48:41.986622: step 9704, loss 0.000320574, acc 1\n",
      "2018-10-26T16:48:42.316740: step 9705, loss 0.00277718, acc 1\n",
      "2018-10-26T16:48:42.639874: step 9706, loss 2.13024e-05, acc 1\n",
      "2018-10-26T16:48:42.968994: step 9707, loss 0.000322025, acc 1\n",
      "2018-10-26T16:48:43.288143: step 9708, loss 3.15907e-05, acc 1\n",
      "2018-10-26T16:48:43.605295: step 9709, loss 2.46064e-05, acc 1\n",
      "2018-10-26T16:48:43.964340: step 9710, loss 6.37323e-05, acc 1\n",
      "2018-10-26T16:48:44.311453: step 9711, loss 0.00236271, acc 1\n",
      "2018-10-26T16:48:44.652501: step 9712, loss 0.0100083, acc 1\n",
      "2018-10-26T16:48:45.017526: step 9713, loss 0.000216897, acc 1\n",
      "2018-10-26T16:48:45.360607: step 9714, loss 0.00116817, acc 1\n",
      "2018-10-26T16:48:45.719646: step 9715, loss 9.93754e-06, acc 1\n",
      "2018-10-26T16:48:46.081866: step 9716, loss 0.00115853, acc 1\n",
      "2018-10-26T16:48:46.413862: step 9717, loss 9.55213e-06, acc 1\n",
      "2018-10-26T16:48:46.753886: step 9718, loss 0.000408562, acc 1\n",
      "2018-10-26T16:48:47.225683: step 9719, loss 0.000305875, acc 1\n",
      "2018-10-26T16:48:47.634529: step 9720, loss 0.000261891, acc 1\n",
      "2018-10-26T16:48:47.957702: step 9721, loss 0.00596697, acc 1\n",
      "2018-10-26T16:48:48.367577: step 9722, loss 0.000192639, acc 1\n",
      "2018-10-26T16:48:48.854271: step 9723, loss 0.000142475, acc 1\n",
      "2018-10-26T16:48:49.280131: step 9724, loss 0.000804132, acc 1\n",
      "2018-10-26T16:48:49.667351: step 9725, loss 0.000148335, acc 1\n",
      "2018-10-26T16:48:50.117373: step 9726, loss 0.000117047, acc 1\n",
      "2018-10-26T16:48:50.468958: step 9727, loss 7.52424e-06, acc 1\n",
      "2018-10-26T16:48:50.857916: step 9728, loss 0.00012954, acc 1\n",
      "2018-10-26T16:48:51.273806: step 9729, loss 0.00162522, acc 1\n",
      "2018-10-26T16:48:51.664762: step 9730, loss 0.000367903, acc 1\n",
      "2018-10-26T16:48:52.008843: step 9731, loss 6.4004e-05, acc 1\n",
      "2018-10-26T16:48:52.523467: step 9732, loss 0.000294069, acc 1\n",
      "2018-10-26T16:48:52.954317: step 9733, loss 0.000103911, acc 1\n",
      "2018-10-26T16:48:53.438058: step 9734, loss 0.00144607, acc 1\n",
      "2018-10-26T16:48:53.850972: step 9735, loss 0.00173771, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:48:54.263861: step 9736, loss 0.00219809, acc 1\n",
      "2018-10-26T16:48:54.764642: step 9737, loss 0.000313048, acc 1\n",
      "2018-10-26T16:48:55.254171: step 9738, loss 0.000184537, acc 1\n",
      "2018-10-26T16:48:55.878504: step 9739, loss 0.000306782, acc 1\n",
      "2018-10-26T16:48:56.580744: step 9740, loss 0.00125906, acc 1\n",
      "2018-10-26T16:48:57.189999: step 9741, loss 9.08336e-06, acc 1\n",
      "2018-10-26T16:48:57.844250: step 9742, loss 0.00797858, acc 1\n",
      "2018-10-26T16:48:58.375831: step 9743, loss 0.00675639, acc 1\n",
      "2018-10-26T16:48:58.799697: step 9744, loss 4.4456e-05, acc 1\n",
      "2018-10-26T16:48:59.220738: step 9745, loss 2.94287e-06, acc 1\n",
      "2018-10-26T16:48:59.669374: step 9746, loss 7.18376e-05, acc 1\n",
      "2018-10-26T16:49:00.149107: step 9747, loss 3.97944e-05, acc 1\n",
      "2018-10-26T16:49:00.557999: step 9748, loss 9.10827e-07, acc 1\n",
      "2018-10-26T16:49:00.938211: step 9749, loss 3.2638e-05, acc 1\n",
      "2018-10-26T16:49:01.263115: step 9750, loss 4.81451e-05, acc 1\n",
      "2018-10-26T16:49:01.617169: step 9751, loss 4.55815e-05, acc 1\n",
      "2018-10-26T16:49:02.005135: step 9752, loss 3.93549e-06, acc 1\n",
      "2018-10-26T16:49:02.407332: step 9753, loss 5.6636e-06, acc 1\n",
      "2018-10-26T16:49:02.774171: step 9754, loss 5.15245e-05, acc 1\n",
      "2018-10-26T16:49:03.142097: step 9755, loss 0.000237733, acc 1\n",
      "2018-10-26T16:49:03.505126: step 9756, loss 1.52665e-05, acc 1\n",
      "2018-10-26T16:49:03.889102: step 9757, loss 0.00105335, acc 1\n",
      "2018-10-26T16:49:04.249137: step 9758, loss 4.07716e-05, acc 1\n",
      "2018-10-26T16:49:04.636105: step 9759, loss 0.000330932, acc 1\n",
      "2018-10-26T16:49:05.126806: step 9760, loss 5.98755e-06, acc 1\n",
      "2018-10-26T16:49:05.481845: step 9761, loss 8.37143e-05, acc 1\n",
      "2018-10-26T16:49:05.914737: step 9762, loss 0.00028062, acc 1\n",
      "2018-10-26T16:49:06.250858: step 9763, loss 0.00021516, acc 1\n",
      "2018-10-26T16:49:06.604844: step 9764, loss 0.0105722, acc 1\n",
      "2018-10-26T16:49:06.938950: step 9765, loss 0.000897699, acc 1\n",
      "2018-10-26T16:49:07.284029: step 9766, loss 4.51404e-05, acc 1\n",
      "2018-10-26T16:49:07.627163: step 9767, loss 0.000996851, acc 1\n",
      "2018-10-26T16:49:08.095864: step 9768, loss 1.44182e-05, acc 1\n",
      "2018-10-26T16:49:08.446923: step 9769, loss 0.0256128, acc 0.984375\n",
      "2018-10-26T16:49:08.899713: step 9770, loss 0.049225, acc 0.984375\n",
      "2018-10-26T16:49:09.263739: step 9771, loss 0.000158791, acc 1\n",
      "2018-10-26T16:49:09.609818: step 9772, loss 0.000249833, acc 1\n",
      "2018-10-26T16:49:09.946917: step 9773, loss 2.72818e-05, acc 1\n",
      "2018-10-26T16:49:10.377766: step 9774, loss 1.38037e-05, acc 1\n",
      "2018-10-26T16:49:10.900366: step 9775, loss 0.000260579, acc 1\n",
      "2018-10-26T16:49:11.324235: step 9776, loss 0.000474895, acc 1\n",
      "2018-10-26T16:49:11.727158: step 9777, loss 0.000408477, acc 1\n",
      "2018-10-26T16:49:12.096172: step 9778, loss 0.000569345, acc 1\n",
      "2018-10-26T16:49:12.424294: step 9779, loss 0.000221985, acc 1\n",
      "2018-10-26T16:49:12.814252: step 9780, loss 0.117515, acc 0.984375\n",
      "2018-10-26T16:49:13.164319: step 9781, loss 0.000191022, acc 1\n",
      "2018-10-26T16:49:13.529344: step 9782, loss 0.00194357, acc 1\n",
      "2018-10-26T16:49:13.915315: step 9783, loss 0.00156183, acc 1\n",
      "2018-10-26T16:49:14.249418: step 9784, loss 1.20221e-05, acc 1\n",
      "2018-10-26T16:49:14.603474: step 9785, loss 0.0203872, acc 0.984375\n",
      "2018-10-26T16:49:14.938577: step 9786, loss 0.000265118, acc 1\n",
      "2018-10-26T16:49:15.319559: step 9787, loss 0.00181114, acc 1\n",
      "2018-10-26T16:49:15.643693: step 9788, loss 0.000765152, acc 1\n",
      "2018-10-26T16:49:15.984918: step 9789, loss 0.00631947, acc 1\n",
      "2018-10-26T16:49:16.336841: step 9790, loss 8.8678e-05, acc 1\n",
      "2018-10-26T16:49:16.786643: step 9791, loss 0.000570965, acc 1\n",
      "2018-10-26T16:49:17.143686: step 9792, loss 0.00197493, acc 1\n",
      "2018-10-26T16:49:17.497775: step 9793, loss 0.00070497, acc 1\n",
      "2018-10-26T16:49:17.843816: step 9794, loss 0.000530454, acc 1\n",
      "2018-10-26T16:49:18.225794: step 9795, loss 1.04243e-05, acc 1\n",
      "2018-10-26T16:49:18.595183: step 9796, loss 0.00050063, acc 1\n",
      "2018-10-26T16:49:18.954846: step 9797, loss 0.000339619, acc 1\n",
      "2018-10-26T16:49:19.314889: step 9798, loss 0.00540386, acc 1\n",
      "2018-10-26T16:49:19.648991: step 9799, loss 0.000391398, acc 1\n",
      "2018-10-26T16:49:20.002048: step 9800, loss 0.000655019, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:49:20.847816: step 9800, loss 3.40831, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9800\n",
      "\n",
      "2018-10-26T16:49:21.456164: step 9801, loss 6.06372e-05, acc 1\n",
      "2018-10-26T16:49:21.783313: step 9802, loss 8.01239e-05, acc 1\n",
      "2018-10-26T16:49:22.293391: step 9803, loss 3.11599e-05, acc 1\n",
      "2018-10-26T16:49:22.728762: step 9804, loss 0.000373147, acc 1\n",
      "2018-10-26T16:49:23.065942: step 9805, loss 3.27003e-05, acc 1\n",
      "2018-10-26T16:49:23.398975: step 9806, loss 2.63861e-05, acc 1\n",
      "2018-10-26T16:49:23.759579: step 9807, loss 2.86637e-05, acc 1\n",
      "2018-10-26T16:49:24.220813: step 9808, loss 0.000103147, acc 1\n",
      "2018-10-26T16:49:24.587832: step 9809, loss 0.000275123, acc 1\n",
      "2018-10-26T16:49:25.025888: step 9810, loss 7.92641e-05, acc 1\n",
      "2018-10-26T16:49:25.375692: step 9811, loss 0.000774804, acc 1\n",
      "2018-10-26T16:49:25.773702: step 9812, loss 0.000171415, acc 1\n",
      "2018-10-26T16:49:26.310240: step 9813, loss 0.00268932, acc 1\n",
      "2018-10-26T16:49:26.650285: step 9814, loss 7.00495e-05, acc 1\n",
      "2018-10-26T16:49:27.069168: step 9815, loss 0.000638404, acc 1\n",
      "2018-10-26T16:49:27.427209: step 9816, loss 0.000289933, acc 1\n",
      "2018-10-26T16:49:27.769295: step 9817, loss 2.241e-05, acc 1\n",
      "2018-10-26T16:49:28.117365: step 9818, loss 4.66937e-06, acc 1\n",
      "2018-10-26T16:49:28.442496: step 9819, loss 6.77012e-06, acc 1\n",
      "2018-10-26T16:49:28.767632: step 9820, loss 0.00227869, acc 1\n",
      "2018-10-26T16:49:29.118693: step 9821, loss 3.03604e-06, acc 1\n",
      "2018-10-26T16:49:29.453798: step 9822, loss 5.40381e-05, acc 1\n",
      "2018-10-26T16:49:29.797875: step 9823, loss 0.000179447, acc 1\n",
      "2018-10-26T16:49:30.113058: step 9824, loss 8.19665e-06, acc 1\n",
      "2018-10-26T16:49:30.528923: step 9825, loss 2.50009e-05, acc 1\n",
      "2018-10-26T16:49:30.880985: step 9826, loss 0.0174008, acc 0.984375\n",
      "2018-10-26T16:49:31.271982: step 9827, loss 3.42215e-05, acc 1\n",
      "2018-10-26T16:49:31.635967: step 9828, loss 0.00386396, acc 1\n",
      "2018-10-26T16:49:31.982040: step 9829, loss 0.000493666, acc 1\n",
      "2018-10-26T16:49:32.309167: step 9830, loss 0.00033551, acc 1\n",
      "2018-10-26T16:49:32.666212: step 9831, loss 1.17152e-05, acc 1\n",
      "2018-10-26T16:49:33.055173: step 9832, loss 0.000251266, acc 1\n",
      "2018-10-26T16:49:33.402246: step 9833, loss 1.80089e-05, acc 1\n",
      "2018-10-26T16:49:33.741419: step 9834, loss 4.36939e-05, acc 1\n",
      "2018-10-26T16:49:34.102374: step 9835, loss 0.000555679, acc 1\n",
      "2018-10-26T16:49:34.433489: step 9836, loss 0.000407303, acc 1\n",
      "2018-10-26T16:49:34.784555: step 9837, loss 8.79135e-06, acc 1\n",
      "2018-10-26T16:49:35.123645: step 9838, loss 0.0220002, acc 0.984375\n",
      "2018-10-26T16:49:35.465813: step 9839, loss 0.00450026, acc 1\n",
      "2018-10-26T16:49:35.790864: step 9840, loss 2.54171e-05, acc 1\n",
      "2018-10-26T16:49:36.155887: step 9841, loss 0.00391499, acc 1\n",
      "2018-10-26T16:49:36.586738: step 9842, loss 4.29103e-06, acc 1\n",
      "2018-10-26T16:49:36.968719: step 9843, loss 0.000365536, acc 1\n",
      "2018-10-26T16:49:37.314793: step 9844, loss 0.00109759, acc 1\n",
      "2018-10-26T16:49:37.670841: step 9845, loss 0.000246456, acc 1\n",
      "2018-10-26T16:49:38.014919: step 9846, loss 0.000121501, acc 1\n",
      "2018-10-26T16:49:38.389922: step 9847, loss 0.000281628, acc 1\n",
      "2018-10-26T16:49:38.757939: step 9848, loss 0.00223065, acc 1\n",
      "2018-10-26T16:49:39.108004: step 9849, loss 9.18802e-05, acc 1\n",
      "2018-10-26T16:49:39.443105: step 9850, loss 0.00025002, acc 1\n",
      "2018-10-26T16:49:39.788184: step 9851, loss 3.53702e-05, acc 1\n",
      "2018-10-26T16:49:40.177144: step 9852, loss 0.000285477, acc 1\n",
      "2018-10-26T16:49:40.576077: step 9853, loss 0.00952667, acc 1\n",
      "2018-10-26T16:49:40.982992: step 9854, loss 3.42725e-07, acc 1\n",
      "2018-10-26T16:49:41.365968: step 9855, loss 0.00247362, acc 1\n",
      "2018-10-26T16:49:41.737024: step 9856, loss 0.00170611, acc 1\n",
      "2018-10-26T16:49:42.101999: step 9857, loss 0.000124233, acc 1\n",
      "2018-10-26T16:49:42.438105: step 9858, loss 0.00280822, acc 1\n",
      "2018-10-26T16:49:42.831094: step 9859, loss 5.55243e-05, acc 1\n",
      "2018-10-26T16:49:43.154189: step 9860, loss 0.000473147, acc 1\n",
      "2018-10-26T16:49:43.506252: step 9861, loss 0.00437851, acc 1\n",
      "2018-10-26T16:49:43.835370: step 9862, loss 9.81854e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:49:44.217384: step 9863, loss 3.57954e-05, acc 1\n",
      "2018-10-26T16:49:44.629250: step 9864, loss 0.000193464, acc 1\n",
      "2018-10-26T16:49:44.962360: step 9865, loss 5.81624e-05, acc 1\n",
      "2018-10-26T16:49:45.280508: step 9866, loss 0.000308558, acc 1\n",
      "2018-10-26T16:49:45.635562: step 9867, loss 0.000173183, acc 1\n",
      "2018-10-26T16:49:45.983645: step 9868, loss 0.0272018, acc 0.984375\n",
      "2018-10-26T16:49:46.290808: step 9869, loss 0.00322428, acc 1\n",
      "2018-10-26T16:49:46.635887: step 9870, loss 6.28913e-05, acc 1\n",
      "2018-10-26T16:49:47.087679: step 9871, loss 6.0661e-05, acc 1\n",
      "2018-10-26T16:49:47.464674: step 9872, loss 0.000722793, acc 1\n",
      "2018-10-26T16:49:47.864605: step 9873, loss 0.000159835, acc 1\n",
      "2018-10-26T16:49:48.215667: step 9874, loss 1.09157e-05, acc 1\n",
      "2018-10-26T16:49:48.556754: step 9875, loss 0.000315116, acc 1\n",
      "2018-10-26T16:49:48.851968: step 9876, loss 0.0208143, acc 0.984375\n",
      "2018-10-26T16:49:49.187071: step 9877, loss 0.000320366, acc 1\n",
      "2018-10-26T16:49:49.531203: step 9878, loss 0.000492664, acc 1\n",
      "2018-10-26T16:49:49.874234: step 9879, loss 0.000644843, acc 1\n",
      "2018-10-26T16:49:50.204353: step 9880, loss 0.0032365, acc 1\n",
      "2018-10-26T16:49:50.530480: step 9881, loss 0.0049113, acc 1\n",
      "2018-10-26T16:49:50.866583: step 9882, loss 0.0012458, acc 1\n",
      "2018-10-26T16:49:51.171834: step 9883, loss 0.040906, acc 0.984375\n",
      "2018-10-26T16:49:51.488920: step 9884, loss 0.0165582, acc 0.984375\n",
      "2018-10-26T16:49:51.852948: step 9885, loss 0.0283487, acc 0.984375\n",
      "2018-10-26T16:49:52.167110: step 9886, loss 0.00912222, acc 1\n",
      "2018-10-26T16:49:52.468332: step 9887, loss 2.61696e-06, acc 1\n",
      "2018-10-26T16:49:52.785455: step 9888, loss 0.0365397, acc 0.984375\n",
      "2018-10-26T16:49:53.152503: step 9889, loss 5.32491e-06, acc 1\n",
      "2018-10-26T16:49:53.486586: step 9890, loss 6.54828e-05, acc 1\n",
      "2018-10-26T16:49:53.801869: step 9891, loss 0.000442039, acc 1\n",
      "2018-10-26T16:49:54.208656: step 9892, loss 0.00477525, acc 1\n",
      "2018-10-26T16:49:54.620587: step 9893, loss 9.86298e-06, acc 1\n",
      "2018-10-26T16:49:54.930724: step 9894, loss 7.32113e-05, acc 1\n",
      "2018-10-26T16:49:55.276207: step 9895, loss 0.00122282, acc 1\n",
      "2018-10-26T16:49:55.733578: step 9896, loss 6.2701e-05, acc 1\n",
      "2018-10-26T16:49:56.072675: step 9897, loss 0.000259101, acc 1\n",
      "2018-10-26T16:49:56.429720: step 9898, loss 0.0030113, acc 1\n",
      "2018-10-26T16:49:56.792750: step 9899, loss 2.94101e-06, acc 1\n",
      "2018-10-26T16:49:57.121871: step 9900, loss 0.000357887, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:49:58.000569: step 9900, loss 3.53878, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-9900\n",
      "\n",
      "2018-10-26T16:49:58.622860: step 9901, loss 1.13745e-05, acc 1\n",
      "2018-10-26T16:49:59.035756: step 9902, loss 9.63348e-05, acc 1\n",
      "2018-10-26T16:49:59.509489: step 9903, loss 9.48989e-05, acc 1\n",
      "2018-10-26T16:50:00.067998: step 9904, loss 0.00182551, acc 1\n",
      "2018-10-26T16:50:00.458953: step 9905, loss 0.000453097, acc 1\n",
      "2018-10-26T16:50:00.844922: step 9906, loss 1.38309e-05, acc 1\n",
      "2018-10-26T16:50:01.277766: step 9907, loss 4.04193e-07, acc 1\n",
      "2018-10-26T16:50:01.761474: step 9908, loss 0.000140149, acc 1\n",
      "2018-10-26T16:50:02.236204: step 9909, loss 3.60127e-05, acc 1\n",
      "2018-10-26T16:50:02.646160: step 9910, loss 0.00415928, acc 1\n",
      "2018-10-26T16:50:03.026094: step 9911, loss 2.94659e-06, acc 1\n",
      "2018-10-26T16:50:03.453952: step 9912, loss 0.000186749, acc 1\n",
      "2018-10-26T16:50:03.902752: step 9913, loss 1.918e-05, acc 1\n",
      "2018-10-26T16:50:04.381472: step 9914, loss 0.000239484, acc 1\n",
      "2018-10-26T16:50:04.789383: step 9915, loss 0.00260158, acc 1\n",
      "2018-10-26T16:50:05.205273: step 9916, loss 0.00144551, acc 1\n",
      "2018-10-26T16:50:05.674085: step 9917, loss 0.000242736, acc 1\n",
      "2018-10-26T16:50:06.093901: step 9918, loss 0.00169937, acc 1\n",
      "2018-10-26T16:50:06.442463: step 9919, loss 0.0366688, acc 0.984375\n",
      "2018-10-26T16:50:06.806124: step 9920, loss 0.00341015, acc 1\n",
      "2018-10-26T16:50:07.164038: step 9921, loss 6.83551e-05, acc 1\n",
      "2018-10-26T16:50:07.492162: step 9922, loss 0.0235731, acc 0.984375\n",
      "2018-10-26T16:50:07.848210: step 9923, loss 3.32279e-06, acc 1\n",
      "2018-10-26T16:50:08.239166: step 9924, loss 1.48509e-05, acc 1\n",
      "2018-10-26T16:50:08.599205: step 9925, loss 5.61327e-06, acc 1\n",
      "2018-10-26T16:50:09.030054: step 9926, loss 4.10704e-06, acc 1\n",
      "2018-10-26T16:50:09.372226: step 9927, loss 0.000373247, acc 1\n",
      "2018-10-26T16:50:09.712230: step 9928, loss 6.27241e-05, acc 1\n",
      "2018-10-26T16:50:10.066284: step 9929, loss 0.00077316, acc 1\n",
      "2018-10-26T16:50:10.495141: step 9930, loss 0.0221266, acc 0.984375\n",
      "2018-10-26T16:50:10.872132: step 9931, loss 0.000226868, acc 1\n",
      "2018-10-26T16:50:11.244136: step 9932, loss 0.000190695, acc 1\n",
      "2018-10-26T16:50:11.649056: step 9933, loss 0.000934337, acc 1\n",
      "2018-10-26T16:50:11.966208: step 9934, loss 0.000149839, acc 1\n",
      "2018-10-26T16:50:12.308324: step 9935, loss 0.000211371, acc 1\n",
      "2018-10-26T16:50:12.638535: step 9936, loss 7.61029e-06, acc 1\n",
      "2018-10-26T16:50:12.987478: step 9937, loss 2.96784e-05, acc 1\n",
      "2018-10-26T16:50:13.367464: step 9938, loss 1.92582e-05, acc 1\n",
      "2018-10-26T16:50:13.774376: step 9939, loss 0.000185455, acc 1\n",
      "2018-10-26T16:50:14.116463: step 9940, loss 0.0197099, acc 0.984375\n",
      "2018-10-26T16:50:14.463536: step 9941, loss 3.92169e-05, acc 1\n",
      "2018-10-26T16:50:14.825567: step 9942, loss 0.000371698, acc 1\n",
      "2018-10-26T16:50:15.130752: step 9943, loss 6.9662e-05, acc 1\n",
      "2018-10-26T16:50:15.421974: step 9944, loss 4.01811e-05, acc 1\n",
      "2018-10-26T16:50:15.712526: step 9945, loss 0.00158168, acc 1\n",
      "2018-10-26T16:50:16.031453: step 9946, loss 0.00169493, acc 1\n",
      "2018-10-26T16:50:16.363459: step 9947, loss 0.000285488, acc 1\n",
      "2018-10-26T16:50:16.742446: step 9948, loss 1.04144e-05, acc 1\n",
      "2018-10-26T16:50:17.206208: step 9949, loss 3.85102e-05, acc 1\n",
      "2018-10-26T16:50:17.544305: step 9950, loss 0.000792258, acc 1\n",
      "2018-10-26T16:50:17.965179: step 9951, loss 0.000231324, acc 1\n",
      "2018-10-26T16:50:18.307305: step 9952, loss 0.000541061, acc 1\n",
      "2018-10-26T16:50:18.643372: step 9953, loss 0.0114616, acc 0.984375\n",
      "2018-10-26T16:50:18.946567: step 9954, loss 7.41097e-06, acc 1\n",
      "2018-10-26T16:50:19.248749: step 9955, loss 0.000758866, acc 1\n",
      "2018-10-26T16:50:19.534984: step 9956, loss 0.000106219, acc 1\n",
      "2018-10-26T16:50:19.819225: step 9957, loss 3.19018e-05, acc 1\n",
      "2018-10-26T16:50:20.144357: step 9958, loss 0.000158498, acc 1\n",
      "2018-10-26T16:50:20.450539: step 9959, loss 4.52357e-05, acc 1\n",
      "2018-10-26T16:50:20.843492: step 9960, loss 0.000202129, acc 1\n",
      "2018-10-26T16:50:21.172644: step 9961, loss 0.000654183, acc 1\n",
      "2018-10-26T16:50:21.519684: step 9962, loss 0.000372184, acc 1\n",
      "2018-10-26T16:50:21.887700: step 9963, loss 0.000103409, acc 1\n",
      "2018-10-26T16:50:22.224798: step 9964, loss 3.3165e-05, acc 1\n",
      "2018-10-26T16:50:22.550989: step 9965, loss 1.15584e-05, acc 1\n",
      "2018-10-26T16:50:22.885161: step 9966, loss 0.00030365, acc 1\n",
      "2018-10-26T16:50:23.230112: step 9967, loss 0.000286535, acc 1\n",
      "2018-10-26T16:50:23.584230: step 9968, loss 0.000141095, acc 1\n",
      "2018-10-26T16:50:24.133697: step 9969, loss 0.000108457, acc 1\n",
      "2018-10-26T16:50:24.499720: step 9970, loss 2.58574e-05, acc 1\n",
      "2018-10-26T16:50:24.856766: step 9971, loss 0.00638338, acc 1\n",
      "2018-10-26T16:50:25.226783: step 9972, loss 0.000281016, acc 1\n",
      "2018-10-26T16:50:25.529966: step 9973, loss 0.00108066, acc 1\n",
      "2018-10-26T16:50:25.862079: step 9974, loss 0.000809684, acc 1\n",
      "2018-10-26T16:50:26.222118: step 9975, loss 0.000259466, acc 1\n",
      "2018-10-26T16:50:26.659950: step 9976, loss 0.0500032, acc 0.984375\n",
      "2018-10-26T16:50:27.017993: step 9977, loss 0.00124176, acc 1\n",
      "2018-10-26T16:50:27.402963: step 9978, loss 5.29526e-06, acc 1\n",
      "2018-10-26T16:50:27.789930: step 9979, loss 0.00306682, acc 1\n",
      "2018-10-26T16:50:28.142987: step 9980, loss 0.00266947, acc 1\n",
      "2018-10-26T16:50:28.485072: step 9981, loss 0.00111036, acc 1\n",
      "2018-10-26T16:50:28.896973: step 9982, loss 0.00060545, acc 1\n",
      "2018-10-26T16:50:29.210136: step 9983, loss 0.000211472, acc 1\n",
      "2018-10-26T16:50:29.583138: step 9984, loss 3.17288e-05, acc 1\n",
      "2018-10-26T16:50:29.948251: step 9985, loss 6.18581e-05, acc 1\n",
      "2018-10-26T16:50:30.321233: step 9986, loss 0.00291049, acc 1\n",
      "2018-10-26T16:50:30.658266: step 9987, loss 0.000515416, acc 1\n",
      "2018-10-26T16:50:30.998423: step 9988, loss 0.00426131, acc 1\n",
      "2018-10-26T16:50:31.358397: step 9989, loss 1.09788e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:50:31.760320: step 9990, loss 7.55412e-05, acc 1\n",
      "2018-10-26T16:50:32.098417: step 9991, loss 0.000665944, acc 1\n",
      "2018-10-26T16:50:32.436551: step 9992, loss 0.00038073, acc 1\n",
      "2018-10-26T16:50:32.816500: step 9993, loss 0.000196772, acc 1\n",
      "2018-10-26T16:50:33.135649: step 9994, loss 6.84104e-06, acc 1\n",
      "2018-10-26T16:50:33.486708: step 9995, loss 0.000163063, acc 1\n",
      "2018-10-26T16:50:33.870683: step 9996, loss 0.000997304, acc 1\n",
      "2018-10-26T16:50:34.200801: step 9997, loss 4.25261e-05, acc 1\n",
      "2018-10-26T16:50:34.539139: step 9998, loss 2.34553e-05, acc 1\n",
      "2018-10-26T16:50:34.872006: step 9999, loss 0.00163209, acc 1\n",
      "2018-10-26T16:50:35.221119: step 10000, loss 2.16637e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:50:36.000032: step 10000, loss 3.47376, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10000\n",
      "\n",
      "2018-10-26T16:50:36.727052: step 10001, loss 5.5941e-05, acc 1\n",
      "2018-10-26T16:50:37.182839: step 10002, loss 7.12347e-05, acc 1\n",
      "2018-10-26T16:50:37.673525: step 10003, loss 0.000125123, acc 1\n",
      "2018-10-26T16:50:38.023588: step 10004, loss 0.000403594, acc 1\n",
      "2018-10-26T16:50:38.375649: step 10005, loss 7.40924e-05, acc 1\n",
      "2018-10-26T16:50:38.693863: step 10006, loss 0.0013734, acc 1\n",
      "2018-10-26T16:50:38.994991: step 10007, loss 4.14985e-06, acc 1\n",
      "2018-10-26T16:50:39.298281: step 10008, loss 0.000563459, acc 1\n",
      "2018-10-26T16:50:39.678183: step 10009, loss 0.00174796, acc 1\n",
      "2018-10-26T16:50:40.037206: step 10010, loss 4.53781e-05, acc 1\n",
      "2018-10-26T16:50:40.369318: step 10011, loss 6.17724e-05, acc 1\n",
      "2018-10-26T16:50:40.684476: step 10012, loss 9.51882e-05, acc 1\n",
      "2018-10-26T16:50:41.043517: step 10013, loss 2.41806e-05, acc 1\n",
      "2018-10-26T16:50:41.386600: step 10014, loss 0.00015056, acc 1\n",
      "2018-10-26T16:50:41.800528: step 10015, loss 0.00109802, acc 1\n",
      "2018-10-26T16:50:42.176539: step 10016, loss 0.000191734, acc 1\n",
      "2018-10-26T16:50:42.572443: step 10017, loss 0.000290166, acc 1\n",
      "2018-10-26T16:50:42.903550: step 10018, loss 4.32859e-06, acc 1\n",
      "2018-10-26T16:50:43.279551: step 10019, loss 0.000986005, acc 1\n",
      "2018-10-26T16:50:43.664527: step 10020, loss 0.000439139, acc 1\n",
      "2018-10-26T16:50:44.046499: step 10021, loss 1.56004e-05, acc 1\n",
      "2018-10-26T16:50:44.474351: step 10022, loss 0.000437752, acc 1\n",
      "2018-10-26T16:50:44.791554: step 10023, loss 0.00118784, acc 1\n",
      "2018-10-26T16:50:45.194426: step 10024, loss 0.00107153, acc 1\n",
      "2018-10-26T16:50:45.525545: step 10025, loss 0.000142574, acc 1\n",
      "2018-10-26T16:50:45.908589: step 10026, loss 0.000384915, acc 1\n",
      "2018-10-26T16:50:46.325408: step 10027, loss 0.000617315, acc 1\n",
      "2018-10-26T16:50:46.874148: step 10028, loss 4.42175e-06, acc 1\n",
      "2018-10-26T16:50:47.224006: step 10029, loss 0.00107061, acc 1\n",
      "2018-10-26T16:50:47.586036: step 10030, loss 0.000404535, acc 1\n",
      "2018-10-26T16:50:47.934113: step 10031, loss 0.0393286, acc 0.984375\n",
      "2018-10-26T16:50:48.271255: step 10032, loss 1.63441e-05, acc 1\n",
      "2018-10-26T16:50:48.598362: step 10033, loss 7.32143e-05, acc 1\n",
      "2018-10-26T16:50:48.936430: step 10034, loss 0.000181715, acc 1\n",
      "2018-10-26T16:50:49.262560: step 10035, loss 0.000402593, acc 1\n",
      "2018-10-26T16:50:49.621599: step 10036, loss 0.000177732, acc 1\n",
      "2018-10-26T16:50:49.976653: step 10037, loss 0.000997982, acc 1\n",
      "2018-10-26T16:50:50.379575: step 10038, loss 9.3006e-06, acc 1\n",
      "2018-10-26T16:50:50.727670: step 10039, loss 6.63266e-05, acc 1\n",
      "2018-10-26T16:50:51.096656: step 10040, loss 1.55283e-05, acc 1\n",
      "2018-10-26T16:50:51.421832: step 10041, loss 1.52473e-05, acc 1\n",
      "2018-10-26T16:50:51.754898: step 10042, loss 0.00326363, acc 1\n",
      "2018-10-26T16:50:52.069059: step 10043, loss 0.000122889, acc 1\n",
      "2018-10-26T16:50:52.456028: step 10044, loss 0.000197038, acc 1\n",
      "2018-10-26T16:50:52.839001: step 10045, loss 1.32627e-05, acc 1\n",
      "2018-10-26T16:50:53.201077: step 10046, loss 0.0055314, acc 1\n",
      "2018-10-26T16:50:53.545115: step 10047, loss 1.51614e-06, acc 1\n",
      "2018-10-26T16:50:53.916124: step 10048, loss 0.000101504, acc 1\n",
      "2018-10-26T16:50:54.238392: step 10049, loss 0.00530056, acc 1\n",
      "2018-10-26T16:50:54.595346: step 10050, loss 2.61205e-05, acc 1\n",
      "2018-10-26T16:50:54.900034: step 10051, loss 0.00305944, acc 1\n",
      "2018-10-26T16:50:55.210706: step 10052, loss 0.0110301, acc 0.984375\n",
      "2018-10-26T16:50:55.552752: step 10053, loss 0.000308231, acc 1\n",
      "2018-10-26T16:50:55.894909: step 10054, loss 0.000150841, acc 1\n",
      "2018-10-26T16:50:56.243906: step 10055, loss 0.000104418, acc 1\n",
      "2018-10-26T16:50:56.607932: step 10056, loss 0.0106662, acc 1\n",
      "2018-10-26T16:50:56.950152: step 10057, loss 0.00142115, acc 1\n",
      "2018-10-26T16:50:57.288117: step 10058, loss 0.000902602, acc 1\n",
      "2018-10-26T16:50:57.694031: step 10059, loss 6.18264e-05, acc 1\n",
      "2018-10-26T16:50:58.053082: step 10060, loss 1.00901e-05, acc 1\n",
      "2018-10-26T16:50:58.479931: step 10061, loss 0.00046521, acc 1\n",
      "2018-10-26T16:50:58.955662: step 10062, loss 1.72989e-05, acc 1\n",
      "2018-10-26T16:50:59.410501: step 10063, loss 0.00217194, acc 1\n",
      "2018-10-26T16:50:59.819353: step 10064, loss 0.0016488, acc 1\n",
      "2018-10-26T16:51:00.181439: step 10065, loss 2.89062e-05, acc 1\n",
      "2018-10-26T16:51:00.499537: step 10066, loss 0.000909243, acc 1\n",
      "2018-10-26T16:51:00.871652: step 10067, loss 3.15014e-05, acc 1\n",
      "2018-10-26T16:51:01.262495: step 10068, loss 1.70049e-05, acc 1\n",
      "2018-10-26T16:51:01.603584: step 10069, loss 9.3554e-06, acc 1\n",
      "2018-10-26T16:51:01.937721: step 10070, loss 0.00091333, acc 1\n",
      "2018-10-26T16:51:02.259899: step 10071, loss 0.00024116, acc 1\n",
      "2018-10-26T16:51:02.623861: step 10072, loss 9.60104e-06, acc 1\n",
      "2018-10-26T16:51:02.966942: step 10073, loss 1.70732e-05, acc 1\n",
      "2018-10-26T16:51:03.354905: step 10074, loss 0.000176431, acc 1\n",
      "2018-10-26T16:51:03.787749: step 10075, loss 6.12497e-05, acc 1\n",
      "2018-10-26T16:51:04.149781: step 10076, loss 0.000103376, acc 1\n",
      "2018-10-26T16:51:04.554893: step 10077, loss 0.000856917, acc 1\n",
      "2018-10-26T16:51:04.935681: step 10078, loss 0.00843642, acc 1\n",
      "2018-10-26T16:51:05.292773: step 10079, loss 1.52487e-05, acc 1\n",
      "2018-10-26T16:51:05.666731: step 10080, loss 1.37722e-05, acc 1\n",
      "2018-10-26T16:51:06.097609: step 10081, loss 0.00047716, acc 1\n",
      "2018-10-26T16:51:06.427695: step 10082, loss 0.0129232, acc 1\n",
      "2018-10-26T16:51:06.884473: step 10083, loss 0.000263206, acc 1\n",
      "2018-10-26T16:51:07.327291: step 10084, loss 0.0018561, acc 1\n",
      "2018-10-26T16:51:07.795041: step 10085, loss 1.47298e-05, acc 1\n",
      "2018-10-26T16:51:08.309668: step 10086, loss 1.86324e-05, acc 1\n",
      "2018-10-26T16:51:08.728549: step 10087, loss 0.000786281, acc 1\n",
      "2018-10-26T16:51:09.186323: step 10088, loss 6.87574e-05, acc 1\n",
      "2018-10-26T16:51:09.635245: step 10089, loss 8.68026e-05, acc 1\n",
      "2018-10-26T16:51:10.091904: step 10090, loss 0.00237886, acc 1\n",
      "2018-10-26T16:51:10.567668: step 10091, loss 0.000245417, acc 1\n",
      "2018-10-26T16:51:10.984519: step 10092, loss 2.74539e-06, acc 1\n",
      "2018-10-26T16:51:11.432323: step 10093, loss 5.53636e-05, acc 1\n",
      "2018-10-26T16:51:11.772415: step 10094, loss 1.40613e-05, acc 1\n",
      "2018-10-26T16:51:12.191296: step 10095, loss 0.000489918, acc 1\n",
      "2018-10-26T16:51:12.594218: step 10096, loss 0.00191615, acc 1\n",
      "2018-10-26T16:51:12.959244: step 10097, loss 0.0233676, acc 0.984375\n",
      "2018-10-26T16:51:13.320282: step 10098, loss 3.79985e-05, acc 1\n",
      "2018-10-26T16:51:13.763096: step 10099, loss 9.34363e-06, acc 1\n",
      "2018-10-26T16:51:14.131113: step 10100, loss 0.00276565, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:51:14.946935: step 10100, loss 3.47568, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10100\n",
      "\n",
      "2018-10-26T16:51:15.638087: step 10101, loss 6.57191e-05, acc 1\n",
      "2018-10-26T16:51:16.008097: step 10102, loss 1.2107e-05, acc 1\n",
      "2018-10-26T16:51:16.443074: step 10103, loss 7.74108e-05, acc 1\n",
      "2018-10-26T16:51:16.887747: step 10104, loss 3.05835e-06, acc 1\n",
      "2018-10-26T16:51:17.281744: step 10105, loss 5.59936e-05, acc 1\n",
      "2018-10-26T16:51:17.618862: step 10106, loss 4.41717e-05, acc 1\n",
      "2018-10-26T16:51:17.985816: step 10107, loss 0.000410741, acc 1\n",
      "2018-10-26T16:51:18.366796: step 10108, loss 4.84631e-05, acc 1\n",
      "2018-10-26T16:51:18.708880: step 10109, loss 2.22611e-05, acc 1\n",
      "2018-10-26T16:51:19.063933: step 10110, loss 0.000537352, acc 1\n",
      "2018-10-26T16:51:19.541657: step 10111, loss 3.03057e-05, acc 1\n",
      "2018-10-26T16:51:19.877799: step 10112, loss 8.04038e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:51:20.231947: step 10113, loss 7.82069e-06, acc 1\n",
      "2018-10-26T16:51:20.547969: step 10114, loss 4.32831e-06, acc 1\n",
      "2018-10-26T16:51:20.911000: step 10115, loss 5.35283e-05, acc 1\n",
      "2018-10-26T16:51:21.257073: step 10116, loss 9.53641e-05, acc 1\n",
      "2018-10-26T16:51:21.593174: step 10117, loss 6.60584e-05, acc 1\n",
      "2018-10-26T16:51:21.935263: step 10118, loss 4.45885e-05, acc 1\n",
      "2018-10-26T16:51:22.256403: step 10119, loss 2.02661e-05, acc 1\n",
      "2018-10-26T16:51:22.573590: step 10120, loss 1.22171e-05, acc 1\n",
      "2018-10-26T16:51:22.904752: step 10121, loss 2.84593e-06, acc 1\n",
      "2018-10-26T16:51:23.235785: step 10122, loss 6.8787e-05, acc 1\n",
      "2018-10-26T16:51:23.560045: step 10123, loss 0.000112903, acc 1\n",
      "2018-10-26T16:51:23.896022: step 10124, loss 2.04598e-05, acc 1\n",
      "2018-10-26T16:51:24.230130: step 10125, loss 0.0041677, acc 1\n",
      "2018-10-26T16:51:24.562241: step 10126, loss 0.000523944, acc 1\n",
      "2018-10-26T16:51:24.902333: step 10127, loss 0.000757255, acc 1\n",
      "2018-10-26T16:51:25.265366: step 10128, loss 6.76217e-05, acc 1\n",
      "2018-10-26T16:51:25.593490: step 10129, loss 0.000303176, acc 1\n",
      "2018-10-26T16:51:25.931729: step 10130, loss 0.00223024, acc 1\n",
      "2018-10-26T16:51:26.298605: step 10131, loss 0.00147065, acc 1\n",
      "2018-10-26T16:51:26.653653: step 10132, loss 0.0140657, acc 0.984375\n",
      "2018-10-26T16:51:26.978786: step 10133, loss 3.87604e-06, acc 1\n",
      "2018-10-26T16:51:27.314888: step 10134, loss 4.75683e-05, acc 1\n",
      "2018-10-26T16:51:27.713823: step 10135, loss 3.17002e-05, acc 1\n",
      "2018-10-26T16:51:28.144837: step 10136, loss 3.49794e-06, acc 1\n",
      "2018-10-26T16:51:28.556570: step 10137, loss 0.000221971, acc 1\n",
      "2018-10-26T16:51:28.950517: step 10138, loss 4.40812e-05, acc 1\n",
      "2018-10-26T16:51:29.334619: step 10139, loss 7.68355e-05, acc 1\n",
      "2018-10-26T16:51:29.690544: step 10140, loss 4.47905e-05, acc 1\n",
      "2018-10-26T16:51:30.045626: step 10141, loss 9.9305e-06, acc 1\n",
      "2018-10-26T16:51:30.401643: step 10142, loss 5.70677e-06, acc 1\n",
      "2018-10-26T16:51:30.765713: step 10143, loss 4.86295e-06, acc 1\n",
      "2018-10-26T16:51:31.239523: step 10144, loss 0.000274013, acc 1\n",
      "2018-10-26T16:51:31.616395: step 10145, loss 0.00147975, acc 1\n",
      "2018-10-26T16:51:31.999371: step 10146, loss 0.00180483, acc 1\n",
      "2018-10-26T16:51:32.338503: step 10147, loss 6.11614e-05, acc 1\n",
      "2018-10-26T16:51:32.687569: step 10148, loss 0.000108798, acc 1\n",
      "2018-10-26T16:51:33.058542: step 10149, loss 0.000136401, acc 1\n",
      "2018-10-26T16:51:33.429550: step 10150, loss 9.46543e-05, acc 1\n",
      "2018-10-26T16:51:33.812526: step 10151, loss 8.18038e-05, acc 1\n",
      "2018-10-26T16:51:34.141694: step 10152, loss 3.22406e-06, acc 1\n",
      "2018-10-26T16:51:34.480955: step 10153, loss 6.61719e-06, acc 1\n",
      "2018-10-26T16:51:34.812856: step 10154, loss 1.19208e-06, acc 1\n",
      "2018-10-26T16:51:35.178876: step 10155, loss 0.000501331, acc 1\n",
      "2018-10-26T16:51:35.521037: step 10156, loss 4.60971e-05, acc 1\n",
      "2018-10-26T16:51:35.885986: step 10157, loss 0.000208248, acc 1\n",
      "2018-10-26T16:51:36.381664: step 10158, loss 5.23907e-06, acc 1\n",
      "2018-10-26T16:51:36.832459: step 10159, loss 0.00167096, acc 1\n",
      "2018-10-26T16:51:37.212446: step 10160, loss 5.56317e-06, acc 1\n",
      "2018-10-26T16:51:37.551596: step 10161, loss 0.000150843, acc 1\n",
      "2018-10-26T16:51:37.888635: step 10162, loss 0.000127852, acc 1\n",
      "2018-10-26T16:51:38.254661: step 10163, loss 0.000100253, acc 1\n",
      "2018-10-26T16:51:38.579788: step 10164, loss 0.000120796, acc 1\n",
      "2018-10-26T16:51:38.899935: step 10165, loss 0.000190907, acc 1\n",
      "2018-10-26T16:51:39.227060: step 10166, loss 2.56494e-05, acc 1\n",
      "2018-10-26T16:51:39.560171: step 10167, loss 0.000380433, acc 1\n",
      "2018-10-26T16:51:39.887295: step 10168, loss 0.000198075, acc 1\n",
      "2018-10-26T16:51:40.217414: step 10169, loss 0.000219567, acc 1\n",
      "2018-10-26T16:51:40.549526: step 10170, loss 2.499e-05, acc 1\n",
      "2018-10-26T16:51:40.883632: step 10171, loss 5.10359e-07, acc 1\n",
      "2018-10-26T16:51:41.236693: step 10172, loss 0.00139159, acc 1\n",
      "2018-10-26T16:51:41.565812: step 10173, loss 5.12772e-05, acc 1\n",
      "2018-10-26T16:51:41.933828: step 10174, loss 0.00105527, acc 1\n",
      "2018-10-26T16:51:42.238046: step 10175, loss 2.17965e-05, acc 1\n",
      "2018-10-26T16:51:42.573119: step 10176, loss 6.27977e-05, acc 1\n",
      "2018-10-26T16:51:42.937147: step 10177, loss 0.000169388, acc 1\n",
      "2018-10-26T16:51:43.242332: step 10178, loss 1.01356e-05, acc 1\n",
      "2018-10-26T16:51:43.581426: step 10179, loss 0.0184868, acc 0.984375\n",
      "2018-10-26T16:51:43.902566: step 10180, loss 5.57804e-05, acc 1\n",
      "2018-10-26T16:51:44.230694: step 10181, loss 0.000916156, acc 1\n",
      "2018-10-26T16:51:44.566815: step 10182, loss 0.000512695, acc 1\n",
      "2018-10-26T16:51:44.915883: step 10183, loss 0.000190833, acc 1\n",
      "2018-10-26T16:51:45.237006: step 10184, loss 0.000225601, acc 1\n",
      "2018-10-26T16:51:45.568119: step 10185, loss 0.000133594, acc 1\n",
      "2018-10-26T16:51:45.902225: step 10186, loss 0.000973371, acc 1\n",
      "2018-10-26T16:51:46.202456: step 10187, loss 0.000120973, acc 1\n",
      "2018-10-26T16:51:46.535563: step 10188, loss 9.62735e-05, acc 1\n",
      "2018-10-26T16:51:46.887595: step 10189, loss 0.000259913, acc 1\n",
      "2018-10-26T16:51:47.213724: step 10190, loss 1.83767e-05, acc 1\n",
      "2018-10-26T16:51:47.526887: step 10191, loss 3.27522e-05, acc 1\n",
      "2018-10-26T16:51:47.857001: step 10192, loss 0.000483635, acc 1\n",
      "2018-10-26T16:51:48.152250: step 10193, loss 0.0038861, acc 1\n",
      "2018-10-26T16:51:48.469367: step 10194, loss 0.000504244, acc 1\n",
      "2018-10-26T16:51:48.792602: step 10195, loss 9.67182e-06, acc 1\n",
      "2018-10-26T16:51:49.106666: step 10196, loss 1.82305e-05, acc 1\n",
      "2018-10-26T16:51:49.406874: step 10197, loss 0.0105355, acc 1\n",
      "2018-10-26T16:51:49.737979: step 10198, loss 0.00193727, acc 1\n",
      "2018-10-26T16:51:50.065168: step 10199, loss 0.00010156, acc 1\n",
      "2018-10-26T16:51:50.411179: step 10200, loss 0.00162944, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:51:51.325734: step 10200, loss 3.51974, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10200\n",
      "\n",
      "2018-10-26T16:51:51.947078: step 10201, loss 0.000758315, acc 1\n",
      "2018-10-26T16:51:52.316089: step 10202, loss 0.000806255, acc 1\n",
      "2018-10-26T16:51:52.753917: step 10203, loss 0.00143593, acc 1\n",
      "2018-10-26T16:51:53.082041: step 10204, loss 0.000880825, acc 1\n",
      "2018-10-26T16:51:53.386228: step 10205, loss 0.0153713, acc 0.984375\n",
      "2018-10-26T16:51:53.712361: step 10206, loss 0.000573526, acc 1\n",
      "2018-10-26T16:51:54.040480: step 10207, loss 2.32271e-05, acc 1\n",
      "2018-10-26T16:51:54.370600: step 10208, loss 0.00164059, acc 1\n",
      "2018-10-26T16:51:54.682937: step 10209, loss 0.000167376, acc 1\n",
      "2018-10-26T16:51:55.005903: step 10210, loss 2.26679e-06, acc 1\n",
      "2018-10-26T16:51:55.331033: step 10211, loss 7.97966e-05, acc 1\n",
      "2018-10-26T16:51:55.645193: step 10212, loss 8.31578e-05, acc 1\n",
      "2018-10-26T16:51:55.994063: step 10213, loss 0.000145455, acc 1\n",
      "2018-10-26T16:51:56.394296: step 10214, loss 0.00127528, acc 1\n",
      "2018-10-26T16:51:56.758224: step 10215, loss 0.00139962, acc 1\n",
      "2018-10-26T16:51:57.231959: step 10216, loss 0.0023551, acc 1\n",
      "2018-10-26T16:51:57.604959: step 10217, loss 0.00030154, acc 1\n",
      "2018-10-26T16:51:58.028824: step 10218, loss 2.27242e-07, acc 1\n",
      "2018-10-26T16:51:58.386909: step 10219, loss 1.6675e-05, acc 1\n",
      "2018-10-26T16:51:58.755884: step 10220, loss 3.79447e-05, acc 1\n",
      "2018-10-26T16:51:59.102953: step 10221, loss 1.952e-06, acc 1\n",
      "2018-10-26T16:51:59.432075: step 10222, loss 0.000156679, acc 1\n",
      "2018-10-26T16:51:59.750389: step 10223, loss 0.00198593, acc 1\n",
      "2018-10-26T16:52:00.096303: step 10224, loss 0.000488801, acc 1\n",
      "2018-10-26T16:52:00.424424: step 10225, loss 4.01043e-05, acc 1\n",
      "2018-10-26T16:52:00.745566: step 10226, loss 3.05627e-05, acc 1\n",
      "2018-10-26T16:52:01.079674: step 10227, loss 1.32391e-05, acc 1\n",
      "2018-10-26T16:52:01.416900: step 10228, loss 5.21336e-06, acc 1\n",
      "2018-10-26T16:52:01.748888: step 10229, loss 2.65921e-05, acc 1\n",
      "2018-10-26T16:52:02.097718: step 10230, loss 0.000184567, acc 1\n",
      "2018-10-26T16:52:02.412191: step 10231, loss 1.75266e-05, acc 1\n",
      "2018-10-26T16:52:02.746224: step 10232, loss 0.000241927, acc 1\n",
      "2018-10-26T16:52:03.073414: step 10233, loss 3.13092e-06, acc 1\n",
      "2018-10-26T16:52:03.392493: step 10234, loss 1.58443e-05, acc 1\n",
      "2018-10-26T16:52:03.716635: step 10235, loss 0.00021005, acc 1\n",
      "2018-10-26T16:52:04.069684: step 10236, loss 3.43232e-05, acc 1\n",
      "2018-10-26T16:52:04.365896: step 10237, loss 4.21307e-06, acc 1\n",
      "2018-10-26T16:52:04.693019: step 10238, loss 2.36456e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:52:05.008176: step 10239, loss 0.000130386, acc 1\n",
      "2018-10-26T16:52:05.330318: step 10240, loss 0.00101066, acc 1\n",
      "2018-10-26T16:52:05.639562: step 10241, loss 2.15615e-05, acc 1\n",
      "2018-10-26T16:52:05.952655: step 10242, loss 1.76588e-05, acc 1\n",
      "2018-10-26T16:52:06.287792: step 10243, loss 0.000229192, acc 1\n",
      "2018-10-26T16:52:06.596931: step 10244, loss 6.18055e-05, acc 1\n",
      "2018-10-26T16:52:06.939020: step 10245, loss 2.68436e-05, acc 1\n",
      "2018-10-26T16:52:07.264151: step 10246, loss 0.000533569, acc 1\n",
      "2018-10-26T16:52:07.613218: step 10247, loss 0.000264773, acc 1\n",
      "2018-10-26T16:52:07.920395: step 10248, loss 0.00715916, acc 1\n",
      "2018-10-26T16:52:08.268466: step 10249, loss 0.00017524, acc 1\n",
      "2018-10-26T16:52:08.649448: step 10250, loss 6.06882e-05, acc 1\n",
      "2018-10-26T16:52:09.000510: step 10251, loss 0.000192424, acc 1\n",
      "2018-10-26T16:52:09.342597: step 10252, loss 3.77003e-05, acc 1\n",
      "2018-10-26T16:52:09.700738: step 10253, loss 0.000804462, acc 1\n",
      "2018-10-26T16:52:10.064670: step 10254, loss 0.000661019, acc 1\n",
      "2018-10-26T16:52:10.442656: step 10255, loss 8.18943e-06, acc 1\n",
      "2018-10-26T16:52:10.846577: step 10256, loss 3.93331e-05, acc 1\n",
      "2018-10-26T16:52:11.208613: step 10257, loss 3.28463e-05, acc 1\n",
      "2018-10-26T16:52:11.564663: step 10258, loss 0.00926466, acc 1\n",
      "2018-10-26T16:52:11.923700: step 10259, loss 0.000410965, acc 1\n",
      "2018-10-26T16:52:12.279748: step 10260, loss 0.00273191, acc 1\n",
      "2018-10-26T16:52:12.671700: step 10261, loss 0.000460208, acc 1\n",
      "2018-10-26T16:52:13.115516: step 10262, loss 5.6913e-05, acc 1\n",
      "2018-10-26T16:52:13.520434: step 10263, loss 5.56905e-06, acc 1\n",
      "2018-10-26T16:52:13.988183: step 10264, loss 0.00314584, acc 1\n",
      "2018-10-26T16:52:14.450986: step 10265, loss 6.13673e-06, acc 1\n",
      "2018-10-26T16:52:14.887781: step 10266, loss 0.000895421, acc 1\n",
      "2018-10-26T16:52:15.269892: step 10267, loss 7.43493e-06, acc 1\n",
      "2018-10-26T16:52:15.684651: step 10268, loss 0.00012507, acc 1\n",
      "2018-10-26T16:52:16.115500: step 10269, loss 8.02709e-05, acc 1\n",
      "2018-10-26T16:52:16.548377: step 10270, loss 0.00190111, acc 1\n",
      "2018-10-26T16:52:17.024072: step 10271, loss 0.000109353, acc 1\n",
      "2018-10-26T16:52:17.444947: step 10272, loss 3.49155e-05, acc 1\n",
      "2018-10-26T16:52:17.808973: step 10273, loss 1.46402e-06, acc 1\n",
      "2018-10-26T16:52:18.161036: step 10274, loss 4.6197e-05, acc 1\n",
      "2018-10-26T16:52:18.514091: step 10275, loss 6.14414e-05, acc 1\n",
      "2018-10-26T16:52:18.871208: step 10276, loss 0.000850699, acc 1\n",
      "2018-10-26T16:52:19.178344: step 10277, loss 7.86633e-05, acc 1\n",
      "2018-10-26T16:52:19.505443: step 10278, loss 4.57625e-06, acc 1\n",
      "2018-10-26T16:52:19.829576: step 10279, loss 0.000933788, acc 1\n",
      "2018-10-26T16:52:20.158697: step 10280, loss 1.5322e-05, acc 1\n",
      "2018-10-26T16:52:20.475853: step 10281, loss 0.000258838, acc 1\n",
      "2018-10-26T16:52:20.801977: step 10282, loss 0.000102421, acc 1\n",
      "2018-10-26T16:52:21.120130: step 10283, loss 5.55232e-05, acc 1\n",
      "2018-10-26T16:52:21.451286: step 10284, loss 0.000248372, acc 1\n",
      "2018-10-26T16:52:21.778369: step 10285, loss 0.000222567, acc 1\n",
      "2018-10-26T16:52:22.106493: step 10286, loss 0.000172967, acc 1\n",
      "2018-10-26T16:52:22.446288: step 10287, loss 1.82115e-05, acc 1\n",
      "2018-10-26T16:52:22.819648: step 10288, loss 0.000146428, acc 1\n",
      "2018-10-26T16:52:23.133751: step 10289, loss 6.81721e-07, acc 1\n",
      "2018-10-26T16:52:23.450900: step 10290, loss 1.42071e-05, acc 1\n",
      "2018-10-26T16:52:23.760202: step 10291, loss 0.000299319, acc 1\n",
      "2018-10-26T16:52:24.125101: step 10292, loss 5.33755e-06, acc 1\n",
      "2018-10-26T16:52:24.489127: step 10293, loss 0.0206866, acc 0.984375\n",
      "2018-10-26T16:52:24.841250: step 10294, loss 6.04914e-06, acc 1\n",
      "2018-10-26T16:52:25.175296: step 10295, loss 2.37228e-05, acc 1\n",
      "2018-10-26T16:52:25.519378: step 10296, loss 0.000534178, acc 1\n",
      "2018-10-26T16:52:25.850489: step 10297, loss 0.00116836, acc 1\n",
      "2018-10-26T16:52:26.189586: step 10298, loss 0.000108851, acc 1\n",
      "2018-10-26T16:52:26.529675: step 10299, loss 0.0125359, acc 0.984375\n",
      "2018-10-26T16:52:26.855803: step 10300, loss 0.000388528, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:52:27.618764: step 10300, loss 3.53905, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10300\n",
      "\n",
      "2018-10-26T16:52:28.210484: step 10301, loss 0.000107703, acc 1\n",
      "2018-10-26T16:52:28.548282: step 10302, loss 0.00180818, acc 1\n",
      "2018-10-26T16:52:28.980375: step 10303, loss 0.00013552, acc 1\n",
      "2018-10-26T16:52:29.401002: step 10304, loss 5.07278e-05, acc 1\n",
      "2018-10-26T16:52:29.770019: step 10305, loss 0.00114492, acc 1\n",
      "2018-10-26T16:52:30.203859: step 10306, loss 0.000164019, acc 1\n",
      "2018-10-26T16:52:30.579854: step 10307, loss 5.69374e-05, acc 1\n",
      "2018-10-26T16:52:30.982981: step 10308, loss 0.000635094, acc 1\n",
      "2018-10-26T16:52:31.301021: step 10309, loss 1.28147e-06, acc 1\n",
      "2018-10-26T16:52:31.624062: step 10310, loss 0.000332765, acc 1\n",
      "2018-10-26T16:52:31.953184: step 10311, loss 9.91616e-05, acc 1\n",
      "2018-10-26T16:52:32.284298: step 10312, loss 0.000435452, acc 1\n",
      "2018-10-26T16:52:32.601452: step 10313, loss 2.03596e-05, acc 1\n",
      "2018-10-26T16:52:32.924587: step 10314, loss 4.18125e-05, acc 1\n",
      "2018-10-26T16:52:33.251715: step 10315, loss 0.000630177, acc 1\n",
      "2018-10-26T16:52:33.586822: step 10316, loss 3.32567e-05, acc 1\n",
      "2018-10-26T16:52:33.886020: step 10317, loss 0.000135612, acc 1\n",
      "2018-10-26T16:52:34.268996: step 10318, loss 1.74154e-06, acc 1\n",
      "2018-10-26T16:52:34.660949: step 10319, loss 8.11831e-05, acc 1\n",
      "2018-10-26T16:52:35.059884: step 10320, loss 0.00685002, acc 1\n",
      "2018-10-26T16:52:35.447847: step 10321, loss 2.83114e-05, acc 1\n",
      "2018-10-26T16:52:35.809879: step 10322, loss 2.40728e-05, acc 1\n",
      "2018-10-26T16:52:36.129026: step 10323, loss 1.76826e-05, acc 1\n",
      "2018-10-26T16:52:36.447177: step 10324, loss 2.74361e-06, acc 1\n",
      "2018-10-26T16:52:36.758348: step 10325, loss 2.54541e-05, acc 1\n",
      "2018-10-26T16:52:37.082481: step 10326, loss 7.33585e-05, acc 1\n",
      "2018-10-26T16:52:37.428554: step 10327, loss 0.000134701, acc 1\n",
      "2018-10-26T16:52:37.811553: step 10328, loss 5.90376e-06, acc 1\n",
      "2018-10-26T16:52:38.162593: step 10329, loss 5.82181e-05, acc 1\n",
      "2018-10-26T16:52:38.523632: step 10330, loss 0.000555928, acc 1\n",
      "2018-10-26T16:52:38.859763: step 10331, loss 2.859e-06, acc 1\n",
      "2018-10-26T16:52:39.187030: step 10332, loss 9.40462e-05, acc 1\n",
      "2018-10-26T16:52:39.529940: step 10333, loss 0.000196806, acc 1\n",
      "2018-10-26T16:52:39.908927: step 10334, loss 0.00185712, acc 1\n",
      "2018-10-26T16:52:40.284925: step 10335, loss 3.217e-05, acc 1\n",
      "2018-10-26T16:52:40.757659: step 10336, loss 5.23375e-06, acc 1\n",
      "2018-10-26T16:52:41.085786: step 10337, loss 1.78135e-05, acc 1\n",
      "2018-10-26T16:52:41.418893: step 10338, loss 0.0001442, acc 1\n",
      "2018-10-26T16:52:41.772947: step 10339, loss 0.000544351, acc 1\n",
      "2018-10-26T16:52:42.129996: step 10340, loss 0.00775219, acc 1\n",
      "2018-10-26T16:52:42.491031: step 10341, loss 0.000160642, acc 1\n",
      "2018-10-26T16:52:42.818153: step 10342, loss 2.27685e-05, acc 1\n",
      "2018-10-26T16:52:43.139295: step 10343, loss 3.16332e-05, acc 1\n",
      "2018-10-26T16:52:43.489361: step 10344, loss 0.0193602, acc 0.984375\n",
      "2018-10-26T16:52:43.917414: step 10345, loss 0.000158478, acc 1\n",
      "2018-10-26T16:52:44.262383: step 10346, loss 1.86611e-05, acc 1\n",
      "2018-10-26T16:52:44.652253: step 10347, loss 4.80459e-05, acc 1\n",
      "2018-10-26T16:52:45.016280: step 10348, loss 1.89989e-07, acc 1\n",
      "2018-10-26T16:52:45.362356: step 10349, loss 8.59873e-05, acc 1\n",
      "2018-10-26T16:52:45.695465: step 10350, loss 6.28981e-06, acc 1\n",
      "2018-10-26T16:52:46.160224: step 10351, loss 3.30609e-06, acc 1\n",
      "2018-10-26T16:52:46.620416: step 10352, loss 0.000336151, acc 1\n",
      "2018-10-26T16:52:47.037878: step 10353, loss 2.28534e-06, acc 1\n",
      "2018-10-26T16:52:47.484685: step 10354, loss 0.000416988, acc 1\n",
      "2018-10-26T16:52:47.865668: step 10355, loss 3.09187e-06, acc 1\n",
      "2018-10-26T16:52:48.243658: step 10356, loss 0.000459003, acc 1\n",
      "2018-10-26T16:52:48.574772: step 10357, loss 0.000224828, acc 1\n",
      "2018-10-26T16:52:49.028560: step 10358, loss 0.00618006, acc 1\n",
      "2018-10-26T16:52:49.375632: step 10359, loss 0.000821299, acc 1\n",
      "2018-10-26T16:52:49.744649: step 10360, loss 1.46729e-05, acc 1\n",
      "2018-10-26T16:52:50.254471: step 10361, loss 9.7542e-06, acc 1\n",
      "2018-10-26T16:52:50.582408: step 10362, loss 1.8165e-05, acc 1\n",
      "2018-10-26T16:52:50.985714: step 10363, loss 2.62856e-05, acc 1\n",
      "2018-10-26T16:52:51.408241: step 10364, loss 0.00353884, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:52:51.785195: step 10365, loss 0.0020974, acc 1\n",
      "2018-10-26T16:52:52.118307: step 10366, loss 4.78148e-05, acc 1\n",
      "2018-10-26T16:52:52.433463: step 10367, loss 0.000129198, acc 1\n",
      "2018-10-26T16:52:52.779541: step 10368, loss 0.00020323, acc 1\n",
      "2018-10-26T16:52:53.196424: step 10369, loss 3.03673e-05, acc 1\n",
      "2018-10-26T16:52:53.546488: step 10370, loss 6.15016e-05, acc 1\n",
      "2018-10-26T16:52:53.894559: step 10371, loss 4.7532e-06, acc 1\n",
      "2018-10-26T16:52:54.233653: step 10372, loss 3.86886e-05, acc 1\n",
      "2018-10-26T16:52:54.569914: step 10373, loss 2.24337e-05, acc 1\n",
      "2018-10-26T16:52:54.948743: step 10374, loss 0.000693597, acc 1\n",
      "2018-10-26T16:52:55.309783: step 10375, loss 1.20006e-05, acc 1\n",
      "2018-10-26T16:52:55.716693: step 10376, loss 0.00112913, acc 1\n",
      "2018-10-26T16:52:56.197408: step 10377, loss 0.000216816, acc 1\n",
      "2018-10-26T16:52:56.652194: step 10378, loss 0.000365264, acc 1\n",
      "2018-10-26T16:52:57.072073: step 10379, loss 5.44363e-05, acc 1\n",
      "2018-10-26T16:52:57.393211: step 10380, loss 1.10826e-06, acc 1\n",
      "2018-10-26T16:52:57.786161: step 10381, loss 2.50296e-05, acc 1\n",
      "2018-10-26T16:52:58.147196: step 10382, loss 1.00277e-05, acc 1\n",
      "2018-10-26T16:52:58.479313: step 10383, loss 0.000217261, acc 1\n",
      "2018-10-26T16:52:58.866280: step 10384, loss 6.05347e-05, acc 1\n",
      "2018-10-26T16:52:59.245267: step 10385, loss 0.000511356, acc 1\n",
      "2018-10-26T16:52:59.607433: step 10386, loss 4.6733e-05, acc 1\n",
      "2018-10-26T16:52:59.990273: step 10387, loss 0.000268692, acc 1\n",
      "2018-10-26T16:53:00.384219: step 10388, loss 2.30909e-05, acc 1\n",
      "2018-10-26T16:53:00.712345: step 10389, loss 4.25923e-05, acc 1\n",
      "2018-10-26T16:53:01.110280: step 10390, loss 4.07338e-06, acc 1\n",
      "2018-10-26T16:53:01.471319: step 10391, loss 3.73167e-05, acc 1\n",
      "2018-10-26T16:53:01.789548: step 10392, loss 3.89718e-05, acc 1\n",
      "2018-10-26T16:53:02.173472: step 10393, loss 7.03601e-05, acc 1\n",
      "2018-10-26T16:53:02.622240: step 10394, loss 0.000154493, acc 1\n",
      "2018-10-26T16:53:02.944382: step 10395, loss 2.86122e-05, acc 1\n",
      "2018-10-26T16:53:03.289457: step 10396, loss 0.000138785, acc 1\n",
      "2018-10-26T16:53:03.634658: step 10397, loss 4.57639e-05, acc 1\n",
      "2018-10-26T16:53:03.985599: step 10398, loss 4.20379e-06, acc 1\n",
      "2018-10-26T16:53:04.323696: step 10399, loss 8.6768e-06, acc 1\n",
      "2018-10-26T16:53:04.708665: step 10400, loss 0.00129589, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:53:05.558395: step 10400, loss 3.60057, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10400\n",
      "\n",
      "2018-10-26T16:53:06.184721: step 10401, loss 2.17138e-05, acc 1\n",
      "2018-10-26T16:53:06.500880: step 10402, loss 0.000660616, acc 1\n",
      "2018-10-26T16:53:06.998592: step 10403, loss 7.21479e-06, acc 1\n",
      "2018-10-26T16:53:07.343655: step 10404, loss 3.81173e-05, acc 1\n",
      "2018-10-26T16:53:07.753531: step 10405, loss 0.0337678, acc 0.96875\n",
      "2018-10-26T16:53:08.140498: step 10406, loss 6.05147e-06, acc 1\n",
      "2018-10-26T16:53:08.454657: step 10407, loss 9.5739e-07, acc 1\n",
      "2018-10-26T16:53:08.794753: step 10408, loss 0.000109685, acc 1\n",
      "2018-10-26T16:53:09.173736: step 10409, loss 0.000183269, acc 1\n",
      "2018-10-26T16:53:09.502010: step 10410, loss 0.000107603, acc 1\n",
      "2018-10-26T16:53:09.828010: step 10411, loss 2.36169e-06, acc 1\n",
      "2018-10-26T16:53:10.171117: step 10412, loss 5.42886e-06, acc 1\n",
      "2018-10-26T16:53:10.520203: step 10413, loss 0.000500089, acc 1\n",
      "2018-10-26T16:53:10.858238: step 10414, loss 0.000277244, acc 1\n",
      "2018-10-26T16:53:11.210296: step 10415, loss 0.000110091, acc 1\n",
      "2018-10-26T16:53:11.550384: step 10416, loss 0.0029295, acc 1\n",
      "2018-10-26T16:53:11.880645: step 10417, loss 0.000258676, acc 1\n",
      "2018-10-26T16:53:12.233560: step 10418, loss 0.000412263, acc 1\n",
      "2018-10-26T16:53:12.558694: step 10419, loss 0.00658604, acc 1\n",
      "2018-10-26T16:53:12.902774: step 10420, loss 0.00293126, acc 1\n",
      "2018-10-26T16:53:13.231892: step 10421, loss 9.09826e-06, acc 1\n",
      "2018-10-26T16:53:13.575973: step 10422, loss 0.000139499, acc 1\n",
      "2018-10-26T16:53:13.887256: step 10423, loss 4.79978e-05, acc 1\n",
      "2018-10-26T16:53:14.271213: step 10424, loss 2.33563e-06, acc 1\n",
      "2018-10-26T16:53:14.673043: step 10425, loss 2.99134e-06, acc 1\n",
      "2018-10-26T16:53:15.031159: step 10426, loss 0.0343402, acc 0.984375\n",
      "2018-10-26T16:53:15.404089: step 10427, loss 2.23425e-05, acc 1\n",
      "2018-10-26T16:53:15.806016: step 10428, loss 0.000472866, acc 1\n",
      "2018-10-26T16:53:16.186997: step 10429, loss 0.000569928, acc 1\n",
      "2018-10-26T16:53:16.575959: step 10430, loss 0.000124413, acc 1\n",
      "2018-10-26T16:53:16.980877: step 10431, loss 0.00101117, acc 1\n",
      "2018-10-26T16:53:17.370865: step 10432, loss 3.91153e-07, acc 1\n",
      "2018-10-26T16:53:17.794705: step 10433, loss 8.3012e-06, acc 1\n",
      "2018-10-26T16:53:18.211664: step 10434, loss 2.47827e-05, acc 1\n",
      "2018-10-26T16:53:18.645429: step 10435, loss 0.000534282, acc 1\n",
      "2018-10-26T16:53:19.139110: step 10436, loss 7.32811e-06, acc 1\n",
      "2018-10-26T16:53:19.595890: step 10437, loss 8.48726e-06, acc 1\n",
      "2018-10-26T16:53:20.083586: step 10438, loss 1.64595e-05, acc 1\n",
      "2018-10-26T16:53:20.506457: step 10439, loss 9.92853e-06, acc 1\n",
      "2018-10-26T16:53:20.919352: step 10440, loss 0.000824302, acc 1\n",
      "2018-10-26T16:53:21.370149: step 10441, loss 4.33397e-06, acc 1\n",
      "2018-10-26T16:53:21.812965: step 10442, loss 4.95691e-05, acc 1\n",
      "2018-10-26T16:53:22.230847: step 10443, loss 3.17387e-05, acc 1\n",
      "2018-10-26T16:53:22.664689: step 10444, loss 0.000307843, acc 1\n",
      "2018-10-26T16:53:23.066616: step 10445, loss 3.94112e-06, acc 1\n",
      "2018-10-26T16:53:23.472532: step 10446, loss 0.000124783, acc 1\n",
      "2018-10-26T16:53:23.853606: step 10447, loss 0.000507858, acc 1\n",
      "2018-10-26T16:53:24.231271: step 10448, loss 6.56951e-05, acc 1\n",
      "2018-10-26T16:53:24.564614: step 10449, loss 0.000250679, acc 1\n",
      "2018-10-26T16:53:24.904705: step 10450, loss 0.0186142, acc 0.984375\n",
      "2018-10-26T16:53:25.245832: step 10451, loss 0.000335544, acc 1\n",
      "2018-10-26T16:53:25.590872: step 10452, loss 2.67154e-05, acc 1\n",
      "2018-10-26T16:53:25.933955: step 10453, loss 1.03733e-05, acc 1\n",
      "2018-10-26T16:53:26.272051: step 10454, loss 0.000122099, acc 1\n",
      "2018-10-26T16:53:26.585216: step 10455, loss 0.000693023, acc 1\n",
      "2018-10-26T16:53:26.928331: step 10456, loss 6.66172e-05, acc 1\n",
      "2018-10-26T16:53:27.267392: step 10457, loss 1.24344e-05, acc 1\n",
      "2018-10-26T16:53:27.611513: step 10458, loss 6.44979e-06, acc 1\n",
      "2018-10-26T16:53:27.940623: step 10459, loss 6.23946e-05, acc 1\n",
      "2018-10-26T16:53:28.266722: step 10460, loss 0.000228744, acc 1\n",
      "2018-10-26T16:53:28.610916: step 10461, loss 2.54055e-06, acc 1\n",
      "2018-10-26T16:53:29.020747: step 10462, loss 0.000461705, acc 1\n",
      "2018-10-26T16:53:29.413658: step 10463, loss 0.000464059, acc 1\n",
      "2018-10-26T16:53:29.742779: step 10464, loss 5.22753e-05, acc 1\n",
      "2018-10-26T16:53:30.120795: step 10465, loss 0.000331763, acc 1\n",
      "2018-10-26T16:53:30.470947: step 10466, loss 0.00056612, acc 1\n",
      "2018-10-26T16:53:30.832869: step 10467, loss 0.000167304, acc 1\n",
      "2018-10-26T16:53:31.177946: step 10468, loss 8.81267e-05, acc 1\n",
      "2018-10-26T16:53:31.549949: step 10469, loss 0.00132287, acc 1\n",
      "2018-10-26T16:53:31.874086: step 10470, loss 0.000134661, acc 1\n",
      "2018-10-26T16:53:32.211184: step 10471, loss 0.000221168, acc 1\n",
      "2018-10-26T16:53:32.550280: step 10472, loss 0.000197478, acc 1\n",
      "2018-10-26T16:53:32.911362: step 10473, loss 3.18319e-05, acc 1\n",
      "2018-10-26T16:53:33.249409: step 10474, loss 0.000491272, acc 1\n",
      "2018-10-26T16:53:33.603465: step 10475, loss 0.000210448, acc 1\n",
      "2018-10-26T16:53:33.938569: step 10476, loss 3.42178e-05, acc 1\n",
      "2018-10-26T16:53:34.290627: step 10477, loss 4.9781e-05, acc 1\n",
      "2018-10-26T16:53:34.615759: step 10478, loss 1.22188e-05, acc 1\n",
      "2018-10-26T16:53:34.964829: step 10479, loss 0.00997577, acc 1\n",
      "2018-10-26T16:53:35.310963: step 10480, loss 0.00367107, acc 1\n",
      "2018-10-26T16:53:35.681909: step 10481, loss 1.63281e-05, acc 1\n",
      "2018-10-26T16:53:36.035964: step 10482, loss 0.000179082, acc 1\n",
      "2018-10-26T16:53:36.381041: step 10483, loss 0.000241631, acc 1\n",
      "2018-10-26T16:53:36.721136: step 10484, loss 2.22013e-05, acc 1\n",
      "2018-10-26T16:53:37.068209: step 10485, loss 0.00149353, acc 1\n",
      "2018-10-26T16:53:37.403312: step 10486, loss 0.000135563, acc 1\n",
      "2018-10-26T16:53:37.764346: step 10487, loss 0.00010667, acc 1\n",
      "2018-10-26T16:53:38.172258: step 10488, loss 1.34485e-05, acc 1\n",
      "2018-10-26T16:53:38.500382: step 10489, loss 0.000195071, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:53:38.856429: step 10490, loss 0.00226517, acc 1\n",
      "2018-10-26T16:53:39.180562: step 10491, loss 0.000255962, acc 1\n",
      "2018-10-26T16:53:39.501769: step 10492, loss 5.28885e-05, acc 1\n",
      "2018-10-26T16:53:39.845823: step 10493, loss 8.02703e-05, acc 1\n",
      "2018-10-26T16:53:40.178895: step 10494, loss 2.60175e-05, acc 1\n",
      "2018-10-26T16:53:40.508016: step 10495, loss 0.0001674, acc 1\n",
      "2018-10-26T16:53:40.858082: step 10496, loss 0.0177415, acc 0.984375\n",
      "2018-10-26T16:53:41.184208: step 10497, loss 1.52563e-05, acc 1\n",
      "2018-10-26T16:53:41.503357: step 10498, loss 0.000608835, acc 1\n",
      "2018-10-26T16:53:41.841518: step 10499, loss 5.68907e-05, acc 1\n",
      "2018-10-26T16:53:42.195509: step 10500, loss 9.0822e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:53:42.978533: step 10500, loss 3.61863, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10500\n",
      "\n",
      "2018-10-26T16:53:43.637654: step 10501, loss 0.000316352, acc 1\n",
      "2018-10-26T16:53:43.995697: step 10502, loss 7.16854e-06, acc 1\n",
      "2018-10-26T16:53:44.456499: step 10503, loss 4.8012e-05, acc 1\n",
      "2018-10-26T16:53:44.829469: step 10504, loss 0.000158826, acc 1\n",
      "2018-10-26T16:53:45.169560: step 10505, loss 0.00147843, acc 1\n",
      "2018-10-26T16:53:45.543563: step 10506, loss 8.40578e-05, acc 1\n",
      "2018-10-26T16:53:45.911578: step 10507, loss 0.00327132, acc 1\n",
      "2018-10-26T16:53:46.262689: step 10508, loss 0.000354046, acc 1\n",
      "2018-10-26T16:53:46.604729: step 10509, loss 0.000228789, acc 1\n",
      "2018-10-26T16:53:46.922879: step 10510, loss 0.00089157, acc 1\n",
      "2018-10-26T16:53:47.254988: step 10511, loss 0.000142009, acc 1\n",
      "2018-10-26T16:53:47.600070: step 10512, loss 1.27e-05, acc 1\n",
      "2018-10-26T16:53:47.910260: step 10513, loss 0.000142397, acc 1\n",
      "2018-10-26T16:53:48.238364: step 10514, loss 4.45507e-06, acc 1\n",
      "2018-10-26T16:53:48.575459: step 10515, loss 5.04364e-05, acc 1\n",
      "2018-10-26T16:53:48.913559: step 10516, loss 1.3572e-05, acc 1\n",
      "2018-10-26T16:53:49.250659: step 10517, loss 2.82889e-05, acc 1\n",
      "2018-10-26T16:53:49.584762: step 10518, loss 0.000832978, acc 1\n",
      "2018-10-26T16:53:49.914884: step 10519, loss 1.50323e-05, acc 1\n",
      "2018-10-26T16:53:50.254974: step 10520, loss 0.000159953, acc 1\n",
      "2018-10-26T16:53:50.622990: step 10521, loss 9.26029e-05, acc 1\n",
      "2018-10-26T16:53:50.974050: step 10522, loss 0.000407945, acc 1\n",
      "2018-10-26T16:53:51.317138: step 10523, loss 4.34131e-05, acc 1\n",
      "2018-10-26T16:53:51.661275: step 10524, loss 0.000108186, acc 1\n",
      "2018-10-26T16:53:52.016270: step 10525, loss 2.06348e-05, acc 1\n",
      "2018-10-26T16:53:52.353367: step 10526, loss 0.000182641, acc 1\n",
      "2018-10-26T16:53:52.723449: step 10527, loss 0.00284009, acc 1\n",
      "2018-10-26T16:53:53.062471: step 10528, loss 3.84642e-05, acc 1\n",
      "2018-10-26T16:53:53.387602: step 10529, loss 0.00251598, acc 1\n",
      "2018-10-26T16:53:53.725792: step 10530, loss 8.57889e-05, acc 1\n",
      "2018-10-26T16:53:54.071848: step 10531, loss 2.11833e-05, acc 1\n",
      "2018-10-26T16:53:54.429821: step 10532, loss 4.78865e-06, acc 1\n",
      "2018-10-26T16:53:54.759939: step 10533, loss 1.48538e-05, acc 1\n",
      "2018-10-26T16:53:55.116982: step 10534, loss 0.000172661, acc 1\n",
      "2018-10-26T16:53:55.458073: step 10535, loss 5.54003e-05, acc 1\n",
      "2018-10-26T16:53:55.833068: step 10536, loss 5.34163e-06, acc 1\n",
      "2018-10-26T16:53:56.174156: step 10537, loss 1.90439e-05, acc 1\n",
      "2018-10-26T16:53:56.507271: step 10538, loss 0.00012047, acc 1\n",
      "2018-10-26T16:53:56.836389: step 10539, loss 0.000195054, acc 1\n",
      "2018-10-26T16:53:57.188447: step 10540, loss 9.40044e-06, acc 1\n",
      "2018-10-26T16:53:57.516574: step 10541, loss 8.73048e-05, acc 1\n",
      "2018-10-26T16:53:57.856751: step 10542, loss 1.61675e-06, acc 1\n",
      "2018-10-26T16:53:58.191768: step 10543, loss 3.46326e-05, acc 1\n",
      "2018-10-26T16:53:58.533854: step 10544, loss 0.00060345, acc 1\n",
      "2018-10-26T16:53:58.868957: step 10545, loss 1.62911e-05, acc 1\n",
      "2018-10-26T16:53:59.232985: step 10546, loss 0.000230863, acc 1\n",
      "2018-10-26T16:53:59.569089: step 10547, loss 4.14219e-06, acc 1\n",
      "2018-10-26T16:53:59.908185: step 10548, loss 0.000204284, acc 1\n",
      "2018-10-26T16:54:00.251267: step 10549, loss 5.15326e-06, acc 1\n",
      "2018-10-26T16:54:00.588364: step 10550, loss 1.30414e-05, acc 1\n",
      "2018-10-26T16:54:00.932446: step 10551, loss 2.5998e-05, acc 1\n",
      "2018-10-26T16:54:01.269543: step 10552, loss 0.000159999, acc 1\n",
      "2018-10-26T16:54:01.610693: step 10553, loss 0.000996258, acc 1\n",
      "2018-10-26T16:54:01.949727: step 10554, loss 0.00017024, acc 1\n",
      "2018-10-26T16:54:02.272125: step 10555, loss 1.00582e-06, acc 1\n",
      "2018-10-26T16:54:02.609963: step 10556, loss 0.00775597, acc 1\n",
      "2018-10-26T16:54:02.940082: step 10557, loss 2.41199e-05, acc 1\n",
      "2018-10-26T16:54:03.262220: step 10558, loss 5.88926e-05, acc 1\n",
      "2018-10-26T16:54:03.601397: step 10559, loss 1.8364e-05, acc 1\n",
      "2018-10-26T16:54:03.937414: step 10560, loss 0.00119577, acc 1\n",
      "2018-10-26T16:54:04.276509: step 10561, loss 0.00195629, acc 1\n",
      "2018-10-26T16:54:04.616599: step 10562, loss 3.54408e-05, acc 1\n",
      "2018-10-26T16:54:04.954697: step 10563, loss 0.00043395, acc 1\n",
      "2018-10-26T16:54:05.297783: step 10564, loss 2.6077e-08, acc 1\n",
      "2018-10-26T16:54:05.620920: step 10565, loss 0.00113182, acc 1\n",
      "2018-10-26T16:54:05.960011: step 10566, loss 0.000151051, acc 1\n",
      "2018-10-26T16:54:06.303098: step 10567, loss 5.33324e-05, acc 1\n",
      "2018-10-26T16:54:06.655189: step 10568, loss 3.25756e-06, acc 1\n",
      "2018-10-26T16:54:06.996244: step 10569, loss 4.02798e-05, acc 1\n",
      "2018-10-26T16:54:07.349298: step 10570, loss 1.31255e-05, acc 1\n",
      "2018-10-26T16:54:07.686402: step 10571, loss 4.31728e-06, acc 1\n",
      "2018-10-26T16:54:08.023501: step 10572, loss 1.2743e-05, acc 1\n",
      "2018-10-26T16:54:08.363589: step 10573, loss 6.3999e-05, acc 1\n",
      "2018-10-26T16:54:08.703682: step 10574, loss 6.11132e-05, acc 1\n",
      "2018-10-26T16:54:09.047790: step 10575, loss 3.01248e-05, acc 1\n",
      "2018-10-26T16:54:09.384861: step 10576, loss 9.93962e-06, acc 1\n",
      "2018-10-26T16:54:09.717973: step 10577, loss 0.000837854, acc 1\n",
      "2018-10-26T16:54:10.079068: step 10578, loss 3.54804e-06, acc 1\n",
      "2018-10-26T16:54:10.458054: step 10579, loss 2.8162e-06, acc 1\n",
      "2018-10-26T16:54:10.813044: step 10580, loss 0.000181948, acc 1\n",
      "2018-10-26T16:54:11.174096: step 10581, loss 0.00270922, acc 1\n",
      "2018-10-26T16:54:11.507189: step 10582, loss 0.00122758, acc 1\n",
      "2018-10-26T16:54:11.847282: step 10583, loss 0.00276151, acc 1\n",
      "2018-10-26T16:54:12.188370: step 10584, loss 1.5577e-05, acc 1\n",
      "2018-10-26T16:54:12.534506: step 10585, loss 0.000187086, acc 1\n",
      "2018-10-26T16:54:12.906488: step 10586, loss 2.78243e-05, acc 1\n",
      "2018-10-26T16:54:13.256519: step 10587, loss 6.57511e-07, acc 1\n",
      "2018-10-26T16:54:13.605582: step 10588, loss 0.000112482, acc 1\n",
      "2018-10-26T16:54:13.938694: step 10589, loss 5.38373e-05, acc 1\n",
      "2018-10-26T16:54:14.288758: step 10590, loss 0.024874, acc 0.984375\n",
      "2018-10-26T16:54:14.619876: step 10591, loss 0.000341827, acc 1\n",
      "2018-10-26T16:54:14.959964: step 10592, loss 0.00056311, acc 1\n",
      "2018-10-26T16:54:15.322001: step 10593, loss 0.0015241, acc 1\n",
      "2018-10-26T16:54:15.744867: step 10594, loss 0.000293505, acc 1\n",
      "2018-10-26T16:54:16.140810: step 10595, loss 0.000101468, acc 1\n",
      "2018-10-26T16:54:16.559691: step 10596, loss 5.35276e-06, acc 1\n",
      "2018-10-26T16:54:16.959624: step 10597, loss 7.8789e-07, acc 1\n",
      "2018-10-26T16:54:17.376513: step 10598, loss 3.68064e-05, acc 1\n",
      "2018-10-26T16:54:17.751506: step 10599, loss 0.000226884, acc 1\n",
      "2018-10-26T16:54:18.162407: step 10600, loss 5.22633e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:54:18.955290: step 10600, loss 3.64575, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10600\n",
      "\n",
      "2018-10-26T16:54:19.558679: step 10601, loss 3.64459e-05, acc 1\n",
      "2018-10-26T16:54:19.910737: step 10602, loss 1.80048e-05, acc 1\n",
      "2018-10-26T16:54:20.358639: step 10603, loss 0.000104837, acc 1\n",
      "2018-10-26T16:54:20.704615: step 10604, loss 0.000277753, acc 1\n",
      "2018-10-26T16:54:21.118509: step 10605, loss 0.000273211, acc 1\n",
      "2018-10-26T16:54:21.504480: step 10606, loss 6.31191e-05, acc 1\n",
      "2018-10-26T16:54:21.926353: step 10607, loss 0.00020376, acc 1\n",
      "2018-10-26T16:54:22.309327: step 10608, loss 0.000442209, acc 1\n",
      "2018-10-26T16:54:22.753472: step 10609, loss 0.00483555, acc 1\n",
      "2018-10-26T16:54:23.128299: step 10610, loss 5.03405e-05, acc 1\n",
      "2018-10-26T16:54:23.521159: step 10611, loss 0.0029569, acc 1\n",
      "2018-10-26T16:54:23.920126: step 10612, loss 0.000160706, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:54:24.378800: step 10613, loss 7.88508e-05, acc 1\n",
      "2018-10-26T16:54:24.844555: step 10614, loss 1.81505e-05, acc 1\n",
      "2018-10-26T16:54:25.328262: step 10615, loss 0.000539217, acc 1\n",
      "2018-10-26T16:54:25.806113: step 10616, loss 1.68252e-05, acc 1\n",
      "2018-10-26T16:54:26.267787: step 10617, loss 5.40008e-05, acc 1\n",
      "2018-10-26T16:54:26.704584: step 10618, loss 2.33231e-05, acc 1\n",
      "2018-10-26T16:54:27.131444: step 10619, loss 2.74056e-05, acc 1\n",
      "2018-10-26T16:54:27.514422: step 10620, loss 9.31289e-05, acc 1\n",
      "2018-10-26T16:54:27.863487: step 10621, loss 5.19462e-06, acc 1\n",
      "2018-10-26T16:54:28.275388: step 10622, loss 5.50074e-05, acc 1\n",
      "2018-10-26T16:54:28.686290: step 10623, loss 8.5249e-06, acc 1\n",
      "2018-10-26T16:54:29.075250: step 10624, loss 3.21287e-06, acc 1\n",
      "2018-10-26T16:54:29.448258: step 10625, loss 0.000107486, acc 1\n",
      "2018-10-26T16:54:29.804303: step 10626, loss 1.46402e-06, acc 1\n",
      "2018-10-26T16:54:30.148382: step 10627, loss 3.18608e-05, acc 1\n",
      "2018-10-26T16:54:30.466613: step 10628, loss 3.55681e-05, acc 1\n",
      "2018-10-26T16:54:30.857489: step 10629, loss 0.00103923, acc 1\n",
      "2018-10-26T16:54:31.208552: step 10630, loss 0.000745645, acc 1\n",
      "2018-10-26T16:54:31.545653: step 10631, loss 9.50968e-06, acc 1\n",
      "2018-10-26T16:54:31.879759: step 10632, loss 1.87484e-05, acc 1\n",
      "2018-10-26T16:54:32.229825: step 10633, loss 0.0395568, acc 0.984375\n",
      "2018-10-26T16:54:32.566927: step 10634, loss 8.86518e-05, acc 1\n",
      "2018-10-26T16:54:32.917983: step 10635, loss 0.000329684, acc 1\n",
      "2018-10-26T16:54:33.237130: step 10636, loss 0.00273965, acc 1\n",
      "2018-10-26T16:54:33.609141: step 10637, loss 0.000911451, acc 1\n",
      "2018-10-26T16:54:33.941308: step 10638, loss 0.000309055, acc 1\n",
      "2018-10-26T16:54:34.299293: step 10639, loss 0.00010299, acc 1\n",
      "2018-10-26T16:54:34.648360: step 10640, loss 0.000499662, acc 1\n",
      "2018-10-26T16:54:34.983465: step 10641, loss 1.28147e-06, acc 1\n",
      "2018-10-26T16:54:35.304607: step 10642, loss 2.04828e-05, acc 1\n",
      "2018-10-26T16:54:35.682597: step 10643, loss 2.70244e-05, acc 1\n",
      "2018-10-26T16:54:36.093500: step 10644, loss 0.00326747, acc 1\n",
      "2018-10-26T16:54:36.459523: step 10645, loss 2.01165e-07, acc 1\n",
      "2018-10-26T16:54:36.874411: step 10646, loss 0.00613356, acc 1\n",
      "2018-10-26T16:54:37.204594: step 10647, loss 0.000184197, acc 1\n",
      "2018-10-26T16:54:37.533651: step 10648, loss 1.27907e-05, acc 1\n",
      "2018-10-26T16:54:37.883715: step 10649, loss 6.96323e-05, acc 1\n",
      "2018-10-26T16:54:38.255722: step 10650, loss 2.14857e-05, acc 1\n",
      "2018-10-26T16:54:38.592823: step 10651, loss 2.53961e-05, acc 1\n",
      "2018-10-26T16:54:38.921945: step 10652, loss 6.90874e-05, acc 1\n",
      "2018-10-26T16:54:39.253133: step 10653, loss 2.80661e-05, acc 1\n",
      "2018-10-26T16:54:39.605126: step 10654, loss 0.000355071, acc 1\n",
      "2018-10-26T16:54:39.938229: step 10655, loss 0.00686923, acc 1\n",
      "2018-10-26T16:54:40.271337: step 10656, loss 0.00198207, acc 1\n",
      "2018-10-26T16:54:40.628384: step 10657, loss 8.09391e-06, acc 1\n",
      "2018-10-26T16:54:41.006371: step 10658, loss 8.79326e-05, acc 1\n",
      "2018-10-26T16:54:41.339483: step 10659, loss 4.56639e-05, acc 1\n",
      "2018-10-26T16:54:41.668603: step 10660, loss 0.000167901, acc 1\n",
      "2018-10-26T16:54:42.008695: step 10661, loss 0.000101304, acc 1\n",
      "2018-10-26T16:54:42.357762: step 10662, loss 1.85932e-05, acc 1\n",
      "2018-10-26T16:54:42.737750: step 10663, loss 0.000168098, acc 1\n",
      "2018-10-26T16:54:43.087836: step 10664, loss 0.000163658, acc 1\n",
      "2018-10-26T16:54:43.419924: step 10665, loss 9.79591e-05, acc 1\n",
      "2018-10-26T16:54:43.755032: step 10666, loss 6.86299e-05, acc 1\n",
      "2018-10-26T16:54:44.073235: step 10667, loss 1.8887e-06, acc 1\n",
      "2018-10-26T16:54:44.405291: step 10668, loss 1.05161e-05, acc 1\n",
      "2018-10-26T16:54:44.724441: step 10669, loss 0.000195004, acc 1\n",
      "2018-10-26T16:54:45.041639: step 10670, loss 7.00314e-06, acc 1\n",
      "2018-10-26T16:54:45.373704: step 10671, loss 0.000156099, acc 1\n",
      "2018-10-26T16:54:45.713866: step 10672, loss 2.25563e-05, acc 1\n",
      "2018-10-26T16:54:46.055883: step 10673, loss 9.40625e-07, acc 1\n",
      "2018-10-26T16:54:46.405949: step 10674, loss 0.000326437, acc 1\n",
      "2018-10-26T16:54:46.743044: step 10675, loss 7.06369e-05, acc 1\n",
      "2018-10-26T16:54:47.112153: step 10676, loss 3.5579e-05, acc 1\n",
      "2018-10-26T16:54:47.500023: step 10677, loss 3.12143e-05, acc 1\n",
      "2018-10-26T16:54:47.930959: step 10678, loss 0.000146672, acc 1\n",
      "2018-10-26T16:54:48.273955: step 10679, loss 0.000976107, acc 1\n",
      "2018-10-26T16:54:48.612055: step 10680, loss 0.000100226, acc 1\n",
      "2018-10-26T16:54:48.946162: step 10681, loss 9.5864e-06, acc 1\n",
      "2018-10-26T16:54:49.285256: step 10682, loss 3.11974e-05, acc 1\n",
      "2018-10-26T16:54:49.640305: step 10683, loss 1.37166e-05, acc 1\n",
      "2018-10-26T16:54:49.983428: step 10684, loss 0.000173071, acc 1\n",
      "2018-10-26T16:54:50.346416: step 10685, loss 0.00266043, acc 1\n",
      "2018-10-26T16:54:50.691535: step 10686, loss 0.00343047, acc 1\n",
      "2018-10-26T16:54:51.054529: step 10687, loss 0.00636563, acc 1\n",
      "2018-10-26T16:54:51.416557: step 10688, loss 9.18272e-07, acc 1\n",
      "2018-10-26T16:54:51.747673: step 10689, loss 8.37738e-05, acc 1\n",
      "2018-10-26T16:54:52.092753: step 10690, loss 0.0004054, acc 1\n",
      "2018-10-26T16:54:52.423870: step 10691, loss 1.87142e-05, acc 1\n",
      "2018-10-26T16:54:52.755979: step 10692, loss 0.00145211, acc 1\n",
      "2018-10-26T16:54:53.087097: step 10693, loss 2.43252e-06, acc 1\n",
      "2018-10-26T16:54:53.458135: step 10694, loss 0.000600153, acc 1\n",
      "2018-10-26T16:54:53.794209: step 10695, loss 1.66703e-06, acc 1\n",
      "2018-10-26T16:54:54.145267: step 10696, loss 0.000659781, acc 1\n",
      "2018-10-26T16:54:54.546197: step 10697, loss 4.17602e-05, acc 1\n",
      "2018-10-26T16:54:54.901345: step 10698, loss 0.00100122, acc 1\n",
      "2018-10-26T16:54:55.236355: step 10699, loss 1.75403e-05, acc 1\n",
      "2018-10-26T16:54:55.599382: step 10700, loss 0.000368035, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:54:56.386309: step 10700, loss 3.65836, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10700\n",
      "\n",
      "2018-10-26T16:54:56.984682: step 10701, loss 0.000246575, acc 1\n",
      "2018-10-26T16:54:57.319785: step 10702, loss 3.36374e-05, acc 1\n",
      "2018-10-26T16:54:57.778670: step 10703, loss 1.00104e-05, acc 1\n",
      "2018-10-26T16:54:58.149647: step 10704, loss 0.00136612, acc 1\n",
      "2018-10-26T16:54:58.499632: step 10705, loss 8.84679e-05, acc 1\n",
      "2018-10-26T16:54:58.835738: step 10706, loss 1.28e-05, acc 1\n",
      "2018-10-26T16:54:59.183807: step 10707, loss 0.00134959, acc 1\n",
      "2018-10-26T16:54:59.537862: step 10708, loss 0.00759605, acc 1\n",
      "2018-10-26T16:54:59.872965: step 10709, loss 3.60028e-06, acc 1\n",
      "2018-10-26T16:55:00.234997: step 10710, loss 3.08116e-05, acc 1\n",
      "2018-10-26T16:55:00.589149: step 10711, loss 4.9008e-05, acc 1\n",
      "2018-10-26T16:55:00.970034: step 10712, loss 0.00283061, acc 1\n",
      "2018-10-26T16:55:01.307133: step 10713, loss 0.000323856, acc 1\n",
      "2018-10-26T16:55:01.645260: step 10714, loss 0.000313331, acc 1\n",
      "2018-10-26T16:55:01.991307: step 10715, loss 0.000767589, acc 1\n",
      "2018-10-26T16:55:02.325411: step 10716, loss 9.79304e-05, acc 1\n",
      "2018-10-26T16:55:02.696423: step 10717, loss 4.32286e-05, acc 1\n",
      "2018-10-26T16:55:03.047486: step 10718, loss 0.000134312, acc 1\n",
      "2018-10-26T16:55:03.379597: step 10719, loss 2.94661e-06, acc 1\n",
      "2018-10-26T16:55:03.729710: step 10720, loss 0.000184611, acc 1\n",
      "2018-10-26T16:55:04.107651: step 10721, loss 0.000109235, acc 1\n",
      "2018-10-26T16:55:04.443751: step 10722, loss 0.000180512, acc 1\n",
      "2018-10-26T16:55:04.775867: step 10723, loss 0.000158624, acc 1\n",
      "2018-10-26T16:55:05.131916: step 10724, loss 2.02261e-05, acc 1\n",
      "2018-10-26T16:55:05.478048: step 10725, loss 0.00203682, acc 1\n",
      "2018-10-26T16:55:05.819079: step 10726, loss 0.000172978, acc 1\n",
      "2018-10-26T16:55:06.166153: step 10727, loss 7.26426e-07, acc 1\n",
      "2018-10-26T16:55:06.508263: step 10728, loss 0.00025181, acc 1\n",
      "2018-10-26T16:55:06.849324: step 10729, loss 4.26831e-05, acc 1\n",
      "2018-10-26T16:55:07.191477: step 10730, loss 6.91971e-05, acc 1\n",
      "2018-10-26T16:55:07.536491: step 10731, loss 4.84286e-07, acc 1\n",
      "2018-10-26T16:55:07.866608: step 10732, loss 0.000199626, acc 1\n",
      "2018-10-26T16:55:08.207695: step 10733, loss 3.03801e-05, acc 1\n",
      "2018-10-26T16:55:08.556764: step 10734, loss 0.00037729, acc 1\n",
      "2018-10-26T16:55:08.913807: step 10735, loss 1.39324e-06, acc 1\n",
      "2018-10-26T16:55:09.288805: step 10736, loss 0.00805957, acc 1\n",
      "2018-10-26T16:55:09.632888: step 10737, loss 0.00338311, acc 1\n",
      "2018-10-26T16:55:09.977965: step 10738, loss 2.08267e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:55:10.319056: step 10739, loss 3.77344e-06, acc 1\n",
      "2018-10-26T16:55:10.655154: step 10740, loss 0.000410959, acc 1\n",
      "2018-10-26T16:55:11.001234: step 10741, loss 0.000590977, acc 1\n",
      "2018-10-26T16:55:11.331348: step 10742, loss 1.08312e-05, acc 1\n",
      "2018-10-26T16:55:11.668447: step 10743, loss 0.000518376, acc 1\n",
      "2018-10-26T16:55:12.007545: step 10744, loss 8.81526e-05, acc 1\n",
      "2018-10-26T16:55:12.363592: step 10745, loss 4.86147e-07, acc 1\n",
      "2018-10-26T16:55:12.689718: step 10746, loss 0.00394489, acc 1\n",
      "2018-10-26T16:55:13.019836: step 10747, loss 5.64222e-05, acc 1\n",
      "2018-10-26T16:55:13.363917: step 10748, loss 2.52564e-05, acc 1\n",
      "2018-10-26T16:55:13.691045: step 10749, loss 0.00280381, acc 1\n",
      "2018-10-26T16:55:14.060058: step 10750, loss 0.000216621, acc 1\n",
      "2018-10-26T16:55:14.457995: step 10751, loss 1.26462e-05, acc 1\n",
      "2018-10-26T16:55:14.828007: step 10752, loss 0.0131124, acc 0.984375\n",
      "2018-10-26T16:55:15.252872: step 10753, loss 0.00030256, acc 1\n",
      "2018-10-26T16:55:15.607922: step 10754, loss 3.9014e-05, acc 1\n",
      "2018-10-26T16:55:15.924077: step 10755, loss 0.00223017, acc 1\n",
      "2018-10-26T16:55:16.279196: step 10756, loss 6.77849e-05, acc 1\n",
      "2018-10-26T16:55:16.614236: step 10757, loss 5.19338e-05, acc 1\n",
      "2018-10-26T16:55:16.947347: step 10758, loss 8.62399e-07, acc 1\n",
      "2018-10-26T16:55:17.281493: step 10759, loss 6.26927e-06, acc 1\n",
      "2018-10-26T16:55:17.612565: step 10760, loss 0.000163602, acc 1\n",
      "2018-10-26T16:55:17.953654: step 10761, loss 0.00332171, acc 1\n",
      "2018-10-26T16:55:18.316684: step 10762, loss 2.15314e-06, acc 1\n",
      "2018-10-26T16:55:18.642814: step 10763, loss 0.00101612, acc 1\n",
      "2018-10-26T16:55:18.978916: step 10764, loss 0.000110403, acc 1\n",
      "2018-10-26T16:55:19.314024: step 10765, loss 0.000818912, acc 1\n",
      "2018-10-26T16:55:19.647133: step 10766, loss 0.000189389, acc 1\n",
      "2018-10-26T16:55:19.989219: step 10767, loss 0.00569088, acc 1\n",
      "2018-10-26T16:55:20.308395: step 10768, loss 0.00153749, acc 1\n",
      "2018-10-26T16:55:20.649454: step 10769, loss 0.000121667, acc 1\n",
      "2018-10-26T16:55:20.979642: step 10770, loss 0.00469933, acc 1\n",
      "2018-10-26T16:55:21.299716: step 10771, loss 7.45461e-05, acc 1\n",
      "2018-10-26T16:55:21.618861: step 10772, loss 6.37856e-05, acc 1\n",
      "2018-10-26T16:55:21.951972: step 10773, loss 9.4544e-05, acc 1\n",
      "2018-10-26T16:55:22.286080: step 10774, loss 0.000309517, acc 1\n",
      "2018-10-26T16:55:22.626171: step 10775, loss 3.99988e-05, acc 1\n",
      "2018-10-26T16:55:22.974240: step 10776, loss 0.000192112, acc 1\n",
      "2018-10-26T16:55:23.315328: step 10777, loss 0.000729764, acc 1\n",
      "2018-10-26T16:55:23.651430: step 10778, loss 0.000989799, acc 1\n",
      "2018-10-26T16:55:24.003523: step 10779, loss 9.46198e-07, acc 1\n",
      "2018-10-26T16:55:24.319830: step 10780, loss 0.000435663, acc 1\n",
      "2018-10-26T16:55:24.658741: step 10781, loss 5.26265e-05, acc 1\n",
      "2018-10-26T16:55:24.996977: step 10782, loss 5.39652e-05, acc 1\n",
      "2018-10-26T16:55:25.323962: step 10783, loss 3.99199e-05, acc 1\n",
      "2018-10-26T16:55:25.650150: step 10784, loss 2.44743e-06, acc 1\n",
      "2018-10-26T16:55:25.981211: step 10785, loss 0.000207798, acc 1\n",
      "2018-10-26T16:55:26.312322: step 10786, loss 2.93971e-05, acc 1\n",
      "2018-10-26T16:55:26.644434: step 10787, loss 9.93273e-06, acc 1\n",
      "2018-10-26T16:55:27.038382: step 10788, loss 9.07091e-05, acc 1\n",
      "2018-10-26T16:55:27.393467: step 10789, loss 7.22173e-05, acc 1\n",
      "2018-10-26T16:55:27.759454: step 10790, loss 0.00269486, acc 1\n",
      "2018-10-26T16:55:28.144431: step 10791, loss 0.000292304, acc 1\n",
      "2018-10-26T16:55:28.507460: step 10792, loss 0.000145956, acc 1\n",
      "2018-10-26T16:55:28.890474: step 10793, loss 5.92967e-05, acc 1\n",
      "2018-10-26T16:55:29.262440: step 10794, loss 9.39558e-06, acc 1\n",
      "2018-10-26T16:55:29.651468: step 10795, loss 0.000256695, acc 1\n",
      "2018-10-26T16:55:30.103193: step 10796, loss 0.000732244, acc 1\n",
      "2018-10-26T16:55:30.528058: step 10797, loss 0.00300217, acc 1\n",
      "2018-10-26T16:55:31.012920: step 10798, loss 0.000254785, acc 1\n",
      "2018-10-26T16:55:31.496470: step 10799, loss 0.00027111, acc 1\n",
      "2018-10-26T16:55:31.916348: step 10800, loss 8.6901e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:55:32.863816: step 10800, loss 3.71691, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10800\n",
      "\n",
      "2018-10-26T16:55:33.606832: step 10801, loss 0.00190398, acc 1\n",
      "2018-10-26T16:55:34.018731: step 10802, loss 0.000958758, acc 1\n",
      "2018-10-26T16:55:34.521389: step 10803, loss 3.9226e-06, acc 1\n",
      "2018-10-26T16:55:34.973182: step 10804, loss 7.74486e-05, acc 1\n",
      "2018-10-26T16:55:35.355161: step 10805, loss 7.34759e-06, acc 1\n",
      "2018-10-26T16:55:35.687277: step 10806, loss 8.73969e-05, acc 1\n",
      "2018-10-26T16:55:36.054292: step 10807, loss 3.46065e-06, acc 1\n",
      "2018-10-26T16:55:36.409346: step 10808, loss 2.78272e-06, acc 1\n",
      "2018-10-26T16:55:36.752427: step 10809, loss 3.78649e-06, acc 1\n",
      "2018-10-26T16:55:37.088528: step 10810, loss 0.000186037, acc 1\n",
      "2018-10-26T16:55:37.460580: step 10811, loss 1.87702e-05, acc 1\n",
      "2018-10-26T16:55:37.817614: step 10812, loss 0.000124292, acc 1\n",
      "2018-10-26T16:55:38.204048: step 10813, loss 2.73593e-05, acc 1\n",
      "2018-10-26T16:55:38.566580: step 10814, loss 4.3402e-05, acc 1\n",
      "2018-10-26T16:55:38.925623: step 10815, loss 0.0312828, acc 0.984375\n",
      "2018-10-26T16:55:39.269702: step 10816, loss 6.76018e-06, acc 1\n",
      "2018-10-26T16:55:39.613783: step 10817, loss 6.26307e-05, acc 1\n",
      "2018-10-26T16:55:39.958860: step 10818, loss 0.000104097, acc 1\n",
      "2018-10-26T16:55:40.308927: step 10819, loss 0.000842954, acc 1\n",
      "2018-10-26T16:55:40.672951: step 10820, loss 0.000437006, acc 1\n",
      "2018-10-26T16:55:40.996088: step 10821, loss 5.07162e-06, acc 1\n",
      "2018-10-26T16:55:41.322217: step 10822, loss 0.000401344, acc 1\n",
      "2018-10-26T16:55:41.657322: step 10823, loss 2.04172e-05, acc 1\n",
      "2018-10-26T16:55:41.991537: step 10824, loss 1.09895e-06, acc 1\n",
      "2018-10-26T16:55:42.363435: step 10825, loss 2.28566e-05, acc 1\n",
      "2018-10-26T16:55:42.704527: step 10826, loss 4.71578e-05, acc 1\n",
      "2018-10-26T16:55:43.046612: step 10827, loss 0.000277917, acc 1\n",
      "2018-10-26T16:55:43.383710: step 10828, loss 4.71054e-05, acc 1\n",
      "2018-10-26T16:55:43.728790: step 10829, loss 0.000270365, acc 1\n",
      "2018-10-26T16:55:44.091817: step 10830, loss 0.0011543, acc 1\n",
      "2018-10-26T16:55:44.433903: step 10831, loss 3.46238e-05, acc 1\n",
      "2018-10-26T16:55:44.762062: step 10832, loss 6.97399e-05, acc 1\n",
      "2018-10-26T16:55:45.115083: step 10833, loss 0.000137475, acc 1\n",
      "2018-10-26T16:55:45.457168: step 10834, loss 0.00264527, acc 1\n",
      "2018-10-26T16:55:45.791277: step 10835, loss 0.000252801, acc 1\n",
      "2018-10-26T16:55:46.134360: step 10836, loss 7.94981e-06, acc 1\n",
      "2018-10-26T16:55:46.452512: step 10837, loss 1.83775e-05, acc 1\n",
      "2018-10-26T16:55:46.795593: step 10838, loss 0.00167563, acc 1\n",
      "2018-10-26T16:55:47.163611: step 10839, loss 0.0129635, acc 0.984375\n",
      "2018-10-26T16:55:47.505698: step 10840, loss 5.9825e-06, acc 1\n",
      "2018-10-26T16:55:47.856759: step 10841, loss 1.57162e-05, acc 1\n",
      "2018-10-26T16:55:48.204830: step 10842, loss 0.000171666, acc 1\n",
      "2018-10-26T16:55:48.546914: step 10843, loss 0.00319999, acc 1\n",
      "2018-10-26T16:55:48.919919: step 10844, loss 0.00324705, acc 1\n",
      "2018-10-26T16:55:49.298907: step 10845, loss 0.00678375, acc 1\n",
      "2018-10-26T16:55:49.647976: step 10846, loss 0.000355083, acc 1\n",
      "2018-10-26T16:55:50.022970: step 10847, loss 0.000178605, acc 1\n",
      "2018-10-26T16:55:50.374091: step 10848, loss 3.51849e-05, acc 1\n",
      "2018-10-26T16:55:50.725097: step 10849, loss 1.74341e-05, acc 1\n",
      "2018-10-26T16:55:51.101133: step 10850, loss 8.83935e-05, acc 1\n",
      "2018-10-26T16:55:51.473099: step 10851, loss 3.20519e-05, acc 1\n",
      "2018-10-26T16:55:51.806206: step 10852, loss 1.82535e-06, acc 1\n",
      "2018-10-26T16:55:52.136351: step 10853, loss 0.000577392, acc 1\n",
      "2018-10-26T16:55:52.486391: step 10854, loss 9.39223e-05, acc 1\n",
      "2018-10-26T16:55:52.829508: step 10855, loss 3.33214e-06, acc 1\n",
      "2018-10-26T16:55:53.183529: step 10856, loss 2.06802e-05, acc 1\n",
      "2018-10-26T16:55:53.526609: step 10857, loss 0.000358515, acc 1\n",
      "2018-10-26T16:55:53.896624: step 10858, loss 9.46497e-06, acc 1\n",
      "2018-10-26T16:55:54.253670: step 10859, loss 6.48803e-05, acc 1\n",
      "2018-10-26T16:55:54.608721: step 10860, loss 0.000116585, acc 1\n",
      "2018-10-26T16:55:54.946813: step 10861, loss 0.00026243, acc 1\n",
      "2018-10-26T16:55:55.298877: step 10862, loss 0.00022189, acc 1\n",
      "2018-10-26T16:55:55.648941: step 10863, loss 0.000198157, acc 1\n",
      "2018-10-26T16:55:55.999006: step 10864, loss 0.00050654, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:55:56.350069: step 10865, loss 0.00542477, acc 1\n",
      "2018-10-26T16:55:56.708107: step 10866, loss 4.50376e-05, acc 1\n",
      "2018-10-26T16:55:57.052190: step 10867, loss 0.00122463, acc 1\n",
      "2018-10-26T16:55:57.411304: step 10868, loss 0.000253966, acc 1\n",
      "2018-10-26T16:55:57.758324: step 10869, loss 0.014288, acc 0.984375\n",
      "2018-10-26T16:55:58.110433: step 10870, loss 5.06195e-06, acc 1\n",
      "2018-10-26T16:55:58.457434: step 10871, loss 9.05639e-05, acc 1\n",
      "2018-10-26T16:55:58.795599: step 10872, loss 0.000418451, acc 1\n",
      "2018-10-26T16:55:59.137617: step 10873, loss 0.000576879, acc 1\n",
      "2018-10-26T16:55:59.484689: step 10874, loss 9.29747e-05, acc 1\n",
      "2018-10-26T16:55:59.855699: step 10875, loss 1.05208e-05, acc 1\n",
      "2018-10-26T16:56:00.237677: step 10876, loss 1.74165e-05, acc 1\n",
      "2018-10-26T16:56:00.579766: step 10877, loss 1.53357e-05, acc 1\n",
      "2018-10-26T16:56:00.938805: step 10878, loss 0.00178873, acc 1\n",
      "2018-10-26T16:56:01.280934: step 10879, loss 0.000164107, acc 1\n",
      "2018-10-26T16:56:01.615994: step 10880, loss 0.000214981, acc 1\n",
      "2018-10-26T16:56:01.942126: step 10881, loss 5.33594e-06, acc 1\n",
      "2018-10-26T16:56:02.275264: step 10882, loss 3.54934e-05, acc 1\n",
      "2018-10-26T16:56:02.598400: step 10883, loss 2.64904e-05, acc 1\n",
      "2018-10-26T16:56:02.955419: step 10884, loss 0.000330984, acc 1\n",
      "2018-10-26T16:56:03.286535: step 10885, loss 0.000620596, acc 1\n",
      "2018-10-26T16:56:03.634601: step 10886, loss 0.000412963, acc 1\n",
      "2018-10-26T16:56:03.973699: step 10887, loss 2.96513e-06, acc 1\n",
      "2018-10-26T16:56:04.314783: step 10888, loss 0.0033019, acc 1\n",
      "2018-10-26T16:56:04.639919: step 10889, loss 1.51563e-05, acc 1\n",
      "2018-10-26T16:56:04.988983: step 10890, loss 2.79396e-07, acc 1\n",
      "2018-10-26T16:56:05.328084: step 10891, loss 6.19555e-05, acc 1\n",
      "2018-10-26T16:56:05.669165: step 10892, loss 0.00021845, acc 1\n",
      "2018-10-26T16:56:06.021225: step 10893, loss 9.85549e-05, acc 1\n",
      "2018-10-26T16:56:06.357349: step 10894, loss 2.83535e-05, acc 1\n",
      "2018-10-26T16:56:06.690437: step 10895, loss 2.17929e-07, acc 1\n",
      "2018-10-26T16:56:07.043493: step 10896, loss 6.14288e-05, acc 1\n",
      "2018-10-26T16:56:07.358652: step 10897, loss 3.50559e-05, acc 1\n",
      "2018-10-26T16:56:07.689770: step 10898, loss 0.000262226, acc 1\n",
      "2018-10-26T16:56:08.048807: step 10899, loss 2.21653e-05, acc 1\n",
      "2018-10-26T16:56:08.404857: step 10900, loss 0.000145848, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:56:09.195743: step 10900, loss 3.77799, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-10900\n",
      "\n",
      "2018-10-26T16:56:09.801129: step 10901, loss 0.0001402, acc 1\n",
      "2018-10-26T16:56:10.138228: step 10902, loss 6.82045e-06, acc 1\n",
      "2018-10-26T16:56:10.603981: step 10903, loss 0.000111211, acc 1\n",
      "2018-10-26T16:56:10.966014: step 10904, loss 4.46896e-05, acc 1\n",
      "2018-10-26T16:56:11.309096: step 10905, loss 0.000195545, acc 1\n",
      "2018-10-26T16:56:11.658167: step 10906, loss 9.53447e-05, acc 1\n",
      "2018-10-26T16:56:11.999252: step 10907, loss 5.11516e-05, acc 1\n",
      "2018-10-26T16:56:12.343336: step 10908, loss 1.76631e-05, acc 1\n",
      "2018-10-26T16:56:12.666470: step 10909, loss 0.000280583, acc 1\n",
      "2018-10-26T16:56:13.002573: step 10910, loss 0.000106029, acc 1\n",
      "2018-10-26T16:56:13.324711: step 10911, loss 2.94552e-05, acc 1\n",
      "2018-10-26T16:56:13.658822: step 10912, loss 2.37873e-05, acc 1\n",
      "2018-10-26T16:56:14.001901: step 10913, loss 5.25207e-06, acc 1\n",
      "2018-10-26T16:56:14.323077: step 10914, loss 7.65491e-05, acc 1\n",
      "2018-10-26T16:56:14.667129: step 10915, loss 0.000139999, acc 1\n",
      "2018-10-26T16:56:14.996246: step 10916, loss 8.58969e-06, acc 1\n",
      "2018-10-26T16:56:15.325564: step 10917, loss 0.000536506, acc 1\n",
      "2018-10-26T16:56:15.660471: step 10918, loss 0.000150613, acc 1\n",
      "2018-10-26T16:56:15.979618: step 10919, loss 9.87823e-06, acc 1\n",
      "2018-10-26T16:56:16.317718: step 10920, loss 5.30266e-06, acc 1\n",
      "2018-10-26T16:56:16.660797: step 10921, loss 0.000251672, acc 1\n",
      "2018-10-26T16:56:17.009868: step 10922, loss 0.00372835, acc 1\n",
      "2018-10-26T16:56:17.342978: step 10923, loss 0.00430215, acc 1\n",
      "2018-10-26T16:56:17.686061: step 10924, loss 5.31096e-05, acc 1\n",
      "2018-10-26T16:56:18.033133: step 10925, loss 0.000804214, acc 1\n",
      "2018-10-26T16:56:18.401191: step 10926, loss 6.75712e-06, acc 1\n",
      "2018-10-26T16:56:18.741303: step 10927, loss 7.48546e-06, acc 1\n",
      "2018-10-26T16:56:19.076379: step 10928, loss 0.0458395, acc 0.984375\n",
      "2018-10-26T16:56:19.409453: step 10929, loss 1.05323e-05, acc 1\n",
      "2018-10-26T16:56:19.741611: step 10930, loss 7.69916e-05, acc 1\n",
      "2018-10-26T16:56:20.082658: step 10931, loss 2.89568e-05, acc 1\n",
      "2018-10-26T16:56:20.436730: step 10932, loss 0.00134895, acc 1\n",
      "2018-10-26T16:56:20.778798: step 10933, loss 0.00688874, acc 1\n",
      "2018-10-26T16:56:21.122922: step 10934, loss 4.67809e-05, acc 1\n",
      "2018-10-26T16:56:21.474935: step 10935, loss 0.000163044, acc 1\n",
      "2018-10-26T16:56:21.821014: step 10936, loss 2.67763e-05, acc 1\n",
      "2018-10-26T16:56:22.160105: step 10937, loss 6.52779e-06, acc 1\n",
      "2018-10-26T16:56:22.493215: step 10938, loss 6.07054e-05, acc 1\n",
      "2018-10-26T16:56:22.834341: step 10939, loss 5.17303e-05, acc 1\n",
      "2018-10-26T16:56:23.193343: step 10940, loss 9.53333e-05, acc 1\n",
      "2018-10-26T16:56:23.524462: step 10941, loss 0.00042776, acc 1\n",
      "2018-10-26T16:56:23.861606: step 10942, loss 0.000565759, acc 1\n",
      "2018-10-26T16:56:24.199707: step 10943, loss 2.62105e-05, acc 1\n",
      "2018-10-26T16:56:24.538750: step 10944, loss 0.000683879, acc 1\n",
      "2018-10-26T16:56:24.871859: step 10945, loss 2.03953e-06, acc 1\n",
      "2018-10-26T16:56:25.204969: step 10946, loss 0.0364292, acc 0.984375\n",
      "2018-10-26T16:56:25.549053: step 10947, loss 0.000189333, acc 1\n",
      "2018-10-26T16:56:25.880168: step 10948, loss 0.000371292, acc 1\n",
      "2018-10-26T16:56:26.211326: step 10949, loss 7.55437e-05, acc 1\n",
      "2018-10-26T16:56:26.539407: step 10950, loss 5.06636e-07, acc 1\n",
      "2018-10-26T16:56:26.865532: step 10951, loss 0.000557876, acc 1\n",
      "2018-10-26T16:56:27.218590: step 10952, loss 2.94894e-05, acc 1\n",
      "2018-10-26T16:56:27.589666: step 10953, loss 0.014793, acc 0.984375\n",
      "2018-10-26T16:56:27.923709: step 10954, loss 0.00558307, acc 1\n",
      "2018-10-26T16:56:28.263843: step 10955, loss 0.000563647, acc 1\n",
      "2018-10-26T16:56:28.642783: step 10956, loss 0.000391822, acc 1\n",
      "2018-10-26T16:56:28.986868: step 10957, loss 2.66464e-05, acc 1\n",
      "2018-10-26T16:56:29.315987: step 10958, loss 2.78498e-05, acc 1\n",
      "2018-10-26T16:56:29.660109: step 10959, loss 0.00146912, acc 1\n",
      "2018-10-26T16:56:30.002153: step 10960, loss 0.000177442, acc 1\n",
      "2018-10-26T16:56:30.346405: step 10961, loss 9.22906e-05, acc 1\n",
      "2018-10-26T16:56:30.683333: step 10962, loss 0.00157953, acc 1\n",
      "2018-10-26T16:56:31.024422: step 10963, loss 0.0001728, acc 1\n",
      "2018-10-26T16:56:31.362519: step 10964, loss 0.000223576, acc 1\n",
      "2018-10-26T16:56:31.703605: step 10965, loss 0.000433959, acc 1\n",
      "2018-10-26T16:56:32.069629: step 10966, loss 9.44875e-05, acc 1\n",
      "2018-10-26T16:56:32.466618: step 10967, loss 0.00100674, acc 1\n",
      "2018-10-26T16:56:32.839571: step 10968, loss 6.64627e-05, acc 1\n",
      "2018-10-26T16:56:33.274409: step 10969, loss 0.000863278, acc 1\n",
      "2018-10-26T16:56:33.633453: step 10970, loss 5.9388e-05, acc 1\n",
      "2018-10-26T16:56:34.055352: step 10971, loss 7.1289e-06, acc 1\n",
      "2018-10-26T16:56:34.435307: step 10972, loss 0.00016185, acc 1\n",
      "2018-10-26T16:56:34.809307: step 10973, loss 0.000283204, acc 1\n",
      "2018-10-26T16:56:35.205297: step 10974, loss 2.57036e-06, acc 1\n",
      "2018-10-26T16:56:35.586272: step 10975, loss 2.31514e-06, acc 1\n",
      "2018-10-26T16:56:36.006110: step 10976, loss 0.00649519, acc 1\n",
      "2018-10-26T16:56:36.431054: step 10977, loss 7.66021e-06, acc 1\n",
      "2018-10-26T16:56:36.888750: step 10978, loss 0.00312971, acc 1\n",
      "2018-10-26T16:56:37.347587: step 10979, loss 6.20256e-07, acc 1\n",
      "2018-10-26T16:56:37.796327: step 10980, loss 8.8206e-06, acc 1\n",
      "2018-10-26T16:56:38.196258: step 10981, loss 5.42146e-06, acc 1\n",
      "2018-10-26T16:56:38.615139: step 10982, loss 0.00644468, acc 1\n",
      "2018-10-26T16:56:39.034020: step 10983, loss 4.64646e-05, acc 1\n",
      "2018-10-26T16:56:39.478831: step 10984, loss 2.91163e-05, acc 1\n",
      "2018-10-26T16:56:39.955559: step 10985, loss 8.50426e-06, acc 1\n",
      "2018-10-26T16:56:40.403362: step 10986, loss 2.69215e-05, acc 1\n",
      "2018-10-26T16:56:40.815260: step 10987, loss 3.40863e-07, acc 1\n",
      "2018-10-26T16:56:41.217187: step 10988, loss 2.76335e-05, acc 1\n",
      "2018-10-26T16:56:41.572239: step 10989, loss 2.42139e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:56:41.919312: step 10990, loss 5.67526e-05, acc 1\n",
      "2018-10-26T16:56:42.242450: step 10991, loss 7.82305e-07, acc 1\n",
      "2018-10-26T16:56:42.562591: step 10992, loss 0.000162442, acc 1\n",
      "2018-10-26T16:56:42.914730: step 10993, loss 9.36622e-05, acc 1\n",
      "2018-10-26T16:56:43.257760: step 10994, loss 0.000252611, acc 1\n",
      "2018-10-26T16:56:43.591887: step 10995, loss 5.5497e-05, acc 1\n",
      "2018-10-26T16:56:43.925949: step 10996, loss 0.000134038, acc 1\n",
      "2018-10-26T16:56:44.245098: step 10997, loss 5.50198e-05, acc 1\n",
      "2018-10-26T16:56:44.593206: step 10998, loss 9.86433e-05, acc 1\n",
      "2018-10-26T16:56:44.968164: step 10999, loss 0.00018737, acc 1\n",
      "2018-10-26T16:56:45.294341: step 11000, loss 1.78625e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:56:46.083186: step 11000, loss 3.87048, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11000\n",
      "\n",
      "2018-10-26T16:56:46.713549: step 11001, loss 3.50622e-05, acc 1\n",
      "2018-10-26T16:56:47.059697: step 11002, loss 2.27654e-05, acc 1\n",
      "2018-10-26T16:56:47.552322: step 11003, loss 0.00215654, acc 1\n",
      "2018-10-26T16:56:47.906315: step 11004, loss 0.0471351, acc 0.984375\n",
      "2018-10-26T16:56:48.218481: step 11005, loss 0.000136344, acc 1\n",
      "2018-10-26T16:56:48.549597: step 11006, loss 0.00034463, acc 1\n",
      "2018-10-26T16:56:48.901709: step 11007, loss 0.000451617, acc 1\n",
      "2018-10-26T16:56:49.239752: step 11008, loss 2.06753e-07, acc 1\n",
      "2018-10-26T16:56:49.581875: step 11009, loss 0.000985012, acc 1\n",
      "2018-10-26T16:56:49.941875: step 11010, loss 0.00649381, acc 1\n",
      "2018-10-26T16:56:50.277979: step 11011, loss 1.18542e-05, acc 1\n",
      "2018-10-26T16:56:50.610094: step 11012, loss 9.2083e-05, acc 1\n",
      "2018-10-26T16:56:50.950267: step 11013, loss 0.000948851, acc 1\n",
      "2018-10-26T16:56:51.280372: step 11014, loss 0.000174793, acc 1\n",
      "2018-10-26T16:56:51.623383: step 11015, loss 0.00296305, acc 1\n",
      "2018-10-26T16:56:51.977464: step 11016, loss 6.60948e-05, acc 1\n",
      "2018-10-26T16:56:52.322519: step 11017, loss 6.20949e-05, acc 1\n",
      "2018-10-26T16:56:52.659614: step 11018, loss 0.00017892, acc 1\n",
      "2018-10-26T16:56:53.000704: step 11019, loss 8.75435e-05, acc 1\n",
      "2018-10-26T16:56:53.347775: step 11020, loss 4.16535e-05, acc 1\n",
      "2018-10-26T16:56:53.682882: step 11021, loss 4.13566e-05, acc 1\n",
      "2018-10-26T16:56:54.018984: step 11022, loss 6.76126e-07, acc 1\n",
      "2018-10-26T16:56:54.360071: step 11023, loss 0.000613367, acc 1\n",
      "2018-10-26T16:56:54.710139: step 11024, loss 2.35566e-05, acc 1\n",
      "2018-10-26T16:56:55.041281: step 11025, loss 3.95459e-05, acc 1\n",
      "2018-10-26T16:56:55.408311: step 11026, loss 9.19817e-06, acc 1\n",
      "2018-10-26T16:56:55.731410: step 11027, loss 7.09746e-06, acc 1\n",
      "2018-10-26T16:56:56.072497: step 11028, loss 1.9744e-07, acc 1\n",
      "2018-10-26T16:56:56.403610: step 11029, loss 5.53416e-05, acc 1\n",
      "2018-10-26T16:56:56.739714: step 11030, loss 9.05294e-05, acc 1\n",
      "2018-10-26T16:56:57.093767: step 11031, loss 1.72707e-05, acc 1\n",
      "2018-10-26T16:56:57.435856: step 11032, loss 3.87455e-05, acc 1\n",
      "2018-10-26T16:56:57.776942: step 11033, loss 4.69366e-06, acc 1\n",
      "2018-10-26T16:56:58.117033: step 11034, loss 5.77555e-05, acc 1\n",
      "2018-10-26T16:56:58.428202: step 11035, loss 0.0365194, acc 0.984375\n",
      "2018-10-26T16:56:58.769293: step 11036, loss 0.000314344, acc 1\n",
      "2018-10-26T16:56:59.129400: step 11037, loss 2.91781e-05, acc 1\n",
      "2018-10-26T16:56:59.464436: step 11038, loss 0.000585601, acc 1\n",
      "2018-10-26T16:56:59.806518: step 11039, loss 3.18511e-07, acc 1\n",
      "2018-10-26T16:57:00.152594: step 11040, loss 0.000381727, acc 1\n",
      "2018-10-26T16:57:00.478724: step 11041, loss 6.97355e-05, acc 1\n",
      "2018-10-26T16:57:00.820813: step 11042, loss 0.000168755, acc 1\n",
      "2018-10-26T16:57:01.154918: step 11043, loss 3.25198e-06, acc 1\n",
      "2018-10-26T16:57:01.498996: step 11044, loss 0.0108444, acc 1\n",
      "2018-10-26T16:57:01.859072: step 11045, loss 0.000118542, acc 1\n",
      "2018-10-26T16:57:02.183172: step 11046, loss 0.00143058, acc 1\n",
      "2018-10-26T16:57:02.526502: step 11047, loss 3.88965e-05, acc 1\n",
      "2018-10-26T16:57:02.878312: step 11048, loss 0.00901588, acc 1\n",
      "2018-10-26T16:57:03.212418: step 11049, loss 0.000612274, acc 1\n",
      "2018-10-26T16:57:03.563539: step 11050, loss 3.53266e-05, acc 1\n",
      "2018-10-26T16:57:03.925513: step 11051, loss 0.00913863, acc 1\n",
      "2018-10-26T16:57:04.250644: step 11052, loss 0.000377266, acc 1\n",
      "2018-10-26T16:57:04.573784: step 11053, loss 2.74917e-06, acc 1\n",
      "2018-10-26T16:57:04.922848: step 11054, loss 0.000218104, acc 1\n",
      "2018-10-26T16:57:05.263941: step 11055, loss 6.146e-06, acc 1\n",
      "2018-10-26T16:57:05.608049: step 11056, loss 4.46442e-06, acc 1\n",
      "2018-10-26T16:57:05.991037: step 11057, loss 0.00479878, acc 1\n",
      "2018-10-26T16:57:06.376966: step 11058, loss 0.00158382, acc 1\n",
      "2018-10-26T16:57:06.728062: step 11059, loss 0.000184261, acc 1\n",
      "2018-10-26T16:57:07.104021: step 11060, loss 0.000225449, acc 1\n",
      "2018-10-26T16:57:07.434139: step 11061, loss 0.000194654, acc 1\n",
      "2018-10-26T16:57:07.796171: step 11062, loss 0.00135746, acc 1\n",
      "2018-10-26T16:57:08.127347: step 11063, loss 7.2085e-05, acc 1\n",
      "2018-10-26T16:57:08.472367: step 11064, loss 6.37021e-07, acc 1\n",
      "2018-10-26T16:57:08.849360: step 11065, loss 3.04344e-06, acc 1\n",
      "2018-10-26T16:57:09.199422: step 11066, loss 3.58614e-05, acc 1\n",
      "2018-10-26T16:57:09.543502: step 11067, loss 9.22462e-05, acc 1\n",
      "2018-10-26T16:57:09.967372: step 11068, loss 0.000131541, acc 1\n",
      "2018-10-26T16:57:10.308524: step 11069, loss 3.34517e-06, acc 1\n",
      "2018-10-26T16:57:10.657528: step 11070, loss 3.13263e-05, acc 1\n",
      "2018-10-26T16:57:10.996621: step 11071, loss 4.3897e-05, acc 1\n",
      "2018-10-26T16:57:11.365634: step 11072, loss 6.05962e-05, acc 1\n",
      "2018-10-26T16:57:11.742626: step 11073, loss 6.86663e-05, acc 1\n",
      "2018-10-26T16:57:12.094686: step 11074, loss 9.59512e-06, acc 1\n",
      "2018-10-26T16:57:12.441761: step 11075, loss 1.27234e-05, acc 1\n",
      "2018-10-26T16:57:12.779859: step 11076, loss 0.000190463, acc 1\n",
      "2018-10-26T16:57:13.122939: step 11077, loss 1.24463e-05, acc 1\n",
      "2018-10-26T16:57:13.458080: step 11078, loss 3.72526e-07, acc 1\n",
      "2018-10-26T16:57:13.789162: step 11079, loss 0.00235114, acc 1\n",
      "2018-10-26T16:57:14.135238: step 11080, loss 0.000308552, acc 1\n",
      "2018-10-26T16:57:14.480314: step 11081, loss 3.77889e-06, acc 1\n",
      "2018-10-26T16:57:14.836397: step 11082, loss 9.2759e-07, acc 1\n",
      "2018-10-26T16:57:15.168474: step 11083, loss 0.000424159, acc 1\n",
      "2018-10-26T16:57:15.516544: step 11084, loss 0.000264154, acc 1\n",
      "2018-10-26T16:57:15.863618: step 11085, loss 4.89776e-05, acc 1\n",
      "2018-10-26T16:57:16.210720: step 11086, loss 1.5076e-05, acc 1\n",
      "2018-10-26T16:57:16.543803: step 11087, loss 8.05168e-06, acc 1\n",
      "2018-10-26T16:57:16.864940: step 11088, loss 0.0048698, acc 1\n",
      "2018-10-26T16:57:17.243928: step 11089, loss 1.013e-05, acc 1\n",
      "2018-10-26T16:57:17.633890: step 11090, loss 0.0192649, acc 0.984375\n",
      "2018-10-26T16:57:18.011876: step 11091, loss 1.76386e-05, acc 1\n",
      "2018-10-26T16:57:18.364009: step 11092, loss 1.89422e-06, acc 1\n",
      "2018-10-26T16:57:18.699041: step 11093, loss 0.000164595, acc 1\n",
      "2018-10-26T16:57:19.043120: step 11094, loss 4.20956e-07, acc 1\n",
      "2018-10-26T16:57:19.393247: step 11095, loss 0.000756252, acc 1\n",
      "2018-10-26T16:57:19.751286: step 11096, loss 6.11648e-06, acc 1\n",
      "2018-10-26T16:57:20.085417: step 11097, loss 1.89507e-05, acc 1\n",
      "2018-10-26T16:57:20.437398: step 11098, loss 0.000223349, acc 1\n",
      "2018-10-26T16:57:20.782476: step 11099, loss 5.36959e-05, acc 1\n",
      "2018-10-26T16:57:21.156474: step 11100, loss 2.99605e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:57:21.937389: step 11100, loss 4.10281, acc 0.705441\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11100\n",
      "\n",
      "2018-10-26T16:57:22.560723: step 11101, loss 1.10917e-05, acc 1\n",
      "2018-10-26T16:57:22.894829: step 11102, loss 8.39491e-05, acc 1\n",
      "2018-10-26T16:57:23.353371: step 11103, loss 0.002553, acc 1\n",
      "2018-10-26T16:57:23.732593: step 11104, loss 0.000123513, acc 1\n",
      "2018-10-26T16:57:24.082661: step 11105, loss 1.12057e-05, acc 1\n",
      "2018-10-26T16:57:24.423749: step 11106, loss 0.00977061, acc 1\n",
      "2018-10-26T16:57:24.760848: step 11107, loss 0.00966393, acc 1\n",
      "2018-10-26T16:57:25.108915: step 11108, loss 0.000205364, acc 1\n",
      "2018-10-26T16:57:25.453993: step 11109, loss 0.000102537, acc 1\n",
      "2018-10-26T16:57:25.790095: step 11110, loss 4.04257e-05, acc 1\n",
      "2018-10-26T16:57:26.134175: step 11111, loss 4.14041e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:57:26.477259: step 11112, loss 1.30889e-05, acc 1\n",
      "2018-10-26T16:57:26.810369: step 11113, loss 1.8781e-05, acc 1\n",
      "2018-10-26T16:57:27.143482: step 11114, loss 3.9501e-05, acc 1\n",
      "2018-10-26T16:57:27.474593: step 11115, loss 7.07802e-07, acc 1\n",
      "2018-10-26T16:57:27.814684: step 11116, loss 0.00126136, acc 1\n",
      "2018-10-26T16:57:28.134832: step 11117, loss 0.000158068, acc 1\n",
      "2018-10-26T16:57:28.478914: step 11118, loss 6.87313e-07, acc 1\n",
      "2018-10-26T16:57:28.810026: step 11119, loss 0.0265737, acc 0.984375\n",
      "2018-10-26T16:57:29.141145: step 11120, loss 7.99615e-05, acc 1\n",
      "2018-10-26T16:57:29.474252: step 11121, loss 0.000114152, acc 1\n",
      "2018-10-26T16:57:29.823318: step 11122, loss 0.000561497, acc 1\n",
      "2018-10-26T16:57:30.151524: step 11123, loss 0.036222, acc 0.984375\n",
      "2018-10-26T16:57:30.480604: step 11124, loss 0.000599673, acc 1\n",
      "2018-10-26T16:57:30.813671: step 11125, loss 1.0021e-06, acc 1\n",
      "2018-10-26T16:57:31.149779: step 11126, loss 2.50895e-05, acc 1\n",
      "2018-10-26T16:57:31.477897: step 11127, loss 7.02214e-07, acc 1\n",
      "2018-10-26T16:57:31.820984: step 11128, loss 0.000486123, acc 1\n",
      "2018-10-26T16:57:32.155089: step 11129, loss 2.04259e-05, acc 1\n",
      "2018-10-26T16:57:32.491194: step 11130, loss 1.47149e-05, acc 1\n",
      "2018-10-26T16:57:32.844248: step 11131, loss 3.53908e-05, acc 1\n",
      "2018-10-26T16:57:33.164396: step 11132, loss 0.0652144, acc 0.984375\n",
      "2018-10-26T16:57:33.493541: step 11133, loss 0.00118974, acc 1\n",
      "2018-10-26T16:57:33.837597: step 11134, loss 0.000222283, acc 1\n",
      "2018-10-26T16:57:34.172700: step 11135, loss 0.000359642, acc 1\n",
      "2018-10-26T16:57:34.497833: step 11136, loss 1.70982e-06, acc 1\n",
      "2018-10-26T16:57:34.846896: step 11137, loss 7.63694e-06, acc 1\n",
      "2018-10-26T16:57:35.209930: step 11138, loss 1.66886e-06, acc 1\n",
      "2018-10-26T16:57:35.551054: step 11139, loss 8.50299e-05, acc 1\n",
      "2018-10-26T16:57:35.905069: step 11140, loss 2.15835e-05, acc 1\n",
      "2018-10-26T16:57:36.231350: step 11141, loss 3.38005e-05, acc 1\n",
      "2018-10-26T16:57:36.570292: step 11142, loss 0.000432311, acc 1\n",
      "2018-10-26T16:57:36.931408: step 11143, loss 1.29538e-05, acc 1\n",
      "2018-10-26T16:57:37.298345: step 11144, loss 0.000780806, acc 1\n",
      "2018-10-26T16:57:37.650439: step 11145, loss 6.0053e-05, acc 1\n",
      "2018-10-26T16:57:37.991576: step 11146, loss 0.00229614, acc 1\n",
      "2018-10-26T16:57:38.334678: step 11147, loss 7.13057e-06, acc 1\n",
      "2018-10-26T16:57:38.740494: step 11148, loss 0.000717498, acc 1\n",
      "2018-10-26T16:57:39.101594: step 11149, loss 0.00806479, acc 1\n",
      "2018-10-26T16:57:39.509439: step 11150, loss 0.000954169, acc 1\n",
      "2018-10-26T16:57:39.884437: step 11151, loss 0.000115701, acc 1\n",
      "2018-10-26T16:57:40.291528: step 11152, loss 0.00119794, acc 1\n",
      "2018-10-26T16:57:40.698263: step 11153, loss 1.50678e-05, acc 1\n",
      "2018-10-26T16:57:41.035362: step 11154, loss 0.000867824, acc 1\n",
      "2018-10-26T16:57:41.407371: step 11155, loss 6.16492e-05, acc 1\n",
      "2018-10-26T16:57:41.827245: step 11156, loss 0.0010472, acc 1\n",
      "2018-10-26T16:57:42.271103: step 11157, loss 2.16941e-05, acc 1\n",
      "2018-10-26T16:57:42.727840: step 11158, loss 9.07413e-06, acc 1\n",
      "2018-10-26T16:57:43.168662: step 11159, loss 0.00209993, acc 1\n",
      "2018-10-26T16:57:43.559617: step 11160, loss 1.94994e-05, acc 1\n",
      "2018-10-26T16:57:43.958550: step 11161, loss 3.28191e-06, acc 1\n",
      "2018-10-26T16:57:44.356488: step 11162, loss 0.000136762, acc 1\n",
      "2018-10-26T16:57:44.764398: step 11163, loss 1.90731e-06, acc 1\n",
      "2018-10-26T16:57:45.193348: step 11164, loss 0.000685352, acc 1\n",
      "2018-10-26T16:57:45.631084: step 11165, loss 1.00955e-06, acc 1\n",
      "2018-10-26T16:57:46.048966: step 11166, loss 0.00191713, acc 1\n",
      "2018-10-26T16:57:46.364127: step 11167, loss 6.38004e-05, acc 1\n",
      "2018-10-26T16:57:46.759072: step 11168, loss 0.00435498, acc 1\n",
      "2018-10-26T16:57:47.132071: step 11169, loss 7.48516e-05, acc 1\n",
      "2018-10-26T16:57:47.462190: step 11170, loss 6.04524e-05, acc 1\n",
      "2018-10-26T16:57:47.794332: step 11171, loss 1.65401e-05, acc 1\n",
      "2018-10-26T16:57:48.128412: step 11172, loss 5.51461e-05, acc 1\n",
      "2018-10-26T16:57:48.468502: step 11173, loss 8.02418e-06, acc 1\n",
      "2018-10-26T16:57:48.835551: step 11174, loss 3.48702e-05, acc 1\n",
      "2018-10-26T16:57:49.168632: step 11175, loss 1.32285e-05, acc 1\n",
      "2018-10-26T16:57:49.498750: step 11176, loss 7.04075e-07, acc 1\n",
      "2018-10-26T16:57:49.831908: step 11177, loss 0.000148678, acc 1\n",
      "2018-10-26T16:57:50.163971: step 11178, loss 0.000142622, acc 1\n",
      "2018-10-26T16:57:50.504066: step 11179, loss 0.000309325, acc 1\n",
      "2018-10-26T16:57:50.839229: step 11180, loss 6.30037e-05, acc 1\n",
      "2018-10-26T16:57:51.181322: step 11181, loss 1.49214e-05, acc 1\n",
      "2018-10-26T16:57:51.571273: step 11182, loss 0.000111573, acc 1\n",
      "2018-10-26T16:57:51.902326: step 11183, loss 0.000128572, acc 1\n",
      "2018-10-26T16:57:52.231489: step 11184, loss 3.16017e-05, acc 1\n",
      "2018-10-26T16:57:52.568574: step 11185, loss 0.00148791, acc 1\n",
      "2018-10-26T16:57:52.923599: step 11186, loss 1.63705e-05, acc 1\n",
      "2018-10-26T16:57:53.253717: step 11187, loss 8.72202e-06, acc 1\n",
      "2018-10-26T16:57:53.603783: step 11188, loss 2.02446e-05, acc 1\n",
      "2018-10-26T16:57:53.941952: step 11189, loss 0.00117519, acc 1\n",
      "2018-10-26T16:57:54.276981: step 11190, loss 5.69977e-05, acc 1\n",
      "2018-10-26T16:57:54.616117: step 11191, loss 0.0065019, acc 1\n",
      "2018-10-26T16:57:54.961154: step 11192, loss 9.42533e-05, acc 1\n",
      "2018-10-26T16:57:55.332164: step 11193, loss 0.00133425, acc 1\n",
      "2018-10-26T16:57:55.671257: step 11194, loss 0.0122192, acc 0.984375\n",
      "2018-10-26T16:57:56.008392: step 11195, loss 9.33177e-07, acc 1\n",
      "2018-10-26T16:57:56.343460: step 11196, loss 0.00035426, acc 1\n",
      "2018-10-26T16:57:56.677569: step 11197, loss 7.82304e-07, acc 1\n",
      "2018-10-26T16:57:57.019657: step 11198, loss 0.000342619, acc 1\n",
      "2018-10-26T16:57:57.347778: step 11199, loss 2.74296e-05, acc 1\n",
      "2018-10-26T16:57:57.703829: step 11200, loss 0.000111136, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:57:58.564526: step 11200, loss 3.9555, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11200\n",
      "\n",
      "2018-10-26T16:57:59.228754: step 11201, loss 7.0812e-06, acc 1\n",
      "2018-10-26T16:57:59.579813: step 11202, loss 1.78573e-05, acc 1\n",
      "2018-10-26T16:58:00.018640: step 11203, loss 2.39676e-05, acc 1\n",
      "2018-10-26T16:58:00.397630: step 11204, loss 2.50136e-06, acc 1\n",
      "2018-10-26T16:58:00.767643: step 11205, loss 0.000909851, acc 1\n",
      "2018-10-26T16:58:01.129060: step 11206, loss 9.59095e-06, acc 1\n",
      "2018-10-26T16:58:01.471760: step 11207, loss 6.9857e-05, acc 1\n",
      "2018-10-26T16:58:01.859723: step 11208, loss 5.73234e-06, acc 1\n",
      "2018-10-26T16:58:02.224748: step 11209, loss 3.61349e-05, acc 1\n",
      "2018-10-26T16:58:02.592763: step 11210, loss 1.60991e-05, acc 1\n",
      "2018-10-26T16:58:02.941882: step 11211, loss 0.000226767, acc 1\n",
      "2018-10-26T16:58:03.263002: step 11212, loss 1.07047e-05, acc 1\n",
      "2018-10-26T16:58:03.600164: step 11213, loss 2.22547e-05, acc 1\n",
      "2018-10-26T16:58:03.933181: step 11214, loss 2.60471e-05, acc 1\n",
      "2018-10-26T16:58:04.311172: step 11215, loss 0.000800912, acc 1\n",
      "2018-10-26T16:58:04.699169: step 11216, loss 0.000112331, acc 1\n",
      "2018-10-26T16:58:05.070144: step 11217, loss 0.000374694, acc 1\n",
      "2018-10-26T16:58:05.401260: step 11218, loss 0.00379405, acc 1\n",
      "2018-10-26T16:58:05.732375: step 11219, loss 9.13455e-05, acc 1\n",
      "2018-10-26T16:58:06.050525: step 11220, loss 1.56727e-05, acc 1\n",
      "2018-10-26T16:58:06.345894: step 11221, loss 0.000353518, acc 1\n",
      "2018-10-26T16:58:06.738875: step 11222, loss 0.000214953, acc 1\n",
      "2018-10-26T16:58:07.118717: step 11223, loss 0.00107426, acc 1\n",
      "2018-10-26T16:58:07.450813: step 11224, loss 0.00108224, acc 1\n",
      "2018-10-26T16:58:07.769933: step 11225, loss 1.47831e-05, acc 1\n",
      "2018-10-26T16:58:08.110094: step 11226, loss 3.14612e-05, acc 1\n",
      "2018-10-26T16:58:08.448205: step 11227, loss 0.000155206, acc 1\n",
      "2018-10-26T16:58:08.789208: step 11228, loss 6.80294e-06, acc 1\n",
      "2018-10-26T16:58:09.155229: step 11229, loss 0.00266406, acc 1\n",
      "2018-10-26T16:58:09.477368: step 11230, loss 5.14178e-05, acc 1\n",
      "2018-10-26T16:58:09.776586: step 11231, loss 0.000523578, acc 1\n",
      "2018-10-26T16:58:10.128627: step 11232, loss 2.34087e-05, acc 1\n",
      "2018-10-26T16:58:10.451769: step 11233, loss 0.000138547, acc 1\n",
      "2018-10-26T16:58:10.773908: step 11234, loss 1.05036e-05, acc 1\n",
      "2018-10-26T16:58:11.085072: step 11235, loss 3.72887e-06, acc 1\n",
      "2018-10-26T16:58:11.423174: step 11236, loss 0.00499739, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:58:11.747303: step 11237, loss 0.000541847, acc 1\n",
      "2018-10-26T16:58:12.083405: step 11238, loss 1.94044e-05, acc 1\n",
      "2018-10-26T16:58:12.406584: step 11239, loss 5.06327e-05, acc 1\n",
      "2018-10-26T16:58:12.750623: step 11240, loss 5.1036e-07, acc 1\n",
      "2018-10-26T16:58:13.065780: step 11241, loss 1.39696e-06, acc 1\n",
      "2018-10-26T16:58:13.399892: step 11242, loss 0.000574511, acc 1\n",
      "2018-10-26T16:58:13.750954: step 11243, loss 0.00904032, acc 1\n",
      "2018-10-26T16:58:14.083065: step 11244, loss 0.000153019, acc 1\n",
      "2018-10-26T16:58:14.405201: step 11245, loss 0.000125291, acc 1\n",
      "2018-10-26T16:58:14.733326: step 11246, loss 0.000295228, acc 1\n",
      "2018-10-26T16:58:15.055466: step 11247, loss 5.62302e-05, acc 1\n",
      "2018-10-26T16:58:15.375610: step 11248, loss 8.25401e-06, acc 1\n",
      "2018-10-26T16:58:15.687813: step 11249, loss 8.26073e-06, acc 1\n",
      "2018-10-26T16:58:15.998200: step 11250, loss 5.66236e-07, acc 1\n",
      "2018-10-26T16:58:16.313105: step 11251, loss 0.00160752, acc 1\n",
      "2018-10-26T16:58:16.649209: step 11252, loss 0.0469244, acc 0.984375\n",
      "2018-10-26T16:58:16.983345: step 11253, loss 4.03801e-06, acc 1\n",
      "2018-10-26T16:58:17.319419: step 11254, loss 2.33935e-06, acc 1\n",
      "2018-10-26T16:58:17.637566: step 11255, loss 0.000318903, acc 1\n",
      "2018-10-26T16:58:17.965723: step 11256, loss 3.48482e-05, acc 1\n",
      "2018-10-26T16:58:18.278852: step 11257, loss 6.56839e-05, acc 1\n",
      "2018-10-26T16:58:18.591018: step 11258, loss 0.000142695, acc 1\n",
      "2018-10-26T16:58:18.918145: step 11259, loss 0.000450788, acc 1\n",
      "2018-10-26T16:58:19.245269: step 11260, loss 5.68044e-06, acc 1\n",
      "2018-10-26T16:58:19.577383: step 11261, loss 8.17687e-07, acc 1\n",
      "2018-10-26T16:58:19.904510: step 11262, loss 0.000331148, acc 1\n",
      "2018-10-26T16:58:20.251648: step 11263, loss 7.52472e-05, acc 1\n",
      "2018-10-26T16:58:20.577752: step 11264, loss 5.58007e-05, acc 1\n",
      "2018-10-26T16:58:20.900891: step 11265, loss 1.58501e-05, acc 1\n",
      "2018-10-26T16:58:21.213016: step 11266, loss 0.000358874, acc 1\n",
      "2018-10-26T16:58:21.521227: step 11267, loss 7.65773e-05, acc 1\n",
      "2018-10-26T16:58:21.839405: step 11268, loss 0.00253759, acc 1\n",
      "2018-10-26T16:58:22.167466: step 11269, loss 3.83736e-05, acc 1\n",
      "2018-10-26T16:58:22.480707: step 11270, loss 6.48052e-05, acc 1\n",
      "2018-10-26T16:58:22.816727: step 11271, loss 4.28921e-05, acc 1\n",
      "2018-10-26T16:58:23.150835: step 11272, loss 0.000657923, acc 1\n",
      "2018-10-26T16:58:23.465994: step 11273, loss 0.000103412, acc 1\n",
      "2018-10-26T16:58:23.785178: step 11274, loss 4.60603e-06, acc 1\n",
      "2018-10-26T16:58:24.148202: step 11275, loss 0.00170192, acc 1\n",
      "2018-10-26T16:58:24.465322: step 11276, loss 8.54944e-07, acc 1\n",
      "2018-10-26T16:58:24.781538: step 11277, loss 0.000140307, acc 1\n",
      "2018-10-26T16:58:25.110598: step 11278, loss 1.87376e-06, acc 1\n",
      "2018-10-26T16:58:25.460663: step 11279, loss 0.000981305, acc 1\n",
      "2018-10-26T16:58:25.794770: step 11280, loss 0.000173023, acc 1\n",
      "2018-10-26T16:58:26.124891: step 11281, loss 0.00029285, acc 1\n",
      "2018-10-26T16:58:26.458001: step 11282, loss 1.24049e-06, acc 1\n",
      "2018-10-26T16:58:26.774154: step 11283, loss 0.00904478, acc 1\n",
      "2018-10-26T16:58:27.088315: step 11284, loss 0.000498124, acc 1\n",
      "2018-10-26T16:58:27.399485: step 11285, loss 6.27937e-05, acc 1\n",
      "2018-10-26T16:58:27.739574: step 11286, loss 0.000888839, acc 1\n",
      "2018-10-26T16:58:28.068696: step 11287, loss 1.03138e-05, acc 1\n",
      "2018-10-26T16:58:28.398813: step 11288, loss 6.7868e-06, acc 1\n",
      "2018-10-26T16:58:28.784783: step 11289, loss 3.5364e-05, acc 1\n",
      "2018-10-26T16:58:29.128863: step 11290, loss 0.000271847, acc 1\n",
      "2018-10-26T16:58:29.464968: step 11291, loss 0.0104812, acc 1\n",
      "2018-10-26T16:58:29.792090: step 11292, loss 1.25001e-05, acc 1\n",
      "2018-10-26T16:58:30.114230: step 11293, loss 4.04192e-07, acc 1\n",
      "2018-10-26T16:58:30.443355: step 11294, loss 0.00311139, acc 1\n",
      "2018-10-26T16:58:30.770554: step 11295, loss 3.77728e-06, acc 1\n",
      "2018-10-26T16:58:31.099597: step 11296, loss 2.89697e-05, acc 1\n",
      "2018-10-26T16:58:31.424729: step 11297, loss 0.000225782, acc 1\n",
      "2018-10-26T16:58:31.749934: step 11298, loss 0.000208853, acc 1\n",
      "2018-10-26T16:58:32.086962: step 11299, loss 6.15018e-05, acc 1\n",
      "2018-10-26T16:58:32.412152: step 11300, loss 9.33185e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:58:33.194998: step 11300, loss 4.03949, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11300\n",
      "\n",
      "2018-10-26T16:58:33.823356: step 11301, loss 0.000335781, acc 1\n",
      "2018-10-26T16:58:34.142469: step 11302, loss 1.57386e-06, acc 1\n",
      "2018-10-26T16:58:34.570323: step 11303, loss 7.39597e-06, acc 1\n",
      "2018-10-26T16:58:34.931426: step 11304, loss 2.89562e-05, acc 1\n",
      "2018-10-26T16:58:35.269471: step 11305, loss 0.0255526, acc 0.984375\n",
      "2018-10-26T16:58:35.647450: step 11306, loss 2.25553e-06, acc 1\n",
      "2018-10-26T16:58:36.004495: step 11307, loss 5.26187e-05, acc 1\n",
      "2018-10-26T16:58:36.316659: step 11308, loss 1.03999e-05, acc 1\n",
      "2018-10-26T16:58:36.639795: step 11309, loss 0.000350008, acc 1\n",
      "2018-10-26T16:58:36.967918: step 11310, loss 0.000103692, acc 1\n",
      "2018-10-26T16:58:37.285074: step 11311, loss 4.94499e-06, acc 1\n",
      "2018-10-26T16:58:37.608208: step 11312, loss 0.000248462, acc 1\n",
      "2018-10-26T16:58:37.936334: step 11313, loss 0.000321295, acc 1\n",
      "2018-10-26T16:58:38.246500: step 11314, loss 0.000139567, acc 1\n",
      "2018-10-26T16:58:38.561662: step 11315, loss 5.24128e-06, acc 1\n",
      "2018-10-26T16:58:38.880807: step 11316, loss 0.000158267, acc 1\n",
      "2018-10-26T16:58:39.204944: step 11317, loss 0.000487164, acc 1\n",
      "2018-10-26T16:58:39.532066: step 11318, loss 1.11008e-05, acc 1\n",
      "2018-10-26T16:58:39.842238: step 11319, loss 0.000528781, acc 1\n",
      "2018-10-26T16:58:40.153406: step 11320, loss 9.27078e-05, acc 1\n",
      "2018-10-26T16:58:40.476546: step 11321, loss 0.000309685, acc 1\n",
      "2018-10-26T16:58:40.803670: step 11322, loss 2.49204e-06, acc 1\n",
      "2018-10-26T16:58:41.149813: step 11323, loss 4.05643e-06, acc 1\n",
      "2018-10-26T16:58:41.463906: step 11324, loss 0.000144192, acc 1\n",
      "2018-10-26T16:58:41.782057: step 11325, loss 0.00515996, acc 1\n",
      "2018-10-26T16:58:42.111243: step 11326, loss 0.000154605, acc 1\n",
      "2018-10-26T16:58:42.432321: step 11327, loss 0.000115416, acc 1\n",
      "2018-10-26T16:58:42.749471: step 11328, loss 0.000624991, acc 1\n",
      "2018-10-26T16:58:43.074738: step 11329, loss 2.61695e-06, acc 1\n",
      "2018-10-26T16:58:43.383775: step 11330, loss 0.000218624, acc 1\n",
      "2018-10-26T16:58:43.702950: step 11331, loss 1.49454e-05, acc 1\n",
      "2018-10-26T16:58:44.028159: step 11332, loss 1.22962e-05, acc 1\n",
      "2018-10-26T16:58:44.373163: step 11333, loss 4.04715e-05, acc 1\n",
      "2018-10-26T16:58:44.768078: step 11334, loss 7.35015e-05, acc 1\n",
      "2018-10-26T16:58:45.125124: step 11335, loss 0.040541, acc 0.984375\n",
      "2018-10-26T16:58:45.468207: step 11336, loss 6.34775e-05, acc 1\n",
      "2018-10-26T16:58:45.833230: step 11337, loss 2.31721e-05, acc 1\n",
      "2018-10-26T16:58:46.201298: step 11338, loss 0.000101945, acc 1\n",
      "2018-10-26T16:58:46.558294: step 11339, loss 7.84387e-06, acc 1\n",
      "2018-10-26T16:58:46.866474: step 11340, loss 0.000118339, acc 1\n",
      "2018-10-26T16:58:47.252440: step 11341, loss 0.000105533, acc 1\n",
      "2018-10-26T16:58:47.635416: step 11342, loss 3.0345e-05, acc 1\n",
      "2018-10-26T16:58:48.086213: step 11343, loss 1.62388e-05, acc 1\n",
      "2018-10-26T16:58:48.545982: step 11344, loss 0.000144662, acc 1\n",
      "2018-10-26T16:58:48.996779: step 11345, loss 0.000387721, acc 1\n",
      "2018-10-26T16:58:49.384744: step 11346, loss 0.000109279, acc 1\n",
      "2018-10-26T16:58:49.795644: step 11347, loss 0.00727719, acc 1\n",
      "2018-10-26T16:58:50.225495: step 11348, loss 3.13998e-05, acc 1\n",
      "2018-10-26T16:58:50.638393: step 11349, loss 0.000117705, acc 1\n",
      "2018-10-26T16:58:51.033337: step 11350, loss 3.33128e-05, acc 1\n",
      "2018-10-26T16:58:51.417311: step 11351, loss 0.000542812, acc 1\n",
      "2018-10-26T16:58:51.825220: step 11352, loss 2.94987e-05, acc 1\n",
      "2018-10-26T16:58:52.252083: step 11353, loss 0.00135497, acc 1\n",
      "2018-10-26T16:58:52.649020: step 11354, loss 9.37249e-06, acc 1\n",
      "2018-10-26T16:58:53.032994: step 11355, loss 8.08343e-05, acc 1\n",
      "2018-10-26T16:58:53.350151: step 11356, loss 3.17096e-05, acc 1\n",
      "2018-10-26T16:58:53.677273: step 11357, loss 3.2634e-05, acc 1\n",
      "2018-10-26T16:58:54.028406: step 11358, loss 0.000130852, acc 1\n",
      "2018-10-26T16:58:54.371468: step 11359, loss 1.1328e-05, acc 1\n",
      "2018-10-26T16:58:54.677680: step 11360, loss 3.55764e-07, acc 1\n",
      "2018-10-26T16:58:54.994781: step 11361, loss 0.00238101, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:58:55.310976: step 11362, loss 0.100861, acc 0.984375\n",
      "2018-10-26T16:58:55.649004: step 11363, loss 0.0560474, acc 0.984375\n",
      "2018-10-26T16:58:55.983114: step 11364, loss 6.15474e-05, acc 1\n",
      "2018-10-26T16:58:56.309241: step 11365, loss 6.2842e-05, acc 1\n",
      "2018-10-26T16:58:56.626393: step 11366, loss 0.000217734, acc 1\n",
      "2018-10-26T16:58:56.951526: step 11367, loss 5.3457e-07, acc 1\n",
      "2018-10-26T16:58:57.279668: step 11368, loss 5.85835e-05, acc 1\n",
      "2018-10-26T16:58:57.638719: step 11369, loss 1.22705e-05, acc 1\n",
      "2018-10-26T16:58:57.974790: step 11370, loss 0.000786612, acc 1\n",
      "2018-10-26T16:58:58.320870: step 11371, loss 2.95677e-05, acc 1\n",
      "2018-10-26T16:58:58.662951: step 11372, loss 1.56461e-05, acc 1\n",
      "2018-10-26T16:58:59.036953: step 11373, loss 0.000474263, acc 1\n",
      "2018-10-26T16:58:59.399982: step 11374, loss 0.0125849, acc 0.984375\n",
      "2018-10-26T16:58:59.727109: step 11375, loss 7.96804e-05, acc 1\n",
      "2018-10-26T16:59:00.069195: step 11376, loss 0.000350642, acc 1\n",
      "2018-10-26T16:59:00.390337: step 11377, loss 0.000375476, acc 1\n",
      "2018-10-26T16:59:00.707561: step 11378, loss 0.000411932, acc 1\n",
      "2018-10-26T16:59:01.022647: step 11379, loss 3.86378e-05, acc 1\n",
      "2018-10-26T16:59:01.353762: step 11380, loss 0.000454998, acc 1\n",
      "2018-10-26T16:59:01.681002: step 11381, loss 0.00653478, acc 1\n",
      "2018-10-26T16:59:01.999040: step 11382, loss 4.73164e-05, acc 1\n",
      "2018-10-26T16:59:02.341125: step 11383, loss 8.47996e-05, acc 1\n",
      "2018-10-26T16:59:02.676230: step 11384, loss 0.000115909, acc 1\n",
      "2018-10-26T16:59:03.015324: step 11385, loss 0.000168114, acc 1\n",
      "2018-10-26T16:59:03.382343: step 11386, loss 0.00304399, acc 1\n",
      "2018-10-26T16:59:03.710525: step 11387, loss 0.00067567, acc 1\n",
      "2018-10-26T16:59:04.042581: step 11388, loss 0.00203567, acc 1\n",
      "2018-10-26T16:59:04.368708: step 11389, loss 2.44006e-07, acc 1\n",
      "2018-10-26T16:59:04.699825: step 11390, loss 0.0890504, acc 0.96875\n",
      "2018-10-26T16:59:05.020039: step 11391, loss 0.000135154, acc 1\n",
      "2018-10-26T16:59:05.345102: step 11392, loss 2.38918e-05, acc 1\n",
      "2018-10-26T16:59:05.656267: step 11393, loss 0.000149001, acc 1\n",
      "2018-10-26T16:59:05.979404: step 11394, loss 1.80112e-06, acc 1\n",
      "2018-10-26T16:59:06.315509: step 11395, loss 3.75544e-05, acc 1\n",
      "2018-10-26T16:59:06.646624: step 11396, loss 0.000224018, acc 1\n",
      "2018-10-26T16:59:06.989703: step 11397, loss 2.07121e-06, acc 1\n",
      "2018-10-26T16:59:07.322818: step 11398, loss 4.37502e-06, acc 1\n",
      "2018-10-26T16:59:07.652935: step 11399, loss 3.24915e-05, acc 1\n",
      "2018-10-26T16:59:07.970085: step 11400, loss 0.0165835, acc 0.983333\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:59:08.773984: step 11400, loss 4.06765, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11400\n",
      "\n",
      "2018-10-26T16:59:09.424199: step 11401, loss 0.000214617, acc 1\n",
      "2018-10-26T16:59:09.748376: step 11402, loss 2.96874e-05, acc 1\n",
      "2018-10-26T16:59:10.191150: step 11403, loss 0.000141697, acc 1\n",
      "2018-10-26T16:59:10.526254: step 11404, loss 8.67417e-05, acc 1\n",
      "2018-10-26T16:59:10.866349: step 11405, loss 0.00122984, acc 1\n",
      "2018-10-26T16:59:11.200555: step 11406, loss 0.000167288, acc 1\n",
      "2018-10-26T16:59:11.543540: step 11407, loss 8.73727e-05, acc 1\n",
      "2018-10-26T16:59:11.868672: step 11408, loss 4.84288e-08, acc 1\n",
      "2018-10-26T16:59:12.211752: step 11409, loss 0.000864899, acc 1\n",
      "2018-10-26T16:59:12.537882: step 11410, loss 2.79384e-06, acc 1\n",
      "2018-10-26T16:59:12.862016: step 11411, loss 0.0730649, acc 0.984375\n",
      "2018-10-26T16:59:13.194127: step 11412, loss 3.17263e-05, acc 1\n",
      "2018-10-26T16:59:13.522250: step 11413, loss 0.00019192, acc 1\n",
      "2018-10-26T16:59:13.862346: step 11414, loss 4.20654e-05, acc 1\n",
      "2018-10-26T16:59:14.165531: step 11415, loss 4.2971e-05, acc 1\n",
      "2018-10-26T16:59:14.480689: step 11416, loss 0.00242106, acc 1\n",
      "2018-10-26T16:59:14.790863: step 11417, loss 3.5871e-06, acc 1\n",
      "2018-10-26T16:59:15.113997: step 11418, loss 0.00130865, acc 1\n",
      "2018-10-26T16:59:15.428193: step 11419, loss 8.01816e-06, acc 1\n",
      "2018-10-26T16:59:15.748513: step 11420, loss 0.000143782, acc 1\n",
      "2018-10-26T16:59:16.050524: step 11421, loss 2.24071e-06, acc 1\n",
      "2018-10-26T16:59:16.377622: step 11422, loss 5.99874e-05, acc 1\n",
      "2018-10-26T16:59:16.701800: step 11423, loss 1.93992e-05, acc 1\n",
      "2018-10-26T16:59:17.071768: step 11424, loss 0.00171021, acc 1\n",
      "2018-10-26T16:59:17.394902: step 11425, loss 0.000478998, acc 1\n",
      "2018-10-26T16:59:17.732042: step 11426, loss 0.00532909, acc 1\n",
      "2018-10-26T16:59:18.061123: step 11427, loss 0.000737953, acc 1\n",
      "2018-10-26T16:59:18.368303: step 11428, loss 0.000482119, acc 1\n",
      "2018-10-26T16:59:18.695432: step 11429, loss 0.000210289, acc 1\n",
      "2018-10-26T16:59:19.022554: step 11430, loss 0.000190238, acc 1\n",
      "2018-10-26T16:59:19.348683: step 11431, loss 1.85033e-05, acc 1\n",
      "2018-10-26T16:59:19.664838: step 11432, loss 1.24051e-06, acc 1\n",
      "2018-10-26T16:59:19.998019: step 11433, loss 0.0340356, acc 0.984375\n",
      "2018-10-26T16:59:20.313107: step 11434, loss 0.000267006, acc 1\n",
      "2018-10-26T16:59:20.642304: step 11435, loss 0.00136721, acc 1\n",
      "2018-10-26T16:59:20.933451: step 11436, loss 0.0206648, acc 0.984375\n",
      "2018-10-26T16:59:21.260575: step 11437, loss 1.4116e-05, acc 1\n",
      "2018-10-26T16:59:21.587811: step 11438, loss 0.00031685, acc 1\n",
      "2018-10-26T16:59:21.908028: step 11439, loss 2.1716e-05, acc 1\n",
      "2018-10-26T16:59:22.242951: step 11440, loss 3.57626e-07, acc 1\n",
      "2018-10-26T16:59:22.575062: step 11441, loss 2.62989e-06, acc 1\n",
      "2018-10-26T16:59:22.930172: step 11442, loss 7.24416e-05, acc 1\n",
      "2018-10-26T16:59:23.264232: step 11443, loss 1.91733e-05, acc 1\n",
      "2018-10-26T16:59:23.587358: step 11444, loss 0.00231524, acc 1\n",
      "2018-10-26T16:59:23.896533: step 11445, loss 2.1642e-05, acc 1\n",
      "2018-10-26T16:59:24.219695: step 11446, loss 9.14952e-05, acc 1\n",
      "2018-10-26T16:59:24.556767: step 11447, loss 0.00017818, acc 1\n",
      "2018-10-26T16:59:24.876912: step 11448, loss 0.000164695, acc 1\n",
      "2018-10-26T16:59:25.195063: step 11449, loss 4.67126e-06, acc 1\n",
      "2018-10-26T16:59:25.513214: step 11450, loss 5.01508e-05, acc 1\n",
      "2018-10-26T16:59:25.826451: step 11451, loss 0.00022921, acc 1\n",
      "2018-10-26T16:59:26.146520: step 11452, loss 2.15705e-05, acc 1\n",
      "2018-10-26T16:59:26.468660: step 11453, loss 0.100451, acc 0.984375\n",
      "2018-10-26T16:59:26.793789: step 11454, loss 0.00135169, acc 1\n",
      "2018-10-26T16:59:27.111941: step 11455, loss 0.000147183, acc 1\n",
      "2018-10-26T16:59:27.431090: step 11456, loss 0.00374305, acc 1\n",
      "2018-10-26T16:59:27.745248: step 11457, loss 7.77757e-05, acc 1\n",
      "2018-10-26T16:59:28.071424: step 11458, loss 9.14866e-06, acc 1\n",
      "2018-10-26T16:59:28.387567: step 11459, loss 9.6735e-05, acc 1\n",
      "2018-10-26T16:59:28.699717: step 11460, loss 0.000895796, acc 1\n",
      "2018-10-26T16:59:29.014857: step 11461, loss 0.000790897, acc 1\n",
      "2018-10-26T16:59:29.330092: step 11462, loss 0.0709639, acc 0.984375\n",
      "2018-10-26T16:59:29.686066: step 11463, loss 9.43152e-05, acc 1\n",
      "2018-10-26T16:59:30.040120: step 11464, loss 3.45025e-05, acc 1\n",
      "2018-10-26T16:59:30.372230: step 11465, loss 0.0742423, acc 0.984375\n",
      "2018-10-26T16:59:30.685392: step 11466, loss 4.95453e-05, acc 1\n",
      "2018-10-26T16:59:31.016509: step 11467, loss 0.000623546, acc 1\n",
      "2018-10-26T16:59:31.334657: step 11468, loss 0.00121077, acc 1\n",
      "2018-10-26T16:59:31.654803: step 11469, loss 0.000285462, acc 1\n",
      "2018-10-26T16:59:31.979934: step 11470, loss 7.58086e-07, acc 1\n",
      "2018-10-26T16:59:32.306066: step 11471, loss 0.0123253, acc 0.984375\n",
      "2018-10-26T16:59:32.627204: step 11472, loss 0.000109843, acc 1\n",
      "2018-10-26T16:59:32.974281: step 11473, loss 3.32112e-05, acc 1\n",
      "2018-10-26T16:59:33.275471: step 11474, loss 5.0984e-05, acc 1\n",
      "2018-10-26T16:59:33.592939: step 11475, loss 0.000355984, acc 1\n",
      "2018-10-26T16:59:33.903795: step 11476, loss 0.000678233, acc 1\n",
      "2018-10-26T16:59:34.240893: step 11477, loss 8.47126e-05, acc 1\n",
      "2018-10-26T16:59:34.553060: step 11478, loss 4.84998e-06, acc 1\n",
      "2018-10-26T16:59:34.866222: step 11479, loss 8.394e-05, acc 1\n",
      "2018-10-26T16:59:35.184474: step 11480, loss 8.92442e-05, acc 1\n",
      "2018-10-26T16:59:35.504519: step 11481, loss 0.00010831, acc 1\n",
      "2018-10-26T16:59:35.830647: step 11482, loss 7.85636e-05, acc 1\n",
      "2018-10-26T16:59:36.158771: step 11483, loss 0.000293589, acc 1\n",
      "2018-10-26T16:59:36.486895: step 11484, loss 1.03624e-05, acc 1\n",
      "2018-10-26T16:59:36.849922: step 11485, loss 0.00118297, acc 1\n",
      "2018-10-26T16:59:37.205971: step 11486, loss 0.000108167, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T16:59:37.535090: step 11487, loss 0.0287514, acc 0.984375\n",
      "2018-10-26T16:59:37.897126: step 11488, loss 3.45128e-06, acc 1\n",
      "2018-10-26T16:59:38.233265: step 11489, loss 1.47289e-05, acc 1\n",
      "2018-10-26T16:59:38.590351: step 11490, loss 2.99106e-05, acc 1\n",
      "2018-10-26T16:59:38.926374: step 11491, loss 3.12544e-05, acc 1\n",
      "2018-10-26T16:59:39.259487: step 11492, loss 3.5742e-06, acc 1\n",
      "2018-10-26T16:59:39.611580: step 11493, loss 0.000303578, acc 1\n",
      "2018-10-26T16:59:39.985548: step 11494, loss 1.61487e-06, acc 1\n",
      "2018-10-26T16:59:40.349571: step 11495, loss 0.00134008, acc 1\n",
      "2018-10-26T16:59:40.673739: step 11496, loss 0.0235996, acc 0.984375\n",
      "2018-10-26T16:59:41.004849: step 11497, loss 0.0191825, acc 0.984375\n",
      "2018-10-26T16:59:41.341921: step 11498, loss 0.00885531, acc 1\n",
      "2018-10-26T16:59:41.667051: step 11499, loss 2.81813e-05, acc 1\n",
      "2018-10-26T16:59:42.001230: step 11500, loss 2.27789e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T16:59:42.792046: step 11500, loss 4.15926, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11500\n",
      "\n",
      "2018-10-26T16:59:43.390489: step 11501, loss 2.54189e-05, acc 1\n",
      "2018-10-26T16:59:43.734531: step 11502, loss 2.02277e-06, acc 1\n",
      "2018-10-26T16:59:44.144441: step 11503, loss 0.036476, acc 0.984375\n",
      "2018-10-26T16:59:44.495497: step 11504, loss 0.000294262, acc 1\n",
      "2018-10-26T16:59:44.836583: step 11505, loss 5.93948e-06, acc 1\n",
      "2018-10-26T16:59:45.156727: step 11506, loss 4.25618e-05, acc 1\n",
      "2018-10-26T16:59:45.475879: step 11507, loss 2.13318e-05, acc 1\n",
      "2018-10-26T16:59:45.784050: step 11508, loss 4.20167e-06, acc 1\n",
      "2018-10-26T16:59:46.086248: step 11509, loss 6.44644e-05, acc 1\n",
      "2018-10-26T16:59:46.413437: step 11510, loss 0.000805954, acc 1\n",
      "2018-10-26T16:59:46.738559: step 11511, loss 0.000136661, acc 1\n",
      "2018-10-26T16:59:47.050669: step 11512, loss 0.000134178, acc 1\n",
      "2018-10-26T16:59:47.371808: step 11513, loss 3.06015e-06, acc 1\n",
      "2018-10-26T16:59:47.674001: step 11514, loss 0.00045738, acc 1\n",
      "2018-10-26T16:59:48.002125: step 11515, loss 8.23569e-05, acc 1\n",
      "2018-10-26T16:59:48.312299: step 11516, loss 0.000201506, acc 1\n",
      "2018-10-26T16:59:48.637427: step 11517, loss 0.000120382, acc 1\n",
      "2018-10-26T16:59:48.971596: step 11518, loss 5.09048e-05, acc 1\n",
      "2018-10-26T16:59:49.298664: step 11519, loss 0.00106177, acc 1\n",
      "2018-10-26T16:59:49.656703: step 11520, loss 0.00995561, acc 1\n",
      "2018-10-26T16:59:50.027715: step 11521, loss 0.108221, acc 0.984375\n",
      "2018-10-26T16:59:50.382764: step 11522, loss 0.000881647, acc 1\n",
      "2018-10-26T16:59:50.748786: step 11523, loss 0.0001771, acc 1\n",
      "2018-10-26T16:59:51.108828: step 11524, loss 2.16247e-06, acc 1\n",
      "2018-10-26T16:59:51.491842: step 11525, loss 2.27108e-05, acc 1\n",
      "2018-10-26T16:59:51.855167: step 11526, loss 0.00347709, acc 1\n",
      "2018-10-26T16:59:52.216864: step 11527, loss 8.03192e-05, acc 1\n",
      "2018-10-26T16:59:52.565935: step 11528, loss 0.000124425, acc 1\n",
      "2018-10-26T16:59:52.933951: step 11529, loss 4.50004e-05, acc 1\n",
      "2018-10-26T16:59:53.281022: step 11530, loss 5.24306e-06, acc 1\n",
      "2018-10-26T16:59:53.742788: step 11531, loss 0.00028316, acc 1\n",
      "2018-10-26T16:59:54.164660: step 11532, loss 6.7876e-05, acc 1\n",
      "2018-10-26T16:59:54.573568: step 11533, loss 6.7366e-06, acc 1\n",
      "2018-10-26T16:59:54.970507: step 11534, loss 0.000543194, acc 1\n",
      "2018-10-26T16:59:55.381501: step 11535, loss 0.000672193, acc 1\n",
      "2018-10-26T16:59:55.766381: step 11536, loss 5.36501e-05, acc 1\n",
      "2018-10-26T16:59:56.124531: step 11537, loss 1.79988e-05, acc 1\n",
      "2018-10-26T16:59:56.472494: step 11538, loss 1.64599e-05, acc 1\n",
      "2018-10-26T16:59:56.837518: step 11539, loss 1.09708e-06, acc 1\n",
      "2018-10-26T16:59:57.265417: step 11540, loss 4.16897e-05, acc 1\n",
      "2018-10-26T16:59:57.662314: step 11541, loss 0.00903809, acc 1\n",
      "2018-10-26T16:59:58.067468: step 11542, loss 3.54798e-06, acc 1\n",
      "2018-10-26T16:59:58.457270: step 11543, loss 4.9583e-05, acc 1\n",
      "2018-10-26T16:59:58.863108: step 11544, loss 0.0878464, acc 0.984375\n",
      "2018-10-26T16:59:59.197217: step 11545, loss 0.0827209, acc 0.984375\n",
      "2018-10-26T16:59:59.515425: step 11546, loss 0.00142923, acc 1\n",
      "2018-10-26T16:59:59.830521: step 11547, loss 0.000121799, acc 1\n",
      "2018-10-26T17:00:00.173704: step 11548, loss 0.00037887, acc 1\n",
      "2018-10-26T17:00:00.510798: step 11549, loss 0.00250277, acc 1\n",
      "2018-10-26T17:00:00.844811: step 11550, loss 1.02709e-05, acc 1\n",
      "2018-10-26T17:00:01.184903: step 11551, loss 7.5995e-07, acc 1\n",
      "2018-10-26T17:00:01.485100: step 11552, loss 2.59079e-06, acc 1\n",
      "2018-10-26T17:00:01.810231: step 11553, loss 4.36732e-05, acc 1\n",
      "2018-10-26T17:00:02.124391: step 11554, loss 0.000714889, acc 1\n",
      "2018-10-26T17:00:02.456506: step 11555, loss 0.000375322, acc 1\n",
      "2018-10-26T17:00:02.791613: step 11556, loss 0.0218851, acc 0.984375\n",
      "2018-10-26T17:00:03.138769: step 11557, loss 0.00217084, acc 1\n",
      "2018-10-26T17:00:03.537616: step 11558, loss 8.97786e-07, acc 1\n",
      "2018-10-26T17:00:03.907659: step 11559, loss 4.47945e-06, acc 1\n",
      "2018-10-26T17:00:04.242734: step 11560, loss 0.000876506, acc 1\n",
      "2018-10-26T17:00:04.564872: step 11561, loss 4.17402e-05, acc 1\n",
      "2018-10-26T17:00:04.904012: step 11562, loss 8.06907e-06, acc 1\n",
      "2018-10-26T17:00:05.224110: step 11563, loss 8.73567e-07, acc 1\n",
      "2018-10-26T17:00:05.552295: step 11564, loss 0.0348708, acc 0.984375\n",
      "2018-10-26T17:00:05.888339: step 11565, loss 0.00865876, acc 1\n",
      "2018-10-26T17:00:06.200600: step 11566, loss 8.60928e-06, acc 1\n",
      "2018-10-26T17:00:06.506687: step 11567, loss 0.000153441, acc 1\n",
      "2018-10-26T17:00:06.828823: step 11568, loss 8.79664e-05, acc 1\n",
      "2018-10-26T17:00:07.139995: step 11569, loss 0.00142067, acc 1\n",
      "2018-10-26T17:00:07.443246: step 11570, loss 0.000690378, acc 1\n",
      "2018-10-26T17:00:07.757343: step 11571, loss 1.23277e-05, acc 1\n",
      "2018-10-26T17:00:08.081517: step 11572, loss 0.000353773, acc 1\n",
      "2018-10-26T17:00:08.402696: step 11573, loss 1.72099e-05, acc 1\n",
      "2018-10-26T17:00:08.769638: step 11574, loss 7.61709e-06, acc 1\n",
      "2018-10-26T17:00:09.127680: step 11575, loss 0.000268581, acc 1\n",
      "2018-10-26T17:00:09.444833: step 11576, loss 6.7896e-05, acc 1\n",
      "2018-10-26T17:00:09.755005: step 11577, loss 0.000498398, acc 1\n",
      "2018-10-26T17:00:10.085123: step 11578, loss 1.83081e-05, acc 1\n",
      "2018-10-26T17:00:10.453186: step 11579, loss 3.9952e-06, acc 1\n",
      "2018-10-26T17:00:10.779268: step 11580, loss 0.000472021, acc 1\n",
      "2018-10-26T17:00:11.105405: step 11581, loss 0.0173838, acc 0.984375\n",
      "2018-10-26T17:00:11.439574: step 11582, loss 0.00413416, acc 1\n",
      "2018-10-26T17:00:11.772613: step 11583, loss 2.95404e-06, acc 1\n",
      "2018-10-26T17:00:12.092757: step 11584, loss 0.000242424, acc 1\n",
      "2018-10-26T17:00:12.419887: step 11585, loss 6.27294e-06, acc 1\n",
      "2018-10-26T17:00:12.719087: step 11586, loss 0.000225392, acc 1\n",
      "2018-10-26T17:00:13.039230: step 11587, loss 0.00103172, acc 1\n",
      "2018-10-26T17:00:13.346408: step 11588, loss 5.88616e-05, acc 1\n",
      "2018-10-26T17:00:13.673535: step 11589, loss 0.00674087, acc 1\n",
      "2018-10-26T17:00:14.006645: step 11590, loss 0.000215385, acc 1\n",
      "2018-10-26T17:00:14.325809: step 11591, loss 0.000264775, acc 1\n",
      "2018-10-26T17:00:14.632972: step 11592, loss 0.000554001, acc 1\n",
      "2018-10-26T17:00:14.950125: step 11593, loss 0.0121711, acc 0.984375\n",
      "2018-10-26T17:00:15.276252: step 11594, loss 0.000862828, acc 1\n",
      "2018-10-26T17:00:15.585430: step 11595, loss 1.01858e-05, acc 1\n",
      "2018-10-26T17:00:15.902626: step 11596, loss 0.00258915, acc 1\n",
      "2018-10-26T17:00:16.235690: step 11597, loss 0.00833456, acc 1\n",
      "2018-10-26T17:00:16.587749: step 11598, loss 2.16055e-06, acc 1\n",
      "2018-10-26T17:00:16.911882: step 11599, loss 7.02447e-05, acc 1\n",
      "2018-10-26T17:00:17.230036: step 11600, loss 0.000990284, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:00:17.996983: step 11600, loss 4.25453, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11600\n",
      "\n",
      "2018-10-26T17:00:18.639268: step 11601, loss 5.03253e-06, acc 1\n",
      "2018-10-26T17:00:18.971417: step 11602, loss 4.79355e-05, acc 1\n",
      "2018-10-26T17:00:19.460075: step 11603, loss 0.00156161, acc 1\n",
      "2018-10-26T17:00:19.814129: step 11604, loss 1.25549e-05, acc 1\n",
      "2018-10-26T17:00:20.145247: step 11605, loss 2.89061e-06, acc 1\n",
      "2018-10-26T17:00:20.488357: step 11606, loss 1.93961e-05, acc 1\n",
      "2018-10-26T17:00:20.857342: step 11607, loss 9.90399e-05, acc 1\n",
      "2018-10-26T17:00:21.205414: step 11608, loss 0.00157584, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:00:21.515582: step 11609, loss 1.75429e-05, acc 1\n",
      "2018-10-26T17:00:21.836727: step 11610, loss 9.45724e-06, acc 1\n",
      "2018-10-26T17:00:22.165960: step 11611, loss 0.0132522, acc 0.984375\n",
      "2018-10-26T17:00:22.503945: step 11612, loss 0.0014795, acc 1\n",
      "2018-10-26T17:00:22.822092: step 11613, loss 0.000324449, acc 1\n",
      "2018-10-26T17:00:23.167170: step 11614, loss 0.000617146, acc 1\n",
      "2018-10-26T17:00:23.485322: step 11615, loss 4.98978e-06, acc 1\n",
      "2018-10-26T17:00:23.803472: step 11616, loss 1.02566e-05, acc 1\n",
      "2018-10-26T17:00:24.132589: step 11617, loss 0.000262031, acc 1\n",
      "2018-10-26T17:00:24.470686: step 11618, loss 0.106504, acc 0.984375\n",
      "2018-10-26T17:00:24.787868: step 11619, loss 0.00094282, acc 1\n",
      "2018-10-26T17:00:25.118953: step 11620, loss 0.00107277, acc 1\n",
      "2018-10-26T17:00:25.441153: step 11621, loss 0.0543582, acc 0.984375\n",
      "2018-10-26T17:00:25.767222: step 11622, loss 2.07502e-05, acc 1\n",
      "2018-10-26T17:00:26.088400: step 11623, loss 9.67884e-06, acc 1\n",
      "2018-10-26T17:00:26.409506: step 11624, loss 0.00116941, acc 1\n",
      "2018-10-26T17:00:26.730748: step 11625, loss 0.000277002, acc 1\n",
      "2018-10-26T17:00:27.050827: step 11626, loss 0.000422949, acc 1\n",
      "2018-10-26T17:00:27.366952: step 11627, loss 0.00598543, acc 1\n",
      "2018-10-26T17:00:27.690085: step 11628, loss 1.92324e-05, acc 1\n",
      "2018-10-26T17:00:28.001362: step 11629, loss 0.000106204, acc 1\n",
      "2018-10-26T17:00:28.314475: step 11630, loss 3.3625e-05, acc 1\n",
      "2018-10-26T17:00:28.621596: step 11631, loss 0.000137409, acc 1\n",
      "2018-10-26T17:00:28.980640: step 11632, loss 0.083712, acc 0.984375\n",
      "2018-10-26T17:00:29.332695: step 11633, loss 0.00108788, acc 1\n",
      "2018-10-26T17:00:29.651844: step 11634, loss 7.43187e-07, acc 1\n",
      "2018-10-26T17:00:29.999914: step 11635, loss 2.1087e-05, acc 1\n",
      "2018-10-26T17:00:30.330031: step 11636, loss 0.000114805, acc 1\n",
      "2018-10-26T17:00:30.644193: step 11637, loss 1.20437e-05, acc 1\n",
      "2018-10-26T17:00:30.984283: step 11638, loss 1.98774e-05, acc 1\n",
      "2018-10-26T17:00:31.347313: step 11639, loss 5.03299e-05, acc 1\n",
      "2018-10-26T17:00:31.658486: step 11640, loss 6.80739e-05, acc 1\n",
      "2018-10-26T17:00:31.986606: step 11641, loss 1.80672e-06, acc 1\n",
      "2018-10-26T17:00:32.303963: step 11642, loss 6.16531e-07, acc 1\n",
      "2018-10-26T17:00:32.636882: step 11643, loss 0.11744, acc 0.984375\n",
      "2018-10-26T17:00:32.978955: step 11644, loss 3.809e-06, acc 1\n",
      "2018-10-26T17:00:33.307076: step 11645, loss 0.000118419, acc 1\n",
      "2018-10-26T17:00:33.628259: step 11646, loss 0.0114577, acc 1\n",
      "2018-10-26T17:00:33.954348: step 11647, loss 6.31381e-05, acc 1\n",
      "2018-10-26T17:00:34.275491: step 11648, loss 0.00489837, acc 1\n",
      "2018-10-26T17:00:34.600655: step 11649, loss 0.000265193, acc 1\n",
      "2018-10-26T17:00:34.925755: step 11650, loss 0.000866225, acc 1\n",
      "2018-10-26T17:00:35.253874: step 11651, loss 5.28971e-05, acc 1\n",
      "2018-10-26T17:00:35.577013: step 11652, loss 0.0111641, acc 0.984375\n",
      "2018-10-26T17:00:35.902204: step 11653, loss 0.000658048, acc 1\n",
      "2018-10-26T17:00:36.227278: step 11654, loss 7.31411e-05, acc 1\n",
      "2018-10-26T17:00:36.553405: step 11655, loss 0.000105329, acc 1\n",
      "2018-10-26T17:00:36.882523: step 11656, loss 7.58673e-05, acc 1\n",
      "2018-10-26T17:00:37.236577: step 11657, loss 6.57595e-05, acc 1\n",
      "2018-10-26T17:00:37.565702: step 11658, loss 0.000258291, acc 1\n",
      "2018-10-26T17:00:37.881925: step 11659, loss 0.00348018, acc 1\n",
      "2018-10-26T17:00:38.208980: step 11660, loss 2.89573e-05, acc 1\n",
      "2018-10-26T17:00:38.519151: step 11661, loss 4.16463e-06, acc 1\n",
      "2018-10-26T17:00:38.845284: step 11662, loss 0.00368369, acc 1\n",
      "2018-10-26T17:00:39.172410: step 11663, loss 2.10097e-06, acc 1\n",
      "2018-10-26T17:00:39.509509: step 11664, loss 0.0543112, acc 0.984375\n",
      "2018-10-26T17:00:39.845918: step 11665, loss 2.1646e-05, acc 1\n",
      "2018-10-26T17:00:40.166748: step 11666, loss 0.00202766, acc 1\n",
      "2018-10-26T17:00:40.498862: step 11667, loss 7.15901e-05, acc 1\n",
      "2018-10-26T17:00:40.821998: step 11668, loss 8.44492e-06, acc 1\n",
      "2018-10-26T17:00:41.153116: step 11669, loss 3.48944e-05, acc 1\n",
      "2018-10-26T17:00:41.476254: step 11670, loss 0.000166063, acc 1\n",
      "2018-10-26T17:00:41.794400: step 11671, loss 1.95085e-05, acc 1\n",
      "2018-10-26T17:00:42.121527: step 11672, loss 0.0989517, acc 0.984375\n",
      "2018-10-26T17:00:42.435725: step 11673, loss 7.73532e-05, acc 1\n",
      "2018-10-26T17:00:42.752871: step 11674, loss 9.78736e-06, acc 1\n",
      "2018-10-26T17:00:43.063009: step 11675, loss 8.44238e-05, acc 1\n",
      "2018-10-26T17:00:43.388143: step 11676, loss 1.06641e-05, acc 1\n",
      "2018-10-26T17:00:43.709286: step 11677, loss 0.0104549, acc 1\n",
      "2018-10-26T17:00:44.025440: step 11678, loss 0.000178688, acc 1\n",
      "2018-10-26T17:00:44.339602: step 11679, loss 7.33893e-05, acc 1\n",
      "2018-10-26T17:00:44.667726: step 11680, loss 1.09953e-05, acc 1\n",
      "2018-10-26T17:00:44.981884: step 11681, loss 9.53738e-06, acc 1\n",
      "2018-10-26T17:00:45.311004: step 11682, loss 6.32288e-05, acc 1\n",
      "2018-10-26T17:00:45.633144: step 11683, loss 3.84055e-06, acc 1\n",
      "2018-10-26T17:00:45.978224: step 11684, loss 0.0188953, acc 0.984375\n",
      "2018-10-26T17:00:46.344247: step 11685, loss 0.0255292, acc 0.96875\n",
      "2018-10-26T17:00:46.671438: step 11686, loss 0.000124797, acc 1\n",
      "2018-10-26T17:00:46.999576: step 11687, loss 0.000694197, acc 1\n",
      "2018-10-26T17:00:47.328615: step 11688, loss 0.000725115, acc 1\n",
      "2018-10-26T17:00:47.660727: step 11689, loss 0.0041231, acc 1\n",
      "2018-10-26T17:00:47.997860: step 11690, loss 0.000135614, acc 1\n",
      "2018-10-26T17:00:48.315977: step 11691, loss 5.49397e-05, acc 1\n",
      "2018-10-26T17:00:48.633127: step 11692, loss 2.34311e-05, acc 1\n",
      "2018-10-26T17:00:48.954272: step 11693, loss 0.00176979, acc 1\n",
      "2018-10-26T17:00:49.270424: step 11694, loss 2.44075e-05, acc 1\n",
      "2018-10-26T17:00:49.585584: step 11695, loss 0.000210179, acc 1\n",
      "2018-10-26T17:00:49.948614: step 11696, loss 0.000738452, acc 1\n",
      "2018-10-26T17:00:50.310649: step 11697, loss 0.000553487, acc 1\n",
      "2018-10-26T17:00:50.642802: step 11698, loss 4.4028e-06, acc 1\n",
      "2018-10-26T17:00:50.985845: step 11699, loss 0.0183423, acc 0.984375\n",
      "2018-10-26T17:00:51.296016: step 11700, loss 4.94806e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:00:52.106850: step 11700, loss 4.29772, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11700\n",
      "\n",
      "2018-10-26T17:00:52.714226: step 11701, loss 8.31387e-05, acc 1\n",
      "2018-10-26T17:00:53.047333: step 11702, loss 6.47942e-06, acc 1\n",
      "2018-10-26T17:00:53.449307: step 11703, loss 6.61179e-05, acc 1\n",
      "2018-10-26T17:00:53.829245: step 11704, loss 8.58566e-05, acc 1\n",
      "2018-10-26T17:00:54.161402: step 11705, loss 2.3295e-05, acc 1\n",
      "2018-10-26T17:00:54.481536: step 11706, loss 2.87582e-06, acc 1\n",
      "2018-10-26T17:00:54.802645: step 11707, loss 9.10149e-06, acc 1\n",
      "2018-10-26T17:00:55.123789: step 11708, loss 8.08195e-05, acc 1\n",
      "2018-10-26T17:00:55.498784: step 11709, loss 0.00017761, acc 1\n",
      "2018-10-26T17:00:55.852880: step 11710, loss 0.00454014, acc 1\n",
      "2018-10-26T17:00:56.206894: step 11711, loss 0.0382941, acc 0.984375\n",
      "2018-10-26T17:00:56.605825: step 11712, loss 0.000189259, acc 1\n",
      "2018-10-26T17:00:56.963900: step 11713, loss 0.000398048, acc 1\n",
      "2018-10-26T17:00:57.333881: step 11714, loss 0.0260398, acc 0.984375\n",
      "2018-10-26T17:00:57.703891: step 11715, loss 0.00469406, acc 1\n",
      "2018-10-26T17:00:58.081883: step 11716, loss 0.00143107, acc 1\n",
      "2018-10-26T17:00:58.462866: step 11717, loss 4.37568e-05, acc 1\n",
      "2018-10-26T17:00:58.802955: step 11718, loss 2.94283e-05, acc 1\n",
      "2018-10-26T17:00:59.228818: step 11719, loss 0.00061096, acc 1\n",
      "2018-10-26T17:00:59.653683: step 11720, loss 6.60418e-06, acc 1\n",
      "2018-10-26T17:01:00.163321: step 11721, loss 8.38888e-05, acc 1\n",
      "2018-10-26T17:01:00.592175: step 11722, loss 6.38646e-06, acc 1\n",
      "2018-10-26T17:01:01.016043: step 11723, loss 8.61575e-05, acc 1\n",
      "2018-10-26T17:01:01.418966: step 11724, loss 0.000908031, acc 1\n",
      "2018-10-26T17:01:01.848818: step 11725, loss 1.25726e-06, acc 1\n",
      "2018-10-26T17:01:02.222818: step 11726, loss 1.37224e-05, acc 1\n",
      "2018-10-26T17:01:02.617804: step 11727, loss 2.72068e-05, acc 1\n",
      "2018-10-26T17:01:02.977802: step 11728, loss 0.0142717, acc 0.984375\n",
      "2018-10-26T17:01:03.400671: step 11729, loss 2.61865e-05, acc 1\n",
      "2018-10-26T17:01:03.796614: step 11730, loss 0.00175219, acc 1\n",
      "2018-10-26T17:01:04.144683: step 11731, loss 0.00518681, acc 1\n",
      "2018-10-26T17:01:04.479789: step 11732, loss 0.000562831, acc 1\n",
      "2018-10-26T17:01:04.822873: step 11733, loss 7.24484e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:01:05.137033: step 11734, loss 0.00243517, acc 1\n",
      "2018-10-26T17:01:05.467154: step 11735, loss 9.16487e-05, acc 1\n",
      "2018-10-26T17:01:05.810298: step 11736, loss 0.0622741, acc 0.984375\n",
      "2018-10-26T17:01:06.128387: step 11737, loss 6.57843e-06, acc 1\n",
      "2018-10-26T17:01:06.451564: step 11738, loss 0.000273861, acc 1\n",
      "2018-10-26T17:01:06.786625: step 11739, loss 7.04066e-07, acc 1\n",
      "2018-10-26T17:01:07.116806: step 11740, loss 4.38257e-06, acc 1\n",
      "2018-10-26T17:01:07.429906: step 11741, loss 2.57216e-06, acc 1\n",
      "2018-10-26T17:01:07.782962: step 11742, loss 7.44177e-05, acc 1\n",
      "2018-10-26T17:01:08.119063: step 11743, loss 1.4068e-05, acc 1\n",
      "2018-10-26T17:01:08.443202: step 11744, loss 0.00425647, acc 1\n",
      "2018-10-26T17:01:08.759402: step 11745, loss 7.55702e-06, acc 1\n",
      "2018-10-26T17:01:09.084484: step 11746, loss 2.6923e-05, acc 1\n",
      "2018-10-26T17:01:09.407621: step 11747, loss 2.922e-05, acc 1\n",
      "2018-10-26T17:01:09.727805: step 11748, loss 2.73589e-05, acc 1\n",
      "2018-10-26T17:01:10.054893: step 11749, loss 0.000262113, acc 1\n",
      "2018-10-26T17:01:10.401039: step 11750, loss 0.0535474, acc 0.984375\n",
      "2018-10-26T17:01:10.738066: step 11751, loss 1.662e-05, acc 1\n",
      "2018-10-26T17:01:11.054223: step 11752, loss 4.36557e-06, acc 1\n",
      "2018-10-26T17:01:11.378360: step 11753, loss 0.021242, acc 0.984375\n",
      "2018-10-26T17:01:11.710468: step 11754, loss 0.00353725, acc 1\n",
      "2018-10-26T17:01:12.024633: step 11755, loss 0.000331509, acc 1\n",
      "2018-10-26T17:01:12.358751: step 11756, loss 0.000123738, acc 1\n",
      "2018-10-26T17:01:12.681876: step 11757, loss 0.000659465, acc 1\n",
      "2018-10-26T17:01:13.008032: step 11758, loss 4.63189e-06, acc 1\n",
      "2018-10-26T17:01:13.336124: step 11759, loss 7.96542e-05, acc 1\n",
      "2018-10-26T17:01:13.669235: step 11760, loss 0.0194032, acc 0.984375\n",
      "2018-10-26T17:01:13.995427: step 11761, loss 2.91297e-06, acc 1\n",
      "2018-10-26T17:01:14.325485: step 11762, loss 2.5327e-05, acc 1\n",
      "2018-10-26T17:01:14.666574: step 11763, loss 0.00569341, acc 1\n",
      "2018-10-26T17:01:14.981764: step 11764, loss 3.42725e-07, acc 1\n",
      "2018-10-26T17:01:15.284921: step 11765, loss 0.000153051, acc 1\n",
      "2018-10-26T17:01:15.611050: step 11766, loss 1.26658e-06, acc 1\n",
      "2018-10-26T17:01:15.929196: step 11767, loss 0.000338767, acc 1\n",
      "2018-10-26T17:01:16.261309: step 11768, loss 5.92321e-05, acc 1\n",
      "2018-10-26T17:01:16.606386: step 11769, loss 0.000172693, acc 1\n",
      "2018-10-26T17:01:16.929524: step 11770, loss 1.99646e-05, acc 1\n",
      "2018-10-26T17:01:17.252662: step 11771, loss 1.21072e-07, acc 1\n",
      "2018-10-26T17:01:17.580827: step 11772, loss 0.00027226, acc 1\n",
      "2018-10-26T17:01:17.896941: step 11773, loss 1.31161e-05, acc 1\n",
      "2018-10-26T17:01:18.220079: step 11774, loss 0.000139943, acc 1\n",
      "2018-10-26T17:01:18.545297: step 11775, loss 6.14672e-08, acc 1\n",
      "2018-10-26T17:01:18.863359: step 11776, loss 0.00156942, acc 1\n",
      "2018-10-26T17:01:19.180512: step 11777, loss 1.72486e-05, acc 1\n",
      "2018-10-26T17:01:19.514618: step 11778, loss 0.0148906, acc 0.984375\n",
      "2018-10-26T17:01:19.827784: step 11779, loss 8.89935e-05, acc 1\n",
      "2018-10-26T17:01:20.141941: step 11780, loss 0.0444036, acc 0.984375\n",
      "2018-10-26T17:01:20.447126: step 11781, loss 0.000255198, acc 1\n",
      "2018-10-26T17:01:20.770265: step 11782, loss 5.45751e-07, acc 1\n",
      "2018-10-26T17:01:21.097426: step 11783, loss 7.70645e-05, acc 1\n",
      "2018-10-26T17:01:21.419600: step 11784, loss 3.41968e-06, acc 1\n",
      "2018-10-26T17:01:21.749649: step 11785, loss 3.1261e-05, acc 1\n",
      "2018-10-26T17:01:22.099714: step 11786, loss 0.000811866, acc 1\n",
      "2018-10-26T17:01:22.412930: step 11787, loss 4.89628e-05, acc 1\n",
      "2018-10-26T17:01:22.757002: step 11788, loss 9.99969e-06, acc 1\n",
      "2018-10-26T17:01:23.095050: step 11789, loss 3.93893e-05, acc 1\n",
      "2018-10-26T17:01:23.445118: step 11790, loss 0.000121282, acc 1\n",
      "2018-10-26T17:01:23.774236: step 11791, loss 0.000122254, acc 1\n",
      "2018-10-26T17:01:24.105354: step 11792, loss 0.000826497, acc 1\n",
      "2018-10-26T17:01:24.432479: step 11793, loss 1.18602e-05, acc 1\n",
      "2018-10-26T17:01:24.754618: step 11794, loss 0.000164994, acc 1\n",
      "2018-10-26T17:01:25.074761: step 11795, loss 0.00329532, acc 1\n",
      "2018-10-26T17:01:25.397990: step 11796, loss 0.00349837, acc 1\n",
      "2018-10-26T17:01:25.721078: step 11797, loss 4.87649e-05, acc 1\n",
      "2018-10-26T17:01:26.048191: step 11798, loss 0.000177832, acc 1\n",
      "2018-10-26T17:01:26.379279: step 11799, loss 4.90188e-05, acc 1\n",
      "2018-10-26T17:01:26.710391: step 11800, loss 0.000552261, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:01:27.487314: step 11800, loss 4.36386, acc 0.706379\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11800\n",
      "\n",
      "2018-10-26T17:01:28.116637: step 11801, loss 0.000731084, acc 1\n",
      "2018-10-26T17:01:28.438775: step 11802, loss 0.00152542, acc 1\n",
      "2018-10-26T17:01:28.862640: step 11803, loss 0.0266444, acc 0.984375\n",
      "2018-10-26T17:01:29.217725: step 11804, loss 2.03579e-06, acc 1\n",
      "2018-10-26T17:01:29.559779: step 11805, loss 8.52838e-05, acc 1\n",
      "2018-10-26T17:01:29.889897: step 11806, loss 0.00109383, acc 1\n",
      "2018-10-26T17:01:30.206051: step 11807, loss 0.000180096, acc 1\n",
      "2018-10-26T17:01:30.550136: step 11808, loss 2.40513e-05, acc 1\n",
      "2018-10-26T17:01:30.874269: step 11809, loss 0.00249262, acc 1\n",
      "2018-10-26T17:01:31.200547: step 11810, loss 0.000185756, acc 1\n",
      "2018-10-26T17:01:31.527521: step 11811, loss 0.00696355, acc 1\n",
      "2018-10-26T17:01:31.866616: step 11812, loss 0.00115873, acc 1\n",
      "2018-10-26T17:01:32.213687: step 11813, loss 8.32792e-05, acc 1\n",
      "2018-10-26T17:01:32.530843: step 11814, loss 0.00374064, acc 1\n",
      "2018-10-26T17:01:32.853980: step 11815, loss 0.000473635, acc 1\n",
      "2018-10-26T17:01:33.185095: step 11816, loss 0.00126103, acc 1\n",
      "2018-10-26T17:01:33.514213: step 11817, loss 4.60945e-06, acc 1\n",
      "2018-10-26T17:01:33.808429: step 11818, loss 1.45286e-07, acc 1\n",
      "2018-10-26T17:01:34.133558: step 11819, loss 3.98507e-05, acc 1\n",
      "2018-10-26T17:01:34.465673: step 11820, loss 1.22746e-06, acc 1\n",
      "2018-10-26T17:01:34.837707: step 11821, loss 3.97109e-05, acc 1\n",
      "2018-10-26T17:01:35.163809: step 11822, loss 0.000408734, acc 1\n",
      "2018-10-26T17:01:35.482051: step 11823, loss 0.000919934, acc 1\n",
      "2018-10-26T17:01:35.797112: step 11824, loss 0.0157702, acc 0.984375\n",
      "2018-10-26T17:01:36.132218: step 11825, loss 0.000409006, acc 1\n",
      "2018-10-26T17:01:36.489265: step 11826, loss 1.55861e-05, acc 1\n",
      "2018-10-26T17:01:36.822375: step 11827, loss 0.00353053, acc 1\n",
      "2018-10-26T17:01:37.148505: step 11828, loss 5.47905e-05, acc 1\n",
      "2018-10-26T17:01:37.476628: step 11829, loss 0.0824967, acc 0.984375\n",
      "2018-10-26T17:01:37.799766: step 11830, loss 4.08803e-06, acc 1\n",
      "2018-10-26T17:01:38.122903: step 11831, loss 0.0025112, acc 1\n",
      "2018-10-26T17:01:38.455015: step 11832, loss 0.000535954, acc 1\n",
      "2018-10-26T17:01:38.762274: step 11833, loss 1.28145e-06, acc 1\n",
      "2018-10-26T17:01:39.100352: step 11834, loss 0.0071224, acc 1\n",
      "2018-10-26T17:01:39.423428: step 11835, loss 6.73472e-06, acc 1\n",
      "2018-10-26T17:01:39.750553: step 11836, loss 1.01798e-05, acc 1\n",
      "2018-10-26T17:01:40.068702: step 11837, loss 0.00156164, acc 1\n",
      "2018-10-26T17:01:40.391837: step 11838, loss 0.0105098, acc 1\n",
      "2018-10-26T17:01:40.712030: step 11839, loss 8.90282e-06, acc 1\n",
      "2018-10-26T17:01:41.033122: step 11840, loss 5.36541e-06, acc 1\n",
      "2018-10-26T17:01:41.354301: step 11841, loss 0.0246437, acc 0.984375\n",
      "2018-10-26T17:01:41.686378: step 11842, loss 0.00153021, acc 1\n",
      "2018-10-26T17:01:42.042426: step 11843, loss 4.72748e-05, acc 1\n",
      "2018-10-26T17:01:42.358584: step 11844, loss 0.000220867, acc 1\n",
      "2018-10-26T17:01:42.674740: step 11845, loss 3.01781e-05, acc 1\n",
      "2018-10-26T17:01:43.002862: step 11846, loss 0.00011311, acc 1\n",
      "2018-10-26T17:01:43.329990: step 11847, loss 0.00108446, acc 1\n",
      "2018-10-26T17:01:43.660103: step 11848, loss 6.15008e-06, acc 1\n",
      "2018-10-26T17:01:43.996210: step 11849, loss 8.74595e-06, acc 1\n",
      "2018-10-26T17:01:44.297402: step 11850, loss 0.000522692, acc 1\n",
      "2018-10-26T17:01:44.618542: step 11851, loss 5.69548e-05, acc 1\n",
      "2018-10-26T17:01:44.943674: step 11852, loss 0.000888303, acc 1\n",
      "2018-10-26T17:01:45.267842: step 11853, loss 5.43045e-05, acc 1\n",
      "2018-10-26T17:01:45.601918: step 11854, loss 0.0176303, acc 0.984375\n",
      "2018-10-26T17:01:45.929041: step 11855, loss 6.24588e-05, acc 1\n",
      "2018-10-26T17:01:46.229239: step 11856, loss 0.000302017, acc 1\n",
      "2018-10-26T17:01:46.539412: step 11857, loss 6.26247e-05, acc 1\n",
      "2018-10-26T17:01:46.858560: step 11858, loss 1.6745e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:01:47.187682: step 11859, loss 3.82597e-05, acc 1\n",
      "2018-10-26T17:01:47.505829: step 11860, loss 3.91296e-06, acc 1\n",
      "2018-10-26T17:01:47.833953: step 11861, loss 1.713e-05, acc 1\n",
      "2018-10-26T17:01:48.194988: step 11862, loss 4.24023e-05, acc 1\n",
      "2018-10-26T17:01:48.504160: step 11863, loss 0.000224692, acc 1\n",
      "2018-10-26T17:01:48.834278: step 11864, loss 1.84299e-05, acc 1\n",
      "2018-10-26T17:01:49.157416: step 11865, loss 3.03412e-06, acc 1\n",
      "2018-10-26T17:01:49.470582: step 11866, loss 0.00114236, acc 1\n",
      "2018-10-26T17:01:49.845581: step 11867, loss 0.00725856, acc 1\n",
      "2018-10-26T17:01:50.170710: step 11868, loss 5.82227e-06, acc 1\n",
      "2018-10-26T17:01:50.499833: step 11869, loss 1.5369e-05, acc 1\n",
      "2018-10-26T17:01:50.825958: step 11870, loss 0.000411062, acc 1\n",
      "2018-10-26T17:01:51.151089: step 11871, loss 5.91556e-05, acc 1\n",
      "2018-10-26T17:01:51.488243: step 11872, loss 1.47193e-05, acc 1\n",
      "2018-10-26T17:01:51.831272: step 11873, loss 0.000161129, acc 1\n",
      "2018-10-26T17:01:52.152413: step 11874, loss 1.0654e-06, acc 1\n",
      "2018-10-26T17:01:52.467573: step 11875, loss 8.0467e-05, acc 1\n",
      "2018-10-26T17:01:52.798686: step 11876, loss 0.118617, acc 0.984375\n",
      "2018-10-26T17:01:53.151782: step 11877, loss 0.000170089, acc 1\n",
      "2018-10-26T17:01:53.505801: step 11878, loss 2.83778e-05, acc 1\n",
      "2018-10-26T17:01:53.834919: step 11879, loss 2.7827e-06, acc 1\n",
      "2018-10-26T17:01:54.156094: step 11880, loss 0.0708767, acc 0.984375\n",
      "2018-10-26T17:01:54.461291: step 11881, loss 2.38946e-05, acc 1\n",
      "2018-10-26T17:01:54.775409: step 11882, loss 0.000587103, acc 1\n",
      "2018-10-26T17:01:55.086575: step 11883, loss 0.00335757, acc 1\n",
      "2018-10-26T17:01:55.409807: step 11884, loss 3.32617e-05, acc 1\n",
      "2018-10-26T17:01:55.732848: step 11885, loss 0.00992126, acc 1\n",
      "2018-10-26T17:01:56.051096: step 11886, loss 0.0224845, acc 0.984375\n",
      "2018-10-26T17:01:56.387291: step 11887, loss 6.88968e-05, acc 1\n",
      "2018-10-26T17:01:56.707244: step 11888, loss 0.000711038, acc 1\n",
      "2018-10-26T17:01:57.026394: step 11889, loss 5.6621e-06, acc 1\n",
      "2018-10-26T17:01:57.346537: step 11890, loss 0.000528659, acc 1\n",
      "2018-10-26T17:01:57.693608: step 11891, loss 3.12095e-05, acc 1\n",
      "2018-10-26T17:01:58.027716: step 11892, loss 0.00108202, acc 1\n",
      "2018-10-26T17:01:58.357872: step 11893, loss 0.0948799, acc 0.984375\n",
      "2018-10-26T17:01:58.684148: step 11894, loss 1.78253e-05, acc 1\n",
      "2018-10-26T17:01:59.021263: step 11895, loss 0.00418663, acc 1\n",
      "2018-10-26T17:01:59.334228: step 11896, loss 1.50874e-07, acc 1\n",
      "2018-10-26T17:01:59.674319: step 11897, loss 0.00600586, acc 1\n",
      "2018-10-26T17:01:59.984489: step 11898, loss 7.75333e-05, acc 1\n",
      "2018-10-26T17:02:00.297654: step 11899, loss 0.000395255, acc 1\n",
      "2018-10-26T17:02:00.632760: step 11900, loss 0.000226112, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:02:01.481488: step 11900, loss 4.30952, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-11900\n",
      "\n",
      "2018-10-26T17:02:02.132749: step 11901, loss 0.00498175, acc 1\n",
      "2018-10-26T17:02:02.508743: step 11902, loss 0.00376219, acc 1\n",
      "2018-10-26T17:02:02.957544: step 11903, loss 0.00391422, acc 1\n",
      "2018-10-26T17:02:03.329551: step 11904, loss 2.84411e-06, acc 1\n",
      "2018-10-26T17:02:03.688591: step 11905, loss 1.05798e-05, acc 1\n",
      "2018-10-26T17:02:04.029712: step 11906, loss 1.49012e-08, acc 1\n",
      "2018-10-26T17:02:04.381791: step 11907, loss 7.42079e-05, acc 1\n",
      "2018-10-26T17:02:04.751751: step 11908, loss 7.35075e-06, acc 1\n",
      "2018-10-26T17:02:05.198557: step 11909, loss 0.0783913, acc 0.984375\n",
      "2018-10-26T17:02:05.632397: step 11910, loss 0.000771668, acc 1\n",
      "2018-10-26T17:02:06.097155: step 11911, loss 1.2759e-06, acc 1\n",
      "2018-10-26T17:02:06.508092: step 11912, loss 1.47149e-07, acc 1\n",
      "2018-10-26T17:02:06.900065: step 11913, loss 6.3955e-06, acc 1\n",
      "2018-10-26T17:02:07.266065: step 11914, loss 0.0533163, acc 0.984375\n",
      "2018-10-26T17:02:07.653995: step 11915, loss 0.00167516, acc 1\n",
      "2018-10-26T17:02:08.041963: step 11916, loss 0.000576807, acc 1\n",
      "2018-10-26T17:02:08.464828: step 11917, loss 0.00037188, acc 1\n",
      "2018-10-26T17:02:08.834841: step 11918, loss 0.00288486, acc 1\n",
      "2018-10-26T17:02:09.208841: step 11919, loss 0.000170265, acc 1\n",
      "2018-10-26T17:02:09.632710: step 11920, loss 3.26236e-05, acc 1\n",
      "2018-10-26T17:02:09.954848: step 11921, loss 3.3042e-06, acc 1\n",
      "2018-10-26T17:02:10.339823: step 11922, loss 7.03971e-05, acc 1\n",
      "2018-10-26T17:02:10.648992: step 11923, loss 1.22631e-05, acc 1\n",
      "2018-10-26T17:02:11.006040: step 11924, loss 0.000553857, acc 1\n",
      "2018-10-26T17:02:11.405144: step 11925, loss 0.00634883, acc 1\n",
      "2018-10-26T17:02:11.739082: step 11926, loss 2.06753e-07, acc 1\n",
      "2018-10-26T17:02:12.074185: step 11927, loss 0.00382744, acc 1\n",
      "2018-10-26T17:02:12.393334: step 11928, loss 1.68871e-05, acc 1\n",
      "2018-10-26T17:02:12.718495: step 11929, loss 7.93852e-06, acc 1\n",
      "2018-10-26T17:02:13.067534: step 11930, loss 2.57803e-05, acc 1\n",
      "2018-10-26T17:02:13.397654: step 11931, loss 0.00308463, acc 1\n",
      "2018-10-26T17:02:13.731877: step 11932, loss 1.07557e-05, acc 1\n",
      "2018-10-26T17:02:14.044922: step 11933, loss 1.12204e-05, acc 1\n",
      "2018-10-26T17:02:14.356102: step 11934, loss 5.49478e-07, acc 1\n",
      "2018-10-26T17:02:14.678232: step 11935, loss 4.52777e-05, acc 1\n",
      "2018-10-26T17:02:14.995381: step 11936, loss 1.9755e-05, acc 1\n",
      "2018-10-26T17:02:15.313533: step 11937, loss 0.00379288, acc 1\n",
      "2018-10-26T17:02:15.635671: step 11938, loss 2.78828e-06, acc 1\n",
      "2018-10-26T17:02:15.955815: step 11939, loss 0.000337081, acc 1\n",
      "2018-10-26T17:02:16.300894: step 11940, loss 0.00386465, acc 1\n",
      "2018-10-26T17:02:16.622034: step 11941, loss 5.36353e-05, acc 1\n",
      "2018-10-26T17:02:16.926272: step 11942, loss 9.63113e-05, acc 1\n",
      "2018-10-26T17:02:17.261370: step 11943, loss 0.00032839, acc 1\n",
      "2018-10-26T17:02:17.585464: step 11944, loss 6.89171e-07, acc 1\n",
      "2018-10-26T17:02:17.909595: step 11945, loss 0.000144224, acc 1\n",
      "2018-10-26T17:02:18.231819: step 11946, loss 0.000466022, acc 1\n",
      "2018-10-26T17:02:18.554870: step 11947, loss 8.64928e-06, acc 1\n",
      "2018-10-26T17:02:18.915907: step 11948, loss 7.31674e-06, acc 1\n",
      "2018-10-26T17:02:19.274946: step 11949, loss 5.30457e-05, acc 1\n",
      "2018-10-26T17:02:19.606066: step 11950, loss 0.020603, acc 0.984375\n",
      "2018-10-26T17:02:19.923213: step 11951, loss 0.000185647, acc 1\n",
      "2018-10-26T17:02:20.241410: step 11952, loss 1.65593e-05, acc 1\n",
      "2018-10-26T17:02:20.548546: step 11953, loss 5.84213e-05, acc 1\n",
      "2018-10-26T17:02:20.882654: step 11954, loss 4.57614e-06, acc 1\n",
      "2018-10-26T17:02:21.213885: step 11955, loss 0.000223768, acc 1\n",
      "2018-10-26T17:02:21.519947: step 11956, loss 2.00259e-05, acc 1\n",
      "2018-10-26T17:02:21.836109: step 11957, loss 5.60653e-07, acc 1\n",
      "2018-10-26T17:02:22.165225: step 11958, loss 0.000373696, acc 1\n",
      "2018-10-26T17:02:22.527257: step 11959, loss 9.66363e-06, acc 1\n",
      "2018-10-26T17:02:22.837432: step 11960, loss 4.92254e-06, acc 1\n",
      "2018-10-26T17:02:23.169541: step 11961, loss 0.000316753, acc 1\n",
      "2018-10-26T17:02:23.487694: step 11962, loss 1.81418e-06, acc 1\n",
      "2018-10-26T17:02:23.801852: step 11963, loss 1.42678e-05, acc 1\n",
      "2018-10-26T17:02:24.134961: step 11964, loss 0.000108071, acc 1\n",
      "2018-10-26T17:02:24.496102: step 11965, loss 6.24604e-05, acc 1\n",
      "2018-10-26T17:02:24.824147: step 11966, loss 0.000166521, acc 1\n",
      "2018-10-26T17:02:25.137284: step 11967, loss 1.76618e-05, acc 1\n",
      "2018-10-26T17:02:25.469396: step 11968, loss 0.00013582, acc 1\n",
      "2018-10-26T17:02:25.803572: step 11969, loss 7.45044e-07, acc 1\n",
      "2018-10-26T17:02:26.125643: step 11970, loss 1.54411e-06, acc 1\n",
      "2018-10-26T17:02:26.442795: step 11971, loss 0.000557971, acc 1\n",
      "2018-10-26T17:02:26.764937: step 11972, loss 1.95663e-05, acc 1\n",
      "2018-10-26T17:02:27.080126: step 11973, loss 4.74193e-06, acc 1\n",
      "2018-10-26T17:02:27.401235: step 11974, loss 1.3536e-05, acc 1\n",
      "2018-10-26T17:02:27.719387: step 11975, loss 3.81253e-06, acc 1\n",
      "2018-10-26T17:02:28.046509: step 11976, loss 2.39133e-05, acc 1\n",
      "2018-10-26T17:02:28.401563: step 11977, loss 0.000255272, acc 1\n",
      "2018-10-26T17:02:28.715724: step 11978, loss 0.00093796, acc 1\n",
      "2018-10-26T17:02:29.041851: step 11979, loss 0.000243753, acc 1\n",
      "2018-10-26T17:02:29.377979: step 11980, loss 4.75657e-06, acc 1\n",
      "2018-10-26T17:02:29.722034: step 11981, loss 0.00110799, acc 1\n",
      "2018-10-26T17:02:30.042178: step 11982, loss 1.93711e-06, acc 1\n",
      "2018-10-26T17:02:30.349358: step 11983, loss 0.0047345, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:02:30.648664: step 11984, loss 0.00475231, acc 1\n",
      "2018-10-26T17:02:30.966940: step 11985, loss 0.000371727, acc 1\n",
      "2018-10-26T17:02:31.278875: step 11986, loss 0.00268949, acc 1\n",
      "2018-10-26T17:02:31.597023: step 11987, loss 0.00103135, acc 1\n",
      "2018-10-26T17:02:31.917171: step 11988, loss 0.000106057, acc 1\n",
      "2018-10-26T17:02:32.248317: step 11989, loss 5.11584e-05, acc 1\n",
      "2018-10-26T17:02:32.584388: step 11990, loss 4.05475e-06, acc 1\n",
      "2018-10-26T17:02:32.912510: step 11991, loss 4.79758e-06, acc 1\n",
      "2018-10-26T17:02:33.241630: step 11992, loss 8.01987e-05, acc 1\n",
      "2018-10-26T17:02:33.565767: step 11993, loss 3.09197e-07, acc 1\n",
      "2018-10-26T17:02:33.859976: step 11994, loss 0.000868489, acc 1\n",
      "2018-10-26T17:02:34.188166: step 11995, loss 3.57626e-07, acc 1\n",
      "2018-10-26T17:02:34.511279: step 11996, loss 4.06725e-05, acc 1\n",
      "2018-10-26T17:02:34.847343: step 11997, loss 0.000614362, acc 1\n",
      "2018-10-26T17:02:35.178483: step 11998, loss 6.71572e-05, acc 1\n",
      "2018-10-26T17:02:35.496652: step 11999, loss 1.02373e-05, acc 1\n",
      "2018-10-26T17:02:35.797803: step 12000, loss 1.64951e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:02:36.578834: step 12000, loss 4.34397, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12000\n",
      "\n",
      "2018-10-26T17:02:37.198103: step 12001, loss 9.74156e-06, acc 1\n",
      "2018-10-26T17:02:37.526182: step 12002, loss 8.94053e-07, acc 1\n",
      "2018-10-26T17:02:37.963032: step 12003, loss 6.94623e-06, acc 1\n",
      "2018-10-26T17:02:38.287153: step 12004, loss 5.68101e-07, acc 1\n",
      "2018-10-26T17:02:38.625246: step 12005, loss 0.00124542, acc 1\n",
      "2018-10-26T17:02:38.938409: step 12006, loss 7.35449e-05, acc 1\n",
      "2018-10-26T17:02:39.257556: step 12007, loss 0.000611832, acc 1\n",
      "2018-10-26T17:02:39.603632: step 12008, loss 0.000441397, acc 1\n",
      "2018-10-26T17:02:39.953697: step 12009, loss 0.000148146, acc 1\n",
      "2018-10-26T17:02:40.276874: step 12010, loss 0.000696494, acc 1\n",
      "2018-10-26T17:02:40.636870: step 12011, loss 0.000286733, acc 1\n",
      "2018-10-26T17:02:40.994998: step 12012, loss 0.000138626, acc 1\n",
      "2018-10-26T17:02:41.321089: step 12013, loss 5.14824e-05, acc 1\n",
      "2018-10-26T17:02:41.641248: step 12014, loss 7.59296e-05, acc 1\n",
      "2018-10-26T17:02:41.973302: step 12015, loss 8.61031e-06, acc 1\n",
      "2018-10-26T17:02:42.298432: step 12016, loss 0.0012414, acc 1\n",
      "2018-10-26T17:02:42.647617: step 12017, loss 2.63201e-05, acc 1\n",
      "2018-10-26T17:02:42.984601: step 12018, loss 0.000194688, acc 1\n",
      "2018-10-26T17:02:43.322696: step 12019, loss 1.41244e-05, acc 1\n",
      "2018-10-26T17:02:43.671804: step 12020, loss 0.000185276, acc 1\n",
      "2018-10-26T17:02:43.995897: step 12021, loss 0.000134972, acc 1\n",
      "2018-10-26T17:02:44.317060: step 12022, loss 2.61242e-05, acc 1\n",
      "2018-10-26T17:02:44.647159: step 12023, loss 8.72695e-05, acc 1\n",
      "2018-10-26T17:02:44.980269: step 12024, loss 8.11366e-06, acc 1\n",
      "2018-10-26T17:02:45.296422: step 12025, loss 5.38302e-07, acc 1\n",
      "2018-10-26T17:02:45.609584: step 12026, loss 7.63684e-08, acc 1\n",
      "2018-10-26T17:02:45.938705: step 12027, loss 1.81129e-05, acc 1\n",
      "2018-10-26T17:02:46.253862: step 12028, loss 0.000193514, acc 1\n",
      "2018-10-26T17:02:46.578001: step 12029, loss 2.64892e-05, acc 1\n",
      "2018-10-26T17:02:46.905126: step 12030, loss 7.31083e-05, acc 1\n",
      "2018-10-26T17:02:47.235241: step 12031, loss 4.60597e-06, acc 1\n",
      "2018-10-26T17:02:47.566398: step 12032, loss 0.000772121, acc 1\n",
      "2018-10-26T17:02:47.920414: step 12033, loss 8.22158e-05, acc 1\n",
      "2018-10-26T17:02:48.240554: step 12034, loss 6.14668e-07, acc 1\n",
      "2018-10-26T17:02:48.557773: step 12035, loss 5.45778e-05, acc 1\n",
      "2018-10-26T17:02:48.956642: step 12036, loss 4.62084e-06, acc 1\n",
      "2018-10-26T17:02:49.312690: step 12037, loss 0.000189406, acc 1\n",
      "2018-10-26T17:02:49.629842: step 12038, loss 3.85986e-05, acc 1\n",
      "2018-10-26T17:02:49.979002: step 12039, loss 1.04308e-07, acc 1\n",
      "2018-10-26T17:02:50.286093: step 12040, loss 4.35462e-05, acc 1\n",
      "2018-10-26T17:02:50.606237: step 12041, loss 8.27608e-06, acc 1\n",
      "2018-10-26T17:02:50.909426: step 12042, loss 6.26844e-05, acc 1\n",
      "2018-10-26T17:02:51.276444: step 12043, loss 3.63018e-06, acc 1\n",
      "2018-10-26T17:02:51.627508: step 12044, loss 2.89432e-06, acc 1\n",
      "2018-10-26T17:02:51.950643: step 12045, loss 3.97625e-06, acc 1\n",
      "2018-10-26T17:02:52.288741: step 12046, loss 0.000969317, acc 1\n",
      "2018-10-26T17:02:52.608885: step 12047, loss 9.32667e-05, acc 1\n",
      "2018-10-26T17:02:52.924045: step 12048, loss 0.000232022, acc 1\n",
      "2018-10-26T17:02:53.252164: step 12049, loss 6.52914e-06, acc 1\n",
      "2018-10-26T17:02:53.569317: step 12050, loss 0.00380421, acc 1\n",
      "2018-10-26T17:02:53.887466: step 12051, loss 0.0494506, acc 0.984375\n",
      "2018-10-26T17:02:54.204621: step 12052, loss 1.54824e-05, acc 1\n",
      "2018-10-26T17:02:54.554684: step 12053, loss 8.56938e-05, acc 1\n",
      "2018-10-26T17:02:54.875829: step 12054, loss 1.18133e-05, acc 1\n",
      "2018-10-26T17:02:55.206943: step 12055, loss 4.57632e-06, acc 1\n",
      "2018-10-26T17:02:55.557009: step 12056, loss 1.43889e-05, acc 1\n",
      "2018-10-26T17:02:55.874159: step 12057, loss 4.13473e-06, acc 1\n",
      "2018-10-26T17:02:56.192309: step 12058, loss 0.000115703, acc 1\n",
      "2018-10-26T17:02:56.514451: step 12059, loss 0.000148267, acc 1\n",
      "2018-10-26T17:02:56.840579: step 12060, loss 0.00157035, acc 1\n",
      "2018-10-26T17:02:57.202677: step 12061, loss 0.0150166, acc 0.984375\n",
      "2018-10-26T17:02:57.515775: step 12062, loss 4.39278e-05, acc 1\n",
      "2018-10-26T17:02:57.859902: step 12063, loss 5.12188e-06, acc 1\n",
      "2018-10-26T17:02:58.182994: step 12064, loss 5.33431e-06, acc 1\n",
      "2018-10-26T17:02:58.533055: step 12065, loss 8.19221e-06, acc 1\n",
      "2018-10-26T17:02:58.848211: step 12066, loss 0.00426058, acc 1\n",
      "2018-10-26T17:02:59.179329: step 12067, loss 1.63877e-05, acc 1\n",
      "2018-10-26T17:02:59.501531: step 12068, loss 7.1717e-05, acc 1\n",
      "2018-10-26T17:02:59.833765: step 12069, loss 5.61994e-05, acc 1\n",
      "2018-10-26T17:03:00.150772: step 12070, loss 0.000191881, acc 1\n",
      "2018-10-26T17:03:00.465891: step 12071, loss 9.24783e-05, acc 1\n",
      "2018-10-26T17:03:00.791065: step 12072, loss 1.93039e-05, acc 1\n",
      "2018-10-26T17:03:01.109171: step 12073, loss 3.24099e-07, acc 1\n",
      "2018-10-26T17:03:01.443281: step 12074, loss 1.79927e-06, acc 1\n",
      "2018-10-26T17:03:01.767412: step 12075, loss 0.000151816, acc 1\n",
      "2018-10-26T17:03:02.086561: step 12076, loss 0.081411, acc 0.984375\n",
      "2018-10-26T17:03:02.419670: step 12077, loss 6.16313e-05, acc 1\n",
      "2018-10-26T17:03:02.725852: step 12078, loss 0.000107825, acc 1\n",
      "2018-10-26T17:03:03.052978: step 12079, loss 4.41669e-05, acc 1\n",
      "2018-10-26T17:03:03.368141: step 12080, loss 9.60236e-06, acc 1\n",
      "2018-10-26T17:03:03.674319: step 12081, loss 1.74233e-05, acc 1\n",
      "2018-10-26T17:03:04.007428: step 12082, loss 5.17756e-06, acc 1\n",
      "2018-10-26T17:03:04.360550: step 12083, loss 0.000497497, acc 1\n",
      "2018-10-26T17:03:04.718527: step 12084, loss 0.000259325, acc 1\n",
      "2018-10-26T17:03:05.072581: step 12085, loss 0.000114379, acc 1\n",
      "2018-10-26T17:03:05.402701: step 12086, loss 3.59271e-06, acc 1\n",
      "2018-10-26T17:03:05.753762: step 12087, loss 0.000228832, acc 1\n",
      "2018-10-26T17:03:06.090861: step 12088, loss 9.01534e-05, acc 1\n",
      "2018-10-26T17:03:06.434942: step 12089, loss 5.70539e-05, acc 1\n",
      "2018-10-26T17:03:06.832878: step 12090, loss 8.07708e-05, acc 1\n",
      "2018-10-26T17:03:07.180952: step 12091, loss 2.68832e-05, acc 1\n",
      "2018-10-26T17:03:07.564922: step 12092, loss 0.000832544, acc 1\n",
      "2018-10-26T17:03:07.966850: step 12093, loss 2.66161e-06, acc 1\n",
      "2018-10-26T17:03:08.327923: step 12094, loss 5.4715e-06, acc 1\n",
      "2018-10-26T17:03:08.700888: step 12095, loss 9.36901e-07, acc 1\n",
      "2018-10-26T17:03:09.055979: step 12096, loss 0.000411056, acc 1\n",
      "2018-10-26T17:03:09.416976: step 12097, loss 9.65925e-05, acc 1\n",
      "2018-10-26T17:03:09.786985: step 12098, loss 3.80827e-05, acc 1\n",
      "2018-10-26T17:03:10.180968: step 12099, loss 0.00457969, acc 1\n",
      "2018-10-26T17:03:10.555932: step 12100, loss 4.65956e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:03:11.665967: step 12100, loss 4.36171, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12100\n",
      "\n",
      "2018-10-26T17:03:12.354127: step 12101, loss 0.000170795, acc 1\n",
      "2018-10-26T17:03:12.751134: step 12102, loss 1.32246e-06, acc 1\n",
      "2018-10-26T17:03:13.209841: step 12103, loss 0.000423353, acc 1\n",
      "2018-10-26T17:03:13.604784: step 12104, loss 0.00391399, acc 1\n",
      "2018-10-26T17:03:13.969810: step 12105, loss 2.01033e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:03:14.369849: step 12106, loss 0.00157898, acc 1\n",
      "2018-10-26T17:03:14.705846: step 12107, loss 2.65762e-05, acc 1\n",
      "2018-10-26T17:03:15.127716: step 12108, loss 1.45797e-05, acc 1\n",
      "2018-10-26T17:03:15.545599: step 12109, loss 0.000191916, acc 1\n",
      "2018-10-26T17:03:15.919600: step 12110, loss 0.00203372, acc 1\n",
      "2018-10-26T17:03:16.425249: step 12111, loss 3.14204e-06, acc 1\n",
      "2018-10-26T17:03:16.777308: step 12112, loss 1.92593e-06, acc 1\n",
      "2018-10-26T17:03:17.098450: step 12113, loss 5.05695e-05, acc 1\n",
      "2018-10-26T17:03:17.473448: step 12114, loss 0.0146753, acc 0.984375\n",
      "2018-10-26T17:03:17.789603: step 12115, loss 2.16162e-05, acc 1\n",
      "2018-10-26T17:03:18.173578: step 12116, loss 0.000148885, acc 1\n",
      "2018-10-26T17:03:18.511732: step 12117, loss 0.000136121, acc 1\n",
      "2018-10-26T17:03:18.840799: step 12118, loss 0.00133431, acc 1\n",
      "2018-10-26T17:03:19.191858: step 12119, loss 2.91462e-05, acc 1\n",
      "2018-10-26T17:03:19.604755: step 12120, loss 7.76718e-07, acc 1\n",
      "2018-10-26T17:03:20.003724: step 12121, loss 1.71363e-07, acc 1\n",
      "2018-10-26T17:03:20.380681: step 12122, loss 0.000202133, acc 1\n",
      "2018-10-26T17:03:20.730748: step 12123, loss 0.000183416, acc 1\n",
      "2018-10-26T17:03:21.082846: step 12124, loss 1.85983e-05, acc 1\n",
      "2018-10-26T17:03:21.419908: step 12125, loss 4.52058e-05, acc 1\n",
      "2018-10-26T17:03:21.750023: step 12126, loss 0.00030739, acc 1\n",
      "2018-10-26T17:03:22.108065: step 12127, loss 7.64061e-05, acc 1\n",
      "2018-10-26T17:03:22.493036: step 12128, loss 3.70999e-05, acc 1\n",
      "2018-10-26T17:03:22.851161: step 12129, loss 2.89816e-06, acc 1\n",
      "2018-10-26T17:03:23.175218: step 12130, loss 4.61166e-06, acc 1\n",
      "2018-10-26T17:03:23.503338: step 12131, loss 0.000853392, acc 1\n",
      "2018-10-26T17:03:23.827472: step 12132, loss 0.000962881, acc 1\n",
      "2018-10-26T17:03:24.148617: step 12133, loss 1.53022e-05, acc 1\n",
      "2018-10-26T17:03:24.501669: step 12134, loss 0.000319927, acc 1\n",
      "2018-10-26T17:03:24.838770: step 12135, loss 2.4126e-05, acc 1\n",
      "2018-10-26T17:03:25.205791: step 12136, loss 6.65288e-06, acc 1\n",
      "2018-10-26T17:03:25.608712: step 12137, loss 1.43291e-05, acc 1\n",
      "2018-10-26T17:03:25.997673: step 12138, loss 0.000184815, acc 1\n",
      "2018-10-26T17:03:26.374666: step 12139, loss 5.1678e-05, acc 1\n",
      "2018-10-26T17:03:26.727723: step 12140, loss 8.42618e-05, acc 1\n",
      "2018-10-26T17:03:27.053851: step 12141, loss 0.000227125, acc 1\n",
      "2018-10-26T17:03:27.402584: step 12142, loss 0.000497885, acc 1\n",
      "2018-10-26T17:03:27.781907: step 12143, loss 0.000317477, acc 1\n",
      "2018-10-26T17:03:28.103050: step 12144, loss 0.000159007, acc 1\n",
      "2018-10-26T17:03:28.433166: step 12145, loss 6.13122e-06, acc 1\n",
      "2018-10-26T17:03:28.756302: step 12146, loss 1.78814e-07, acc 1\n",
      "2018-10-26T17:03:29.102382: step 12147, loss 0.000209391, acc 1\n",
      "2018-10-26T17:03:29.445462: step 12148, loss 1.3597e-05, acc 1\n",
      "2018-10-26T17:03:29.765606: step 12149, loss 0.000973335, acc 1\n",
      "2018-10-26T17:03:30.116667: step 12150, loss 0.104734, acc 0.983333\n",
      "2018-10-26T17:03:30.447809: step 12151, loss 1.06866e-05, acc 1\n",
      "2018-10-26T17:03:30.767943: step 12152, loss 0.000991598, acc 1\n",
      "2018-10-26T17:03:31.141931: step 12153, loss 1.28522e-07, acc 1\n",
      "2018-10-26T17:03:31.468099: step 12154, loss 0.00163153, acc 1\n",
      "2018-10-26T17:03:31.802166: step 12155, loss 9.72279e-07, acc 1\n",
      "2018-10-26T17:03:32.139263: step 12156, loss 3.56367e-05, acc 1\n",
      "2018-10-26T17:03:32.477364: step 12157, loss 6.54427e-05, acc 1\n",
      "2018-10-26T17:03:32.812466: step 12158, loss 4.83121e-05, acc 1\n",
      "2018-10-26T17:03:33.167632: step 12159, loss 0.000258705, acc 1\n",
      "2018-10-26T17:03:33.522613: step 12160, loss 1.28522e-07, acc 1\n",
      "2018-10-26T17:03:33.853730: step 12161, loss 3.57811e-05, acc 1\n",
      "2018-10-26T17:03:34.166987: step 12162, loss 3.75754e-05, acc 1\n",
      "2018-10-26T17:03:34.479066: step 12163, loss 1.21072e-07, acc 1\n",
      "2018-10-26T17:03:34.810127: step 12164, loss 2.2712e-05, acc 1\n",
      "2018-10-26T17:03:35.129279: step 12165, loss 0.00210442, acc 1\n",
      "2018-10-26T17:03:35.481451: step 12166, loss 0.000142659, acc 1\n",
      "2018-10-26T17:03:35.827409: step 12167, loss 1.37088e-06, acc 1\n",
      "2018-10-26T17:03:36.163515: step 12168, loss 3.60539e-05, acc 1\n",
      "2018-10-26T17:03:36.499614: step 12169, loss 0.000149475, acc 1\n",
      "2018-10-26T17:03:36.842698: step 12170, loss 0.0001015, acc 1\n",
      "2018-10-26T17:03:37.169823: step 12171, loss 3.06021e-06, acc 1\n",
      "2018-10-26T17:03:37.480065: step 12172, loss 0.0262873, acc 0.984375\n",
      "2018-10-26T17:03:37.820087: step 12173, loss 1.31064e-05, acc 1\n",
      "2018-10-26T17:03:38.143221: step 12174, loss 1.56745e-05, acc 1\n",
      "2018-10-26T17:03:38.459378: step 12175, loss 8.91361e-05, acc 1\n",
      "2018-10-26T17:03:38.781554: step 12176, loss 8.41123e-05, acc 1\n",
      "2018-10-26T17:03:39.109644: step 12177, loss 0.000134998, acc 1\n",
      "2018-10-26T17:03:39.446743: step 12178, loss 4.3357e-06, acc 1\n",
      "2018-10-26T17:03:39.772869: step 12179, loss 1.1753e-06, acc 1\n",
      "2018-10-26T17:03:40.079051: step 12180, loss 0.000134226, acc 1\n",
      "2018-10-26T17:03:40.410168: step 12181, loss 0.000187511, acc 1\n",
      "2018-10-26T17:03:40.736297: step 12182, loss 3.8217e-06, acc 1\n",
      "2018-10-26T17:03:41.060430: step 12183, loss 0.000426223, acc 1\n",
      "2018-10-26T17:03:41.378577: step 12184, loss 9.25905e-05, acc 1\n",
      "2018-10-26T17:03:41.712685: step 12185, loss 6.31122e-05, acc 1\n",
      "2018-10-26T17:03:42.050785: step 12186, loss 0.000676578, acc 1\n",
      "2018-10-26T17:03:42.403842: step 12187, loss 2.54044e-06, acc 1\n",
      "2018-10-26T17:03:42.721033: step 12188, loss 5.88592e-07, acc 1\n",
      "2018-10-26T17:03:43.040141: step 12189, loss 0.000339881, acc 1\n",
      "2018-10-26T17:03:43.352304: step 12190, loss 1.59811e-05, acc 1\n",
      "2018-10-26T17:03:43.680429: step 12191, loss 2.0879e-06, acc 1\n",
      "2018-10-26T17:03:44.050439: step 12192, loss 1.39801e-05, acc 1\n",
      "2018-10-26T17:03:44.416462: step 12193, loss 5.60516e-05, acc 1\n",
      "2018-10-26T17:03:44.743589: step 12194, loss 0.00126391, acc 1\n",
      "2018-10-26T17:03:45.089735: step 12195, loss 0.000107169, acc 1\n",
      "2018-10-26T17:03:45.403906: step 12196, loss 0.0135719, acc 0.984375\n",
      "2018-10-26T17:03:45.733942: step 12197, loss 8.4096e-06, acc 1\n",
      "2018-10-26T17:03:46.049099: step 12198, loss 9.12777e-05, acc 1\n",
      "2018-10-26T17:03:46.383209: step 12199, loss 3.21833e-06, acc 1\n",
      "2018-10-26T17:03:46.701397: step 12200, loss 1.31527e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:03:47.468307: step 12200, loss 4.44555, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12200\n",
      "\n",
      "2018-10-26T17:03:48.101614: step 12201, loss 0.0020127, acc 1\n",
      "2018-10-26T17:03:48.429784: step 12202, loss 1.98867e-05, acc 1\n",
      "2018-10-26T17:03:48.861921: step 12203, loss 3.43882e-05, acc 1\n",
      "2018-10-26T17:03:49.190706: step 12204, loss 9.56488e-06, acc 1\n",
      "2018-10-26T17:03:49.495891: step 12205, loss 1.05889e-05, acc 1\n",
      "2018-10-26T17:03:49.825142: step 12206, loss 2.15681e-06, acc 1\n",
      "2018-10-26T17:03:50.155127: step 12207, loss 2.60837e-05, acc 1\n",
      "2018-10-26T17:03:50.467294: step 12208, loss 1.37832e-06, acc 1\n",
      "2018-10-26T17:03:50.783449: step 12209, loss 9.09374e-06, acc 1\n",
      "2018-10-26T17:03:51.100605: step 12210, loss 4.39238e-05, acc 1\n",
      "2018-10-26T17:03:51.414763: step 12211, loss 7.2829e-07, acc 1\n",
      "2018-10-26T17:03:51.727930: step 12212, loss 1.06096e-05, acc 1\n",
      "2018-10-26T17:03:52.043083: step 12213, loss 4.23546e-06, acc 1\n",
      "2018-10-26T17:03:52.348310: step 12214, loss 1.26079e-05, acc 1\n",
      "2018-10-26T17:03:52.650489: step 12215, loss 0.000564228, acc 1\n",
      "2018-10-26T17:03:52.968612: step 12216, loss 0.00271611, acc 1\n",
      "2018-10-26T17:03:53.279782: step 12217, loss 0.000269126, acc 1\n",
      "2018-10-26T17:03:53.601918: step 12218, loss 0.0127401, acc 0.984375\n",
      "2018-10-26T17:03:53.931039: step 12219, loss 6.51881e-06, acc 1\n",
      "2018-10-26T17:03:54.253179: step 12220, loss 0.00094264, acc 1\n",
      "2018-10-26T17:03:54.575322: step 12221, loss 4.09384e-06, acc 1\n",
      "2018-10-26T17:03:54.876513: step 12222, loss 1.55706e-05, acc 1\n",
      "2018-10-26T17:03:55.201689: step 12223, loss 0.000454153, acc 1\n",
      "2018-10-26T17:03:55.525780: step 12224, loss 0.00205816, acc 1\n",
      "2018-10-26T17:03:55.849913: step 12225, loss 1.15855e-06, acc 1\n",
      "2018-10-26T17:03:56.168062: step 12226, loss 3.83872e-06, acc 1\n",
      "2018-10-26T17:03:56.481274: step 12227, loss 1.9967e-06, acc 1\n",
      "2018-10-26T17:03:56.788408: step 12228, loss 9.47179e-05, acc 1\n",
      "2018-10-26T17:03:57.112538: step 12229, loss 1.73217e-06, acc 1\n",
      "2018-10-26T17:03:57.440663: step 12230, loss 1.94457e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:03:57.752830: step 12231, loss 0.00189267, acc 1\n",
      "2018-10-26T17:03:58.084941: step 12232, loss 1.93522e-06, acc 1\n",
      "2018-10-26T17:03:58.410072: step 12233, loss 0.000262941, acc 1\n",
      "2018-10-26T17:03:58.718291: step 12234, loss 0.000973059, acc 1\n",
      "2018-10-26T17:03:59.063327: step 12235, loss 7.39188e-05, acc 1\n",
      "2018-10-26T17:03:59.411404: step 12236, loss 2.23061e-05, acc 1\n",
      "2018-10-26T17:03:59.751488: step 12237, loss 7.84941e-06, acc 1\n",
      "2018-10-26T17:04:00.093574: step 12238, loss 8.63151e-06, acc 1\n",
      "2018-10-26T17:04:00.414717: step 12239, loss 5.25244e-06, acc 1\n",
      "2018-10-26T17:04:00.787721: step 12240, loss 0.00041376, acc 1\n",
      "2018-10-26T17:04:01.131848: step 12241, loss 1.72476e-06, acc 1\n",
      "2018-10-26T17:04:01.674350: step 12242, loss 0.00573967, acc 1\n",
      "2018-10-26T17:04:02.005557: step 12243, loss 0.00025715, acc 1\n",
      "2018-10-26T17:04:02.433323: step 12244, loss 2.91797e-05, acc 1\n",
      "2018-10-26T17:04:02.855199: step 12245, loss 0.000371413, acc 1\n",
      "2018-10-26T17:04:03.188306: step 12246, loss 2.61037e-05, acc 1\n",
      "2018-10-26T17:04:03.584250: step 12247, loss 6.10091e-06, acc 1\n",
      "2018-10-26T17:04:03.992159: step 12248, loss 0.0071519, acc 1\n",
      "2018-10-26T17:04:04.318290: step 12249, loss 0.000135893, acc 1\n",
      "2018-10-26T17:04:04.656386: step 12250, loss 0.00167532, acc 1\n",
      "2018-10-26T17:04:04.997549: step 12251, loss 0.135333, acc 0.984375\n",
      "2018-10-26T17:04:05.316618: step 12252, loss 7.36759e-06, acc 1\n",
      "2018-10-26T17:04:05.659704: step 12253, loss 0.0102336, acc 1\n",
      "2018-10-26T17:04:06.003786: step 12254, loss 0.00427465, acc 1\n",
      "2018-10-26T17:04:06.325987: step 12255, loss 6.57053e-06, acc 1\n",
      "2018-10-26T17:04:06.654049: step 12256, loss 5.9953e-06, acc 1\n",
      "2018-10-26T17:04:06.994230: step 12257, loss 0.00316278, acc 1\n",
      "2018-10-26T17:04:07.336226: step 12258, loss 0.0128686, acc 0.984375\n",
      "2018-10-26T17:04:07.702246: step 12259, loss 9.04037e-05, acc 1\n",
      "2018-10-26T17:04:08.035355: step 12260, loss 5.68038e-06, acc 1\n",
      "2018-10-26T17:04:08.384425: step 12261, loss 1.39079e-05, acc 1\n",
      "2018-10-26T17:04:08.726509: step 12262, loss 1.66221e-05, acc 1\n",
      "2018-10-26T17:04:09.065604: step 12263, loss 3.71579e-06, acc 1\n",
      "2018-10-26T17:04:09.440601: step 12264, loss 0.0192544, acc 0.984375\n",
      "2018-10-26T17:04:09.823580: step 12265, loss 1.52912e-05, acc 1\n",
      "2018-10-26T17:04:10.186611: step 12266, loss 0.000127273, acc 1\n",
      "2018-10-26T17:04:10.525703: step 12267, loss 3.2559e-05, acc 1\n",
      "2018-10-26T17:04:10.881750: step 12268, loss 6.96165e-05, acc 1\n",
      "2018-10-26T17:04:11.260740: step 12269, loss 7.09662e-07, acc 1\n",
      "2018-10-26T17:04:11.624801: step 12270, loss 3.62454e-06, acc 1\n",
      "2018-10-26T17:04:11.974830: step 12271, loss 2.10623e-05, acc 1\n",
      "2018-10-26T17:04:12.317953: step 12272, loss 1.87523e-05, acc 1\n",
      "2018-10-26T17:04:12.701888: step 12273, loss 1.54416e-05, acc 1\n",
      "2018-10-26T17:04:13.062923: step 12274, loss 0.00011877, acc 1\n",
      "2018-10-26T17:04:13.444544: step 12275, loss 2.77522e-06, acc 1\n",
      "2018-10-26T17:04:13.802058: step 12276, loss 6.24125e-05, acc 1\n",
      "2018-10-26T17:04:14.147025: step 12277, loss 0.0040414, acc 1\n",
      "2018-10-26T17:04:14.532050: step 12278, loss 8.22618e-06, acc 1\n",
      "2018-10-26T17:04:14.922953: step 12279, loss 0.000107149, acc 1\n",
      "2018-10-26T17:04:15.300941: step 12280, loss 0.00196281, acc 1\n",
      "2018-10-26T17:04:15.691897: step 12281, loss 4.36963e-06, acc 1\n",
      "2018-10-26T17:04:16.096815: step 12282, loss 5.23033e-05, acc 1\n",
      "2018-10-26T17:04:16.452864: step 12283, loss 5.34575e-07, acc 1\n",
      "2018-10-26T17:04:16.932583: step 12284, loss 0.00199079, acc 1\n",
      "2018-10-26T17:04:17.374402: step 12285, loss 2.11032e-06, acc 1\n",
      "2018-10-26T17:04:17.820212: step 12286, loss 0.00516975, acc 1\n",
      "2018-10-26T17:04:18.255050: step 12287, loss 2.31892e-06, acc 1\n",
      "2018-10-26T17:04:18.682922: step 12288, loss 0.00082495, acc 1\n",
      "2018-10-26T17:04:19.078221: step 12289, loss 2.01348e-06, acc 1\n",
      "2018-10-26T17:04:19.414951: step 12290, loss 2.91626e-05, acc 1\n",
      "2018-10-26T17:04:19.830842: step 12291, loss 7.54867e-05, acc 1\n",
      "2018-10-26T17:04:20.245730: step 12292, loss 3.3338e-05, acc 1\n",
      "2018-10-26T17:04:20.612749: step 12293, loss 0.000644666, acc 1\n",
      "2018-10-26T17:04:21.075513: step 12294, loss 0.000261944, acc 1\n",
      "2018-10-26T17:04:21.461512: step 12295, loss 6.76898e-05, acc 1\n",
      "2018-10-26T17:04:21.809555: step 12296, loss 6.96623e-07, acc 1\n",
      "2018-10-26T17:04:22.159620: step 12297, loss 0.000190636, acc 1\n",
      "2018-10-26T17:04:22.516664: step 12298, loss 0.000307684, acc 1\n",
      "2018-10-26T17:04:22.857754: step 12299, loss 1.11821e-05, acc 1\n",
      "2018-10-26T17:04:23.214797: step 12300, loss 4.19375e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:04:24.021690: step 12300, loss 4.42686, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12300\n",
      "\n",
      "2018-10-26T17:04:24.639989: step 12301, loss 1.21813e-05, acc 1\n",
      "2018-10-26T17:04:24.982079: step 12302, loss 1.32244e-06, acc 1\n",
      "2018-10-26T17:04:25.502683: step 12303, loss 0.000225157, acc 1\n",
      "2018-10-26T17:04:25.873695: step 12304, loss 5.56926e-07, acc 1\n",
      "2018-10-26T17:04:26.223759: step 12305, loss 3.85919e-05, acc 1\n",
      "2018-10-26T17:04:26.561855: step 12306, loss 1.78625e-06, acc 1\n",
      "2018-10-26T17:04:26.888018: step 12307, loss 2.99949e-05, acc 1\n",
      "2018-10-26T17:04:27.215110: step 12308, loss 6.4708e-05, acc 1\n",
      "2018-10-26T17:04:27.543338: step 12309, loss 0.00126477, acc 1\n",
      "2018-10-26T17:04:27.949149: step 12310, loss 2.22929e-05, acc 1\n",
      "2018-10-26T17:04:28.314212: step 12311, loss 0.00910389, acc 1\n",
      "2018-10-26T17:04:28.689170: step 12312, loss 1.32245e-06, acc 1\n",
      "2018-10-26T17:04:29.031295: step 12313, loss 1.10866e-05, acc 1\n",
      "2018-10-26T17:04:29.399275: step 12314, loss 4.76216e-06, acc 1\n",
      "2018-10-26T17:04:29.847077: step 12315, loss 0.08819, acc 0.984375\n",
      "2018-10-26T17:04:30.283910: step 12316, loss 8.62198e-05, acc 1\n",
      "2018-10-26T17:04:30.655956: step 12317, loss 3.60208e-06, acc 1\n",
      "2018-10-26T17:04:30.999000: step 12318, loss 0.000103371, acc 1\n",
      "2018-10-26T17:04:31.333106: step 12319, loss 0.00777737, acc 1\n",
      "2018-10-26T17:04:31.711177: step 12320, loss 4.34126e-06, acc 1\n",
      "2018-10-26T17:04:32.074190: step 12321, loss 0.000149249, acc 1\n",
      "2018-10-26T17:04:32.430179: step 12322, loss 8.82161e-05, acc 1\n",
      "2018-10-26T17:04:32.803221: step 12323, loss 0.000463869, acc 1\n",
      "2018-10-26T17:04:33.197127: step 12324, loss 8.47482e-07, acc 1\n",
      "2018-10-26T17:04:33.578110: step 12325, loss 2.41765e-06, acc 1\n",
      "2018-10-26T17:04:33.925184: step 12326, loss 0.000210412, acc 1\n",
      "2018-10-26T17:04:34.259310: step 12327, loss 6.38875e-07, acc 1\n",
      "2018-10-26T17:04:34.587413: step 12328, loss 6.04158e-06, acc 1\n",
      "2018-10-26T17:04:34.931529: step 12329, loss 7.77208e-05, acc 1\n",
      "2018-10-26T17:04:35.307489: step 12330, loss 1.53477e-06, acc 1\n",
      "2018-10-26T17:04:35.616665: step 12331, loss 8.0865e-06, acc 1\n",
      "2018-10-26T17:04:35.989667: step 12332, loss 8.21574e-05, acc 1\n",
      "2018-10-26T17:04:36.349730: step 12333, loss 2.82354e-06, acc 1\n",
      "2018-10-26T17:04:36.703819: step 12334, loss 3.96694e-06, acc 1\n",
      "2018-10-26T17:04:37.051870: step 12335, loss 0.00390847, acc 1\n",
      "2018-10-26T17:04:37.405925: step 12336, loss 0.000326669, acc 1\n",
      "2018-10-26T17:04:37.755947: step 12337, loss 2.15498e-06, acc 1\n",
      "2018-10-26T17:04:38.106011: step 12338, loss 0.0028644, acc 1\n",
      "2018-10-26T17:04:38.450091: step 12339, loss 0.000187834, acc 1\n",
      "2018-10-26T17:04:38.790186: step 12340, loss 4.82423e-07, acc 1\n",
      "2018-10-26T17:04:39.173158: step 12341, loss 0.0559379, acc 0.984375\n",
      "2018-10-26T17:04:39.552146: step 12342, loss 1.37703e-05, acc 1\n",
      "2018-10-26T17:04:39.889246: step 12343, loss 6.35661e-06, acc 1\n",
      "2018-10-26T17:04:40.243406: step 12344, loss 4.37628e-05, acc 1\n",
      "2018-10-26T17:04:40.604339: step 12345, loss 3.12083e-05, acc 1\n",
      "2018-10-26T17:04:40.963375: step 12346, loss 0.000470457, acc 1\n",
      "2018-10-26T17:04:41.318427: step 12347, loss 0.000284009, acc 1\n",
      "2018-10-26T17:04:41.671554: step 12348, loss 5.59586e-05, acc 1\n",
      "2018-10-26T17:04:42.023543: step 12349, loss 1.09522e-06, acc 1\n",
      "2018-10-26T17:04:42.399539: step 12350, loss 0.000137377, acc 1\n",
      "2018-10-26T17:04:42.753593: step 12351, loss 0.000693329, acc 1\n",
      "2018-10-26T17:04:43.111635: step 12352, loss 1.03916e-05, acc 1\n",
      "2018-10-26T17:04:43.560437: step 12353, loss 1.67263e-06, acc 1\n",
      "2018-10-26T17:04:43.974331: step 12354, loss 6.4658e-06, acc 1\n",
      "2018-10-26T17:04:44.363294: step 12355, loss 5.15385e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:04:44.706376: step 12356, loss 8.51596e-06, acc 1\n",
      "2018-10-26T17:04:45.047465: step 12357, loss 0.0492476, acc 0.984375\n",
      "2018-10-26T17:04:45.419471: step 12358, loss 5.02268e-05, acc 1\n",
      "2018-10-26T17:04:45.785585: step 12359, loss 0.00814567, acc 1\n",
      "2018-10-26T17:04:46.130573: step 12360, loss 0.000507345, acc 1\n",
      "2018-10-26T17:04:46.473677: step 12361, loss 3.68007e-05, acc 1\n",
      "2018-10-26T17:04:46.835686: step 12362, loss 0.00215589, acc 1\n",
      "2018-10-26T17:04:47.176775: step 12363, loss 1.91283e-06, acc 1\n",
      "2018-10-26T17:04:47.517905: step 12364, loss 5.85204e-05, acc 1\n",
      "2018-10-26T17:04:47.841997: step 12365, loss 5.81141e-07, acc 1\n",
      "2018-10-26T17:04:48.179100: step 12366, loss 0.000221773, acc 1\n",
      "2018-10-26T17:04:48.511210: step 12367, loss 2.23517e-07, acc 1\n",
      "2018-10-26T17:04:48.846315: step 12368, loss 4.93123e-05, acc 1\n",
      "2018-10-26T17:04:49.193387: step 12369, loss 0.000891924, acc 1\n",
      "2018-10-26T17:04:49.566391: step 12370, loss 1.45154e-05, acc 1\n",
      "2018-10-26T17:04:49.895511: step 12371, loss 0.00191138, acc 1\n",
      "2018-10-26T17:04:50.222637: step 12372, loss 0.00156582, acc 1\n",
      "2018-10-26T17:04:50.572703: step 12373, loss 0.000133328, acc 1\n",
      "2018-10-26T17:04:50.914786: step 12374, loss 0.000311245, acc 1\n",
      "2018-10-26T17:04:51.280809: step 12375, loss 4.34339e-06, acc 1\n",
      "2018-10-26T17:04:51.638971: step 12376, loss 6.86897e-06, acc 1\n",
      "2018-10-26T17:04:52.017843: step 12377, loss 4.26149e-06, acc 1\n",
      "2018-10-26T17:04:52.400819: step 12378, loss 1.93779e-05, acc 1\n",
      "2018-10-26T17:04:52.778807: step 12379, loss 5.62475e-06, acc 1\n",
      "2018-10-26T17:04:53.123886: step 12380, loss 0.00123311, acc 1\n",
      "2018-10-26T17:04:53.530847: step 12381, loss 4.37506e-06, acc 1\n",
      "2018-10-26T17:04:53.904800: step 12382, loss 9.13118e-05, acc 1\n",
      "2018-10-26T17:04:54.338640: step 12383, loss 0.000422267, acc 1\n",
      "2018-10-26T17:04:54.681723: step 12384, loss 7.7579e-05, acc 1\n",
      "2018-10-26T17:04:55.034782: step 12385, loss 3.2315e-06, acc 1\n",
      "2018-10-26T17:04:55.380856: step 12386, loss 0.000719048, acc 1\n",
      "2018-10-26T17:04:55.708008: step 12387, loss 6.88302e-06, acc 1\n",
      "2018-10-26T17:04:56.085974: step 12388, loss 3.9579e-06, acc 1\n",
      "2018-10-26T17:04:56.453126: step 12389, loss 0.000941585, acc 1\n",
      "2018-10-26T17:04:56.827002: step 12390, loss 8.87258e-05, acc 1\n",
      "2018-10-26T17:04:57.178057: step 12391, loss 6.15087e-05, acc 1\n",
      "2018-10-26T17:04:57.578981: step 12392, loss 0.00109555, acc 1\n",
      "2018-10-26T17:04:57.957972: step 12393, loss 0.000114603, acc 1\n",
      "2018-10-26T17:04:58.294075: step 12394, loss 0.0001018, acc 1\n",
      "2018-10-26T17:04:58.665079: step 12395, loss 4.76833e-07, acc 1\n",
      "2018-10-26T17:04:59.014148: step 12396, loss 0.0016969, acc 1\n",
      "2018-10-26T17:04:59.355237: step 12397, loss 0.000190094, acc 1\n",
      "2018-10-26T17:04:59.759157: step 12398, loss 1.62548e-05, acc 1\n",
      "2018-10-26T17:05:00.117203: step 12399, loss 0.000107921, acc 1\n",
      "2018-10-26T17:05:00.452303: step 12400, loss 3.68765e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:05:01.255158: step 12400, loss 4.46566, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12400\n",
      "\n",
      "2018-10-26T17:05:01.896449: step 12401, loss 0.00233945, acc 1\n",
      "2018-10-26T17:05:02.237538: step 12402, loss 8.04647e-07, acc 1\n",
      "2018-10-26T17:05:02.730335: step 12403, loss 1.17551e-05, acc 1\n",
      "2018-10-26T17:05:03.068339: step 12404, loss 6.25936e-06, acc 1\n",
      "2018-10-26T17:05:03.414393: step 12405, loss 3.00987e-06, acc 1\n",
      "2018-10-26T17:05:03.763459: step 12406, loss 0.0896182, acc 0.96875\n",
      "2018-10-26T17:05:04.142445: step 12407, loss 2.44005e-07, acc 1\n",
      "2018-10-26T17:05:04.476553: step 12408, loss 0.0241521, acc 0.984375\n",
      "2018-10-26T17:05:04.800686: step 12409, loss 2.44642e-05, acc 1\n",
      "2018-10-26T17:05:05.150753: step 12410, loss 2.61665e-05, acc 1\n",
      "2018-10-26T17:05:05.481865: step 12411, loss 6.2025e-07, acc 1\n",
      "2018-10-26T17:05:05.816970: step 12412, loss 0.000220255, acc 1\n",
      "2018-10-26T17:05:06.145095: step 12413, loss 4.39956e-05, acc 1\n",
      "2018-10-26T17:05:06.486185: step 12414, loss 6.36811e-05, acc 1\n",
      "2018-10-26T17:05:06.806333: step 12415, loss 0.000394383, acc 1\n",
      "2018-10-26T17:05:07.151413: step 12416, loss 4.17227e-07, acc 1\n",
      "2018-10-26T17:05:07.483517: step 12417, loss 2.00589e-05, acc 1\n",
      "2018-10-26T17:05:07.837572: step 12418, loss 1.02072e-06, acc 1\n",
      "2018-10-26T17:05:08.197614: step 12419, loss 5.26091e-05, acc 1\n",
      "2018-10-26T17:05:08.542688: step 12420, loss 0.000307128, acc 1\n",
      "2018-10-26T17:05:08.924668: step 12421, loss 7.91985e-05, acc 1\n",
      "2018-10-26T17:05:09.324600: step 12422, loss 0.00698598, acc 1\n",
      "2018-10-26T17:05:09.689626: step 12423, loss 0.148573, acc 0.984375\n",
      "2018-10-26T17:05:10.098533: step 12424, loss 0.00206321, acc 1\n",
      "2018-10-26T17:05:10.443610: step 12425, loss 2.89817e-06, acc 1\n",
      "2018-10-26T17:05:10.801761: step 12426, loss 0.000215205, acc 1\n",
      "2018-10-26T17:05:11.148725: step 12427, loss 2.31701e-06, acc 1\n",
      "2018-10-26T17:05:11.574586: step 12428, loss 0.00924427, acc 1\n",
      "2018-10-26T17:05:11.938614: step 12429, loss 0.00142718, acc 1\n",
      "2018-10-26T17:05:12.269729: step 12430, loss 0.00013392, acc 1\n",
      "2018-10-26T17:05:12.608825: step 12431, loss 0.000360751, acc 1\n",
      "2018-10-26T17:05:12.960916: step 12432, loss 2.15082e-05, acc 1\n",
      "2018-10-26T17:05:13.316933: step 12433, loss 1.23836e-05, acc 1\n",
      "2018-10-26T17:05:13.650044: step 12434, loss 0.000100003, acc 1\n",
      "2018-10-26T17:05:13.996119: step 12435, loss 7.94801e-06, acc 1\n",
      "2018-10-26T17:05:14.337206: step 12436, loss 0.000161468, acc 1\n",
      "2018-10-26T17:05:14.699238: step 12437, loss 0.00150543, acc 1\n",
      "2018-10-26T17:05:15.045317: step 12438, loss 1.50869e-06, acc 1\n",
      "2018-10-26T17:05:15.402361: step 12439, loss 3.8929e-07, acc 1\n",
      "2018-10-26T17:05:15.757412: step 12440, loss 0.000188487, acc 1\n",
      "2018-10-26T17:05:16.086535: step 12441, loss 6.61703e-05, acc 1\n",
      "2018-10-26T17:05:16.427622: step 12442, loss 4.00625e-06, acc 1\n",
      "2018-10-26T17:05:16.791689: step 12443, loss 7.3859e-06, acc 1\n",
      "2018-10-26T17:05:17.142710: step 12444, loss 7.69258e-07, acc 1\n",
      "2018-10-26T17:05:17.492775: step 12445, loss 2.41304e-05, acc 1\n",
      "2018-10-26T17:05:17.867804: step 12446, loss 1.42128e-05, acc 1\n",
      "2018-10-26T17:05:18.204875: step 12447, loss 1.6205e-07, acc 1\n",
      "2018-10-26T17:05:18.560957: step 12448, loss 0.000231046, acc 1\n",
      "2018-10-26T17:05:18.994850: step 12449, loss 0.000856822, acc 1\n",
      "2018-10-26T17:05:19.367805: step 12450, loss 4.19197e-06, acc 1\n",
      "2018-10-26T17:05:19.786649: step 12451, loss 0.00054503, acc 1\n",
      "2018-10-26T17:05:20.185579: step 12452, loss 0.00132659, acc 1\n",
      "2018-10-26T17:05:20.568559: step 12453, loss 4.4362e-06, acc 1\n",
      "2018-10-26T17:05:21.026333: step 12454, loss 0.000381195, acc 1\n",
      "2018-10-26T17:05:21.404323: step 12455, loss 6.80576e-05, acc 1\n",
      "2018-10-26T17:05:21.745416: step 12456, loss 3.1196e-05, acc 1\n",
      "2018-10-26T17:05:22.221140: step 12457, loss 3.79391e-06, acc 1\n",
      "2018-10-26T17:05:22.682907: step 12458, loss 5.46959e-05, acc 1\n",
      "2018-10-26T17:05:23.167631: step 12459, loss 4.59471e-06, acc 1\n",
      "2018-10-26T17:05:23.633593: step 12460, loss 7.80299e-06, acc 1\n",
      "2018-10-26T17:05:24.072269: step 12461, loss 0.000140627, acc 1\n",
      "2018-10-26T17:05:24.490078: step 12462, loss 5.84247e-06, acc 1\n",
      "2018-10-26T17:05:24.906966: step 12463, loss 0.0318769, acc 0.96875\n",
      "2018-10-26T17:05:25.333860: step 12464, loss 0.000702127, acc 1\n",
      "2018-10-26T17:05:25.724779: step 12465, loss 4.23535e-05, acc 1\n",
      "2018-10-26T17:05:26.148648: step 12466, loss 9.22824e-05, acc 1\n",
      "2018-10-26T17:05:26.534616: step 12467, loss 4.76692e-05, acc 1\n",
      "2018-10-26T17:05:26.917593: step 12468, loss 1.67378e-05, acc 1\n",
      "2018-10-26T17:05:27.277671: step 12469, loss 5.50749e-05, acc 1\n",
      "2018-10-26T17:05:27.618757: step 12470, loss 1.23679e-06, acc 1\n",
      "2018-10-26T17:05:27.961878: step 12471, loss 0.0011128, acc 1\n",
      "2018-10-26T17:05:28.314861: step 12472, loss 1.51645e-05, acc 1\n",
      "2018-10-26T17:05:28.656949: step 12473, loss 1.81179e-05, acc 1\n",
      "2018-10-26T17:05:28.999031: step 12474, loss 4.71909e-05, acc 1\n",
      "2018-10-26T17:05:29.347105: step 12475, loss 1.03899e-05, acc 1\n",
      "2018-10-26T17:05:29.690188: step 12476, loss 0.000138717, acc 1\n",
      "2018-10-26T17:05:30.059218: step 12477, loss 0.0020979, acc 1\n",
      "2018-10-26T17:05:30.406273: step 12478, loss 0.000244702, acc 1\n",
      "2018-10-26T17:05:30.747361: step 12479, loss 0.00133088, acc 1\n",
      "2018-10-26T17:05:31.076481: step 12480, loss 2.73808e-07, acc 1\n",
      "2018-10-26T17:05:31.407600: step 12481, loss 0.000310365, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:05:31.745693: step 12482, loss 4.45483e-06, acc 1\n",
      "2018-10-26T17:05:32.066836: step 12483, loss 7.13863e-05, acc 1\n",
      "2018-10-26T17:05:32.406926: step 12484, loss 2.64814e-05, acc 1\n",
      "2018-10-26T17:05:32.727070: step 12485, loss 0.0147586, acc 0.984375\n",
      "2018-10-26T17:05:33.057190: step 12486, loss 0.000192075, acc 1\n",
      "2018-10-26T17:05:33.379437: step 12487, loss 0.000114301, acc 1\n",
      "2018-10-26T17:05:33.714434: step 12488, loss 0.000284555, acc 1\n",
      "2018-10-26T17:05:34.056518: step 12489, loss 2.51696e-05, acc 1\n",
      "2018-10-26T17:05:34.388633: step 12490, loss 0.000145538, acc 1\n",
      "2018-10-26T17:05:34.735704: step 12491, loss 9.58437e-05, acc 1\n",
      "2018-10-26T17:05:35.108710: step 12492, loss 0.000291456, acc 1\n",
      "2018-10-26T17:05:35.450796: step 12493, loss 2.90284e-05, acc 1\n",
      "2018-10-26T17:05:35.805845: step 12494, loss 1.00583e-07, acc 1\n",
      "2018-10-26T17:05:36.135963: step 12495, loss 8.51476e-05, acc 1\n",
      "2018-10-26T17:05:36.465088: step 12496, loss 4.5726e-06, acc 1\n",
      "2018-10-26T17:05:36.798194: step 12497, loss 1.76389e-06, acc 1\n",
      "2018-10-26T17:05:37.167208: step 12498, loss 0.000435604, acc 1\n",
      "2018-10-26T17:05:37.484364: step 12499, loss 4.55211e-06, acc 1\n",
      "2018-10-26T17:05:37.823490: step 12500, loss 3.68033e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:05:38.609354: step 12500, loss 4.41826, acc 0.726079\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12500\n",
      "\n",
      "2018-10-26T17:05:39.241664: step 12501, loss 0.000114996, acc 1\n",
      "2018-10-26T17:05:39.634615: step 12502, loss 0.00187199, acc 1\n",
      "2018-10-26T17:05:40.088507: step 12503, loss 1.45525e-05, acc 1\n",
      "2018-10-26T17:05:40.412535: step 12504, loss 0.000226961, acc 1\n",
      "2018-10-26T17:05:40.749635: step 12505, loss 0.000436153, acc 1\n",
      "2018-10-26T17:05:41.067787: step 12506, loss 0.0015812, acc 1\n",
      "2018-10-26T17:05:41.390988: step 12507, loss 0.00903761, acc 1\n",
      "2018-10-26T17:05:41.725183: step 12508, loss 0.000695272, acc 1\n",
      "2018-10-26T17:05:42.076091: step 12509, loss 5.6039e-05, acc 1\n",
      "2018-10-26T17:05:42.414187: step 12510, loss 1.84554e-05, acc 1\n",
      "2018-10-26T17:05:42.737329: step 12511, loss 0.0120782, acc 0.984375\n",
      "2018-10-26T17:05:43.080408: step 12512, loss 1.5124e-06, acc 1\n",
      "2018-10-26T17:05:43.406537: step 12513, loss 6.12952e-06, acc 1\n",
      "2018-10-26T17:05:43.754611: step 12514, loss 0.000135182, acc 1\n",
      "2018-10-26T17:05:44.101723: step 12515, loss 6.9472e-06, acc 1\n",
      "2018-10-26T17:05:44.441814: step 12516, loss 6.04874e-05, acc 1\n",
      "2018-10-26T17:05:44.800892: step 12517, loss 7.14784e-05, acc 1\n",
      "2018-10-26T17:05:45.144895: step 12518, loss 0.000144125, acc 1\n",
      "2018-10-26T17:05:45.496955: step 12519, loss 2.52751e-06, acc 1\n",
      "2018-10-26T17:05:45.870023: step 12520, loss 0.000112878, acc 1\n",
      "2018-10-26T17:05:46.214035: step 12521, loss 5.32891e-05, acc 1\n",
      "2018-10-26T17:05:46.553131: step 12522, loss 2.89695e-05, acc 1\n",
      "2018-10-26T17:05:46.898210: step 12523, loss 8.33793e-06, acc 1\n",
      "2018-10-26T17:05:47.247276: step 12524, loss 1.91137e-05, acc 1\n",
      "2018-10-26T17:05:47.580385: step 12525, loss 8.13968e-07, acc 1\n",
      "2018-10-26T17:05:47.896541: step 12526, loss 6.34307e-06, acc 1\n",
      "2018-10-26T17:05:48.230651: step 12527, loss 0.00099632, acc 1\n",
      "2018-10-26T17:05:48.564758: step 12528, loss 0.000120288, acc 1\n",
      "2018-10-26T17:05:48.915818: step 12529, loss 0.00630442, acc 1\n",
      "2018-10-26T17:05:49.252916: step 12530, loss 0.000166704, acc 1\n",
      "2018-10-26T17:05:49.583036: step 12531, loss 8.85579e-05, acc 1\n",
      "2018-10-26T17:05:49.926119: step 12532, loss 1.39075e-05, acc 1\n",
      "2018-10-26T17:05:50.285159: step 12533, loss 0.000372827, acc 1\n",
      "2018-10-26T17:05:50.598321: step 12534, loss 1.09894e-06, acc 1\n",
      "2018-10-26T17:05:50.933428: step 12535, loss 3.07313e-06, acc 1\n",
      "2018-10-26T17:05:51.269528: step 12536, loss 0.000505065, acc 1\n",
      "2018-10-26T17:05:51.640540: step 12537, loss 5.19648e-06, acc 1\n",
      "2018-10-26T17:05:51.976638: step 12538, loss 0.018996, acc 0.984375\n",
      "2018-10-26T17:05:52.316729: step 12539, loss 0.000132326, acc 1\n",
      "2018-10-26T17:05:52.669790: step 12540, loss 5.02909e-07, acc 1\n",
      "2018-10-26T17:05:53.016861: step 12541, loss 7.24036e-05, acc 1\n",
      "2018-10-26T17:05:53.352962: step 12542, loss 3.64133e-06, acc 1\n",
      "2018-10-26T17:05:53.710007: step 12543, loss 9.47457e-06, acc 1\n",
      "2018-10-26T17:05:54.057079: step 12544, loss 0.000296303, acc 1\n",
      "2018-10-26T17:05:54.374234: step 12545, loss 3.93744e-06, acc 1\n",
      "2018-10-26T17:05:54.700366: step 12546, loss 1.42899e-05, acc 1\n",
      "2018-10-26T17:05:55.035513: step 12547, loss 8.69253e-05, acc 1\n",
      "2018-10-26T17:05:55.372564: step 12548, loss 7.84423e-05, acc 1\n",
      "2018-10-26T17:05:55.713654: step 12549, loss 0.0138324, acc 0.984375\n",
      "2018-10-26T17:05:56.071749: step 12550, loss 4.38602e-06, acc 1\n",
      "2018-10-26T17:05:56.421899: step 12551, loss 0.000181752, acc 1\n",
      "2018-10-26T17:05:56.742903: step 12552, loss 9.46632e-06, acc 1\n",
      "2018-10-26T17:05:57.105934: step 12553, loss 2.81434e-06, acc 1\n",
      "2018-10-26T17:05:57.450015: step 12554, loss 4.50581e-05, acc 1\n",
      "2018-10-26T17:05:57.803102: step 12555, loss 1.72658e-06, acc 1\n",
      "2018-10-26T17:05:58.143164: step 12556, loss 1.03748e-06, acc 1\n",
      "2018-10-26T17:05:58.481263: step 12557, loss 0.00690186, acc 1\n",
      "2018-10-26T17:05:58.818359: step 12558, loss 0.000534455, acc 1\n",
      "2018-10-26T17:05:59.167426: step 12559, loss 0.00014205, acc 1\n",
      "2018-10-26T17:05:59.500566: step 12560, loss 2.14203e-07, acc 1\n",
      "2018-10-26T17:05:59.840627: step 12561, loss 6.8358e-07, acc 1\n",
      "2018-10-26T17:06:00.229626: step 12562, loss 3.08135e-05, acc 1\n",
      "2018-10-26T17:06:00.565717: step 12563, loss 0.106013, acc 0.984375\n",
      "2018-10-26T17:06:00.920815: step 12564, loss 1.302e-05, acc 1\n",
      "2018-10-26T17:06:01.256846: step 12565, loss 0.000453336, acc 1\n",
      "2018-10-26T17:06:01.602918: step 12566, loss 1.18462e-06, acc 1\n",
      "2018-10-26T17:06:01.936029: step 12567, loss 9.45304e-06, acc 1\n",
      "2018-10-26T17:06:02.278118: step 12568, loss 1.02629e-06, acc 1\n",
      "2018-10-26T17:06:02.610228: step 12569, loss 9.6856e-07, acc 1\n",
      "2018-10-26T17:06:02.931370: step 12570, loss 5.0477e-07, acc 1\n",
      "2018-10-26T17:06:03.255507: step 12571, loss 5.50449e-05, acc 1\n",
      "2018-10-26T17:06:03.595594: step 12572, loss 5.88593e-07, acc 1\n",
      "2018-10-26T17:06:03.952688: step 12573, loss 0.00028467, acc 1\n",
      "2018-10-26T17:06:04.286749: step 12574, loss 1.9725e-06, acc 1\n",
      "2018-10-26T17:06:04.642797: step 12575, loss 8.40057e-05, acc 1\n",
      "2018-10-26T17:06:04.952968: step 12576, loss 2.98022e-07, acc 1\n",
      "2018-10-26T17:06:05.299046: step 12577, loss 1.19542e-05, acc 1\n",
      "2018-10-26T17:06:05.640133: step 12578, loss 0.00519105, acc 1\n",
      "2018-10-26T17:06:05.979230: step 12579, loss 5.06376e-06, acc 1\n",
      "2018-10-26T17:06:06.319321: step 12580, loss 0.00732508, acc 1\n",
      "2018-10-26T17:06:06.705287: step 12581, loss 8.44199e-05, acc 1\n",
      "2018-10-26T17:06:07.070310: step 12582, loss 7.63684e-08, acc 1\n",
      "2018-10-26T17:06:07.409404: step 12583, loss 2.39709e-06, acc 1\n",
      "2018-10-26T17:06:07.746504: step 12584, loss 1.03731e-05, acc 1\n",
      "2018-10-26T17:06:08.097590: step 12585, loss 7.38023e-06, acc 1\n",
      "2018-10-26T17:06:08.430677: step 12586, loss 2.2673e-05, acc 1\n",
      "2018-10-26T17:06:08.771765: step 12587, loss 0.00281862, acc 1\n",
      "2018-10-26T17:06:09.112855: step 12588, loss 6.03453e-05, acc 1\n",
      "2018-10-26T17:06:09.443969: step 12589, loss 5.57404e-06, acc 1\n",
      "2018-10-26T17:06:09.797028: step 12590, loss 0.000202248, acc 1\n",
      "2018-10-26T17:06:10.138115: step 12591, loss 5.38178e-05, acc 1\n",
      "2018-10-26T17:06:10.515111: step 12592, loss 0.00299775, acc 1\n",
      "2018-10-26T17:06:10.871155: step 12593, loss 5.6585e-05, acc 1\n",
      "2018-10-26T17:06:11.210251: step 12594, loss 5.62331e-05, acc 1\n",
      "2018-10-26T17:06:11.536382: step 12595, loss 0.00144947, acc 1\n",
      "2018-10-26T17:06:11.866498: step 12596, loss 0.0430104, acc 0.984375\n",
      "2018-10-26T17:06:12.216562: step 12597, loss 0.000581007, acc 1\n",
      "2018-10-26T17:06:12.562637: step 12598, loss 0.00566595, acc 1\n",
      "2018-10-26T17:06:12.918685: step 12599, loss 7.40877e-06, acc 1\n",
      "2018-10-26T17:06:13.242817: step 12600, loss 1.24951e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:06:14.041746: step 12600, loss 4.4221, acc 0.726079\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12600\n",
      "\n",
      "2018-10-26T17:06:14.656044: step 12601, loss 0.00142527, acc 1\n",
      "2018-10-26T17:06:15.000123: step 12602, loss 0.0131995, acc 0.984375\n",
      "2018-10-26T17:06:15.448941: step 12603, loss 4.38107e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:06:15.837886: step 12604, loss 0.000156629, acc 1\n",
      "2018-10-26T17:06:16.195929: step 12605, loss 1.65866e-05, acc 1\n",
      "2018-10-26T17:06:16.528043: step 12606, loss 0.000218314, acc 1\n",
      "2018-10-26T17:06:16.863182: step 12607, loss 2.48995e-05, acc 1\n",
      "2018-10-26T17:06:17.238145: step 12608, loss 1.71544e-06, acc 1\n",
      "2018-10-26T17:06:17.556293: step 12609, loss 2.99884e-07, acc 1\n",
      "2018-10-26T17:06:17.885415: step 12610, loss 1.50123e-06, acc 1\n",
      "2018-10-26T17:06:18.208629: step 12611, loss 4.84287e-08, acc 1\n",
      "2018-10-26T17:06:18.531732: step 12612, loss 6.15278e-05, acc 1\n",
      "2018-10-26T17:06:18.847842: step 12613, loss 0.000353084, acc 1\n",
      "2018-10-26T17:06:19.199902: step 12614, loss 0.000128915, acc 1\n",
      "2018-10-26T17:06:19.535011: step 12615, loss 3.59078e-06, acc 1\n",
      "2018-10-26T17:06:19.871152: step 12616, loss 1.63925e-05, acc 1\n",
      "2018-10-26T17:06:20.202227: step 12617, loss 3.01747e-07, acc 1\n",
      "2018-10-26T17:06:20.526486: step 12618, loss 0.000698329, acc 1\n",
      "2018-10-26T17:06:20.853518: step 12619, loss 2.00994e-05, acc 1\n",
      "2018-10-26T17:06:21.232474: step 12620, loss 0.000949392, acc 1\n",
      "2018-10-26T17:06:21.649405: step 12621, loss 3.17378e-06, acc 1\n",
      "2018-10-26T17:06:21.988528: step 12622, loss 1.42564e-05, acc 1\n",
      "2018-10-26T17:06:22.336527: step 12623, loss 9.30561e-06, acc 1\n",
      "2018-10-26T17:06:22.661687: step 12624, loss 2.82051e-05, acc 1\n",
      "2018-10-26T17:06:22.985788: step 12625, loss 0.052229, acc 0.984375\n",
      "2018-10-26T17:06:23.328870: step 12626, loss 8.73575e-07, acc 1\n",
      "2018-10-26T17:06:23.742767: step 12627, loss 0.000117946, acc 1\n",
      "2018-10-26T17:06:24.063906: step 12628, loss 0.000766009, acc 1\n",
      "2018-10-26T17:06:24.467865: step 12629, loss 0.000373759, acc 1\n",
      "2018-10-26T17:06:24.853797: step 12630, loss 1.7881e-06, acc 1\n",
      "2018-10-26T17:06:25.228796: step 12631, loss 8.17691e-07, acc 1\n",
      "2018-10-26T17:06:25.638699: step 12632, loss 1.26471e-06, acc 1\n",
      "2018-10-26T17:06:26.028659: step 12633, loss 4.74971e-07, acc 1\n",
      "2018-10-26T17:06:26.402703: step 12634, loss 6.9476e-07, acc 1\n",
      "2018-10-26T17:06:26.761748: step 12635, loss 0.0250328, acc 0.984375\n",
      "2018-10-26T17:06:27.131709: step 12636, loss 0.00404205, acc 1\n",
      "2018-10-26T17:06:27.503715: step 12637, loss 3.22034e-06, acc 1\n",
      "2018-10-26T17:06:27.856772: step 12638, loss 8.56816e-08, acc 1\n",
      "2018-10-26T17:06:28.325520: step 12639, loss 0.0372307, acc 0.984375\n",
      "2018-10-26T17:06:28.746466: step 12640, loss 3.82555e-05, acc 1\n",
      "2018-10-26T17:06:29.176247: step 12641, loss 7.43098e-05, acc 1\n",
      "2018-10-26T17:06:29.639115: step 12642, loss 2.40523e-05, acc 1\n",
      "2018-10-26T17:06:30.075843: step 12643, loss 0.000181651, acc 1\n",
      "2018-10-26T17:06:30.502702: step 12644, loss 0.000119174, acc 1\n",
      "2018-10-26T17:06:30.953695: step 12645, loss 3.89643e-05, acc 1\n",
      "2018-10-26T17:06:31.394321: step 12646, loss 3.46451e-07, acc 1\n",
      "2018-10-26T17:06:31.842123: step 12647, loss 6.83808e-05, acc 1\n",
      "2018-10-26T17:06:32.275965: step 12648, loss 0.000718563, acc 1\n",
      "2018-10-26T17:06:32.647972: step 12649, loss 7.86391e-05, acc 1\n",
      "2018-10-26T17:06:33.062862: step 12650, loss 2.78638e-06, acc 1\n",
      "2018-10-26T17:06:33.392054: step 12651, loss 1.08031e-06, acc 1\n",
      "2018-10-26T17:06:33.728089: step 12652, loss 0.00299369, acc 1\n",
      "2018-10-26T17:06:34.073163: step 12653, loss 7.83691e-06, acc 1\n",
      "2018-10-26T17:06:34.417244: step 12654, loss 0.000745569, acc 1\n",
      "2018-10-26T17:06:34.739383: step 12655, loss 3.43653e-05, acc 1\n",
      "2018-10-26T17:06:35.080475: step 12656, loss 1.20623e-05, acc 1\n",
      "2018-10-26T17:06:35.418569: step 12657, loss 0.000281062, acc 1\n",
      "2018-10-26T17:06:35.761654: step 12658, loss 0.000913105, acc 1\n",
      "2018-10-26T17:06:36.099748: step 12659, loss 1.38642e-05, acc 1\n",
      "2018-10-26T17:06:36.439843: step 12660, loss 0.00139614, acc 1\n",
      "2018-10-26T17:06:36.787948: step 12661, loss 5.21774e-05, acc 1\n",
      "2018-10-26T17:06:37.196867: step 12662, loss 1.27819e-05, acc 1\n",
      "2018-10-26T17:06:37.596749: step 12663, loss 3.11038e-06, acc 1\n",
      "2018-10-26T17:06:37.971747: step 12664, loss 0.010136, acc 1\n",
      "2018-10-26T17:06:38.309844: step 12665, loss 2.39297e-05, acc 1\n",
      "2018-10-26T17:06:38.647941: step 12666, loss 0.000224318, acc 1\n",
      "2018-10-26T17:06:38.995014: step 12667, loss 0.00023906, acc 1\n",
      "2018-10-26T17:06:39.327129: step 12668, loss 0.00081826, acc 1\n",
      "2018-10-26T17:06:39.704118: step 12669, loss 1.35241e-05, acc 1\n",
      "2018-10-26T17:06:40.055184: step 12670, loss 0.000210543, acc 1\n",
      "2018-10-26T17:06:40.399263: step 12671, loss 0.000119693, acc 1\n",
      "2018-10-26T17:06:40.744339: step 12672, loss 0.000164552, acc 1\n",
      "2018-10-26T17:06:41.070467: step 12673, loss 4.70454e-06, acc 1\n",
      "2018-10-26T17:06:41.405573: step 12674, loss 0.000247187, acc 1\n",
      "2018-10-26T17:06:41.740713: step 12675, loss 0.0013342, acc 1\n",
      "2018-10-26T17:06:42.089747: step 12676, loss 9.31317e-07, acc 1\n",
      "2018-10-26T17:06:42.439851: step 12677, loss 5.96176e-06, acc 1\n",
      "2018-10-26T17:06:42.777907: step 12678, loss 0.000187008, acc 1\n",
      "2018-10-26T17:06:43.129965: step 12679, loss 1.87272e-05, acc 1\n",
      "2018-10-26T17:06:43.487012: step 12680, loss 0.0133134, acc 0.984375\n",
      "2018-10-26T17:06:43.831094: step 12681, loss 5.49471e-07, acc 1\n",
      "2018-10-26T17:06:44.160212: step 12682, loss 0.0243195, acc 0.984375\n",
      "2018-10-26T17:06:44.518532: step 12683, loss 1.59261e-05, acc 1\n",
      "2018-10-26T17:06:44.883280: step 12684, loss 0.00028765, acc 1\n",
      "2018-10-26T17:06:45.230357: step 12685, loss 4.97157e-05, acc 1\n",
      "2018-10-26T17:06:45.580481: step 12686, loss 2.5e-05, acc 1\n",
      "2018-10-26T17:06:45.908543: step 12687, loss 1.99302e-07, acc 1\n",
      "2018-10-26T17:06:46.243646: step 12688, loss 8.48779e-05, acc 1\n",
      "2018-10-26T17:06:46.579751: step 12689, loss 0.00478857, acc 1\n",
      "2018-10-26T17:06:46.931808: step 12690, loss 1.11199e-06, acc 1\n",
      "2018-10-26T17:06:47.285864: step 12691, loss 0.000288555, acc 1\n",
      "2018-10-26T17:06:47.622963: step 12692, loss 5.79054e-06, acc 1\n",
      "2018-10-26T17:06:47.982999: step 12693, loss 7.30255e-05, acc 1\n",
      "2018-10-26T17:06:48.349022: step 12694, loss 2.91484e-06, acc 1\n",
      "2018-10-26T17:06:48.671268: step 12695, loss 5.09813e-05, acc 1\n",
      "2018-10-26T17:06:49.011295: step 12696, loss 0.00529807, acc 1\n",
      "2018-10-26T17:06:49.363519: step 12697, loss 5.19981e-06, acc 1\n",
      "2018-10-26T17:06:49.696423: step 12698, loss 0.000138179, acc 1\n",
      "2018-10-26T17:06:50.054466: step 12699, loss 1.24796e-05, acc 1\n",
      "2018-10-26T17:06:50.400538: step 12700, loss 1.53333e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:06:51.197409: step 12700, loss 4.44381, acc 0.727955\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12700\n",
      "\n",
      "2018-10-26T17:06:51.817755: step 12701, loss 0.000168621, acc 1\n",
      "2018-10-26T17:06:52.165823: step 12702, loss 3.45421e-05, acc 1\n",
      "2018-10-26T17:06:52.622602: step 12703, loss 0.000136694, acc 1\n",
      "2018-10-26T17:06:52.981696: step 12704, loss 1.09052e-05, acc 1\n",
      "2018-10-26T17:06:53.319738: step 12705, loss 0.000257698, acc 1\n",
      "2018-10-26T17:06:53.673832: step 12706, loss 1.32248e-07, acc 1\n",
      "2018-10-26T17:06:54.006905: step 12707, loss 3.52582e-05, acc 1\n",
      "2018-10-26T17:06:54.343006: step 12708, loss 3.00207e-05, acc 1\n",
      "2018-10-26T17:06:54.675119: step 12709, loss 0.00011668, acc 1\n",
      "2018-10-26T17:06:55.022226: step 12710, loss 2.95313e-05, acc 1\n",
      "2018-10-26T17:06:55.359289: step 12711, loss 1.17059e-05, acc 1\n",
      "2018-10-26T17:06:55.701380: step 12712, loss 1.51276e-05, acc 1\n",
      "2018-10-26T17:06:56.044460: step 12713, loss 1.82902e-06, acc 1\n",
      "2018-10-26T17:06:56.369761: step 12714, loss 8.67984e-07, acc 1\n",
      "2018-10-26T17:06:56.722649: step 12715, loss 5.2154e-08, acc 1\n",
      "2018-10-26T17:06:57.072747: step 12716, loss 3.58711e-06, acc 1\n",
      "2018-10-26T17:06:57.416796: step 12717, loss 3.56385e-05, acc 1\n",
      "2018-10-26T17:06:57.746996: step 12718, loss 2.00228e-06, acc 1\n",
      "2018-10-26T17:06:58.083013: step 12719, loss 1.33625e-05, acc 1\n",
      "2018-10-26T17:06:58.422106: step 12720, loss 1.74895e-06, acc 1\n",
      "2018-10-26T17:06:58.758208: step 12721, loss 2.66155e-06, acc 1\n",
      "2018-10-26T17:06:59.114260: step 12722, loss 1.45073e-05, acc 1\n",
      "2018-10-26T17:06:59.458338: step 12723, loss 0.000146771, acc 1\n",
      "2018-10-26T17:06:59.832342: step 12724, loss 1.01038e-05, acc 1\n",
      "2018-10-26T17:07:00.184401: step 12725, loss 0.000212004, acc 1\n",
      "2018-10-26T17:07:00.531471: step 12726, loss 4.02682e-06, acc 1\n",
      "2018-10-26T17:07:00.884527: step 12727, loss 0.000524923, acc 1\n",
      "2018-10-26T17:07:01.234699: step 12728, loss 1.39325e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:07:01.571692: step 12729, loss 0.000348851, acc 1\n",
      "2018-10-26T17:07:01.906797: step 12730, loss 1.18786e-05, acc 1\n",
      "2018-10-26T17:07:02.237914: step 12731, loss 2.8665e-06, acc 1\n",
      "2018-10-26T17:07:02.578004: step 12732, loss 0.000384209, acc 1\n",
      "2018-10-26T17:07:02.920091: step 12733, loss 3.67289e-06, acc 1\n",
      "2018-10-26T17:07:03.260181: step 12734, loss 2.78452e-06, acc 1\n",
      "2018-10-26T17:07:03.596285: step 12735, loss 4.15637e-05, acc 1\n",
      "2018-10-26T17:07:03.936374: step 12736, loss 0.000142717, acc 1\n",
      "2018-10-26T17:07:04.303394: step 12737, loss 7.99066e-07, acc 1\n",
      "2018-10-26T17:07:04.630520: step 12738, loss 3.14188e-05, acc 1\n",
      "2018-10-26T17:07:04.967617: step 12739, loss 0.0975377, acc 0.984375\n",
      "2018-10-26T17:07:05.301726: step 12740, loss 0.000516392, acc 1\n",
      "2018-10-26T17:07:05.627857: step 12741, loss 0.00306938, acc 1\n",
      "2018-10-26T17:07:05.977955: step 12742, loss 0.00173164, acc 1\n",
      "2018-10-26T17:07:06.324990: step 12743, loss 8.93268e-06, acc 1\n",
      "2018-10-26T17:07:06.654111: step 12744, loss 0.000139916, acc 1\n",
      "2018-10-26T17:07:06.977248: step 12745, loss 2.43061e-06, acc 1\n",
      "2018-10-26T17:07:07.319400: step 12746, loss 0.000188204, acc 1\n",
      "2018-10-26T17:07:07.659425: step 12747, loss 2.79395e-07, acc 1\n",
      "2018-10-26T17:07:07.999517: step 12748, loss 0.000145488, acc 1\n",
      "2018-10-26T17:07:08.352575: step 12749, loss 0.00137454, acc 1\n",
      "2018-10-26T17:07:08.681778: step 12750, loss 0.000164413, acc 1\n",
      "2018-10-26T17:07:09.020818: step 12751, loss 0.00425638, acc 1\n",
      "2018-10-26T17:07:09.378834: step 12752, loss 2.63207e-05, acc 1\n",
      "2018-10-26T17:07:09.698980: step 12753, loss 2.7137e-06, acc 1\n",
      "2018-10-26T17:07:10.045055: step 12754, loss 5.80949e-05, acc 1\n",
      "2018-10-26T17:07:10.386141: step 12755, loss 4.63796e-07, acc 1\n",
      "2018-10-26T17:07:10.731219: step 12756, loss 4.15365e-07, acc 1\n",
      "2018-10-26T17:07:11.099270: step 12757, loss 0.000148522, acc 1\n",
      "2018-10-26T17:07:11.433344: step 12758, loss 2.8312e-07, acc 1\n",
      "2018-10-26T17:07:11.765455: step 12759, loss 2.58894e-05, acc 1\n",
      "2018-10-26T17:07:12.131508: step 12760, loss 3.2054e-06, acc 1\n",
      "2018-10-26T17:07:12.530469: step 12761, loss 4.71031e-06, acc 1\n",
      "2018-10-26T17:07:12.876486: step 12762, loss 3.05473e-07, acc 1\n",
      "2018-10-26T17:07:13.234529: step 12763, loss 1.1995e-06, acc 1\n",
      "2018-10-26T17:07:13.562698: step 12764, loss 0.00140313, acc 1\n",
      "2018-10-26T17:07:13.908731: step 12765, loss 6.73066e-05, acc 1\n",
      "2018-10-26T17:07:14.257797: step 12766, loss 1.93958e-05, acc 1\n",
      "2018-10-26T17:07:14.595073: step 12767, loss 0.000378287, acc 1\n",
      "2018-10-26T17:07:14.946958: step 12768, loss 0.000663348, acc 1\n",
      "2018-10-26T17:07:15.299015: step 12769, loss 0.00198261, acc 1\n",
      "2018-10-26T17:07:15.639109: step 12770, loss 1.06171e-07, acc 1\n",
      "2018-10-26T17:07:15.988176: step 12771, loss 0.000374184, acc 1\n",
      "2018-10-26T17:07:16.320355: step 12772, loss 0.000420957, acc 1\n",
      "2018-10-26T17:07:16.659378: step 12773, loss 1.82684e-05, acc 1\n",
      "2018-10-26T17:07:17.003462: step 12774, loss 0.00304222, acc 1\n",
      "2018-10-26T17:07:17.335706: step 12775, loss 0.00010084, acc 1\n",
      "2018-10-26T17:07:17.679653: step 12776, loss 8.88008e-06, acc 1\n",
      "2018-10-26T17:07:18.049665: step 12777, loss 0.000116589, acc 1\n",
      "2018-10-26T17:07:18.363827: step 12778, loss 0.00509974, acc 1\n",
      "2018-10-26T17:07:18.707969: step 12779, loss 2.8167e-05, acc 1\n",
      "2018-10-26T17:07:19.051991: step 12780, loss 1.5576e-05, acc 1\n",
      "2018-10-26T17:07:19.385098: step 12781, loss 6.50942e-06, acc 1\n",
      "2018-10-26T17:07:19.733170: step 12782, loss 3.43084e-06, acc 1\n",
      "2018-10-26T17:07:20.112157: step 12783, loss 2.14171e-05, acc 1\n",
      "2018-10-26T17:07:20.464212: step 12784, loss 7.2643e-08, acc 1\n",
      "2018-10-26T17:07:20.810288: step 12785, loss 3.39417e-05, acc 1\n",
      "2018-10-26T17:07:21.146393: step 12786, loss 0.00233249, acc 1\n",
      "2018-10-26T17:07:21.487482: step 12787, loss 0.00159954, acc 1\n",
      "2018-10-26T17:07:21.816599: step 12788, loss 0.021256, acc 0.984375\n",
      "2018-10-26T17:07:22.152701: step 12789, loss 8.3336e-05, acc 1\n",
      "2018-10-26T17:07:22.497156: step 12790, loss 0.00949996, acc 1\n",
      "2018-10-26T17:07:22.839022: step 12791, loss 6.15377e-06, acc 1\n",
      "2018-10-26T17:07:23.181021: step 12792, loss 2.67862e-05, acc 1\n",
      "2018-10-26T17:07:23.506087: step 12793, loss 2.98471e-05, acc 1\n",
      "2018-10-26T17:07:23.842188: step 12794, loss 0.0968761, acc 0.984375\n",
      "2018-10-26T17:07:24.202225: step 12795, loss 2.99299e-06, acc 1\n",
      "2018-10-26T17:07:24.546343: step 12796, loss 3.92445e-06, acc 1\n",
      "2018-10-26T17:07:24.892385: step 12797, loss 0.000422961, acc 1\n",
      "2018-10-26T17:07:25.257407: step 12798, loss 0.00037823, acc 1\n",
      "2018-10-26T17:07:25.592512: step 12799, loss 0.00011702, acc 1\n",
      "2018-10-26T17:07:25.925644: step 12800, loss 3.23341e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:07:26.739447: step 12800, loss 4.44371, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12800\n",
      "\n",
      "2018-10-26T17:07:27.370763: step 12801, loss 0.000108718, acc 1\n",
      "2018-10-26T17:07:27.708859: step 12802, loss 2.15668e-05, acc 1\n",
      "2018-10-26T17:07:28.172616: step 12803, loss 4.83703e-05, acc 1\n",
      "2018-10-26T17:07:28.509821: step 12804, loss 0.000113506, acc 1\n",
      "2018-10-26T17:07:28.869758: step 12805, loss 0.000669575, acc 1\n",
      "2018-10-26T17:07:29.235776: step 12806, loss 0.000187867, acc 1\n",
      "2018-10-26T17:07:29.596813: step 12807, loss 3.36568e-06, acc 1\n",
      "2018-10-26T17:07:29.931920: step 12808, loss 0.00109609, acc 1\n",
      "2018-10-26T17:07:30.353829: step 12809, loss 2.37286e-06, acc 1\n",
      "2018-10-26T17:07:30.723800: step 12810, loss 7.21036e-05, acc 1\n",
      "2018-10-26T17:07:31.188562: step 12811, loss 1.46284e-05, acc 1\n",
      "2018-10-26T17:07:31.592480: step 12812, loss 7.56094e-06, acc 1\n",
      "2018-10-26T17:07:31.952518: step 12813, loss 0.000746182, acc 1\n",
      "2018-10-26T17:07:32.333499: step 12814, loss 2.11958e-06, acc 1\n",
      "2018-10-26T17:07:32.728446: step 12815, loss 4.24804e-05, acc 1\n",
      "2018-10-26T17:07:33.071597: step 12816, loss 0.00200894, acc 1\n",
      "2018-10-26T17:07:33.498388: step 12817, loss 1.81663e-05, acc 1\n",
      "2018-10-26T17:07:33.949182: step 12818, loss 5.34576e-07, acc 1\n",
      "2018-10-26T17:07:34.436880: step 12819, loss 9.74575e-06, acc 1\n",
      "2018-10-26T17:07:34.898646: step 12820, loss 0.00131392, acc 1\n",
      "2018-10-26T17:07:35.316531: step 12821, loss 0.000490766, acc 1\n",
      "2018-10-26T17:07:35.734413: step 12822, loss 7.34344e-06, acc 1\n",
      "2018-10-26T17:07:36.158281: step 12823, loss 0.0001239, acc 1\n",
      "2018-10-26T17:07:36.588175: step 12824, loss 6.05428e-05, acc 1\n",
      "2018-10-26T17:07:37.024022: step 12825, loss 0.000684836, acc 1\n",
      "2018-10-26T17:07:37.442848: step 12826, loss 1.71733e-06, acc 1\n",
      "2018-10-26T17:07:37.868787: step 12827, loss 6.77993e-07, acc 1\n",
      "2018-10-26T17:07:38.279613: step 12828, loss 0.000586489, acc 1\n",
      "2018-10-26T17:07:38.607771: step 12829, loss 0.000132714, acc 1\n",
      "2018-10-26T17:07:38.998692: step 12830, loss 3.50894e-06, acc 1\n",
      "2018-10-26T17:07:39.328811: step 12831, loss 2.50329e-06, acc 1\n",
      "2018-10-26T17:07:39.697824: step 12832, loss 0.00168378, acc 1\n",
      "2018-10-26T17:07:40.041905: step 12833, loss 6.19117e-06, acc 1\n",
      "2018-10-26T17:07:40.380002: step 12834, loss 7.08401e-05, acc 1\n",
      "2018-10-26T17:07:40.731064: step 12835, loss 6.68307e-05, acc 1\n",
      "2018-10-26T17:07:41.077173: step 12836, loss 3.55763e-07, acc 1\n",
      "2018-10-26T17:07:41.401340: step 12837, loss 0.000164123, acc 1\n",
      "2018-10-26T17:07:41.750343: step 12838, loss 7.55126e-05, acc 1\n",
      "2018-10-26T17:07:42.095421: step 12839, loss 4.84159e-05, acc 1\n",
      "2018-10-26T17:07:42.435513: step 12840, loss 2.54115e-05, acc 1\n",
      "2018-10-26T17:07:42.768623: step 12841, loss 5.60547e-05, acc 1\n",
      "2018-10-26T17:07:43.112699: step 12842, loss 8.49971e-05, acc 1\n",
      "2018-10-26T17:07:43.461767: step 12843, loss 0.00732083, acc 1\n",
      "2018-10-26T17:07:43.800861: step 12844, loss 2.78824e-06, acc 1\n",
      "2018-10-26T17:07:44.124997: step 12845, loss 0.00025975, acc 1\n",
      "2018-10-26T17:07:44.453150: step 12846, loss 0.000129588, acc 1\n",
      "2018-10-26T17:07:44.785232: step 12847, loss 6.89352e-05, acc 1\n",
      "2018-10-26T17:07:45.118405: step 12848, loss 8.93633e-06, acc 1\n",
      "2018-10-26T17:07:45.459458: step 12849, loss 0.00178153, acc 1\n",
      "2018-10-26T17:07:45.807500: step 12850, loss 9.96555e-06, acc 1\n",
      "2018-10-26T17:07:46.145781: step 12851, loss 1.26563e-05, acc 1\n",
      "2018-10-26T17:07:46.489692: step 12852, loss 0.000167192, acc 1\n",
      "2018-10-26T17:07:46.813811: step 12853, loss 1.12875e-06, acc 1\n",
      "2018-10-26T17:07:47.150913: step 12854, loss 8.28516e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:07:47.498012: step 12855, loss 2.98126e-05, acc 1\n",
      "2018-10-26T17:07:47.829216: step 12856, loss 2.41018e-06, acc 1\n",
      "2018-10-26T17:07:48.161212: step 12857, loss 1.65465e-05, acc 1\n",
      "2018-10-26T17:07:48.485345: step 12858, loss 0.0868552, acc 0.984375\n",
      "2018-10-26T17:07:48.825439: step 12859, loss 3.9113e-06, acc 1\n",
      "2018-10-26T17:07:49.164585: step 12860, loss 5.58793e-09, acc 1\n",
      "2018-10-26T17:07:49.492653: step 12861, loss 4.27436e-06, acc 1\n",
      "2018-10-26T17:07:49.821774: step 12862, loss 0.0100588, acc 1\n",
      "2018-10-26T17:07:50.153886: step 12863, loss 0.000175891, acc 1\n",
      "2018-10-26T17:07:50.480016: step 12864, loss 9.77861e-07, acc 1\n",
      "2018-10-26T17:07:50.853021: step 12865, loss 0.000514741, acc 1\n",
      "2018-10-26T17:07:51.187155: step 12866, loss 0.0976629, acc 0.984375\n",
      "2018-10-26T17:07:51.523232: step 12867, loss 0.127221, acc 0.984375\n",
      "2018-10-26T17:07:51.889286: step 12868, loss 0.000267702, acc 1\n",
      "2018-10-26T17:07:52.225352: step 12869, loss 1.25977e-05, acc 1\n",
      "2018-10-26T17:07:52.583396: step 12870, loss 0.000110786, acc 1\n",
      "2018-10-26T17:07:52.923486: step 12871, loss 5.07983e-05, acc 1\n",
      "2018-10-26T17:07:53.252607: step 12872, loss 1.46773e-06, acc 1\n",
      "2018-10-26T17:07:53.581800: step 12873, loss 2.62246e-06, acc 1\n",
      "2018-10-26T17:07:53.914838: step 12874, loss 5.74376e-06, acc 1\n",
      "2018-10-26T17:07:54.247981: step 12875, loss 2.2906e-05, acc 1\n",
      "2018-10-26T17:07:54.567117: step 12876, loss 2.64044e-05, acc 1\n",
      "2018-10-26T17:07:54.884378: step 12877, loss 2.41386e-05, acc 1\n",
      "2018-10-26T17:07:55.222345: step 12878, loss 4.81239e-06, acc 1\n",
      "2018-10-26T17:07:55.560442: step 12879, loss 5.85252e-05, acc 1\n",
      "2018-10-26T17:07:55.897541: step 12880, loss 5.30846e-07, acc 1\n",
      "2018-10-26T17:07:56.213696: step 12881, loss 2.34365e-05, acc 1\n",
      "2018-10-26T17:07:56.563761: step 12882, loss 0.067903, acc 0.984375\n",
      "2018-10-26T17:07:56.883908: step 12883, loss 0.000688187, acc 1\n",
      "2018-10-26T17:07:57.230978: step 12884, loss 1.03188e-06, acc 1\n",
      "2018-10-26T17:07:57.565088: step 12885, loss 0.00100176, acc 1\n",
      "2018-10-26T17:07:57.904181: step 12886, loss 2.58706e-06, acc 1\n",
      "2018-10-26T17:07:58.241327: step 12887, loss 1.11012e-06, acc 1\n",
      "2018-10-26T17:07:58.580374: step 12888, loss 0.000136701, acc 1\n",
      "2018-10-26T17:07:58.914482: step 12889, loss 0.000758528, acc 1\n",
      "2018-10-26T17:07:59.252576: step 12890, loss 4.9529e-05, acc 1\n",
      "2018-10-26T17:07:59.588681: step 12891, loss 0.0110072, acc 0.984375\n",
      "2018-10-26T17:07:59.946723: step 12892, loss 3.30524e-05, acc 1\n",
      "2018-10-26T17:08:00.301846: step 12893, loss 3.71796e-05, acc 1\n",
      "2018-10-26T17:08:00.619927: step 12894, loss 0.000487753, acc 1\n",
      "2018-10-26T17:08:00.953103: step 12895, loss 0.000143417, acc 1\n",
      "2018-10-26T17:08:01.289134: step 12896, loss 0.000166019, acc 1\n",
      "2018-10-26T17:08:01.623247: step 12897, loss 0.000143549, acc 1\n",
      "2018-10-26T17:08:01.958350: step 12898, loss 0.0078295, acc 1\n",
      "2018-10-26T17:08:02.287468: step 12899, loss 0.000330853, acc 1\n",
      "2018-10-26T17:08:02.619585: step 12900, loss 0.000656837, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:08:03.438392: step 12900, loss 4.42467, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-12900\n",
      "\n",
      "2018-10-26T17:08:04.088658: step 12901, loss 0.000495039, acc 1\n",
      "2018-10-26T17:08:04.408861: step 12902, loss 0.00220318, acc 1\n",
      "2018-10-26T17:08:04.817707: step 12903, loss 0.00018792, acc 1\n",
      "2018-10-26T17:08:05.174757: step 12904, loss 0.000527506, acc 1\n",
      "2018-10-26T17:08:05.521826: step 12905, loss 0.00330956, acc 1\n",
      "2018-10-26T17:08:05.854936: step 12906, loss 0.0017007, acc 1\n",
      "2018-10-26T17:08:06.198091: step 12907, loss 2.06929e-06, acc 1\n",
      "2018-10-26T17:08:06.536119: step 12908, loss 3.32307e-05, acc 1\n",
      "2018-10-26T17:08:06.882191: step 12909, loss 5.25199e-06, acc 1\n",
      "2018-10-26T17:08:07.220288: step 12910, loss 1.17904e-06, acc 1\n",
      "2018-10-26T17:08:07.551474: step 12911, loss 5.6286e-06, acc 1\n",
      "2018-10-26T17:08:07.883518: step 12912, loss 0.000427277, acc 1\n",
      "2018-10-26T17:08:08.230592: step 12913, loss 1.49941e-06, acc 1\n",
      "2018-10-26T17:08:08.565727: step 12914, loss 3.5499e-06, acc 1\n",
      "2018-10-26T17:08:08.913796: step 12915, loss 2.07119e-06, acc 1\n",
      "2018-10-26T17:08:09.246874: step 12916, loss 1.33811e-05, acc 1\n",
      "2018-10-26T17:08:09.603922: step 12917, loss 1.5042e-05, acc 1\n",
      "2018-10-26T17:08:09.944014: step 12918, loss 9.04692e-05, acc 1\n",
      "2018-10-26T17:08:10.282143: step 12919, loss 2.12469e-05, acc 1\n",
      "2018-10-26T17:08:10.625192: step 12920, loss 3.31818e-05, acc 1\n",
      "2018-10-26T17:08:10.969271: step 12921, loss 0.00627166, acc 1\n",
      "2018-10-26T17:08:11.295401: step 12922, loss 0.000123883, acc 1\n",
      "2018-10-26T17:08:11.630557: step 12923, loss 3.85567e-07, acc 1\n",
      "2018-10-26T17:08:11.963617: step 12924, loss 4.53141e-06, acc 1\n",
      "2018-10-26T17:08:12.322654: step 12925, loss 0.0131676, acc 0.984375\n",
      "2018-10-26T17:08:12.655769: step 12926, loss 8.15565e-05, acc 1\n",
      "2018-10-26T17:08:12.996882: step 12927, loss 0.00227683, acc 1\n",
      "2018-10-26T17:08:13.341931: step 12928, loss 6.27701e-07, acc 1\n",
      "2018-10-26T17:08:13.686013: step 12929, loss 3.46343e-05, acc 1\n",
      "2018-10-26T17:08:14.022115: step 12930, loss 0.00028845, acc 1\n",
      "2018-10-26T17:08:14.358215: step 12931, loss 1.8221e-05, acc 1\n",
      "2018-10-26T17:08:14.697314: step 12932, loss 0.00037476, acc 1\n",
      "2018-10-26T17:08:15.047379: step 12933, loss 0.00262082, acc 1\n",
      "2018-10-26T17:08:15.388531: step 12934, loss 6.3889e-05, acc 1\n",
      "2018-10-26T17:08:15.713597: step 12935, loss 2.17501e-05, acc 1\n",
      "2018-10-26T17:08:16.034748: step 12936, loss 0.000258135, acc 1\n",
      "2018-10-26T17:08:16.368844: step 12937, loss 0.000191928, acc 1\n",
      "2018-10-26T17:08:16.705946: step 12938, loss 0.000899583, acc 1\n",
      "2018-10-26T17:08:17.050028: step 12939, loss 2.76405e-05, acc 1\n",
      "2018-10-26T17:08:17.391113: step 12940, loss 1.88965e-05, acc 1\n",
      "2018-10-26T17:08:17.735230: step 12941, loss 0.000271565, acc 1\n",
      "2018-10-26T17:08:18.085261: step 12942, loss 0.0181134, acc 1\n",
      "2018-10-26T17:08:18.425353: step 12943, loss 9.00523e-06, acc 1\n",
      "2018-10-26T17:08:18.779404: step 12944, loss 0.000822576, acc 1\n",
      "2018-10-26T17:08:19.241169: step 12945, loss 8.57301e-05, acc 1\n",
      "2018-10-26T17:08:19.632247: step 12946, loss 0.000636537, acc 1\n",
      "2018-10-26T17:08:20.033054: step 12947, loss 3.71221e-05, acc 1\n",
      "2018-10-26T17:08:20.431990: step 12948, loss 2.63661e-05, acc 1\n",
      "2018-10-26T17:08:20.836383: step 12949, loss 0.000665111, acc 1\n",
      "2018-10-26T17:08:21.232877: step 12950, loss 0.00177714, acc 1\n",
      "2018-10-26T17:08:21.526064: step 12951, loss 3.87197e-06, acc 1\n",
      "2018-10-26T17:08:21.820278: step 12952, loss 8.66963e-05, acc 1\n",
      "2018-10-26T17:08:22.110581: step 12953, loss 4.18773e-05, acc 1\n",
      "2018-10-26T17:08:22.401724: step 12954, loss 0.000132816, acc 1\n",
      "2018-10-26T17:08:22.678983: step 12955, loss 7.22374e-06, acc 1\n",
      "2018-10-26T17:08:22.980180: step 12956, loss 0.000178309, acc 1\n",
      "2018-10-26T17:08:23.266414: step 12957, loss 2.24453e-05, acc 1\n",
      "2018-10-26T17:08:23.546666: step 12958, loss 0.00118286, acc 1\n",
      "2018-10-26T17:08:23.838886: step 12959, loss 5.70909e-05, acc 1\n",
      "2018-10-26T17:08:24.141113: step 12960, loss 1.05004e-05, acc 1\n",
      "2018-10-26T17:08:24.444444: step 12961, loss 0.0170997, acc 0.984375\n",
      "2018-10-26T17:08:24.739478: step 12962, loss 0.00012651, acc 1\n",
      "2018-10-26T17:08:25.046658: step 12963, loss 0.02268, acc 0.984375\n",
      "2018-10-26T17:08:25.344863: step 12964, loss 4.32132e-07, acc 1\n",
      "2018-10-26T17:08:25.632093: step 12965, loss 7.48437e-05, acc 1\n",
      "2018-10-26T17:08:25.938275: step 12966, loss 6.83312e-06, acc 1\n",
      "2018-10-26T17:08:26.232489: step 12967, loss 1.911e-06, acc 1\n",
      "2018-10-26T17:08:26.558657: step 12968, loss 4.36144e-05, acc 1\n",
      "2018-10-26T17:08:26.871860: step 12969, loss 7.80436e-05, acc 1\n",
      "2018-10-26T17:08:27.187936: step 12970, loss 0.000133743, acc 1\n",
      "2018-10-26T17:08:27.489158: step 12971, loss 0.000879854, acc 1\n",
      "2018-10-26T17:08:27.790327: step 12972, loss 6.04121e-05, acc 1\n",
      "2018-10-26T17:08:28.126428: step 12973, loss 2.72872e-05, acc 1\n",
      "2018-10-26T17:08:28.447571: step 12974, loss 0.00307263, acc 1\n",
      "2018-10-26T17:08:28.746773: step 12975, loss 0.000188896, acc 1\n",
      "2018-10-26T17:08:29.030014: step 12976, loss 0.00192685, acc 1\n",
      "2018-10-26T17:08:29.332207: step 12977, loss 4.48484e-06, acc 1\n",
      "2018-10-26T17:08:29.626421: step 12978, loss 4.24738e-05, acc 1\n",
      "2018-10-26T17:08:29.922740: step 12979, loss 0.000276535, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:08:30.217866: step 12980, loss 5.55065e-07, acc 1\n",
      "2018-10-26T17:08:30.516044: step 12981, loss 5.7369e-07, acc 1\n",
      "2018-10-26T17:08:30.800285: step 12982, loss 0.000364004, acc 1\n",
      "2018-10-26T17:08:31.083747: step 12983, loss 1.12208e-05, acc 1\n",
      "2018-10-26T17:08:31.375747: step 12984, loss 1.41135e-05, acc 1\n",
      "2018-10-26T17:08:31.667114: step 12985, loss 4.11577e-05, acc 1\n",
      "2018-10-26T17:08:31.962183: step 12986, loss 0.00154657, acc 1\n",
      "2018-10-26T17:08:32.273455: step 12987, loss 1.67638e-08, acc 1\n",
      "2018-10-26T17:08:32.612443: step 12988, loss 4.45932e-05, acc 1\n",
      "2018-10-26T17:08:32.951540: step 12989, loss 5.84074e-06, acc 1\n",
      "2018-10-26T17:08:33.279662: step 12990, loss 2.24873e-05, acc 1\n",
      "2018-10-26T17:08:33.631766: step 12991, loss 1.8794e-05, acc 1\n",
      "2018-10-26T17:08:33.997742: step 12992, loss 0.00802237, acc 1\n",
      "2018-10-26T17:08:34.311902: step 12993, loss 0.000153362, acc 1\n",
      "2018-10-26T17:08:34.652992: step 12994, loss 0.00198662, acc 1\n",
      "2018-10-26T17:08:35.004054: step 12995, loss 0.000836476, acc 1\n",
      "2018-10-26T17:08:35.402993: step 12996, loss 0.113039, acc 0.984375\n",
      "2018-10-26T17:08:35.796252: step 12997, loss 8.32508e-05, acc 1\n",
      "2018-10-26T17:08:36.152041: step 12998, loss 8.9537e-05, acc 1\n",
      "2018-10-26T17:08:36.524005: step 12999, loss 8.77174e-06, acc 1\n",
      "2018-10-26T17:08:36.993736: step 13000, loss 2.22761e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:08:38.176577: step 13000, loss 4.47322, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13000\n",
      "\n",
      "2018-10-26T17:08:38.924583: step 13001, loss 1.38148e-05, acc 1\n",
      "2018-10-26T17:08:39.255694: step 13002, loss 3.10911e-05, acc 1\n",
      "2018-10-26T17:08:39.745386: step 13003, loss 1.16765e-05, acc 1\n",
      "2018-10-26T17:08:40.143322: step 13004, loss 3.40862e-07, acc 1\n",
      "2018-10-26T17:08:40.600101: step 13005, loss 2.3283e-07, acc 1\n",
      "2018-10-26T17:08:41.011091: step 13006, loss 3.22781e-06, acc 1\n",
      "2018-10-26T17:08:41.429886: step 13007, loss 0.000353421, acc 1\n",
      "2018-10-26T17:08:41.852757: step 13008, loss 4.38232e-06, acc 1\n",
      "2018-10-26T17:08:42.263721: step 13009, loss 6.2003e-06, acc 1\n",
      "2018-10-26T17:08:42.590784: step 13010, loss 3.3916e-06, acc 1\n",
      "2018-10-26T17:08:42.992053: step 13011, loss 2.01532e-06, acc 1\n",
      "2018-10-26T17:08:43.404609: step 13012, loss 0.013992, acc 0.984375\n",
      "2018-10-26T17:08:43.725750: step 13013, loss 1.61485e-06, acc 1\n",
      "2018-10-26T17:08:44.128675: step 13014, loss 8.41722e-06, acc 1\n",
      "2018-10-26T17:08:44.508659: step 13015, loss 5.28214e-06, acc 1\n",
      "2018-10-26T17:08:44.828837: step 13016, loss 2.79358e-05, acc 1\n",
      "2018-10-26T17:08:45.145129: step 13017, loss 4.50723e-06, acc 1\n",
      "2018-10-26T17:08:45.479067: step 13018, loss 0.000182511, acc 1\n",
      "2018-10-26T17:08:45.801205: step 13019, loss 0.000387287, acc 1\n",
      "2018-10-26T17:08:46.131322: step 13020, loss 3.7439e-07, acc 1\n",
      "2018-10-26T17:08:46.461442: step 13021, loss 2.24999e-06, acc 1\n",
      "2018-10-26T17:08:46.774673: step 13022, loss 3.01793e-05, acc 1\n",
      "2018-10-26T17:08:47.077808: step 13023, loss 5.90455e-07, acc 1\n",
      "2018-10-26T17:08:47.384973: step 13024, loss 0.000334776, acc 1\n",
      "2018-10-26T17:08:47.708113: step 13025, loss 0.000524582, acc 1\n",
      "2018-10-26T17:08:48.048235: step 13026, loss 0.000453847, acc 1\n",
      "2018-10-26T17:08:48.365355: step 13027, loss 5.96876e-06, acc 1\n",
      "2018-10-26T17:08:48.707487: step 13028, loss 3.65064e-05, acc 1\n",
      "2018-10-26T17:08:49.078496: step 13029, loss 0.00229019, acc 1\n",
      "2018-10-26T17:08:49.393610: step 13030, loss 1.34853e-06, acc 1\n",
      "2018-10-26T17:08:49.712755: step 13031, loss 2.82179e-06, acc 1\n",
      "2018-10-26T17:08:50.034892: step 13032, loss 0.000500367, acc 1\n",
      "2018-10-26T17:08:50.355037: step 13033, loss 6.09079e-07, acc 1\n",
      "2018-10-26T17:08:50.698122: step 13034, loss 2.57865e-05, acc 1\n",
      "2018-10-26T17:08:51.013278: step 13035, loss 1.02445e-07, acc 1\n",
      "2018-10-26T17:08:51.340408: step 13036, loss 4.02414e-05, acc 1\n",
      "2018-10-26T17:08:51.670591: step 13037, loss 7.39524e-06, acc 1\n",
      "2018-10-26T17:08:52.002636: step 13038, loss 0.0344518, acc 0.984375\n",
      "2018-10-26T17:08:52.323778: step 13039, loss 0.00906599, acc 1\n",
      "2018-10-26T17:08:52.655893: step 13040, loss 3.14757e-06, acc 1\n",
      "2018-10-26T17:08:52.982019: step 13041, loss 1.08961e-06, acc 1\n",
      "2018-10-26T17:08:53.301167: step 13042, loss 3.06568e-05, acc 1\n",
      "2018-10-26T17:08:53.635273: step 13043, loss 0.00161904, acc 1\n",
      "2018-10-26T17:08:53.975424: step 13044, loss 1.42369e-05, acc 1\n",
      "2018-10-26T17:08:54.290566: step 13045, loss 0.0362222, acc 0.984375\n",
      "2018-10-26T17:08:54.626625: step 13046, loss 0.000252464, acc 1\n",
      "2018-10-26T17:08:54.926825: step 13047, loss 0.000350731, acc 1\n",
      "2018-10-26T17:08:55.245969: step 13048, loss 0.000103718, acc 1\n",
      "2018-10-26T17:08:55.544173: step 13049, loss 2.11032e-06, acc 1\n",
      "2018-10-26T17:08:55.863348: step 13050, loss 7.98177e-05, acc 1\n",
      "2018-10-26T17:08:56.182470: step 13051, loss 0.00292401, acc 1\n",
      "2018-10-26T17:08:56.507599: step 13052, loss 1.79998e-05, acc 1\n",
      "2018-10-26T17:08:56.814779: step 13053, loss 0.000178914, acc 1\n",
      "2018-10-26T17:08:57.123952: step 13054, loss 0.000126044, acc 1\n",
      "2018-10-26T17:08:57.467035: step 13055, loss 2.98955e-05, acc 1\n",
      "2018-10-26T17:08:57.780260: step 13056, loss 1.77295e-05, acc 1\n",
      "2018-10-26T17:08:58.109322: step 13057, loss 9.3495e-05, acc 1\n",
      "2018-10-26T17:08:58.424482: step 13058, loss 5.2154e-08, acc 1\n",
      "2018-10-26T17:08:58.738641: step 13059, loss 8.38189e-08, acc 1\n",
      "2018-10-26T17:08:59.055794: step 13060, loss 2.93726e-06, acc 1\n",
      "2018-10-26T17:08:59.374939: step 13061, loss 0.000135457, acc 1\n",
      "2018-10-26T17:08:59.725999: step 13062, loss 2.44931e-06, acc 1\n",
      "2018-10-26T17:09:00.063100: step 13063, loss 7.39462e-07, acc 1\n",
      "2018-10-26T17:09:00.382250: step 13064, loss 0.0243451, acc 0.984375\n",
      "2018-10-26T17:09:00.699401: step 13065, loss 7.46447e-06, acc 1\n",
      "2018-10-26T17:09:01.021538: step 13066, loss 1.57015e-06, acc 1\n",
      "2018-10-26T17:09:01.347705: step 13067, loss 0.000870393, acc 1\n",
      "2018-10-26T17:09:01.663826: step 13068, loss 0.00024438, acc 1\n",
      "2018-10-26T17:09:01.986959: step 13069, loss 0.0158494, acc 0.984375\n",
      "2018-10-26T17:09:02.302117: step 13070, loss 0.000562396, acc 1\n",
      "2018-10-26T17:09:02.637221: step 13071, loss 0.000160467, acc 1\n",
      "2018-10-26T17:09:02.935426: step 13072, loss 0.000381146, acc 1\n",
      "2018-10-26T17:09:03.245596: step 13073, loss 2.80621e-05, acc 1\n",
      "2018-10-26T17:09:03.559759: step 13074, loss 0.000899185, acc 1\n",
      "2018-10-26T17:09:03.908823: step 13075, loss 7.33568e-06, acc 1\n",
      "2018-10-26T17:09:04.252962: step 13076, loss 1.44726e-06, acc 1\n",
      "2018-10-26T17:09:04.567069: step 13077, loss 6.73608e-05, acc 1\n",
      "2018-10-26T17:09:04.905162: step 13078, loss 5.18713e-06, acc 1\n",
      "2018-10-26T17:09:05.227303: step 13079, loss 0.0121208, acc 0.984375\n",
      "2018-10-26T17:09:05.543508: step 13080, loss 2.71403e-05, acc 1\n",
      "2018-10-26T17:09:05.869585: step 13081, loss 0.000196838, acc 1\n",
      "2018-10-26T17:09:06.217860: step 13082, loss 0.000157588, acc 1\n",
      "2018-10-26T17:09:06.713484: step 13083, loss 0.00205293, acc 1\n",
      "2018-10-26T17:09:07.099302: step 13084, loss 6.33178e-06, acc 1\n",
      "2018-10-26T17:09:07.478500: step 13085, loss 0.000295146, acc 1\n",
      "2018-10-26T17:09:07.807408: step 13086, loss 1.23696e-05, acc 1\n",
      "2018-10-26T17:09:08.122567: step 13087, loss 0.000796578, acc 1\n",
      "2018-10-26T17:09:08.437728: step 13088, loss 5.45519e-06, acc 1\n",
      "2018-10-26T17:09:08.763853: step 13089, loss 0.0176468, acc 0.984375\n",
      "2018-10-26T17:09:09.084994: step 13090, loss 1.24797e-07, acc 1\n",
      "2018-10-26T17:09:09.411123: step 13091, loss 0.000319878, acc 1\n",
      "2018-10-26T17:09:09.737252: step 13092, loss 0.00592611, acc 1\n",
      "2018-10-26T17:09:10.067471: step 13093, loss 8.4936e-07, acc 1\n",
      "2018-10-26T17:09:10.378539: step 13094, loss 0.00102965, acc 1\n",
      "2018-10-26T17:09:10.685718: step 13095, loss 3.87428e-07, acc 1\n",
      "2018-10-26T17:09:11.014839: step 13096, loss 1.80484e-06, acc 1\n",
      "2018-10-26T17:09:11.328999: step 13097, loss 6.25594e-05, acc 1\n",
      "2018-10-26T17:09:11.643160: step 13098, loss 5.6081e-06, acc 1\n",
      "2018-10-26T17:09:12.011222: step 13099, loss 0.00188058, acc 1\n",
      "2018-10-26T17:09:12.357251: step 13100, loss 5.6698e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:09:13.187035: step 13100, loss 4.50708, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13100\n",
      "\n",
      "2018-10-26T17:09:13.886392: step 13101, loss 7.07804e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:09:14.245209: step 13102, loss 2.22432e-05, acc 1\n",
      "2018-10-26T17:09:14.640152: step 13103, loss 2.5089e-06, acc 1\n",
      "2018-10-26T17:09:14.960332: step 13104, loss 0.00268567, acc 1\n",
      "2018-10-26T17:09:15.296403: step 13105, loss 8.38175e-07, acc 1\n",
      "2018-10-26T17:09:15.613553: step 13106, loss 0.000394202, acc 1\n",
      "2018-10-26T17:09:15.960642: step 13107, loss 0.000180034, acc 1\n",
      "2018-10-26T17:09:16.342604: step 13108, loss 0.000108438, acc 1\n",
      "2018-10-26T17:09:16.667804: step 13109, loss 5.02914e-08, acc 1\n",
      "2018-10-26T17:09:16.999848: step 13110, loss 0.00179446, acc 1\n",
      "2018-10-26T17:09:17.307028: step 13111, loss 0.0106788, acc 1\n",
      "2018-10-26T17:09:17.618198: step 13112, loss 7.22758e-06, acc 1\n",
      "2018-10-26T17:09:17.945221: step 13113, loss 2.3283e-07, acc 1\n",
      "2018-10-26T17:09:18.266467: step 13114, loss 3.74389e-07, acc 1\n",
      "2018-10-26T17:09:18.597579: step 13115, loss 6.94168e-05, acc 1\n",
      "2018-10-26T17:09:18.933685: step 13116, loss 9.26802e-05, acc 1\n",
      "2018-10-26T17:09:19.281752: step 13117, loss 2.47426e-05, acc 1\n",
      "2018-10-26T17:09:19.700630: step 13118, loss 0.000354695, acc 1\n",
      "2018-10-26T17:09:20.205283: step 13119, loss 0.00308541, acc 1\n",
      "2018-10-26T17:09:20.649098: step 13120, loss 1.40194e-05, acc 1\n",
      "2018-10-26T17:09:21.188655: step 13121, loss 0.000128585, acc 1\n",
      "2018-10-26T17:09:21.545701: step 13122, loss 5.40103e-06, acc 1\n",
      "2018-10-26T17:09:21.933790: step 13123, loss 0.000138264, acc 1\n",
      "2018-10-26T17:09:22.252813: step 13124, loss 0.000338047, acc 1\n",
      "2018-10-26T17:09:22.580935: step 13125, loss 0.00370674, acc 1\n",
      "2018-10-26T17:09:22.924018: step 13126, loss 2.42144e-08, acc 1\n",
      "2018-10-26T17:09:23.256130: step 13127, loss 7.75372e-05, acc 1\n",
      "2018-10-26T17:09:23.620158: step 13128, loss 4.29333e-05, acc 1\n",
      "2018-10-26T17:09:23.874479: step 13129, loss 0.000137001, acc 1\n",
      "2018-10-26T17:09:24.115854: step 13130, loss 1.56554e-05, acc 1\n",
      "2018-10-26T17:09:24.367163: step 13131, loss 1.77401e-05, acc 1\n",
      "2018-10-26T17:09:24.622482: step 13132, loss 0.00193636, acc 1\n",
      "2018-10-26T17:09:24.915701: step 13133, loss 1.30195e-06, acc 1\n",
      "2018-10-26T17:09:25.209923: step 13134, loss 0.0250449, acc 0.984375\n",
      "2018-10-26T17:09:36.605440: step 13135, loss 3.91155e-08, acc 1\n",
      "2018-10-26T17:09:38.246183: step 13136, loss 4.37222e-05, acc 1\n",
      "2018-10-26T17:09:38.765796: step 13137, loss 0.00125307, acc 1\n",
      "2018-10-26T17:09:39.270446: step 13138, loss 2.00977e-06, acc 1\n",
      "2018-10-26T17:09:39.889792: step 13139, loss 4.32861e-06, acc 1\n",
      "2018-10-26T17:09:40.390467: step 13140, loss 0.0588568, acc 0.984375\n",
      "2018-10-26T17:09:40.871171: step 13141, loss 0.00015851, acc 1\n",
      "2018-10-26T17:09:41.389785: step 13142, loss 1.09987e-05, acc 1\n",
      "2018-10-26T17:09:42.009131: step 13143, loss 4.40868e-06, acc 1\n",
      "2018-10-26T17:09:42.856865: step 13144, loss 0.00010966, acc 1\n",
      "2018-10-26T17:09:43.470226: step 13145, loss 4.19121e-05, acc 1\n",
      "2018-10-26T17:09:44.150408: step 13146, loss 2.63805e-05, acc 1\n",
      "2018-10-26T17:09:44.634116: step 13147, loss 0.000331923, acc 1\n",
      "2018-10-26T17:09:45.061973: step 13148, loss 4.45342e-05, acc 1\n",
      "2018-10-26T17:09:45.467888: step 13149, loss 5.99292e-06, acc 1\n",
      "2018-10-26T17:09:45.896743: step 13150, loss 0.000171017, acc 1\n",
      "2018-10-26T17:09:46.368482: step 13151, loss 7.64689e-06, acc 1\n",
      "2018-10-26T17:09:46.918013: step 13152, loss 0.00121062, acc 1\n",
      "2018-10-26T17:09:47.535365: step 13153, loss 1.9744e-07, acc 1\n",
      "2018-10-26T17:09:48.056971: step 13154, loss 1.83463e-06, acc 1\n",
      "2018-10-26T17:09:48.633436: step 13155, loss 0.000706051, acc 1\n",
      "2018-10-26T17:09:49.248786: step 13156, loss 1.23009e-05, acc 1\n",
      "2018-10-26T17:09:49.859155: step 13157, loss 0.01232, acc 0.984375\n",
      "2018-10-26T17:09:50.421652: step 13158, loss 0.000364573, acc 1\n",
      "2018-10-26T17:09:50.913338: step 13159, loss 8.632e-05, acc 1\n",
      "2018-10-26T17:09:51.330225: step 13160, loss 0.000161255, acc 1\n",
      "2018-10-26T17:09:51.770050: step 13161, loss 0.0223928, acc 0.984375\n",
      "2018-10-26T17:09:52.163000: step 13162, loss 5.3954e-05, acc 1\n",
      "2018-10-26T17:09:52.560937: step 13163, loss 2.2779e-06, acc 1\n",
      "2018-10-26T17:09:52.963861: step 13164, loss 1.00186e-05, acc 1\n",
      "2018-10-26T17:09:53.369776: step 13165, loss 7.89639e-06, acc 1\n",
      "2018-10-26T17:09:53.740785: step 13166, loss 1.20032e-05, acc 1\n",
      "2018-10-26T17:09:54.166646: step 13167, loss 0.00246792, acc 1\n",
      "2018-10-26T17:09:54.544637: step 13168, loss 6.47589e-06, acc 1\n",
      "2018-10-26T17:09:54.955539: step 13169, loss 0.00162938, acc 1\n",
      "2018-10-26T17:09:55.382399: step 13170, loss 0.000711549, acc 1\n",
      "2018-10-26T17:09:55.749418: step 13171, loss 3.24398e-05, acc 1\n",
      "2018-10-26T17:09:56.151344: step 13172, loss 1.30894e-05, acc 1\n",
      "2018-10-26T17:09:56.542306: step 13173, loss 1.03376e-06, acc 1\n",
      "2018-10-26T17:09:56.902337: step 13174, loss 3.79515e-05, acc 1\n",
      "2018-10-26T17:09:57.252401: step 13175, loss 3.00436e-06, acc 1\n",
      "2018-10-26T17:09:57.566562: step 13176, loss 0.000172281, acc 1\n",
      "2018-10-26T17:09:57.860777: step 13177, loss 6.53538e-06, acc 1\n",
      "2018-10-26T17:09:58.390361: step 13178, loss 4.89646e-05, acc 1\n",
      "2018-10-26T17:09:58.835173: step 13179, loss 3.11051e-06, acc 1\n",
      "2018-10-26T17:09:59.164294: step 13180, loss 3.77173e-06, acc 1\n",
      "2018-10-26T17:09:59.536299: step 13181, loss 0.00872272, acc 1\n",
      "2018-10-26T17:09:59.853476: step 13182, loss 0.00189655, acc 1\n",
      "2018-10-26T17:10:00.174594: step 13183, loss 3.95085e-05, acc 1\n",
      "2018-10-26T17:10:00.704182: step 13184, loss 9.4274e-05, acc 1\n",
      "2018-10-26T17:10:01.062223: step 13185, loss 0.00179997, acc 1\n",
      "2018-10-26T17:10:01.395332: step 13186, loss 3.52037e-07, acc 1\n",
      "2018-10-26T17:10:01.673588: step 13187, loss 0.00011672, acc 1\n",
      "2018-10-26T17:10:01.932895: step 13188, loss 0.000587259, acc 1\n",
      "2018-10-26T17:10:02.194197: step 13189, loss 0.000516256, acc 1\n",
      "2018-10-26T17:10:02.481431: step 13190, loss 0.00925256, acc 1\n",
      "2018-10-26T17:10:02.748718: step 13191, loss 0.0319087, acc 0.984375\n",
      "2018-10-26T17:10:03.217465: step 13192, loss 2.773e-05, acc 1\n",
      "2018-10-26T17:10:03.553566: step 13193, loss 0.000580237, acc 1\n",
      "2018-10-26T17:10:03.846782: step 13194, loss 9.96499e-07, acc 1\n",
      "2018-10-26T17:10:04.129028: step 13195, loss 1.06171e-07, acc 1\n",
      "2018-10-26T17:10:04.510011: step 13196, loss 5.73085e-05, acc 1\n",
      "2018-10-26T17:10:04.774303: step 13197, loss 3.91155e-08, acc 1\n",
      "2018-10-26T17:10:05.054555: step 13198, loss 2.19082e-05, acc 1\n",
      "2018-10-26T17:10:05.432544: step 13199, loss 2.54243e-06, acc 1\n",
      "2018-10-26T17:10:05.683879: step 13200, loss 2.77927e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:10:06.391982: step 13200, loss 4.65694, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13200\n",
      "\n",
      "2018-10-26T17:10:06.833802: step 13201, loss 0.0394013, acc 0.984375\n",
      "2018-10-26T17:10:07.086128: step 13202, loss 0.000876714, acc 1\n",
      "2018-10-26T17:10:07.349423: step 13203, loss 1.13433e-06, acc 1\n",
      "2018-10-26T17:10:07.652614: step 13204, loss 3.58339e-06, acc 1\n",
      "2018-10-26T17:10:08.188183: step 13205, loss 1.05017e-05, acc 1\n",
      "2018-10-26T17:10:08.685853: step 13206, loss 0.000856435, acc 1\n",
      "2018-10-26T17:10:09.185518: step 13207, loss 0.0312762, acc 0.984375\n",
      "2018-10-26T17:10:09.590447: step 13208, loss 5.91216e-05, acc 1\n",
      "2018-10-26T17:10:10.012308: step 13209, loss 1.8514e-06, acc 1\n",
      "2018-10-26T17:10:10.385312: step 13210, loss 0.000145034, acc 1\n",
      "2018-10-26T17:10:10.778263: step 13211, loss 0.00519986, acc 1\n",
      "2018-10-26T17:10:11.050534: step 13212, loss 0.000375208, acc 1\n",
      "2018-10-26T17:10:11.315827: step 13213, loss 0.000212369, acc 1\n",
      "2018-10-26T17:10:11.567154: step 13214, loss 2.25622e-05, acc 1\n",
      "2018-10-26T17:10:11.864359: step 13215, loss 1.49012e-08, acc 1\n",
      "2018-10-26T17:54:08.286676: step 13216, loss 0.000363219, acc 1\n",
      "2018-10-26T17:54:09.297975: step 13217, loss 0.000921349, acc 1\n",
      "2018-10-26T17:54:10.464857: step 13218, loss 0.000159441, acc 1\n",
      "2018-10-26T17:54:11.252751: step 13219, loss 0.000133186, acc 1\n",
      "2018-10-26T17:54:12.041644: step 13220, loss 3.47369e-05, acc 1\n",
      "2018-10-26T17:54:12.624088: step 13221, loss 6.54231e-05, acc 1\n",
      "2018-10-26T17:54:13.170629: step 13222, loss 0.000407274, acc 1\n",
      "2018-10-26T17:54:13.610452: step 13223, loss 2.23517e-08, acc 1\n",
      "2018-10-26T17:54:14.060254: step 13224, loss 1.30982e-05, acc 1\n",
      "2018-10-26T17:54:14.594824: step 13225, loss 0.065523, acc 0.984375\n",
      "2018-10-26T17:54:15.140365: step 13226, loss 1.16042e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:54:15.624073: step 13227, loss 1.01761e-05, acc 1\n",
      "2018-10-26T17:54:16.145679: step 13228, loss 4.17065e-05, acc 1\n",
      "2018-10-26T17:54:16.791953: step 13229, loss 0.0598103, acc 0.984375\n",
      "2018-10-26T17:54:17.493079: step 13230, loss 1.73626e-05, acc 1\n",
      "2018-10-26T17:54:18.154312: step 13231, loss 1.35782e-06, acc 1\n",
      "2018-10-26T17:54:18.599124: step 13232, loss 3.91155e-08, acc 1\n",
      "2018-10-26T17:54:19.048921: step 13233, loss 0.000792169, acc 1\n",
      "2018-10-26T17:54:19.424917: step 13234, loss 5.01573e-06, acc 1\n",
      "2018-10-26T17:54:19.811883: step 13235, loss 0.000984637, acc 1\n",
      "2018-10-26T17:54:20.281629: step 13236, loss 0.000535099, acc 1\n",
      "2018-10-26T17:54:20.861079: step 13237, loss 0.00199898, acc 1\n",
      "2018-10-26T17:54:21.403646: step 13238, loss 0.00687937, acc 1\n",
      "2018-10-26T17:54:21.939205: step 13239, loss 0.000441065, acc 1\n",
      "2018-10-26T17:54:22.439863: step 13240, loss 2.81999e-06, acc 1\n",
      "2018-10-26T17:54:22.971443: step 13241, loss 0.0330562, acc 0.984375\n",
      "2018-10-26T17:54:23.347437: step 13242, loss 3.92355e-05, acc 1\n",
      "2018-10-26T17:54:23.666584: step 13243, loss 0.000358444, acc 1\n",
      "2018-10-26T17:54:24.014654: step 13244, loss 3.76591e-05, acc 1\n",
      "2018-10-26T17:54:24.388657: step 13245, loss 2.80213e-05, acc 1\n",
      "2018-10-26T17:54:24.773627: step 13246, loss 2.64337e-05, acc 1\n",
      "2018-10-26T17:54:25.194502: step 13247, loss 0.000562185, acc 1\n",
      "2018-10-26T17:54:25.513648: step 13248, loss 2.46884e-05, acc 1\n",
      "2018-10-26T17:54:25.792903: step 13249, loss 1.17714e-05, acc 1\n",
      "2018-10-26T17:54:26.064178: step 13250, loss 0.000135798, acc 1\n",
      "2018-10-26T17:54:26.333458: step 13251, loss 1.39595e-05, acc 1\n",
      "2018-10-26T17:54:26.610717: step 13252, loss 2.70254e-06, acc 1\n",
      "2018-10-26T17:54:27.008655: step 13253, loss 4.00465e-07, acc 1\n",
      "2018-10-26T17:54:27.359716: step 13254, loss 7.45673e-05, acc 1\n",
      "2018-10-26T17:54:27.696816: step 13255, loss 3.483e-05, acc 1\n",
      "2018-10-26T17:54:27.990032: step 13256, loss 1.06904e-05, acc 1\n",
      "2018-10-26T17:54:28.296215: step 13257, loss 0.00167695, acc 1\n",
      "2018-10-26T17:54:28.590427: step 13258, loss 0.000359829, acc 1\n",
      "2018-10-26T17:54:28.858731: step 13259, loss 2.91257e-05, acc 1\n",
      "2018-10-26T17:54:29.159906: step 13260, loss 3.97349e-05, acc 1\n",
      "2018-10-26T17:54:29.446141: step 13261, loss 1.05983e-06, acc 1\n",
      "2018-10-26T17:54:29.806179: step 13262, loss 1.72288e-06, acc 1\n",
      "2018-10-26T17:54:30.161235: step 13263, loss 7.58273e-05, acc 1\n",
      "2018-10-26T17:54:30.545207: step 13264, loss 0.00475581, acc 1\n",
      "2018-10-26T17:54:30.816480: step 13265, loss 1.84275e-05, acc 1\n",
      "2018-10-26T17:54:31.099723: step 13266, loss 3.49807e-05, acc 1\n",
      "2018-10-26T17:54:31.383964: step 13267, loss 5.2317e-06, acc 1\n",
      "2018-10-26T17:54:31.665212: step 13268, loss 1.85131e-05, acc 1\n",
      "2018-10-26T17:54:31.929506: step 13269, loss 0.00319414, acc 1\n",
      "2018-10-26T17:54:32.222722: step 13270, loss 0.000265213, acc 1\n",
      "2018-10-26T17:54:32.507962: step 13271, loss 1.75455e-06, acc 1\n",
      "2018-10-26T17:54:32.798185: step 13272, loss 0.00326692, acc 1\n",
      "2018-10-26T17:54:33.089407: step 13273, loss 3.80334e-06, acc 1\n",
      "2018-10-26T17:54:33.357690: step 13274, loss 6.36372e-06, acc 1\n",
      "2018-10-26T17:54:33.624977: step 13275, loss 3.61043e-05, acc 1\n",
      "2018-10-26T17:54:33.888273: step 13276, loss 8.77294e-07, acc 1\n",
      "2018-10-26T17:54:34.149574: step 13277, loss 2.01655e-05, acc 1\n",
      "2018-10-26T17:54:34.417858: step 13278, loss 2.88937e-05, acc 1\n",
      "2018-10-26T17:54:34.679160: step 13279, loss 1.97078e-05, acc 1\n",
      "2018-10-26T17:54:34.944453: step 13280, loss 1.81445e-05, acc 1\n",
      "2018-10-26T17:54:35.202762: step 13281, loss 7.76087e-05, acc 1\n",
      "2018-10-26T17:54:35.470047: step 13282, loss 0.000143498, acc 1\n",
      "2018-10-26T17:54:35.735337: step 13283, loss 1.85592e-05, acc 1\n",
      "2018-10-26T17:54:36.005617: step 13284, loss 0.000288854, acc 1\n",
      "2018-10-26T17:54:36.263926: step 13285, loss 2.18723e-05, acc 1\n",
      "2018-10-26T17:54:36.532208: step 13286, loss 1.25726e-06, acc 1\n",
      "2018-10-26T17:54:36.789521: step 13287, loss 9.50239e-05, acc 1\n",
      "2018-10-26T17:54:37.052817: step 13288, loss 0.000402116, acc 1\n",
      "2018-10-26T17:54:37.315116: step 13289, loss 0.00241575, acc 1\n",
      "2018-10-26T17:54:37.576419: step 13290, loss 4.25162e-05, acc 1\n",
      "2018-10-26T17:54:37.836723: step 13291, loss 7.00257e-05, acc 1\n",
      "2018-10-26T17:54:38.104008: step 13292, loss 0.000203282, acc 1\n",
      "2018-10-26T17:54:38.372292: step 13293, loss 7.3573e-07, acc 1\n",
      "2018-10-26T17:54:38.640575: step 13294, loss 9.77878e-07, acc 1\n",
      "2018-10-26T17:54:38.899882: step 13295, loss 2.28538e-06, acc 1\n",
      "2018-10-26T17:54:39.162181: step 13296, loss 0.000190388, acc 1\n",
      "2018-10-26T17:54:39.427473: step 13297, loss 6.22112e-07, acc 1\n",
      "2018-10-26T17:54:39.722686: step 13298, loss 3.12913e-06, acc 1\n",
      "2018-10-26T17:54:39.996951: step 13299, loss 0.000156165, acc 1\n",
      "2018-10-26T17:54:40.268226: step 13300, loss 2.59931e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:54:40.980324: step 13300, loss 4.5999, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13300\n",
      "\n",
      "2018-10-26T17:54:41.423141: step 13301, loss 0.00067309, acc 1\n",
      "2018-10-26T17:54:41.719349: step 13302, loss 1.11421e-05, acc 1\n",
      "2018-10-26T17:54:41.982645: step 13303, loss 0.000136648, acc 1\n",
      "2018-10-26T17:54:42.477325: step 13304, loss 5.87593e-06, acc 1\n",
      "2018-10-26T17:54:42.932110: step 13305, loss 3.83703e-07, acc 1\n",
      "2018-10-26T17:54:43.431775: step 13306, loss 1.26055e-05, acc 1\n",
      "2018-10-26T17:54:43.810763: step 13307, loss 0.000130882, acc 1\n",
      "2018-10-26T17:54:44.298459: step 13308, loss 1.74526e-06, acc 1\n",
      "2018-10-26T17:54:44.701382: step 13309, loss 2.6077e-08, acc 1\n",
      "2018-10-26T17:54:45.218003: step 13310, loss 9.12677e-07, acc 1\n",
      "2018-10-26T17:54:45.716670: step 13311, loss 3.9813e-05, acc 1\n",
      "2018-10-26T17:54:46.220325: step 13312, loss 0.00183713, acc 1\n",
      "2018-10-26T17:54:46.717995: step 13313, loss 1.88005e-05, acc 1\n",
      "2018-10-26T17:54:47.207713: step 13314, loss 3.24389e-05, acc 1\n",
      "2018-10-26T17:54:47.579691: step 13315, loss 1.9923e-05, acc 1\n",
      "2018-10-26T17:54:47.904823: step 13316, loss 6.68683e-07, acc 1\n",
      "2018-10-26T17:54:48.188065: step 13317, loss 4.66385e-06, acc 1\n",
      "2018-10-26T17:54:48.582014: step 13318, loss 2.12868e-05, acc 1\n",
      "2018-10-26T17:54:48.918117: step 13319, loss 0.000124767, acc 1\n",
      "2018-10-26T17:54:49.278155: step 13320, loss 3.5352e-05, acc 1\n",
      "2018-10-26T17:54:49.619242: step 13321, loss 1.99111e-06, acc 1\n",
      "2018-10-26T17:54:49.935397: step 13322, loss 0.0549855, acc 0.984375\n",
      "2018-10-26T17:54:50.252551: step 13323, loss 0.00145687, acc 1\n",
      "2018-10-26T17:54:50.619570: step 13324, loss 0.000406508, acc 1\n",
      "2018-10-26T17:54:50.936722: step 13325, loss 4.20528e-05, acc 1\n",
      "2018-10-26T17:54:51.217971: step 13326, loss 5.6769e-06, acc 1\n",
      "2018-10-26T17:54:51.507201: step 13327, loss 0.0291779, acc 0.984375\n",
      "2018-10-26T17:54:51.785455: step 13328, loss 0.00333041, acc 1\n",
      "2018-10-26T17:54:52.050745: step 13329, loss 0.000248122, acc 1\n",
      "2018-10-26T17:54:52.313044: step 13330, loss 3.55418e-05, acc 1\n",
      "2018-10-26T17:54:52.573350: step 13331, loss 0.000121074, acc 1\n",
      "2018-10-26T17:54:52.877536: step 13332, loss 4.47681e-05, acc 1\n",
      "2018-10-26T17:54:53.133852: step 13333, loss 7.66014e-05, acc 1\n",
      "2018-10-26T17:54:53.406123: step 13334, loss 2.66335e-05, acc 1\n",
      "2018-10-26T17:54:53.671416: step 13335, loss 0.003539, acc 1\n",
      "2018-10-26T17:54:53.932719: step 13336, loss 4.99344e-06, acc 1\n",
      "2018-10-26T17:54:54.197010: step 13337, loss 5.34182e-06, acc 1\n",
      "2018-10-26T17:54:54.462302: step 13338, loss 6.0827e-06, acc 1\n",
      "2018-10-26T17:54:54.723605: step 13339, loss 1.00925e-05, acc 1\n",
      "2018-10-26T17:54:54.989892: step 13340, loss 7.98583e-05, acc 1\n",
      "2018-10-26T17:54:55.250196: step 13341, loss 0.000452542, acc 1\n",
      "2018-10-26T17:54:55.577323: step 13342, loss 3.54955e-05, acc 1\n",
      "2018-10-26T17:54:55.921403: step 13343, loss 3.96439e-05, acc 1\n",
      "2018-10-26T17:54:56.277453: step 13344, loss 2.1054e-05, acc 1\n",
      "2018-10-26T17:54:56.635495: step 13345, loss 1.91662e-06, acc 1\n",
      "2018-10-26T17:54:56.986559: step 13346, loss 0.0121132, acc 0.984375\n",
      "2018-10-26T17:54:57.320665: step 13347, loss 1.84023e-06, acc 1\n",
      "2018-10-26T17:54:57.582966: step 13348, loss 1.76126e-05, acc 1\n",
      "2018-10-26T17:54:57.845263: step 13349, loss 0.00506873, acc 1\n",
      "2018-10-26T17:54:58.105567: step 13350, loss 1.11262e-07, acc 1\n",
      "2018-10-26T17:54:58.368864: step 13351, loss 1.34667e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:54:58.639142: step 13352, loss 2.23517e-08, acc 1\n",
      "2018-10-26T17:54:58.897452: step 13353, loss 7.85403e-06, acc 1\n",
      "2018-10-26T17:54:59.157758: step 13354, loss 0.00679396, acc 1\n",
      "2018-10-26T17:54:59.411079: step 13355, loss 9.38766e-07, acc 1\n",
      "2018-10-26T17:54:59.670386: step 13356, loss 1.99091e-05, acc 1\n",
      "2018-10-26T17:54:59.923709: step 13357, loss 4.22025e-06, acc 1\n",
      "2018-10-26T17:55:00.189998: step 13358, loss 7.74414e-05, acc 1\n",
      "2018-10-26T17:55:00.445319: step 13359, loss 4.28406e-07, acc 1\n",
      "2018-10-26T17:55:00.700634: step 13360, loss 0.00113365, acc 1\n",
      "2018-10-26T17:55:00.959941: step 13361, loss 2.92392e-05, acc 1\n",
      "2018-10-26T17:55:01.226229: step 13362, loss 3.14786e-07, acc 1\n",
      "2018-10-26T17:55:01.484539: step 13363, loss 8.03907e-06, acc 1\n",
      "2018-10-26T17:55:01.805682: step 13364, loss 5.30848e-07, acc 1\n",
      "2018-10-26T17:55:02.124828: step 13365, loss 5.65438e-06, acc 1\n",
      "2018-10-26T17:55:02.387128: step 13366, loss 2.06246e-05, acc 1\n",
      "2018-10-26T17:55:02.645437: step 13367, loss 6.18161e-06, acc 1\n",
      "2018-10-26T17:55:02.909731: step 13368, loss 1.26169e-05, acc 1\n",
      "2018-10-26T17:55:03.168041: step 13369, loss 0.00111122, acc 1\n",
      "2018-10-26T17:55:03.425355: step 13370, loss 4.28408e-08, acc 1\n",
      "2018-10-26T17:55:03.682668: step 13371, loss 0.000171277, acc 1\n",
      "2018-10-26T17:55:03.943968: step 13372, loss 0.0002122, acc 1\n",
      "2018-10-26T17:55:04.197291: step 13373, loss 2.6038e-06, acc 1\n",
      "2018-10-26T17:55:04.458594: step 13374, loss 4.54002e-05, acc 1\n",
      "2018-10-26T17:55:04.717900: step 13375, loss 0.000670016, acc 1\n",
      "2018-10-26T17:55:04.971224: step 13376, loss 3.63002e-06, acc 1\n",
      "2018-10-26T17:55:05.227539: step 13377, loss 2.4673e-05, acc 1\n",
      "2018-10-26T17:55:05.488871: step 13378, loss 6.28762e-06, acc 1\n",
      "2018-10-26T17:55:05.744159: step 13379, loss 0.00352798, acc 1\n",
      "2018-10-26T17:55:06.006458: step 13380, loss 2.42142e-07, acc 1\n",
      "2018-10-26T17:55:06.262781: step 13381, loss 6.51046e-06, acc 1\n",
      "2018-10-26T17:55:06.533050: step 13382, loss 8.6144e-06, acc 1\n",
      "2018-10-26T17:55:06.791360: step 13383, loss 0.000761718, acc 1\n",
      "2018-10-26T17:55:07.050668: step 13384, loss 0.000181354, acc 1\n",
      "2018-10-26T17:55:07.307980: step 13385, loss 9.86493e-06, acc 1\n",
      "2018-10-26T17:55:07.574269: step 13386, loss 1.01327e-06, acc 1\n",
      "2018-10-26T17:55:07.832579: step 13387, loss 1.41134e-05, acc 1\n",
      "2018-10-26T17:55:08.091885: step 13388, loss 1.2736e-05, acc 1\n",
      "2018-10-26T17:55:08.346207: step 13389, loss 0.000756832, acc 1\n",
      "2018-10-26T17:55:08.608505: step 13390, loss 0.000204355, acc 1\n",
      "2018-10-26T17:55:08.871805: step 13391, loss 1.38844e-05, acc 1\n",
      "2018-10-26T17:55:09.131109: step 13392, loss 6.62427e-06, acc 1\n",
      "2018-10-26T17:55:09.394405: step 13393, loss 5.26317e-06, acc 1\n",
      "2018-10-26T17:55:09.654718: step 13394, loss 1.35599e-06, acc 1\n",
      "2018-10-26T17:55:09.921995: step 13395, loss 7.95336e-07, acc 1\n",
      "2018-10-26T17:55:10.186289: step 13396, loss 7.32118e-06, acc 1\n",
      "2018-10-26T17:55:10.451581: step 13397, loss 0.000154903, acc 1\n",
      "2018-10-26T17:55:10.713880: step 13398, loss 0.000598754, acc 1\n",
      "2018-10-26T17:55:10.971192: step 13399, loss 7.5875e-06, acc 1\n",
      "2018-10-26T17:55:11.230499: step 13400, loss 9.46704e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:55:11.935616: step 13400, loss 4.61073, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13400\n",
      "\n",
      "2018-10-26T17:55:12.377435: step 13401, loss 1.96317e-06, acc 1\n",
      "2018-10-26T17:55:12.637739: step 13402, loss 7.69908e-05, acc 1\n",
      "2018-10-26T17:55:12.898044: step 13403, loss 0.00470762, acc 1\n",
      "2018-10-26T17:55:13.253096: step 13404, loss 4.5262e-07, acc 1\n",
      "2018-10-26T17:55:13.589198: step 13405, loss 0.000122503, acc 1\n",
      "2018-10-26T17:55:14.003093: step 13406, loss 2.89961e-05, acc 1\n",
      "2018-10-26T17:55:14.389061: step 13407, loss 4.76834e-07, acc 1\n",
      "2018-10-26T17:55:14.761067: step 13408, loss 0.00267788, acc 1\n",
      "2018-10-26T17:55:15.105147: step 13409, loss 8.01453e-06, acc 1\n",
      "2018-10-26T17:55:15.448232: step 13410, loss 8.56058e-05, acc 1\n",
      "2018-10-26T17:55:15.760396: step 13411, loss 3.30786e-06, acc 1\n",
      "2018-10-26T17:55:16.120434: step 13412, loss 0.000238035, acc 1\n",
      "2018-10-26T17:55:16.383733: step 13413, loss 8.98892e-05, acc 1\n",
      "2018-10-26T17:55:16.760725: step 13414, loss 1.2703e-06, acc 1\n",
      "2018-10-26T17:55:17.049951: step 13415, loss 0.00105896, acc 1\n",
      "2018-10-26T17:55:17.373088: step 13416, loss 7.43291e-05, acc 1\n",
      "2018-10-26T17:55:17.686251: step 13417, loss 7.94702e-05, acc 1\n",
      "2018-10-26T17:55:18.090172: step 13418, loss 0.000744665, acc 1\n",
      "2018-10-26T17:55:18.498082: step 13419, loss 6.01784e-05, acc 1\n",
      "2018-10-26T17:55:18.837175: step 13420, loss 0.028475, acc 0.984375\n",
      "2018-10-26T17:55:19.163305: step 13421, loss 1.08033e-07, acc 1\n",
      "2018-10-26T17:55:19.431589: step 13422, loss 2.0414e-06, acc 1\n",
      "2018-10-26T17:55:19.726799: step 13423, loss 4.59251e-05, acc 1\n",
      "2018-10-26T17:55:20.055920: step 13424, loss 8.44951e-05, acc 1\n",
      "2018-10-26T17:55:20.387035: step 13425, loss 0.000401931, acc 1\n",
      "2018-10-26T17:55:20.687232: step 13426, loss 0.035405, acc 0.984375\n",
      "2018-10-26T17:55:20.952523: step 13427, loss 1.94487e-05, acc 1\n",
      "2018-10-26T17:55:21.252721: step 13428, loss 0.00228869, acc 1\n",
      "2018-10-26T17:55:21.521005: step 13429, loss 1.68005e-06, acc 1\n",
      "2018-10-26T17:55:21.827187: step 13430, loss 0.000115372, acc 1\n",
      "2018-10-26T17:55:22.089486: step 13431, loss 3.03969e-06, acc 1\n",
      "2018-10-26T17:55:22.393674: step 13432, loss 2.38518e-05, acc 1\n",
      "2018-10-26T17:55:22.663951: step 13433, loss 7.32135e-05, acc 1\n",
      "2018-10-26T17:55:22.942208: step 13434, loss 6.34297e-05, acc 1\n",
      "2018-10-26T17:55:23.205505: step 13435, loss 1.34589e-05, acc 1\n",
      "2018-10-26T17:55:23.469798: step 13436, loss 6.74951e-06, acc 1\n",
      "2018-10-26T17:55:23.731101: step 13437, loss 4.93595e-07, acc 1\n",
      "2018-10-26T17:55:24.006365: step 13438, loss 0.0115632, acc 0.984375\n",
      "2018-10-26T17:55:24.265673: step 13439, loss 0.000518456, acc 1\n",
      "2018-10-26T17:55:24.532958: step 13440, loss 0.000978795, acc 1\n",
      "2018-10-26T17:55:24.799247: step 13441, loss 8.57308e-06, acc 1\n",
      "2018-10-26T17:55:25.069524: step 13442, loss 3.89812e-06, acc 1\n",
      "2018-10-26T17:55:25.327837: step 13443, loss 0.000104773, acc 1\n",
      "2018-10-26T17:55:25.598112: step 13444, loss 1.13453e-05, acc 1\n",
      "2018-10-26T17:55:25.862405: step 13445, loss 0.000218916, acc 1\n",
      "2018-10-26T17:55:26.124704: step 13446, loss 0.000581217, acc 1\n",
      "2018-10-26T17:55:26.389995: step 13447, loss 8.57663e-05, acc 1\n",
      "2018-10-26T17:55:26.648306: step 13448, loss 0.000330852, acc 1\n",
      "2018-10-26T17:55:26.904621: step 13449, loss 0.000302012, acc 1\n",
      "2018-10-26T17:55:27.178887: step 13450, loss 2.03957e-05, acc 1\n",
      "2018-10-26T17:55:27.440190: step 13451, loss 5.4761e-07, acc 1\n",
      "2018-10-26T17:55:27.710468: step 13452, loss 5.47615e-07, acc 1\n",
      "2018-10-26T17:55:27.971769: step 13453, loss 1.911e-06, acc 1\n",
      "2018-10-26T17:55:28.282938: step 13454, loss 0.00333119, acc 1\n",
      "2018-10-26T17:55:28.538255: step 13455, loss 2.4697e-06, acc 1\n",
      "2018-10-26T17:55:28.799558: step 13456, loss 0.00079859, acc 1\n",
      "2018-10-26T17:55:29.067843: step 13457, loss 1.11759e-08, acc 1\n",
      "2018-10-26T17:55:29.372028: step 13458, loss 8.85303e-05, acc 1\n",
      "2018-10-26T17:55:29.695165: step 13459, loss 9.74139e-06, acc 1\n",
      "2018-10-26T17:55:30.016307: step 13460, loss 2.56305e-05, acc 1\n",
      "2018-10-26T17:55:30.329470: step 13461, loss 5.00057e-06, acc 1\n",
      "2018-10-26T17:55:30.677540: step 13462, loss 1.64281e-06, acc 1\n",
      "2018-10-26T17:55:31.029600: step 13463, loss 5.38213e-06, acc 1\n",
      "2018-10-26T17:55:31.399612: step 13464, loss 1.59608e-05, acc 1\n",
      "2018-10-26T17:55:31.751671: step 13465, loss 7.65541e-07, acc 1\n",
      "2018-10-26T17:55:32.107720: step 13466, loss 2.90377e-06, acc 1\n",
      "2018-10-26T17:55:32.409912: step 13467, loss 2.83101e-06, acc 1\n",
      "2018-10-26T17:55:32.676199: step 13468, loss 0.000113469, acc 1\n",
      "2018-10-26T17:55:32.853725: step 13469, loss 8.51664e-06, acc 1\n",
      "2018-10-26T17:55:33.048206: step 13470, loss 6.11414e-06, acc 1\n",
      "2018-10-26T17:55:33.349402: step 13471, loss 7.22699e-07, acc 1\n",
      "2018-10-26T17:55:33.605719: step 13472, loss 0.000600424, acc 1\n",
      "2018-10-26T17:55:33.826129: step 13473, loss 1.45286e-07, acc 1\n",
      "2018-10-26T17:55:34.085435: step 13474, loss 1.32248e-07, acc 1\n",
      "2018-10-26T17:55:34.307841: step 13475, loss 0.000132931, acc 1\n",
      "2018-10-26T17:55:34.524262: step 13476, loss 2.05523e-05, acc 1\n",
      "2018-10-26T17:55:34.715751: step 13477, loss 0.025294, acc 0.984375\n",
      "2018-10-26T17:55:34.909234: step 13478, loss 9.44178e-06, acc 1\n",
      "2018-10-26T17:55:35.108700: step 13479, loss 1.48263e-06, acc 1\n",
      "2018-10-26T17:55:35.306173: step 13480, loss 2.93097e-05, acc 1\n",
      "2018-10-26T17:55:35.501651: step 13481, loss 0.00223967, acc 1\n",
      "2018-10-26T17:55:35.702116: step 13482, loss 7.92555e-06, acc 1\n",
      "2018-10-26T17:55:35.902579: step 13483, loss 8.69163e-05, acc 1\n",
      "2018-10-26T17:55:36.121994: step 13484, loss 0.108566, acc 0.984375\n",
      "2018-10-26T17:55:36.326447: step 13485, loss 1.41284e-05, acc 1\n",
      "2018-10-26T17:55:36.519930: step 13486, loss 1.16414e-06, acc 1\n",
      "2018-10-26T17:55:36.707429: step 13487, loss 4.2895e-05, acc 1\n",
      "2018-10-26T17:55:36.906896: step 13488, loss 1.54151e-05, acc 1\n",
      "2018-10-26T17:55:37.103371: step 13489, loss 0.000198642, acc 1\n",
      "2018-10-26T17:55:37.298849: step 13490, loss 7.12105e-05, acc 1\n",
      "2018-10-26T17:55:37.505299: step 13491, loss 0.000126866, acc 1\n",
      "2018-10-26T17:55:37.807490: step 13492, loss 3.63204e-05, acc 1\n",
      "2018-10-26T17:55:37.977037: step 13493, loss 4.23714e-05, acc 1\n",
      "2018-10-26T17:55:38.164537: step 13494, loss 0.000110434, acc 1\n",
      "2018-10-26T17:55:38.336077: step 13495, loss 0.000336938, acc 1\n",
      "2018-10-26T17:55:38.528564: step 13496, loss 0.0301274, acc 0.984375\n",
      "2018-10-26T17:55:38.760942: step 13497, loss 8.33174e-05, acc 1\n",
      "2018-10-26T17:55:39.016260: step 13498, loss 0.0805109, acc 0.984375\n",
      "2018-10-26T17:55:39.235674: step 13499, loss 0.000952358, acc 1\n",
      "2018-10-26T17:55:39.488001: step 13500, loss 1.86761e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:55:39.899900: step 13500, loss 4.69429, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13500\n",
      "\n",
      "2018-10-26T17:55:40.250961: step 13501, loss 0.000160914, acc 1\n",
      "2018-10-26T17:55:40.437463: step 13502, loss 6.24477e-06, acc 1\n",
      "2018-10-26T17:55:40.642917: step 13503, loss 3.8929e-07, acc 1\n",
      "2018-10-26T17:55:40.828419: step 13504, loss 2.32494e-05, acc 1\n",
      "2018-10-26T17:55:41.096701: step 13505, loss 3.64721e-05, acc 1\n",
      "2018-10-26T17:55:41.314122: step 13506, loss 0.000378866, acc 1\n",
      "2018-10-26T17:55:41.512590: step 13507, loss 2.53679e-06, acc 1\n",
      "2018-10-26T17:55:41.708068: step 13508, loss 6.3101e-06, acc 1\n",
      "2018-10-26T17:55:41.892574: step 13509, loss 0.00286517, acc 1\n",
      "2018-10-26T17:55:42.069103: step 13510, loss 0.0115819, acc 0.984375\n",
      "2018-10-26T17:55:42.250619: step 13511, loss 1.42862e-06, acc 1\n",
      "2018-10-26T17:55:42.414181: step 13512, loss 0.000464309, acc 1\n",
      "2018-10-26T17:55:42.585722: step 13513, loss 0.00478905, acc 1\n",
      "2018-10-26T17:55:42.746295: step 13514, loss 8.48047e-06, acc 1\n",
      "2018-10-26T17:55:42.914854: step 13515, loss 0.0343451, acc 0.984375\n",
      "2018-10-26T17:55:43.072423: step 13516, loss 6.40537e-05, acc 1\n",
      "2018-10-26T17:55:43.253938: step 13517, loss 9.57988e-05, acc 1\n",
      "2018-10-26T17:55:43.418498: step 13518, loss 1.44166e-06, acc 1\n",
      "2018-10-26T17:55:43.589042: step 13519, loss 3.75576e-05, acc 1\n",
      "2018-10-26T17:55:43.751608: step 13520, loss 5.12221e-07, acc 1\n",
      "2018-10-26T17:55:43.924146: step 13521, loss 1.5646e-05, acc 1\n",
      "2018-10-26T17:55:44.090702: step 13522, loss 8.53333e-06, acc 1\n",
      "2018-10-26T17:55:44.263241: step 13523, loss 1.34497e-05, acc 1\n",
      "2018-10-26T17:55:44.426803: step 13524, loss 4.55313e-05, acc 1\n",
      "2018-10-26T17:55:44.596351: step 13525, loss 1.77503e-06, acc 1\n",
      "2018-10-26T17:55:44.761908: step 13526, loss 0.00019104, acc 1\n",
      "2018-10-26T17:55:44.935444: step 13527, loss 6.13439e-05, acc 1\n",
      "2018-10-26T17:55:45.099008: step 13528, loss 2.21649e-06, acc 1\n",
      "2018-10-26T17:55:45.265563: step 13529, loss 0.000613302, acc 1\n",
      "2018-10-26T17:55:45.463036: step 13530, loss 2.92791e-06, acc 1\n",
      "2018-10-26T17:55:45.648539: step 13531, loss 3.82761e-06, acc 1\n",
      "2018-10-26T17:55:45.823074: step 13532, loss 9.9836e-07, acc 1\n",
      "2018-10-26T17:55:45.990625: step 13533, loss 0.000542409, acc 1\n",
      "2018-10-26T17:55:46.160173: step 13534, loss 1.58696e-06, acc 1\n",
      "2018-10-26T17:55:46.322739: step 13535, loss 1.1967e-05, acc 1\n",
      "2018-10-26T17:55:46.497272: step 13536, loss 2.69023e-05, acc 1\n",
      "2018-10-26T17:55:46.656845: step 13537, loss 9.69174e-06, acc 1\n",
      "2018-10-26T17:55:46.826392: step 13538, loss 5.7742e-08, acc 1\n",
      "2018-10-26T17:55:46.990952: step 13539, loss 2.2121e-05, acc 1\n",
      "2018-10-26T17:55:47.162494: step 13540, loss 0.000288595, acc 1\n",
      "2018-10-26T17:55:47.323066: step 13541, loss 7.68776e-06, acc 1\n",
      "2018-10-26T17:55:47.493609: step 13542, loss 0.000635424, acc 1\n",
      "2018-10-26T17:55:47.657172: step 13543, loss 3.46451e-07, acc 1\n",
      "2018-10-26T17:55:47.827718: step 13544, loss 2.40827e-06, acc 1\n",
      "2018-10-26T17:55:47.985297: step 13545, loss 3.01947e-05, acc 1\n",
      "2018-10-26T17:55:48.161861: step 13546, loss 0.00310866, acc 1\n",
      "2018-10-26T17:55:48.327382: step 13547, loss 2.71299e-05, acc 1\n",
      "2018-10-26T17:55:48.495931: step 13548, loss 7.56824e-05, acc 1\n",
      "2018-10-26T17:55:48.652513: step 13549, loss 0.00093238, acc 1\n",
      "2018-10-26T17:55:48.822060: step 13550, loss 5.42226e-05, acc 1\n",
      "2018-10-26T17:55:48.978642: step 13551, loss 4.41008e-05, acc 1\n",
      "2018-10-26T17:55:49.147192: step 13552, loss 0.000167401, acc 1\n",
      "2018-10-26T17:55:49.305768: step 13553, loss 1.92497e-05, acc 1\n",
      "2018-10-26T17:55:49.478308: step 13554, loss 9.90019e-05, acc 1\n",
      "2018-10-26T17:55:49.633891: step 13555, loss 0.00424755, acc 1\n",
      "2018-10-26T17:55:49.806430: step 13556, loss 1.47149e-07, acc 1\n",
      "2018-10-26T17:55:49.969993: step 13557, loss 0.000130287, acc 1\n",
      "2018-10-26T17:55:50.139541: step 13558, loss 1.29263e-06, acc 1\n",
      "2018-10-26T17:55:50.302105: step 13559, loss 4.15923e-05, acc 1\n",
      "2018-10-26T17:55:50.480629: step 13560, loss 0.000441255, acc 1\n",
      "2018-10-26T17:55:50.642197: step 13561, loss 1.80695e-05, acc 1\n",
      "2018-10-26T17:55:50.821717: step 13562, loss 9.01495e-07, acc 1\n",
      "2018-10-26T17:55:50.986277: step 13563, loss 3.86575e-05, acc 1\n",
      "2018-10-26T17:55:51.155825: step 13564, loss 0.00446165, acc 1\n",
      "2018-10-26T17:55:51.315398: step 13565, loss 4.22241e-06, acc 1\n",
      "2018-10-26T17:55:51.487937: step 13566, loss 7.1944e-06, acc 1\n",
      "2018-10-26T17:55:51.645516: step 13567, loss 5.89776e-05, acc 1\n",
      "2018-10-26T17:55:51.818055: step 13568, loss 2.08616e-07, acc 1\n",
      "2018-10-26T17:55:51.973639: step 13569, loss 2.14384e-06, acc 1\n",
      "2018-10-26T17:55:52.144184: step 13570, loss 1.47146e-06, acc 1\n",
      "2018-10-26T17:55:52.304755: step 13571, loss 1.47518e-06, acc 1\n",
      "2018-10-26T17:55:52.482282: step 13572, loss 3.0923e-05, acc 1\n",
      "2018-10-26T17:55:52.649833: step 13573, loss 1.17564e-05, acc 1\n",
      "2018-10-26T17:55:52.824366: step 13574, loss 0.0544898, acc 0.984375\n",
      "2018-10-26T17:55:52.988928: step 13575, loss 0.00122136, acc 1\n",
      "2018-10-26T17:55:53.173434: step 13576, loss 6.16529e-07, acc 1\n",
      "2018-10-26T17:55:53.338991: step 13577, loss 3.61145e-06, acc 1\n",
      "2018-10-26T17:55:53.517515: step 13578, loss 1.08033e-07, acc 1\n",
      "2018-10-26T17:55:53.678086: step 13579, loss 0.0030344, acc 1\n",
      "2018-10-26T17:55:53.845638: step 13580, loss 0.000351459, acc 1\n",
      "2018-10-26T17:55:54.010199: step 13581, loss 0.000229311, acc 1\n",
      "2018-10-26T17:55:54.192711: step 13582, loss 0.000357112, acc 1\n",
      "2018-10-26T17:55:54.354279: step 13583, loss 6.20884e-06, acc 1\n",
      "2018-10-26T17:55:54.524823: step 13584, loss 0.000203377, acc 1\n",
      "2018-10-26T17:55:54.692376: step 13585, loss 0.000205097, acc 1\n",
      "2018-10-26T17:55:54.870898: step 13586, loss 5.39376e-06, acc 1\n",
      "2018-10-26T17:55:55.029475: step 13587, loss 2.99885e-07, acc 1\n",
      "2018-10-26T17:55:55.210991: step 13588, loss 0.0056044, acc 1\n",
      "2018-10-26T17:55:55.366576: step 13589, loss 4.90244e-05, acc 1\n",
      "2018-10-26T17:55:55.544100: step 13590, loss 5.14084e-07, acc 1\n",
      "2018-10-26T17:55:55.701679: step 13591, loss 6.48672e-06, acc 1\n",
      "2018-10-26T17:55:55.875216: step 13592, loss 0.0490286, acc 0.984375\n",
      "2018-10-26T17:55:56.029802: step 13593, loss 0.000839642, acc 1\n",
      "2018-10-26T17:55:56.201344: step 13594, loss 7.63684e-08, acc 1\n",
      "2018-10-26T17:55:56.358923: step 13595, loss 0.0304774, acc 0.984375\n",
      "2018-10-26T17:55:56.532459: step 13596, loss 3.55568e-06, acc 1\n",
      "2018-10-26T17:55:56.689040: step 13597, loss 6.98487e-07, acc 1\n",
      "2018-10-26T17:55:56.865570: step 13598, loss 5.40295e-05, acc 1\n",
      "2018-10-26T17:55:57.026140: step 13599, loss 4.47035e-08, acc 1\n",
      "2018-10-26T17:55:57.201671: step 13600, loss 0.00113265, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:55:57.615566: step 13600, loss 4.89994, acc 0.709193\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13600\n",
      "\n",
      "2018-10-26T17:55:57.995061: step 13601, loss 0.00052791, acc 1\n",
      "2018-10-26T17:55:58.160619: step 13602, loss 0.000220864, acc 1\n",
      "2018-10-26T17:55:58.323185: step 13603, loss 3.57038e-06, acc 1\n",
      "2018-10-26T17:55:58.492731: step 13604, loss 0.000536335, acc 1\n",
      "2018-10-26T17:55:58.663276: step 13605, loss 0.0470798, acc 0.984375\n",
      "2018-10-26T17:55:58.903634: step 13606, loss 0.000552867, acc 1\n",
      "2018-10-26T17:55:59.062210: step 13607, loss 3.20172e-06, acc 1\n",
      "2018-10-26T17:55:59.229763: step 13608, loss 1.46026e-06, acc 1\n",
      "2018-10-26T17:55:59.393325: step 13609, loss 8.68022e-05, acc 1\n",
      "2018-10-26T17:55:59.560912: step 13610, loss 1.6333e-05, acc 1\n",
      "2018-10-26T17:55:59.722446: step 13611, loss 0.000550113, acc 1\n",
      "2018-10-26T17:55:59.892990: step 13612, loss 0.00296849, acc 1\n",
      "2018-10-26T17:56:00.059545: step 13613, loss 8.83708e-05, acc 1\n",
      "2018-10-26T17:56:00.227098: step 13614, loss 0.00115344, acc 1\n",
      "2018-10-26T17:56:00.390660: step 13615, loss 5.40167e-08, acc 1\n",
      "2018-10-26T17:56:00.560209: step 13616, loss 6.48257e-06, acc 1\n",
      "2018-10-26T17:56:00.724768: step 13617, loss 1.81231e-06, acc 1\n",
      "2018-10-26T17:56:00.893317: step 13618, loss 0.000163424, acc 1\n",
      "2018-10-26T17:56:01.052891: step 13619, loss 5.30851e-07, acc 1\n",
      "2018-10-26T17:56:01.217451: step 13620, loss 4.02031e-05, acc 1\n",
      "2018-10-26T17:56:01.380017: step 13621, loss 6.22721e-05, acc 1\n",
      "2018-10-26T17:56:01.549564: step 13622, loss 2.48641e-05, acc 1\n",
      "2018-10-26T17:56:01.712129: step 13623, loss 0.00189084, acc 1\n",
      "2018-10-26T17:56:01.886663: step 13624, loss 1.13619e-06, acc 1\n",
      "2018-10-26T17:56:02.048232: step 13625, loss 0.00177491, acc 1\n",
      "2018-10-26T17:56:02.224760: step 13626, loss 1.65027e-06, acc 1\n",
      "2018-10-26T17:56:02.385331: step 13627, loss 1.17715e-06, acc 1\n",
      "2018-10-26T17:56:02.553881: step 13628, loss 1.21214e-05, acc 1\n",
      "2018-10-26T17:56:02.712457: step 13629, loss 1.6484e-06, acc 1\n",
      "2018-10-26T17:56:02.884995: step 13630, loss 5.38298e-07, acc 1\n",
      "2018-10-26T17:56:03.040580: step 13631, loss 4.39749e-05, acc 1\n",
      "2018-10-26T17:56:03.215114: step 13632, loss 0.000432456, acc 1\n",
      "2018-10-26T17:56:03.381670: step 13633, loss 1.07395e-05, acc 1\n",
      "2018-10-26T17:56:03.555205: step 13634, loss 0.000806006, acc 1\n",
      "2018-10-26T17:56:03.714779: step 13635, loss 1.28055e-05, acc 1\n",
      "2018-10-26T17:56:03.884326: step 13636, loss 0.000247754, acc 1\n",
      "2018-10-26T17:56:04.040907: step 13637, loss 5.19556e-05, acc 1\n",
      "2018-10-26T17:56:04.209457: step 13638, loss 0, acc 1\n",
      "2018-10-26T17:56:04.370027: step 13639, loss 0.000129409, acc 1\n",
      "2018-10-26T17:56:04.535586: step 13640, loss 1.23489e-06, acc 1\n",
      "2018-10-26T17:56:04.694162: step 13641, loss 1.58879e-06, acc 1\n",
      "2018-10-26T17:56:04.865704: step 13642, loss 7.82524e-05, acc 1\n",
      "2018-10-26T17:56:05.026275: step 13643, loss 0.000102429, acc 1\n",
      "2018-10-26T17:56:05.196819: step 13644, loss 1.41419e-05, acc 1\n",
      "2018-10-26T17:56:05.351406: step 13645, loss 5.03428e-05, acc 1\n",
      "2018-10-26T17:56:05.527936: step 13646, loss 2.14754e-06, acc 1\n",
      "2018-10-26T17:56:05.693492: step 13647, loss 5.55064e-07, acc 1\n",
      "2018-10-26T17:56:05.872015: step 13648, loss 0.0111424, acc 0.984375\n",
      "2018-10-26T17:56:06.032587: step 13649, loss 0.000381948, acc 1\n",
      "2018-10-26T17:56:06.198143: step 13650, loss 1.06006e-05, acc 1\n",
      "2018-10-26T17:56:06.354726: step 13651, loss 2.01719e-06, acc 1\n",
      "2018-10-26T17:56:06.533249: step 13652, loss 7.50907e-06, acc 1\n",
      "2018-10-26T17:56:06.692822: step 13653, loss 0.000296322, acc 1\n",
      "2018-10-26T17:56:06.859377: step 13654, loss 1.46038e-05, acc 1\n",
      "2018-10-26T17:56:07.017953: step 13655, loss 0.0020832, acc 1\n",
      "2018-10-26T17:56:07.192486: step 13656, loss 2.53106e-05, acc 1\n",
      "2018-10-26T17:56:07.352061: step 13657, loss 3.76594e-05, acc 1\n",
      "2018-10-26T17:56:07.528588: step 13658, loss 1.31126e-06, acc 1\n",
      "2018-10-26T17:56:07.689160: step 13659, loss 0.000120815, acc 1\n",
      "2018-10-26T17:56:07.857710: step 13660, loss 0.0331816, acc 0.984375\n",
      "2018-10-26T17:56:08.027257: step 13661, loss 3.30373e-05, acc 1\n",
      "2018-10-26T17:56:08.204783: step 13662, loss 0.000541357, acc 1\n",
      "2018-10-26T17:56:08.369343: step 13663, loss 0.00486031, acc 1\n",
      "2018-10-26T17:56:08.551891: step 13664, loss 3.87426e-07, acc 1\n",
      "2018-10-26T17:56:08.713423: step 13665, loss 0.000375292, acc 1\n",
      "2018-10-26T17:56:08.889951: step 13666, loss 1.04493e-06, acc 1\n",
      "2018-10-26T17:56:09.050523: step 13667, loss 2.33494e-05, acc 1\n",
      "2018-10-26T17:56:09.218075: step 13668, loss 0.000142898, acc 1\n",
      "2018-10-26T17:56:09.379644: step 13669, loss 3.36379e-05, acc 1\n",
      "2018-10-26T17:56:09.549190: step 13670, loss 0.000300624, acc 1\n",
      "2018-10-26T17:56:09.707767: step 13671, loss 0.000494229, acc 1\n",
      "2018-10-26T17:56:09.875318: step 13672, loss 2.45522e-05, acc 1\n",
      "2018-10-26T17:56:10.043869: step 13673, loss 1.58503e-06, acc 1\n",
      "2018-10-26T17:56:10.215411: step 13674, loss 0.00012308, acc 1\n",
      "2018-10-26T17:56:10.371991: step 13675, loss 9.26393e-05, acc 1\n",
      "2018-10-26T17:56:10.544531: step 13676, loss 3.7439e-07, acc 1\n",
      "2018-10-26T17:56:10.701113: step 13677, loss 2.25408e-05, acc 1\n",
      "2018-10-26T17:56:10.875646: step 13678, loss 2.43747e-05, acc 1\n",
      "2018-10-26T17:56:11.040206: step 13679, loss 5.0803e-05, acc 1\n",
      "2018-10-26T17:56:11.216735: step 13680, loss 5.3641e-06, acc 1\n",
      "2018-10-26T17:56:11.383290: step 13681, loss 7.50633e-07, acc 1\n",
      "2018-10-26T17:56:11.554831: step 13682, loss 5.15736e-06, acc 1\n",
      "2018-10-26T17:56:11.710416: step 13683, loss 0.000502296, acc 1\n",
      "2018-10-26T17:56:11.885947: step 13684, loss 6.7799e-07, acc 1\n",
      "2018-10-26T17:56:12.043525: step 13685, loss 0.000106436, acc 1\n",
      "2018-10-26T17:56:12.216065: step 13686, loss 0.000102127, acc 1\n",
      "2018-10-26T17:56:12.375638: step 13687, loss 0.000147935, acc 1\n",
      "2018-10-26T17:56:12.544189: step 13688, loss 0.00981015, acc 1\n",
      "2018-10-26T17:56:12.700769: step 13689, loss 9.76004e-07, acc 1\n",
      "2018-10-26T17:56:12.874307: step 13690, loss 0.000929803, acc 1\n",
      "2018-10-26T17:56:13.031916: step 13691, loss 2.47728e-05, acc 1\n",
      "2018-10-26T17:56:13.206419: step 13692, loss 0.000126102, acc 1\n",
      "2018-10-26T17:56:13.362003: step 13693, loss 4.49822e-05, acc 1\n",
      "2018-10-26T17:56:13.537534: step 13694, loss 4.99182e-07, acc 1\n",
      "2018-10-26T17:56:13.693118: step 13695, loss 4.76954e-05, acc 1\n",
      "2018-10-26T17:56:13.866655: step 13696, loss 0.000164337, acc 1\n",
      "2018-10-26T17:56:14.026228: step 13697, loss 0.000104651, acc 1\n",
      "2018-10-26T17:56:14.194777: step 13698, loss 1.43423e-07, acc 1\n",
      "2018-10-26T17:56:14.354352: step 13699, loss 3.27823e-07, acc 1\n",
      "2018-10-26T17:56:14.527887: step 13700, loss 3.41391e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:56:14.936795: step 13700, loss 4.85224, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13700\n",
      "\n",
      "2018-10-26T17:56:15.251473: step 13701, loss 0.000269919, acc 1\n",
      "2018-10-26T17:56:15.421019: step 13702, loss 0.000143907, acc 1\n",
      "2018-10-26T17:56:15.593559: step 13703, loss 1.08033e-07, acc 1\n",
      "2018-10-26T17:56:15.753133: step 13704, loss 1.08145e-05, acc 1\n",
      "2018-10-26T17:56:15.926668: step 13705, loss 3.67305e-06, acc 1\n",
      "2018-10-26T17:56:16.161044: step 13706, loss 1.6164e-05, acc 1\n",
      "2018-10-26T17:56:16.322611: step 13707, loss 6.21843e-06, acc 1\n",
      "2018-10-26T17:56:16.496146: step 13708, loss 1.51615e-06, acc 1\n",
      "2018-10-26T17:56:16.661704: step 13709, loss 4.39938e-06, acc 1\n",
      "2018-10-26T17:56:16.843220: step 13710, loss 7.32045e-05, acc 1\n",
      "2018-10-26T17:56:17.004788: step 13711, loss 1.5837e-05, acc 1\n",
      "2018-10-26T17:56:17.179321: step 13712, loss 7.86024e-07, acc 1\n",
      "2018-10-26T17:56:17.343882: step 13713, loss 0.000907607, acc 1\n",
      "2018-10-26T17:56:17.517418: step 13714, loss 1.17346e-07, acc 1\n",
      "2018-10-26T17:56:17.681978: step 13715, loss 4.28275e-05, acc 1\n",
      "2018-10-26T17:56:17.852524: step 13716, loss 8.16437e-05, acc 1\n",
      "2018-10-26T17:56:18.007110: step 13717, loss 2.13259e-06, acc 1\n",
      "2018-10-26T17:56:18.184636: step 13718, loss 5.39639e-05, acc 1\n",
      "2018-10-26T17:56:18.345207: step 13719, loss 2.82055e-05, acc 1\n",
      "2018-10-26T17:56:18.518743: step 13720, loss 6.6371e-06, acc 1\n",
      "2018-10-26T17:56:18.687292: step 13721, loss 9.05232e-07, acc 1\n",
      "2018-10-26T17:56:18.860829: step 13722, loss 0.000220134, acc 1\n",
      "2018-10-26T17:56:19.025389: step 13723, loss 0.00437276, acc 1\n",
      "2018-10-26T17:56:19.226850: step 13724, loss 0.0016737, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:56:19.433300: step 13725, loss 5.31752e-05, acc 1\n",
      "2018-10-26T17:56:19.664680: step 13726, loss 1.01325e-06, acc 1\n",
      "2018-10-26T17:56:19.885092: step 13727, loss 0.0012285, acc 1\n",
      "2018-10-26T17:56:20.099520: step 13728, loss 1.86264e-08, acc 1\n",
      "2018-10-26T17:56:20.336885: step 13729, loss 1.75088e-07, acc 1\n",
      "2018-10-26T17:56:20.552310: step 13730, loss 1.81266e-05, acc 1\n",
      "2018-10-26T17:56:20.788678: step 13731, loss 1.01895e-05, acc 1\n",
      "2018-10-26T17:56:21.019063: step 13732, loss 1.04865e-06, acc 1\n",
      "2018-10-26T17:56:21.241469: step 13733, loss 3.24098e-07, acc 1\n",
      "2018-10-26T17:56:21.468862: step 13734, loss 4.38407e-06, acc 1\n",
      "2018-10-26T17:56:21.706228: step 13735, loss 1.74588e-05, acc 1\n",
      "2018-10-26T17:56:21.921652: step 13736, loss 5.76762e-06, acc 1\n",
      "2018-10-26T17:56:22.139072: step 13737, loss 6.03847e-05, acc 1\n",
      "2018-10-26T17:56:22.330558: step 13738, loss 3.74389e-07, acc 1\n",
      "2018-10-26T17:56:22.581887: step 13739, loss 0.000161842, acc 1\n",
      "2018-10-26T17:56:22.769386: step 13740, loss 0.000877601, acc 1\n",
      "2018-10-26T17:56:22.977830: step 13741, loss 3.4996e-06, acc 1\n",
      "2018-10-26T17:56:23.198239: step 13742, loss 5.22295e-05, acc 1\n",
      "2018-10-26T17:56:23.403691: step 13743, loss 2.60769e-07, acc 1\n",
      "2018-10-26T17:56:23.627095: step 13744, loss 0.000556274, acc 1\n",
      "2018-10-26T17:56:23.801627: step 13745, loss 0.00014748, acc 1\n",
      "2018-10-26T17:56:24.058940: step 13746, loss 0.000127128, acc 1\n",
      "2018-10-26T17:56:24.283341: step 13747, loss 1.04815e-05, acc 1\n",
      "2018-10-26T17:56:24.453884: step 13748, loss 7.97197e-07, acc 1\n",
      "2018-10-26T17:56:24.644375: step 13749, loss 1.81925e-05, acc 1\n",
      "2018-10-26T17:56:24.811928: step 13750, loss 1.99764e-05, acc 1\n",
      "2018-10-26T17:56:24.981476: step 13751, loss 0.00106533, acc 1\n",
      "2018-10-26T17:56:25.175956: step 13752, loss 0.000291516, acc 1\n",
      "2018-10-26T17:56:25.352483: step 13753, loss 1.92402e-06, acc 1\n",
      "2018-10-26T17:56:25.528015: step 13754, loss 6.13305e-05, acc 1\n",
      "2018-10-26T17:56:25.722495: step 13755, loss 6.5668e-05, acc 1\n",
      "2018-10-26T17:56:25.896032: step 13756, loss 1.86264e-08, acc 1\n",
      "2018-10-26T17:56:26.064581: step 13757, loss 6.04291e-05, acc 1\n",
      "2018-10-26T17:56:26.252080: step 13758, loss 7.36776e-06, acc 1\n",
      "2018-10-26T17:56:26.426615: step 13759, loss 8.32582e-07, acc 1\n",
      "2018-10-26T17:56:26.593169: step 13760, loss 3.31548e-07, acc 1\n",
      "2018-10-26T17:56:26.752743: step 13761, loss 0.000763684, acc 1\n",
      "2018-10-26T17:56:26.943233: step 13762, loss 0.00108447, acc 1\n",
      "2018-10-26T17:56:27.107794: step 13763, loss 0.00154359, acc 1\n",
      "2018-10-26T17:56:27.279335: step 13764, loss 6.34999e-05, acc 1\n",
      "2018-10-26T17:56:27.462847: step 13765, loss 4.20954e-07, acc 1\n",
      "2018-10-26T17:56:27.675278: step 13766, loss 0.000373492, acc 1\n",
      "2018-10-26T17:56:27.849812: step 13767, loss 1.02445e-07, acc 1\n",
      "2018-10-26T17:56:28.031327: step 13768, loss 0.0036077, acc 1\n",
      "2018-10-26T17:56:28.193892: step 13769, loss 1.09938e-05, acc 1\n",
      "2018-10-26T17:56:28.363439: step 13770, loss 0.0201686, acc 0.984375\n",
      "2018-10-26T17:56:28.549947: step 13771, loss 1.19209e-07, acc 1\n",
      "2018-10-26T17:56:28.762373: step 13772, loss 0.00101175, acc 1\n",
      "2018-10-26T17:56:28.949873: step 13773, loss 1.42948e-05, acc 1\n",
      "2018-10-26T17:56:29.123409: step 13774, loss 9.96508e-07, acc 1\n",
      "2018-10-26T17:56:29.292956: step 13775, loss 0.000704821, acc 1\n",
      "2018-10-26T17:56:29.453526: step 13776, loss 0.00222504, acc 1\n",
      "2018-10-26T17:56:29.619085: step 13777, loss 0.000152885, acc 1\n",
      "2018-10-26T17:56:29.787634: step 13778, loss 7.38188e-05, acc 1\n",
      "2018-10-26T17:56:29.959176: step 13779, loss 0.00523127, acc 1\n",
      "2018-10-26T17:56:30.122739: step 13780, loss 5.96046e-08, acc 1\n",
      "2018-10-26T17:56:30.296275: step 13781, loss 0.00417534, acc 1\n",
      "2018-10-26T17:56:30.460835: step 13782, loss 5.83702e-06, acc 1\n",
      "2018-10-26T17:56:30.635369: step 13783, loss 0.0382315, acc 0.984375\n",
      "2018-10-26T17:56:30.797935: step 13784, loss 0.000202745, acc 1\n",
      "2018-10-26T17:56:30.972468: step 13785, loss 0.000683123, acc 1\n",
      "2018-10-26T17:56:31.147002: step 13786, loss 5.12223e-07, acc 1\n",
      "2018-10-26T17:56:31.317546: step 13787, loss 8.19562e-08, acc 1\n",
      "2018-10-26T17:56:31.475125: step 13788, loss 0.000933459, acc 1\n",
      "2018-10-26T17:56:31.639686: step 13789, loss 3.31549e-07, acc 1\n",
      "2018-10-26T17:56:31.807238: step 13790, loss 1.44926e-05, acc 1\n",
      "2018-10-26T17:56:31.973793: step 13791, loss 0.00059896, acc 1\n",
      "2018-10-26T17:56:32.129377: step 13792, loss 3.8929e-07, acc 1\n",
      "2018-10-26T17:56:32.293966: step 13793, loss 5.43662e-06, acc 1\n",
      "2018-10-26T17:56:32.451517: step 13794, loss 2.41876e-05, acc 1\n",
      "2018-10-26T17:56:32.628045: step 13795, loss 0.000463512, acc 1\n",
      "2018-10-26T17:56:32.790610: step 13796, loss 0.000203336, acc 1\n",
      "2018-10-26T17:56:32.963150: step 13797, loss 3.03537e-05, acc 1\n",
      "2018-10-26T17:56:33.116739: step 13798, loss 1.67261e-06, acc 1\n",
      "2018-10-26T17:56:33.283294: step 13799, loss 1.6465e-05, acc 1\n",
      "2018-10-26T17:56:33.437882: step 13800, loss 0.00774518, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:56:33.845791: step 13800, loss 4.91224, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13800\n",
      "\n",
      "2018-10-26T17:56:34.205798: step 13801, loss 5.82574e-06, acc 1\n",
      "2018-10-26T17:56:34.375343: step 13802, loss 0.000587081, acc 1\n",
      "2018-10-26T17:56:34.540901: step 13803, loss 0.000111705, acc 1\n",
      "2018-10-26T17:56:34.718426: step 13804, loss 0.000215618, acc 1\n",
      "2018-10-26T17:56:34.893959: step 13805, loss 8.4903e-05, acc 1\n",
      "2018-10-26T17:56:35.129329: step 13806, loss 6.67473e-06, acc 1\n",
      "2018-10-26T17:56:35.287906: step 13807, loss 1.57018e-06, acc 1\n",
      "2018-10-26T17:56:35.462439: step 13808, loss 0.00010827, acc 1\n",
      "2018-10-26T17:56:35.621015: step 13809, loss 0.000122554, acc 1\n",
      "2018-10-26T17:56:35.796546: step 13810, loss 0.000791538, acc 1\n",
      "2018-10-26T17:56:35.958114: step 13811, loss 3.21096e-05, acc 1\n",
      "2018-10-26T17:56:36.126664: step 13812, loss 2.93916e-06, acc 1\n",
      "2018-10-26T17:56:36.286238: step 13813, loss 0.000753466, acc 1\n",
      "2018-10-26T17:56:36.459775: step 13814, loss 0.0567268, acc 0.984375\n",
      "2018-10-26T17:56:36.616356: step 13815, loss 0.000507135, acc 1\n",
      "2018-10-26T17:56:36.786900: step 13816, loss 0.000134585, acc 1\n",
      "2018-10-26T17:56:36.949466: step 13817, loss 6.62407e-06, acc 1\n",
      "2018-10-26T17:56:37.119022: step 13818, loss 1.39693e-06, acc 1\n",
      "2018-10-26T17:56:37.274597: step 13819, loss 2.79006e-06, acc 1\n",
      "2018-10-26T17:56:37.451126: step 13820, loss 0.00115803, acc 1\n",
      "2018-10-26T17:56:37.609702: step 13821, loss 0.000138544, acc 1\n",
      "2018-10-26T17:56:37.783238: step 13822, loss 0.000841171, acc 1\n",
      "2018-10-26T17:56:37.942811: step 13823, loss 7.84164e-07, acc 1\n",
      "2018-10-26T17:56:38.117345: step 13824, loss 1.56598e-05, acc 1\n",
      "2018-10-26T17:56:38.280909: step 13825, loss 6.31885e-05, acc 1\n",
      "2018-10-26T17:56:38.454444: step 13826, loss 6.89178e-08, acc 1\n",
      "2018-10-26T17:56:38.619005: step 13827, loss 0.000344985, acc 1\n",
      "2018-10-26T17:56:38.799523: step 13828, loss 2.04141e-06, acc 1\n",
      "2018-10-26T17:56:38.956105: step 13829, loss 0.000934689, acc 1\n",
      "2018-10-26T17:56:39.122659: step 13830, loss 0.000190445, acc 1\n",
      "2018-10-26T17:56:39.279241: step 13831, loss 1.13439e-05, acc 1\n",
      "2018-10-26T17:56:39.459758: step 13832, loss 1.96501e-06, acc 1\n",
      "2018-10-26T17:56:39.622324: step 13833, loss 0.0122504, acc 1\n",
      "2018-10-26T17:56:39.793867: step 13834, loss 0.000175229, acc 1\n",
      "2018-10-26T17:56:39.948453: step 13835, loss 2.00966e-05, acc 1\n",
      "2018-10-26T17:56:40.123984: step 13836, loss 0.000341645, acc 1\n",
      "2018-10-26T17:56:40.282561: step 13837, loss 9.43065e-05, acc 1\n",
      "2018-10-26T17:56:40.457094: step 13838, loss 0.00214291, acc 1\n",
      "2018-10-26T17:56:40.617665: step 13839, loss 2.00537e-05, acc 1\n",
      "2018-10-26T17:56:40.794194: step 13840, loss 2.36496e-05, acc 1\n",
      "2018-10-26T17:56:40.951772: step 13841, loss 8.17692e-07, acc 1\n",
      "2018-10-26T17:56:41.126307: step 13842, loss 0.000205168, acc 1\n",
      "2018-10-26T17:56:41.289869: step 13843, loss 2.79687e-05, acc 1\n",
      "2018-10-26T17:56:41.467394: step 13844, loss 2.56229e-05, acc 1\n",
      "2018-10-26T17:56:41.632953: step 13845, loss 0.00113302, acc 1\n",
      "2018-10-26T17:56:41.805491: step 13846, loss 0.000192967, acc 1\n",
      "2018-10-26T17:56:41.965066: step 13847, loss 2.76769e-06, acc 1\n",
      "2018-10-26T17:56:42.137604: step 13848, loss 1.49011e-07, acc 1\n",
      "2018-10-26T17:56:42.307151: step 13849, loss 0.00325208, acc 1\n",
      "2018-10-26T17:56:42.481685: step 13850, loss 1.75088e-07, acc 1\n",
      "2018-10-26T17:56:42.642256: step 13851, loss 1.35785e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:56:42.817786: step 13852, loss 0.000317654, acc 1\n",
      "2018-10-26T17:56:42.977360: step 13853, loss 7.09204e-05, acc 1\n",
      "2018-10-26T17:56:43.151893: step 13854, loss 6.63218e-06, acc 1\n",
      "2018-10-26T17:56:43.308476: step 13855, loss 1.40998e-06, acc 1\n",
      "2018-10-26T17:56:43.475030: step 13856, loss 1.62787e-06, acc 1\n",
      "2018-10-26T17:56:43.635601: step 13857, loss 0.00738544, acc 1\n",
      "2018-10-26T17:56:43.808141: step 13858, loss 4.47035e-08, acc 1\n",
      "2018-10-26T17:56:43.965720: step 13859, loss 4.77704e-05, acc 1\n",
      "2018-10-26T17:56:44.130279: step 13860, loss 2.9055e-06, acc 1\n",
      "2018-10-26T17:56:44.291847: step 13861, loss 6.4623e-05, acc 1\n",
      "2018-10-26T17:56:44.470371: step 13862, loss 4.2468e-07, acc 1\n",
      "2018-10-26T17:56:44.634931: step 13863, loss 6.56337e-05, acc 1\n",
      "2018-10-26T17:56:44.802484: step 13864, loss 0.00229833, acc 1\n",
      "2018-10-26T17:56:44.958068: step 13865, loss 1.4699e-05, acc 1\n",
      "2018-10-26T17:56:45.131605: step 13866, loss 1.39976e-05, acc 1\n",
      "2018-10-26T17:56:45.285194: step 13867, loss 1.35769e-05, acc 1\n",
      "2018-10-26T17:56:45.458730: step 13868, loss 0.000507278, acc 1\n",
      "2018-10-26T17:56:45.618304: step 13869, loss 0.00011742, acc 1\n",
      "2018-10-26T17:56:45.783862: step 13870, loss 1.80676e-07, acc 1\n",
      "2018-10-26T17:56:45.947424: step 13871, loss 0.00244467, acc 1\n",
      "2018-10-26T17:56:46.119964: step 13872, loss 6.37199e-05, acc 1\n",
      "2018-10-26T17:56:46.277542: step 13873, loss 9.35041e-07, acc 1\n",
      "2018-10-26T17:56:46.448087: step 13874, loss 0.000588438, acc 1\n",
      "2018-10-26T17:56:46.603671: step 13875, loss 3.40219e-05, acc 1\n",
      "2018-10-26T17:56:46.769230: step 13876, loss 8.19563e-08, acc 1\n",
      "2018-10-26T17:56:46.923816: step 13877, loss 2.4028e-07, acc 1\n",
      "2018-10-26T17:56:47.091369: step 13878, loss 2.55722e-06, acc 1\n",
      "2018-10-26T17:56:47.253934: step 13879, loss 1.24793e-06, acc 1\n",
      "2018-10-26T17:56:47.424478: step 13880, loss 0.000295405, acc 1\n",
      "2018-10-26T17:56:47.590041: step 13881, loss 1.49566e-06, acc 1\n",
      "2018-10-26T17:56:47.759583: step 13882, loss 2.68198e-06, acc 1\n",
      "2018-10-26T17:56:47.951072: step 13883, loss 2.90002e-06, acc 1\n",
      "2018-10-26T17:56:48.154527: step 13884, loss 1.08524e-05, acc 1\n",
      "2018-10-26T17:56:48.318091: step 13885, loss 3.95789e-06, acc 1\n",
      "2018-10-26T17:56:48.494619: step 13886, loss 1.99522e-05, acc 1\n",
      "2018-10-26T17:56:48.656187: step 13887, loss 5.55947e-06, acc 1\n",
      "2018-10-26T17:56:48.832716: step 13888, loss 0.00498899, acc 1\n",
      "2018-10-26T17:56:48.996278: step 13889, loss 0.000526954, acc 1\n",
      "2018-10-26T17:56:49.171809: step 13890, loss 3.20374e-07, acc 1\n",
      "2018-10-26T17:56:49.330386: step 13891, loss 9.82771e-05, acc 1\n",
      "2018-10-26T17:56:49.497938: step 13892, loss 8.75425e-07, acc 1\n",
      "2018-10-26T17:56:49.658509: step 13893, loss 1.85045e-05, acc 1\n",
      "2018-10-26T17:56:49.829053: step 13894, loss 2.56364e-05, acc 1\n",
      "2018-10-26T17:56:49.988627: step 13895, loss 0.121367, acc 0.984375\n",
      "2018-10-26T17:56:50.159171: step 13896, loss 1.03697e-05, acc 1\n",
      "2018-10-26T17:56:50.317748: step 13897, loss 0.000142501, acc 1\n",
      "2018-10-26T17:56:50.493279: step 13898, loss 6.21894e-05, acc 1\n",
      "2018-10-26T17:56:50.652853: step 13899, loss 1.19209e-07, acc 1\n",
      "2018-10-26T17:56:50.818410: step 13900, loss 1.67071e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:56:51.229313: step 13900, loss 4.93313, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-13900\n",
      "\n",
      "2018-10-26T17:56:51.605504: step 13901, loss 9.262e-06, acc 1\n",
      "2018-10-26T17:56:51.766074: step 13902, loss 4.32129e-07, acc 1\n",
      "2018-10-26T17:56:51.932629: step 13903, loss 1.70988e-06, acc 1\n",
      "2018-10-26T17:56:52.089211: step 13904, loss 4.9334e-06, acc 1\n",
      "2018-10-26T17:56:52.270726: step 13905, loss 0.000180679, acc 1\n",
      "2018-10-26T17:56:52.510088: step 13906, loss 0.208297, acc 0.984375\n",
      "2018-10-26T17:56:52.667666: step 13907, loss 5.80723e-06, acc 1\n",
      "2018-10-26T17:56:52.841203: step 13908, loss 2.08049e-06, acc 1\n",
      "2018-10-26T17:56:53.008754: step 13909, loss 0.000932817, acc 1\n",
      "2018-10-26T17:56:53.174313: step 13910, loss 0.000269667, acc 1\n",
      "2018-10-26T17:56:53.336878: step 13911, loss 3.41954e-06, acc 1\n",
      "2018-10-26T17:56:53.514404: step 13912, loss 4.84287e-08, acc 1\n",
      "2018-10-26T17:56:53.674975: step 13913, loss 5.07879e-06, acc 1\n",
      "2018-10-26T17:56:53.845519: step 13914, loss 1.97446e-05, acc 1\n",
      "2018-10-26T17:56:54.010080: step 13915, loss 8.04644e-07, acc 1\n",
      "2018-10-26T17:56:54.182619: step 13916, loss 2.45868e-07, acc 1\n",
      "2018-10-26T17:56:54.341195: step 13917, loss 2.21458e-06, acc 1\n",
      "2018-10-26T17:56:54.511739: step 13918, loss 0.000177877, acc 1\n",
      "2018-10-26T17:56:54.667324: step 13919, loss 0.000359103, acc 1\n",
      "2018-10-26T17:56:54.841858: step 13920, loss 5.27421e-05, acc 1\n",
      "2018-10-26T17:56:55.008412: step 13921, loss 9.54313e-06, acc 1\n",
      "2018-10-26T17:56:55.176962: step 13922, loss 2.26725e-05, acc 1\n",
      "2018-10-26T17:56:55.338530: step 13923, loss 3.30561e-05, acc 1\n",
      "2018-10-26T17:56:55.520082: step 13924, loss 9.94784e-06, acc 1\n",
      "2018-10-26T17:56:55.683608: step 13925, loss 1.48609e-05, acc 1\n",
      "2018-10-26T17:56:55.859139: step 13926, loss 7.46817e-06, acc 1\n",
      "2018-10-26T17:56:56.027688: step 13927, loss 0.00020113, acc 1\n",
      "2018-10-26T17:56:56.209203: step 13928, loss 7.1981e-06, acc 1\n",
      "2018-10-26T17:56:56.373764: step 13929, loss 0.000122122, acc 1\n",
      "2018-10-26T17:56:56.549294: step 13930, loss 9.77538e-05, acc 1\n",
      "2018-10-26T17:56:56.710863: step 13931, loss 3.61274e-05, acc 1\n",
      "2018-10-26T17:56:56.884400: step 13932, loss 6.90865e-05, acc 1\n",
      "2018-10-26T17:56:57.046965: step 13933, loss 9.20213e-05, acc 1\n",
      "2018-10-26T17:56:57.236459: step 13934, loss 1.66703e-06, acc 1\n",
      "2018-10-26T17:56:57.404011: step 13935, loss 8.79153e-07, acc 1\n",
      "2018-10-26T17:56:57.575552: step 13936, loss 1.695e-07, acc 1\n",
      "2018-10-26T17:56:57.735127: step 13937, loss 4.04142e-06, acc 1\n",
      "2018-10-26T17:56:57.913650: step 13938, loss 9.42216e-06, acc 1\n",
      "2018-10-26T17:56:58.075219: step 13939, loss 3.81363e-05, acc 1\n",
      "2018-10-26T17:56:58.249751: step 13940, loss 0.000298949, acc 1\n",
      "2018-10-26T17:56:58.410323: step 13941, loss 2.05766e-05, acc 1\n",
      "2018-10-26T17:56:58.581864: step 13942, loss 3.11219e-06, acc 1\n",
      "2018-10-26T17:56:58.736452: step 13943, loss 0.00846883, acc 1\n",
      "2018-10-26T17:56:58.913976: step 13944, loss 9.26596e-05, acc 1\n",
      "2018-10-26T17:56:59.073550: step 13945, loss 1.93121e-05, acc 1\n",
      "2018-10-26T17:56:59.249082: step 13946, loss 7.07804e-08, acc 1\n",
      "2018-10-26T17:56:59.410650: step 13947, loss 0.00026471, acc 1\n",
      "2018-10-26T17:56:59.581194: step 13948, loss 3.07334e-07, acc 1\n",
      "2018-10-26T17:56:59.760716: step 13949, loss 0.0200419, acc 0.984375\n",
      "2018-10-26T17:56:59.935248: step 13950, loss 3.5119e-05, acc 1\n",
      "2018-10-26T17:57:00.114768: step 13951, loss 3.03177e-05, acc 1\n",
      "2018-10-26T17:57:00.278331: step 13952, loss 4.23763e-05, acc 1\n",
      "2018-10-26T17:57:00.447878: step 13953, loss 4.99328e-06, acc 1\n",
      "2018-10-26T17:57:00.610444: step 13954, loss 2.1233e-06, acc 1\n",
      "2018-10-26T17:57:00.783981: step 13955, loss 7.45058e-09, acc 1\n",
      "2018-10-26T17:57:00.953527: step 13956, loss 3.05674e-05, acc 1\n",
      "2018-10-26T17:57:01.124071: step 13957, loss 8.6267e-05, acc 1\n",
      "2018-10-26T17:57:01.292623: step 13958, loss 9.68574e-08, acc 1\n",
      "2018-10-26T17:57:01.461171: step 13959, loss 0.000126053, acc 1\n",
      "2018-10-26T17:57:01.617753: step 13960, loss 1.15557e-05, acc 1\n",
      "2018-10-26T17:57:01.792286: step 13961, loss 5.46809e-06, acc 1\n",
      "2018-10-26T17:57:01.949866: step 13962, loss 0.000452563, acc 1\n",
      "2018-10-26T17:57:02.125396: step 13963, loss 7.01058e-05, acc 1\n",
      "2018-10-26T17:57:02.284970: step 13964, loss 0.00070984, acc 1\n",
      "2018-10-26T17:57:02.461499: step 13965, loss 0.000108366, acc 1\n",
      "2018-10-26T17:57:02.627075: step 13966, loss 1.54594e-06, acc 1\n",
      "2018-10-26T17:57:02.803585: step 13967, loss 2.32829e-07, acc 1\n",
      "2018-10-26T17:57:02.961163: step 13968, loss 6.35377e-05, acc 1\n",
      "2018-10-26T17:57:03.135697: step 13969, loss 3.14525e-05, acc 1\n",
      "2018-10-26T17:57:03.293276: step 13970, loss 2.30907e-05, acc 1\n",
      "2018-10-26T17:57:03.468806: step 13971, loss 0.000121432, acc 1\n",
      "2018-10-26T17:57:03.625389: step 13972, loss 9.01912e-05, acc 1\n",
      "2018-10-26T17:57:03.796930: step 13973, loss 0.0755845, acc 0.984375\n",
      "2018-10-26T17:57:03.958498: step 13974, loss 0.000339624, acc 1\n",
      "2018-10-26T17:57:04.133032: step 13975, loss 3.20102e-05, acc 1\n",
      "2018-10-26T17:57:04.300584: step 13976, loss 3.1756e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:57:04.476116: step 13977, loss 1.64088e-05, acc 1\n",
      "2018-10-26T17:57:04.638682: step 13978, loss 0.0853921, acc 0.984375\n",
      "2018-10-26T17:57:04.805236: step 13979, loss 8.86494e-06, acc 1\n",
      "2018-10-26T17:57:04.961818: step 13980, loss 1.423e-06, acc 1\n",
      "2018-10-26T17:57:05.127376: step 13981, loss 4.96884e-06, acc 1\n",
      "2018-10-26T17:57:05.291935: step 13982, loss 0.00164115, acc 1\n",
      "2018-10-26T17:57:05.462481: step 13983, loss 2.50006e-05, acc 1\n",
      "2018-10-26T17:57:05.628050: step 13984, loss 2.23365e-05, acc 1\n",
      "2018-10-26T17:57:05.798582: step 13985, loss 0.00342878, acc 1\n",
      "2018-10-26T17:57:05.955164: step 13986, loss 5.02912e-07, acc 1\n",
      "2018-10-26T17:57:06.129698: step 13987, loss 4.53883e-06, acc 1\n",
      "2018-10-26T17:57:06.291266: step 13988, loss 2.03601e-05, acc 1\n",
      "2018-10-26T17:57:06.457821: step 13989, loss 2.22952e-05, acc 1\n",
      "2018-10-26T17:57:06.620386: step 13990, loss 1.0617e-07, acc 1\n",
      "2018-10-26T17:57:06.786942: step 13991, loss 0.000927617, acc 1\n",
      "2018-10-26T17:57:06.955491: step 13992, loss 2.47726e-06, acc 1\n",
      "2018-10-26T17:57:07.124055: step 13993, loss 0.000374894, acc 1\n",
      "2018-10-26T17:57:07.293588: step 13994, loss 9.98254e-06, acc 1\n",
      "2018-10-26T17:57:07.472111: step 13995, loss 0.000306216, acc 1\n",
      "2018-10-26T17:57:07.636671: step 13996, loss 5.15949e-07, acc 1\n",
      "2018-10-26T17:57:07.804224: step 13997, loss 1.56461e-07, acc 1\n",
      "2018-10-26T17:57:07.960805: step 13998, loss 3.90578e-05, acc 1\n",
      "2018-10-26T17:57:08.126363: step 13999, loss 6.41206e-05, acc 1\n",
      "2018-10-26T17:57:08.283942: step 14000, loss 2.30033e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:57:08.695842: step 14000, loss 5.00983, acc 0.705441\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14000\n",
      "\n",
      "2018-10-26T17:57:09.042367: step 14001, loss 0.000166826, acc 1\n",
      "2018-10-26T17:57:09.212912: step 14002, loss 9.07391e-05, acc 1\n",
      "2018-10-26T17:57:09.382459: step 14003, loss 4.38122e-05, acc 1\n",
      "2018-10-26T17:57:09.540037: step 14004, loss 1.56086e-06, acc 1\n",
      "2018-10-26T17:57:09.723547: step 14005, loss 0.000127755, acc 1\n",
      "2018-10-26T17:57:09.966897: step 14006, loss 1.48218e-05, acc 1\n",
      "2018-10-26T17:57:10.125473: step 14007, loss 7.08164e-05, acc 1\n",
      "2018-10-26T17:57:10.298023: step 14008, loss 0.000159802, acc 1\n",
      "2018-10-26T17:57:10.465564: step 14009, loss 1.34588e-05, acc 1\n",
      "2018-10-26T17:57:10.631122: step 14010, loss 5.12072e-05, acc 1\n",
      "2018-10-26T17:57:10.793687: step 14011, loss 2.19791e-07, acc 1\n",
      "2018-10-26T17:57:10.961240: step 14012, loss 0.00147768, acc 1\n",
      "2018-10-26T17:57:11.127795: step 14013, loss 1.58674e-05, acc 1\n",
      "2018-10-26T17:57:11.302328: step 14014, loss 1.01708e-05, acc 1\n",
      "2018-10-26T17:57:11.465891: step 14015, loss 9.51077e-05, acc 1\n",
      "2018-10-26T17:57:11.638431: step 14016, loss 1.41372e-06, acc 1\n",
      "2018-10-26T17:57:11.798005: step 14017, loss 1.35561e-05, acc 1\n",
      "2018-10-26T17:57:11.974533: step 14018, loss 7.63871e-05, acc 1\n",
      "2018-10-26T17:57:12.131114: step 14019, loss 7.87038e-05, acc 1\n",
      "2018-10-26T17:57:12.298667: step 14020, loss 0.000108133, acc 1\n",
      "2018-10-26T17:57:12.460236: step 14021, loss 1.49011e-07, acc 1\n",
      "2018-10-26T17:57:12.627788: step 14022, loss 2.39711e-06, acc 1\n",
      "2018-10-26T17:57:12.788358: step 14023, loss 2.86439e-05, acc 1\n",
      "2018-10-26T17:57:12.955910: step 14024, loss 1.10638e-06, acc 1\n",
      "2018-10-26T17:57:13.118476: step 14025, loss 6.50366e-06, acc 1\n",
      "2018-10-26T17:57:13.292013: step 14026, loss 2.96513e-06, acc 1\n",
      "2018-10-26T17:57:13.449592: step 14027, loss 2.42144e-08, acc 1\n",
      "2018-10-26T17:57:13.630109: step 14028, loss 0.000285002, acc 1\n",
      "2018-10-26T17:57:13.795667: step 14029, loss 2.98106e-05, acc 1\n",
      "2018-10-26T17:57:13.971198: step 14030, loss 1.26635e-05, acc 1\n",
      "2018-10-26T17:57:14.130771: step 14031, loss 7.17319e-05, acc 1\n",
      "2018-10-26T17:57:14.306303: step 14032, loss 0.0309411, acc 0.984375\n",
      "2018-10-26T17:57:14.471860: step 14033, loss 0.000270947, acc 1\n",
      "2018-10-26T17:57:14.644399: step 14034, loss 4.57091e-05, acc 1\n",
      "2018-10-26T17:57:14.802976: step 14035, loss 4.84287e-08, acc 1\n",
      "2018-10-26T17:57:14.979505: step 14036, loss 6.24221e-05, acc 1\n",
      "2018-10-26T17:57:15.136086: step 14037, loss 3.24454e-06, acc 1\n",
      "2018-10-26T17:57:15.300647: step 14038, loss 0.000248681, acc 1\n",
      "2018-10-26T17:57:15.464209: step 14039, loss 5.05111e-06, acc 1\n",
      "2018-10-26T17:57:15.634753: step 14040, loss 0.000114518, acc 1\n",
      "2018-10-26T17:57:15.793329: step 14041, loss 1.39506e-06, acc 1\n",
      "2018-10-26T17:57:15.969857: step 14042, loss 6.47015e-06, acc 1\n",
      "2018-10-26T17:57:16.131426: step 14043, loss 0.00939124, acc 1\n",
      "2018-10-26T17:57:16.304963: step 14044, loss 2.06753e-07, acc 1\n",
      "2018-10-26T17:57:16.462542: step 14045, loss 6.67482e-06, acc 1\n",
      "2018-10-26T17:57:16.638072: step 14046, loss 2.24092e-05, acc 1\n",
      "2018-10-26T17:57:16.802634: step 14047, loss 0.00063021, acc 1\n",
      "2018-10-26T17:57:16.972179: step 14048, loss 3.11976e-06, acc 1\n",
      "2018-10-26T17:57:17.133749: step 14049, loss 0.0194, acc 0.984375\n",
      "2018-10-26T17:57:17.306288: step 14050, loss 5.91756e-05, acc 1\n",
      "2018-10-26T17:57:17.465862: step 14051, loss 6.86345e-05, acc 1\n",
      "2018-10-26T17:57:17.635409: step 14052, loss 2.6077e-08, acc 1\n",
      "2018-10-26T17:57:17.794981: step 14053, loss 0.000994735, acc 1\n",
      "2018-10-26T17:57:17.965525: step 14054, loss 8.57313e-06, acc 1\n",
      "2018-10-26T17:57:18.128091: step 14055, loss 0.0135193, acc 0.984375\n",
      "2018-10-26T17:57:18.302625: step 14056, loss 1.39698e-07, acc 1\n",
      "2018-10-26T17:57:18.462199: step 14057, loss 1.24116e-05, acc 1\n",
      "2018-10-26T17:57:18.629751: step 14058, loss 0.000138932, acc 1\n",
      "2018-10-26T17:57:18.786332: step 14059, loss 9.22448e-05, acc 1\n",
      "2018-10-26T17:57:18.966850: step 14060, loss 3.55796e-05, acc 1\n",
      "2018-10-26T17:57:19.128418: step 14061, loss 4.34896e-06, acc 1\n",
      "2018-10-26T17:57:19.295971: step 14062, loss 0.000274093, acc 1\n",
      "2018-10-26T17:57:19.456542: step 14063, loss 8.11231e-06, acc 1\n",
      "2018-10-26T17:57:19.627087: step 14064, loss 0.000728843, acc 1\n",
      "2018-10-26T17:57:19.785663: step 14065, loss 8.02415e-05, acc 1\n",
      "2018-10-26T17:57:19.961194: step 14066, loss 2.01155e-06, acc 1\n",
      "2018-10-26T17:57:20.122762: step 14067, loss 0.000103052, acc 1\n",
      "2018-10-26T17:57:20.291311: step 14068, loss 3.81841e-07, acc 1\n",
      "2018-10-26T17:57:20.455872: step 14069, loss 7.82656e-05, acc 1\n",
      "2018-10-26T17:57:20.632401: step 14070, loss 5.90452e-07, acc 1\n",
      "2018-10-26T17:57:20.797958: step 14071, loss 7.1659e-06, acc 1\n",
      "2018-10-26T17:57:20.971495: step 14072, loss 4.01005e-05, acc 1\n",
      "2018-10-26T17:57:21.135057: step 14073, loss 5.85012e-06, acc 1\n",
      "2018-10-26T17:57:21.303608: step 14074, loss 3.64078e-05, acc 1\n",
      "2018-10-26T17:57:21.463181: step 14075, loss 6.94878e-06, acc 1\n",
      "2018-10-26T17:57:21.643698: step 14076, loss 0.000225859, acc 1\n",
      "2018-10-26T17:57:21.804270: step 14077, loss 1.10569e-05, acc 1\n",
      "2018-10-26T17:57:21.985784: step 14078, loss 1.43423e-07, acc 1\n",
      "2018-10-26T17:57:22.152340: step 14079, loss 3.22535e-05, acc 1\n",
      "2018-10-26T17:57:22.324878: step 14080, loss 1.5832e-06, acc 1\n",
      "2018-10-26T17:57:22.489439: step 14081, loss 8.51206e-07, acc 1\n",
      "2018-10-26T17:57:22.665966: step 14082, loss 6.3993e-06, acc 1\n",
      "2018-10-26T17:57:22.823545: step 14083, loss 9.92549e-06, acc 1\n",
      "2018-10-26T17:57:23.010048: step 14084, loss 2.60768e-07, acc 1\n",
      "2018-10-26T17:57:23.180592: step 14085, loss 3.1606e-06, acc 1\n",
      "2018-10-26T17:57:23.350139: step 14086, loss 3.86903e-05, acc 1\n",
      "2018-10-26T17:57:23.505724: step 14087, loss 1.15879e-05, acc 1\n",
      "2018-10-26T17:57:23.694219: step 14088, loss 2.90238e-05, acc 1\n",
      "2018-10-26T17:57:23.851799: step 14089, loss 0.000124064, acc 1\n",
      "2018-10-26T17:57:24.022343: step 14090, loss 0.000351809, acc 1\n",
      "2018-10-26T17:57:24.182914: step 14091, loss 2.88709e-07, acc 1\n",
      "2018-10-26T17:57:24.356450: step 14092, loss 0.00017947, acc 1\n",
      "2018-10-26T17:57:24.523006: step 14093, loss 0.000336008, acc 1\n",
      "2018-10-26T17:57:24.694548: step 14094, loss 3.78134e-05, acc 1\n",
      "2018-10-26T17:57:24.896010: step 14095, loss 4.62056e-06, acc 1\n",
      "2018-10-26T17:57:25.126393: step 14096, loss 2.53299e-06, acc 1\n",
      "2018-10-26T17:57:25.320874: step 14097, loss 1.05944e-05, acc 1\n",
      "2018-10-26T17:57:25.573200: step 14098, loss 3.37016e-05, acc 1\n",
      "2018-10-26T17:57:25.785632: step 14099, loss 0.0003229, acc 1\n",
      "2018-10-26T17:57:26.008037: step 14100, loss 6.48097e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:57:26.522663: step 14100, loss 4.9699, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14100\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:57:26.882887: step 14101, loss 0.000483048, acc 1\n",
      "2018-10-26T17:57:27.109282: step 14102, loss 0.000219952, acc 1\n",
      "2018-10-26T17:57:27.283816: step 14103, loss 2.71572e-05, acc 1\n",
      "2018-10-26T17:57:27.467326: step 14104, loss 0.000164287, acc 1\n",
      "2018-10-26T17:57:27.725635: step 14105, loss 0.000483737, acc 1\n",
      "2018-10-26T17:57:27.902165: step 14106, loss 9.17587e-05, acc 1\n",
      "2018-10-26T17:57:28.110607: step 14107, loss 0.00769728, acc 1\n",
      "2018-10-26T17:57:28.282149: step 14108, loss 0.000163024, acc 1\n",
      "2018-10-26T17:57:28.506549: step 14109, loss 3.50176e-07, acc 1\n",
      "2018-10-26T17:57:28.703024: step 14110, loss 0.0114474, acc 0.984375\n",
      "2018-10-26T17:57:28.886534: step 14111, loss 4.98228e-05, acc 1\n",
      "2018-10-26T17:57:29.089990: step 14112, loss 1.2318e-05, acc 1\n",
      "2018-10-26T17:57:29.285467: step 14113, loss 3.70664e-07, acc 1\n",
      "2018-10-26T17:57:29.522833: step 14114, loss 7.84755e-06, acc 1\n",
      "2018-10-26T17:57:29.752234: step 14115, loss 3.30219e-06, acc 1\n",
      "2018-10-26T17:57:29.936728: step 14116, loss 1.83777e-05, acc 1\n",
      "2018-10-26T17:57:30.145172: step 14117, loss 2.82923e-06, acc 1\n",
      "2018-10-26T17:57:30.319705: step 14118, loss 1.18277e-06, acc 1\n",
      "2018-10-26T17:57:30.518174: step 14119, loss 0.00897022, acc 1\n",
      "2018-10-26T17:57:30.681738: step 14120, loss 0.000762558, acc 1\n",
      "2018-10-26T17:57:30.847294: step 14121, loss 3.16649e-08, acc 1\n",
      "2018-10-26T17:57:31.052746: step 14122, loss 8.66121e-07, acc 1\n",
      "2018-10-26T17:57:31.219301: step 14123, loss 3.2863e-05, acc 1\n",
      "2018-10-26T17:57:31.381866: step 14124, loss 3.65824e-05, acc 1\n",
      "2018-10-26T17:57:31.582331: step 14125, loss 4.63797e-07, acc 1\n",
      "2018-10-26T17:57:31.754870: step 14126, loss 1.02445e-07, acc 1\n",
      "2018-10-26T17:57:31.926412: step 14127, loss 5.58794e-09, acc 1\n",
      "2018-10-26T17:57:32.084987: step 14128, loss 1.11126e-05, acc 1\n",
      "2018-10-26T17:57:32.285452: step 14129, loss 3.72529e-09, acc 1\n",
      "2018-10-26T17:57:32.453005: step 14130, loss 8.49254e-05, acc 1\n",
      "2018-10-26T17:57:32.620558: step 14131, loss 0.000299839, acc 1\n",
      "2018-10-26T17:57:32.781128: step 14132, loss 4.97277e-06, acc 1\n",
      "2018-10-26T17:57:32.974611: step 14133, loss 0.000103034, acc 1\n",
      "2018-10-26T17:57:33.156127: step 14134, loss 1.59997e-06, acc 1\n",
      "2018-10-26T17:57:33.331657: step 14135, loss 0.000117433, acc 1\n",
      "2018-10-26T17:57:33.499210: step 14136, loss 3.20373e-07, acc 1\n",
      "2018-10-26T17:57:33.665764: step 14137, loss 2.01534e-06, acc 1\n",
      "2018-10-26T17:57:33.864234: step 14138, loss 0.000209209, acc 1\n",
      "2018-10-26T17:57:34.049738: step 14139, loss 7.61814e-07, acc 1\n",
      "2018-10-26T17:57:34.230256: step 14140, loss 1.15481e-06, acc 1\n",
      "2018-10-26T17:57:34.405786: step 14141, loss 4.73107e-07, acc 1\n",
      "2018-10-26T17:57:34.587302: step 14142, loss 4.73109e-07, acc 1\n",
      "2018-10-26T17:57:34.762834: step 14143, loss 3.40103e-06, acc 1\n",
      "2018-10-26T17:57:34.947341: step 14144, loss 2.52738e-05, acc 1\n",
      "2018-10-26T17:57:35.125863: step 14145, loss 7.12935e-05, acc 1\n",
      "2018-10-26T17:57:35.299399: step 14146, loss 2.77533e-07, acc 1\n",
      "2018-10-26T17:57:35.490888: step 14147, loss 0.00293155, acc 1\n",
      "2018-10-26T17:57:35.692349: step 14148, loss 2.08616e-07, acc 1\n",
      "2018-10-26T17:57:35.888825: step 14149, loss 6.20883e-06, acc 1\n",
      "2018-10-26T17:57:36.064356: step 14150, loss 0.0250033, acc 0.984375\n",
      "2018-10-26T17:57:36.230911: step 14151, loss 0.00119068, acc 1\n",
      "2018-10-26T17:57:36.394474: step 14152, loss 1.10639e-06, acc 1\n",
      "2018-10-26T17:57:36.567013: step 14153, loss 7.26431e-08, acc 1\n",
      "2018-10-26T17:57:36.726587: step 14154, loss 0.00866725, acc 1\n",
      "2018-10-26T17:57:36.895136: step 14155, loss 1.51032e-05, acc 1\n",
      "2018-10-26T17:57:37.059697: step 14156, loss 0.00193092, acc 1\n",
      "2018-10-26T17:57:37.230241: step 14157, loss 5.61627e-05, acc 1\n",
      "2018-10-26T17:57:37.390812: step 14158, loss 3.5575e-05, acc 1\n",
      "2018-10-26T17:57:37.557366: step 14159, loss 0.00192544, acc 1\n",
      "2018-10-26T17:57:37.721928: step 14160, loss 0.00126744, acc 1\n",
      "2018-10-26T17:57:37.896460: step 14161, loss 0.0028684, acc 1\n",
      "2018-10-26T17:57:38.053042: step 14162, loss 6.46331e-07, acc 1\n",
      "2018-10-26T17:57:38.224584: step 14163, loss 1.98129e-05, acc 1\n",
      "2018-10-26T17:57:38.384158: step 14164, loss 1.82661e-05, acc 1\n",
      "2018-10-26T17:57:38.551710: step 14165, loss 1.38016e-06, acc 1\n",
      "2018-10-26T17:57:38.714276: step 14166, loss 2.41018e-06, acc 1\n",
      "2018-10-26T17:57:38.882825: step 14167, loss 4.71007e-06, acc 1\n",
      "2018-10-26T17:57:39.058357: step 14168, loss 4.81728e-05, acc 1\n",
      "2018-10-26T17:57:39.228901: step 14169, loss 8.18535e-05, acc 1\n",
      "2018-10-26T17:57:39.396453: step 14170, loss 5.07053e-05, acc 1\n",
      "2018-10-26T17:57:39.556026: step 14171, loss 0.000318057, acc 1\n",
      "2018-10-26T17:57:39.723578: step 14172, loss 2.91681e-06, acc 1\n",
      "2018-10-26T17:57:39.891131: step 14173, loss 8.00936e-08, acc 1\n",
      "2018-10-26T17:57:40.069654: step 14174, loss 0.000505744, acc 1\n",
      "2018-10-26T17:57:40.234215: step 14175, loss 2.66357e-07, acc 1\n",
      "2018-10-26T17:57:40.405756: step 14176, loss 2.90327e-05, acc 1\n",
      "2018-10-26T17:57:40.565330: step 14177, loss 6.72418e-05, acc 1\n",
      "2018-10-26T17:57:40.731885: step 14178, loss 0.00010149, acc 1\n",
      "2018-10-26T17:57:40.897442: step 14179, loss 0.00283038, acc 1\n",
      "2018-10-26T17:57:41.071977: step 14180, loss 0.00070212, acc 1\n",
      "2018-10-26T17:57:41.229555: step 14181, loss 6.00475e-06, acc 1\n",
      "2018-10-26T17:57:41.403118: step 14182, loss 2.42144e-08, acc 1\n",
      "2018-10-26T17:57:41.562665: step 14183, loss 1.16453e-05, acc 1\n",
      "2018-10-26T17:57:41.738196: step 14184, loss 3.5052e-06, acc 1\n",
      "2018-10-26T17:57:41.897770: step 14185, loss 1.58325e-07, acc 1\n",
      "2018-10-26T17:57:42.068314: step 14186, loss 2.06561e-06, acc 1\n",
      "2018-10-26T17:57:42.239856: step 14187, loss 0.00360752, acc 1\n",
      "2018-10-26T17:57:42.412395: step 14188, loss 6.333e-05, acc 1\n",
      "2018-10-26T17:57:42.577953: step 14189, loss 0.000256767, acc 1\n",
      "2018-10-26T17:57:42.744507: step 14190, loss 4.84132e-05, acc 1\n",
      "2018-10-26T17:57:42.907083: step 14191, loss 1.48945e-05, acc 1\n",
      "2018-10-26T17:57:43.077617: step 14192, loss 0.000121028, acc 1\n",
      "2018-10-26T17:57:43.240182: step 14193, loss 0.000410402, acc 1\n",
      "2018-10-26T17:57:43.407735: step 14194, loss 6.35632e-06, acc 1\n",
      "2018-10-26T17:57:43.566312: step 14195, loss 5.44022e-06, acc 1\n",
      "2018-10-26T17:57:43.735859: step 14196, loss 0.0510178, acc 0.984375\n",
      "2018-10-26T17:57:43.897427: step 14197, loss 0.00270803, acc 1\n",
      "2018-10-26T17:57:44.076948: step 14198, loss 3.86057e-05, acc 1\n",
      "2018-10-26T17:57:44.242505: step 14199, loss 0.00364186, acc 1\n",
      "2018-10-26T17:57:44.416041: step 14200, loss 3.10374e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:57:44.835919: step 14200, loss 4.99115, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14200\n",
      "\n",
      "2018-10-26T17:57:45.175774: step 14201, loss 3.14197e-06, acc 1\n",
      "2018-10-26T17:57:45.335347: step 14202, loss 0.000432841, acc 1\n",
      "2018-10-26T17:57:45.507887: step 14203, loss 8.54253e-05, acc 1\n",
      "2018-10-26T17:57:45.674441: step 14204, loss 3.94104e-06, acc 1\n",
      "2018-10-26T17:57:45.851967: step 14205, loss 0.0213462, acc 0.984375\n",
      "2018-10-26T17:57:46.090339: step 14206, loss 4.22031e-06, acc 1\n",
      "2018-10-26T17:57:46.260876: step 14207, loss 8.59707e-05, acc 1\n",
      "2018-10-26T17:57:46.435408: step 14208, loss 0.000176329, acc 1\n",
      "2018-10-26T17:57:46.600966: step 14209, loss 1.11373e-05, acc 1\n",
      "2018-10-26T17:57:46.775499: step 14210, loss 0.000288571, acc 1\n",
      "2018-10-26T17:57:46.942055: step 14211, loss 1.18729e-05, acc 1\n",
      "2018-10-26T17:57:47.120577: step 14212, loss 5.90363e-06, acc 1\n",
      "2018-10-26T17:57:47.280152: step 14213, loss 3.07646e-05, acc 1\n",
      "2018-10-26T17:57:47.454685: step 14214, loss 7.71127e-07, acc 1\n",
      "2018-10-26T17:57:47.611267: step 14215, loss 6.21915e-05, acc 1\n",
      "2018-10-26T17:57:47.785801: step 14216, loss 0.00447815, acc 1\n",
      "2018-10-26T17:57:47.948366: step 14217, loss 2.02052e-05, acc 1\n",
      "2018-10-26T17:57:48.124894: step 14218, loss 1.10714e-05, acc 1\n",
      "2018-10-26T17:57:48.279482: step 14219, loss 3.4645e-07, acc 1\n",
      "2018-10-26T17:57:48.455013: step 14220, loss 3.1665e-08, acc 1\n",
      "2018-10-26T17:57:48.613588: step 14221, loss 0.0142559, acc 0.984375\n",
      "2018-10-26T17:57:48.796101: step 14222, loss 4.09782e-08, acc 1\n",
      "2018-10-26T17:57:48.962657: step 14223, loss 0.000451514, acc 1\n",
      "2018-10-26T17:57:49.139184: step 14224, loss 0.000239909, acc 1\n",
      "2018-10-26T17:57:49.332668: step 14225, loss 0.000384205, acc 1\n",
      "2018-10-26T17:57:49.510192: step 14226, loss 8.03251e-06, acc 1\n",
      "2018-10-26T17:57:49.687719: step 14227, loss 1.09171e-05, acc 1\n",
      "2018-10-26T17:57:49.856268: step 14228, loss 3.59541e-05, acc 1\n",
      "2018-10-26T17:57:50.018834: step 14229, loss 3.54586e-05, acc 1\n",
      "2018-10-26T17:57:50.186387: step 14230, loss 9.35095e-05, acc 1\n",
      "2018-10-26T17:57:50.348952: step 14231, loss 0.00270479, acc 1\n",
      "2018-10-26T17:57:50.516504: step 14232, loss 2.18109e-06, acc 1\n",
      "2018-10-26T17:57:50.685054: step 14233, loss 5.01048e-07, acc 1\n",
      "2018-10-26T17:57:50.850612: step 14234, loss 3.79508e-05, acc 1\n",
      "2018-10-26T17:57:51.011183: step 14235, loss 0.0242464, acc 0.984375\n",
      "2018-10-26T17:57:51.184719: step 14236, loss 0.00540242, acc 1\n",
      "2018-10-26T17:57:51.347285: step 14237, loss 0.00045925, acc 1\n",
      "2018-10-26T17:57:51.516832: step 14238, loss 0.000127295, acc 1\n",
      "2018-10-26T17:57:51.675407: step 14239, loss 4.62654e-05, acc 1\n",
      "2018-10-26T17:57:51.846950: step 14240, loss 9.68573e-08, acc 1\n",
      "2018-10-26T17:57:52.007521: step 14241, loss 4.12729e-06, acc 1\n",
      "2018-10-26T17:57:52.175072: step 14242, loss 5.27453e-06, acc 1\n",
      "2018-10-26T17:57:52.338635: step 14243, loss 3.91153e-07, acc 1\n",
      "2018-10-26T17:57:52.507185: step 14244, loss 3.90041e-05, acc 1\n",
      "2018-10-26T17:57:52.670748: step 14245, loss 9.50712e-05, acc 1\n",
      "2018-10-26T17:57:52.837303: step 14246, loss 6.72009e-05, acc 1\n",
      "2018-10-26T17:57:52.995880: step 14247, loss 1.44724e-06, acc 1\n",
      "2018-10-26T17:57:53.164430: step 14248, loss 5.15886e-05, acc 1\n",
      "2018-10-26T17:57:53.328990: step 14249, loss 5.17808e-07, acc 1\n",
      "2018-10-26T17:57:53.500532: step 14250, loss 3.63352e-06, acc 1\n",
      "2018-10-26T17:57:53.665092: step 14251, loss 4.75355e-05, acc 1\n",
      "2018-10-26T17:57:53.838628: step 14252, loss 1.32991e-06, acc 1\n",
      "2018-10-26T17:57:54.002191: step 14253, loss 0.000475988, acc 1\n",
      "2018-10-26T17:57:54.183706: step 14254, loss 3.7625e-07, acc 1\n",
      "2018-10-26T17:57:54.341285: step 14255, loss 6.69699e-06, acc 1\n",
      "2018-10-26T17:57:54.509835: step 14256, loss 5.46996e-06, acc 1\n",
      "2018-10-26T17:57:54.667414: step 14257, loss 6.76572e-06, acc 1\n",
      "2018-10-26T17:57:54.839953: step 14258, loss 6.50935e-06, acc 1\n",
      "2018-10-26T17:57:54.999526: step 14259, loss 1.53853e-05, acc 1\n",
      "2018-10-26T17:57:55.169073: step 14260, loss 7.53236e-05, acc 1\n",
      "2018-10-26T17:57:55.331639: step 14261, loss 0.000176126, acc 1\n",
      "2018-10-26T17:57:55.497196: step 14262, loss 0.000385016, acc 1\n",
      "2018-10-26T17:57:55.664749: step 14263, loss 5.98203e-05, acc 1\n",
      "2018-10-26T17:57:55.836291: step 14264, loss 2.39341e-06, acc 1\n",
      "2018-10-26T17:57:55.994867: step 14265, loss 9.11966e-06, acc 1\n",
      "2018-10-26T17:57:56.164414: step 14266, loss 0.000161179, acc 1\n",
      "2018-10-26T17:57:56.321992: step 14267, loss 0.000253732, acc 1\n",
      "2018-10-26T17:57:56.494533: step 14268, loss 5.03985e-06, acc 1\n",
      "2018-10-26T17:57:56.663081: step 14269, loss 1.29125e-05, acc 1\n",
      "2018-10-26T17:57:56.833626: step 14270, loss 2.18852e-06, acc 1\n",
      "2018-10-26T17:57:56.994196: step 14271, loss 2.30773e-06, acc 1\n",
      "2018-10-26T17:57:57.163743: step 14272, loss 6.03767e-06, acc 1\n",
      "2018-10-26T17:57:57.333291: step 14273, loss 0.000108102, acc 1\n",
      "2018-10-26T17:57:57.503836: step 14274, loss 0.00220504, acc 1\n",
      "2018-10-26T17:57:57.659420: step 14275, loss 0.000312396, acc 1\n",
      "2018-10-26T17:57:57.832956: step 14276, loss 6.39584e-05, acc 1\n",
      "2018-10-26T17:57:57.994524: step 14277, loss 6.08664e-06, acc 1\n",
      "2018-10-26T17:57:58.165069: step 14278, loss 0.00304269, acc 1\n",
      "2018-10-26T17:57:58.327635: step 14279, loss 0.000907366, acc 1\n",
      "2018-10-26T17:57:58.504163: step 14280, loss 0.000365266, acc 1\n",
      "2018-10-26T17:57:58.663736: step 14281, loss 0.000478087, acc 1\n",
      "2018-10-26T17:57:58.830291: step 14282, loss 1.85993e-05, acc 1\n",
      "2018-10-26T17:57:58.988868: step 14283, loss 0.00113183, acc 1\n",
      "2018-10-26T17:57:59.164398: step 14284, loss 5.96045e-08, acc 1\n",
      "2018-10-26T17:57:59.333945: step 14285, loss 1.31271e-05, acc 1\n",
      "2018-10-26T17:57:59.513465: step 14286, loss 1.5888e-06, acc 1\n",
      "2018-10-26T17:57:59.671045: step 14287, loss 8.5342e-06, acc 1\n",
      "2018-10-26T17:57:59.839594: step 14288, loss 2.92715e-05, acc 1\n",
      "2018-10-26T17:58:00.010138: step 14289, loss 1.21254e-06, acc 1\n",
      "2018-10-26T17:58:00.177691: step 14290, loss 1.32248e-07, acc 1\n",
      "2018-10-26T17:58:00.341255: step 14291, loss 0.000124689, acc 1\n",
      "2018-10-26T17:58:00.513794: step 14292, loss 1.95577e-07, acc 1\n",
      "2018-10-26T17:58:00.679350: step 14293, loss 6.88694e-06, acc 1\n",
      "2018-10-26T17:58:00.853885: step 14294, loss 7.00347e-07, acc 1\n",
      "2018-10-26T17:58:01.016450: step 14295, loss 8.47625e-05, acc 1\n",
      "2018-10-26T17:58:01.187992: step 14296, loss 9.25507e-06, acc 1\n",
      "2018-10-26T17:58:01.351554: step 14297, loss 5.96134e-05, acc 1\n",
      "2018-10-26T17:58:01.523097: step 14298, loss 6.44029e-06, acc 1\n",
      "2018-10-26T17:58:01.693641: step 14299, loss 3.25381e-05, acc 1\n",
      "2018-10-26T17:58:01.870168: step 14300, loss 0.000101379, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:58:02.290047: step 14300, loss 5.03476, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14300\n",
      "\n",
      "2018-10-26T17:58:02.630759: step 14301, loss 0.000417091, acc 1\n",
      "2018-10-26T17:58:02.791329: step 14302, loss 1.86264e-07, acc 1\n",
      "2018-10-26T17:58:02.963869: step 14303, loss 8.69809e-06, acc 1\n",
      "2018-10-26T17:58:03.127431: step 14304, loss 2.10379e-05, acc 1\n",
      "2018-10-26T17:58:03.304957: step 14305, loss 7.13473e-06, acc 1\n",
      "2018-10-26T17:58:03.550301: step 14306, loss 0.000717325, acc 1\n",
      "2018-10-26T17:58:03.713865: step 14307, loss 0.000302561, acc 1\n",
      "2018-10-26T17:58:03.892387: step 14308, loss 1.25425e-05, acc 1\n",
      "2018-10-26T17:58:04.058943: step 14309, loss 9.19304e-05, acc 1\n",
      "2018-10-26T17:58:04.232478: step 14310, loss 6.09412e-06, acc 1\n",
      "2018-10-26T17:58:04.393049: step 14311, loss 2.8553e-06, acc 1\n",
      "2018-10-26T17:58:04.570576: step 14312, loss 1.41187e-06, acc 1\n",
      "2018-10-26T17:58:04.732144: step 14313, loss 1.14923e-06, acc 1\n",
      "2018-10-26T17:58:04.899697: step 14314, loss 2.14203e-07, acc 1\n",
      "2018-10-26T17:58:05.061266: step 14315, loss 5.34575e-07, acc 1\n",
      "2018-10-26T17:58:05.235798: step 14316, loss 0.0012726, acc 1\n",
      "2018-10-26T17:58:05.393377: step 14317, loss 5.77909e-05, acc 1\n",
      "2018-10-26T17:58:05.574893: step 14318, loss 4.93839e-05, acc 1\n",
      "2018-10-26T17:58:05.735463: step 14319, loss 1.08761e-05, acc 1\n",
      "2018-10-26T17:58:05.908002: step 14320, loss 1.24053e-05, acc 1\n",
      "2018-10-26T17:58:06.066578: step 14321, loss 1.59993e-06, acc 1\n",
      "2018-10-26T17:58:06.239117: step 14322, loss 5.22188e-05, acc 1\n",
      "2018-10-26T17:58:06.409661: step 14323, loss 3.32458e-06, acc 1\n",
      "2018-10-26T17:58:06.589182: step 14324, loss 4.62998e-06, acc 1\n",
      "2018-10-26T17:58:06.749754: step 14325, loss 0.00173275, acc 1\n",
      "2018-10-26T17:58:06.930270: step 14326, loss 9.77799e-06, acc 1\n",
      "2018-10-26T17:58:07.096826: step 14327, loss 4.11085e-05, acc 1\n",
      "2018-10-26T17:58:07.284324: step 14328, loss 3.28112e-05, acc 1\n",
      "2018-10-26T17:58:07.442901: step 14329, loss 0.00313401, acc 1\n",
      "2018-10-26T17:58:07.627408: step 14330, loss 4.48893e-07, acc 1\n",
      "2018-10-26T17:58:07.795957: step 14331, loss 5.47614e-07, acc 1\n",
      "2018-10-26T17:58:07.968498: step 14332, loss 2.09727e-06, acc 1\n",
      "2018-10-26T17:58:08.134057: step 14333, loss 8.90333e-07, acc 1\n",
      "2018-10-26T17:58:08.310582: step 14334, loss 1.36526e-06, acc 1\n",
      "2018-10-26T17:58:08.470156: step 14335, loss 3.67665e-06, acc 1\n",
      "2018-10-26T17:58:08.648680: step 14336, loss 1.15466e-05, acc 1\n",
      "2018-10-26T17:58:08.814237: step 14337, loss 3.45306e-06, acc 1\n",
      "2018-10-26T17:58:08.988771: step 14338, loss 1.94151e-05, acc 1\n",
      "2018-10-26T17:58:09.152334: step 14339, loss 1.01699e-06, acc 1\n",
      "2018-10-26T17:58:09.327865: step 14340, loss 1.39888e-05, acc 1\n",
      "2018-10-26T17:58:09.486441: step 14341, loss 6.20974e-05, acc 1\n",
      "2018-10-26T17:58:09.666960: step 14342, loss 7.10653e-06, acc 1\n",
      "2018-10-26T17:58:09.827530: step 14343, loss 6.89292e-05, acc 1\n",
      "2018-10-26T17:58:10.010042: step 14344, loss 0.000201568, acc 1\n",
      "2018-10-26T17:58:10.170613: step 14345, loss 5.03207e-06, acc 1\n",
      "2018-10-26T17:58:10.344149: step 14346, loss 1.01649e-05, acc 1\n",
      "2018-10-26T17:58:10.503723: step 14347, loss 1.13619e-06, acc 1\n",
      "2018-10-26T17:58:10.678256: step 14348, loss 2.80223e-05, acc 1\n",
      "2018-10-26T17:58:10.847803: step 14349, loss 0.000737522, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:58:11.029319: step 14350, loss 9.21218e-05, acc 1\n",
      "2018-10-26T17:58:11.195874: step 14351, loss 1.88126e-07, acc 1\n",
      "2018-10-26T17:58:11.360433: step 14352, loss 2.6077e-08, acc 1\n",
      "2018-10-26T17:58:11.526989: step 14353, loss 1.30385e-08, acc 1\n",
      "2018-10-26T17:58:11.697534: step 14354, loss 3.07377e-05, acc 1\n",
      "2018-10-26T17:58:11.861096: step 14355, loss 0.000119567, acc 1\n",
      "2018-10-26T17:58:12.037624: step 14356, loss 0.00030799, acc 1\n",
      "2018-10-26T17:58:12.199193: step 14357, loss 0.000139401, acc 1\n",
      "2018-10-26T17:58:12.371732: step 14358, loss 0.0170457, acc 0.984375\n",
      "2018-10-26T17:58:12.534298: step 14359, loss 2.65947e-05, acc 1\n",
      "2018-10-26T17:58:12.699855: step 14360, loss 2.04891e-08, acc 1\n",
      "2018-10-26T17:58:12.864416: step 14361, loss 2.39901e-06, acc 1\n",
      "2018-10-26T17:58:13.034960: step 14362, loss 0.0321846, acc 0.984375\n",
      "2018-10-26T17:58:13.197526: step 14363, loss 1.78813e-07, acc 1\n",
      "2018-10-26T17:58:13.365078: step 14364, loss 2.36543e-05, acc 1\n",
      "2018-10-26T17:58:13.529638: step 14365, loss 1.19209e-07, acc 1\n",
      "2018-10-26T17:58:13.697190: step 14366, loss 1.27464e-05, acc 1\n",
      "2018-10-26T17:58:13.856764: step 14367, loss 0.00853083, acc 1\n",
      "2018-10-26T17:58:14.029303: step 14368, loss 0.000805668, acc 1\n",
      "2018-10-26T17:58:14.187879: step 14369, loss 1.56814e-05, acc 1\n",
      "2018-10-26T17:58:14.361416: step 14370, loss 3.57989e-05, acc 1\n",
      "2018-10-26T17:58:14.526973: step 14371, loss 8.53462e-06, acc 1\n",
      "2018-10-26T17:58:14.707491: step 14372, loss 2.45677e-06, acc 1\n",
      "2018-10-26T17:58:14.863075: step 14373, loss 0.000241116, acc 1\n",
      "2018-10-26T17:58:15.034617: step 14374, loss 2.0658e-05, acc 1\n",
      "2018-10-26T17:58:15.192196: step 14375, loss 0.00185731, acc 1\n",
      "2018-10-26T17:58:15.363738: step 14376, loss 0.000132266, acc 1\n",
      "2018-10-26T17:58:15.525306: step 14377, loss 0.0686933, acc 0.984375\n",
      "2018-10-26T17:58:15.697845: step 14378, loss 6.85364e-05, acc 1\n",
      "2018-10-26T17:58:15.859413: step 14379, loss 0.000149137, acc 1\n",
      "2018-10-26T17:58:16.033946: step 14380, loss 8.95046e-05, acc 1\n",
      "2018-10-26T17:58:16.200502: step 14381, loss 6.98483e-07, acc 1\n",
      "2018-10-26T17:58:16.376032: step 14382, loss 4.80519e-06, acc 1\n",
      "2018-10-26T17:58:16.532615: step 14383, loss 0.00265451, acc 1\n",
      "2018-10-26T17:58:16.702161: step 14384, loss 1.30385e-08, acc 1\n",
      "2018-10-26T17:58:16.864728: step 14385, loss 9.31322e-09, acc 1\n",
      "2018-10-26T17:58:17.033276: step 14386, loss 3.6745e-05, acc 1\n",
      "2018-10-26T17:58:17.197837: step 14387, loss 7.45057e-08, acc 1\n",
      "2018-10-26T17:58:17.372371: step 14388, loss 9.74737e-05, acc 1\n",
      "2018-10-26T17:58:17.537928: step 14389, loss 9.27637e-05, acc 1\n",
      "2018-10-26T17:58:17.711466: step 14390, loss 1.29636e-05, acc 1\n",
      "2018-10-26T17:58:17.875028: step 14391, loss 0.013472, acc 0.984375\n",
      "2018-10-26T17:58:18.054549: step 14392, loss 3.86227e-05, acc 1\n",
      "2018-10-26T17:58:18.214122: step 14393, loss 0.00110325, acc 1\n",
      "2018-10-26T17:58:18.383668: step 14394, loss 8.08379e-07, acc 1\n",
      "2018-10-26T17:58:18.544240: step 14395, loss 0.000161132, acc 1\n",
      "2018-10-26T17:58:18.723761: step 14396, loss 2.82309e-05, acc 1\n",
      "2018-10-26T17:58:18.887323: step 14397, loss 5.2154e-08, acc 1\n",
      "2018-10-26T17:58:19.071830: step 14398, loss 0.000126078, acc 1\n",
      "2018-10-26T17:58:19.233399: step 14399, loss 4.89865e-05, acc 1\n",
      "2018-10-26T17:58:19.405938: step 14400, loss 1.34506e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:58:19.828808: step 14400, loss 5.04671, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14400\n",
      "\n",
      "2018-10-26T17:58:20.145040: step 14401, loss 1.26333e-05, acc 1\n",
      "2018-10-26T17:58:20.308603: step 14402, loss 0.000283177, acc 1\n",
      "2018-10-26T17:58:20.485131: step 14403, loss 3.99071e-05, acc 1\n",
      "2018-10-26T17:58:20.649692: step 14404, loss 0.000538506, acc 1\n",
      "2018-10-26T17:58:20.821244: step 14405, loss 3.37293e-06, acc 1\n",
      "2018-10-26T17:58:21.058599: step 14406, loss 6.57718e-05, acc 1\n",
      "2018-10-26T17:58:21.228146: step 14407, loss 6.6524e-05, acc 1\n",
      "2018-10-26T17:58:21.398690: step 14408, loss 0.000240224, acc 1\n",
      "2018-10-26T17:58:21.569235: step 14409, loss 0.000510014, acc 1\n",
      "2018-10-26T17:58:21.738782: step 14410, loss 3.2092e-06, acc 1\n",
      "2018-10-26T17:58:21.903342: step 14411, loss 0.0268859, acc 0.984375\n",
      "2018-10-26T17:58:22.073886: step 14412, loss 4.35574e-05, acc 1\n",
      "2018-10-26T17:58:22.230468: step 14413, loss 1.44165e-06, acc 1\n",
      "2018-10-26T17:58:22.407994: step 14414, loss 5.30412e-06, acc 1\n",
      "2018-10-26T17:58:22.577540: step 14415, loss 0.00174378, acc 1\n",
      "2018-10-26T17:58:22.756064: step 14416, loss 1.52145e-05, acc 1\n",
      "2018-10-26T17:58:22.926607: step 14417, loss 2.24631e-06, acc 1\n",
      "2018-10-26T17:58:23.105131: step 14418, loss 0.00019422, acc 1\n",
      "2018-10-26T17:58:23.264705: step 14419, loss 0.0381253, acc 0.984375\n",
      "2018-10-26T17:58:23.440235: step 14420, loss 2.35894e-05, acc 1\n",
      "2018-10-26T17:58:23.604795: step 14421, loss 0.00276625, acc 1\n",
      "2018-10-26T17:58:23.782322: step 14422, loss 1.19209e-07, acc 1\n",
      "2018-10-26T17:58:23.952866: step 14423, loss 3.57625e-07, acc 1\n",
      "2018-10-26T17:58:24.128397: step 14424, loss 2.11956e-06, acc 1\n",
      "2018-10-26T17:58:24.287970: step 14425, loss 7.72092e-06, acc 1\n",
      "2018-10-26T17:58:24.460509: step 14426, loss 0.00105519, acc 1\n",
      "2018-10-26T17:58:24.627065: step 14427, loss 9.87697e-05, acc 1\n",
      "2018-10-26T17:58:24.799603: step 14428, loss 7.20458e-05, acc 1\n",
      "2018-10-26T17:58:24.965161: step 14429, loss 2.06555e-06, acc 1\n",
      "2018-10-26T17:58:25.135706: step 14430, loss 0.000149846, acc 1\n",
      "2018-10-26T17:58:25.297274: step 14431, loss 0.000917887, acc 1\n",
      "2018-10-26T17:58:25.468816: step 14432, loss 0.000163336, acc 1\n",
      "2018-10-26T17:58:25.628389: step 14433, loss 3.20666e-05, acc 1\n",
      "2018-10-26T17:58:25.799931: step 14434, loss 0.00536247, acc 1\n",
      "2018-10-26T17:58:25.966486: step 14435, loss 4.614e-05, acc 1\n",
      "2018-10-26T17:58:26.140022: step 14436, loss 1.95752e-06, acc 1\n",
      "2018-10-26T17:58:26.309569: step 14437, loss 8.72785e-06, acc 1\n",
      "2018-10-26T17:58:26.484103: step 14438, loss 0.000255491, acc 1\n",
      "2018-10-26T17:58:26.645670: step 14439, loss 1.9874e-06, acc 1\n",
      "2018-10-26T17:58:26.827185: step 14440, loss 0.000118337, acc 1\n",
      "2018-10-26T17:58:26.995736: step 14441, loss 0.286113, acc 0.984375\n",
      "2018-10-26T17:58:27.168274: step 14442, loss 3.70506e-05, acc 1\n",
      "2018-10-26T17:58:27.331838: step 14443, loss 2.00598e-05, acc 1\n",
      "2018-10-26T17:58:27.509363: step 14444, loss 1.66418e-05, acc 1\n",
      "2018-10-26T17:58:27.673923: step 14445, loss 0.00856129, acc 1\n",
      "2018-10-26T17:58:27.858432: step 14446, loss 2.04891e-08, acc 1\n",
      "2018-10-26T17:58:28.022991: step 14447, loss 1.60187e-07, acc 1\n",
      "2018-10-26T17:58:28.200517: step 14448, loss 4.2632e-06, acc 1\n",
      "2018-10-26T17:58:28.360091: step 14449, loss 7.62387e-06, acc 1\n",
      "2018-10-26T17:58:28.538613: step 14450, loss 3.11031e-05, acc 1\n",
      "2018-10-26T17:58:28.700181: step 14451, loss 4.56096e-06, acc 1\n",
      "2018-10-26T17:58:28.879702: step 14452, loss 0.000223255, acc 1\n",
      "2018-10-26T17:58:29.048252: step 14453, loss 7.43402e-06, acc 1\n",
      "2018-10-26T17:58:29.226775: step 14454, loss 2.37844e-06, acc 1\n",
      "2018-10-26T17:58:29.396322: step 14455, loss 0.000159319, acc 1\n",
      "2018-10-26T17:58:29.572850: step 14456, loss 0.000664464, acc 1\n",
      "2018-10-26T17:58:29.744391: step 14457, loss 0.000169292, acc 1\n",
      "2018-10-26T17:58:29.916930: step 14458, loss 2.35341e-05, acc 1\n",
      "2018-10-26T17:58:30.074510: step 14459, loss 2.4513e-05, acc 1\n",
      "2018-10-26T17:58:30.256025: step 14460, loss 1.72492e-05, acc 1\n",
      "2018-10-26T17:58:30.424575: step 14461, loss 5.94173e-07, acc 1\n",
      "2018-10-26T17:58:30.657950: step 14462, loss 1.77687e-06, acc 1\n",
      "2018-10-26T17:58:30.879359: step 14463, loss 6.66691e-06, acc 1\n",
      "2018-10-26T17:58:31.102762: step 14464, loss 5.12571e-05, acc 1\n",
      "2018-10-26T17:58:31.344120: step 14465, loss 0.000492549, acc 1\n",
      "2018-10-26T17:58:31.536603: step 14466, loss 0.000247699, acc 1\n",
      "2018-10-26T17:58:31.782944: step 14467, loss 6.92898e-07, acc 1\n",
      "2018-10-26T17:58:31.963462: step 14468, loss 1.28517e-06, acc 1\n",
      "2018-10-26T17:58:32.176892: step 14469, loss 2.62282e-05, acc 1\n",
      "2018-10-26T17:58:32.413261: step 14470, loss 4.03426e-06, acc 1\n",
      "2018-10-26T17:58:32.596770: step 14471, loss 4.83466e-05, acc 1\n",
      "2018-10-26T17:58:32.816184: step 14472, loss 1.64931e-05, acc 1\n",
      "2018-10-26T17:58:33.047567: step 14473, loss 4.39895e-06, acc 1\n",
      "2018-10-26T17:58:33.233071: step 14474, loss 2.89819e-06, acc 1\n",
      "2018-10-26T17:58:33.438523: step 14475, loss 5.62515e-07, acc 1\n",
      "2018-10-26T17:58:33.685861: step 14476, loss 7.63562e-05, acc 1\n",
      "2018-10-26T17:58:33.856405: step 14477, loss 0.0746861, acc 0.984375\n",
      "2018-10-26T17:58:34.062854: step 14478, loss 3.10831e-05, acc 1\n",
      "2018-10-26T17:58:34.265312: step 14479, loss 1.05981e-06, acc 1\n",
      "2018-10-26T17:58:34.481735: step 14480, loss 0.00012847, acc 1\n",
      "2018-10-26T17:58:34.671228: step 14481, loss 4.28954e-06, acc 1\n",
      "2018-10-26T17:58:34.860721: step 14482, loss 0.000661643, acc 1\n",
      "2018-10-26T17:58:35.105070: step 14483, loss 2.37854e-06, acc 1\n",
      "2018-10-26T17:58:35.343432: step 14484, loss 5.02687e-06, acc 1\n",
      "2018-10-26T17:58:35.521955: step 14485, loss 0.00019903, acc 1\n",
      "2018-10-26T17:58:35.730398: step 14486, loss 4.58333e-05, acc 1\n",
      "2018-10-26T17:58:35.896953: step 14487, loss 0.000199348, acc 1\n",
      "2018-10-26T17:58:36.096419: step 14488, loss 4.45472e-05, acc 1\n",
      "2018-10-26T17:58:36.260980: step 14489, loss 5.40322e-05, acc 1\n",
      "2018-10-26T17:58:36.434517: step 14490, loss 0.000591058, acc 1\n",
      "2018-10-26T17:58:36.636976: step 14491, loss 3.18622e-05, acc 1\n",
      "2018-10-26T17:58:36.807520: step 14492, loss 4.15351e-05, acc 1\n",
      "2018-10-26T17:58:36.982054: step 14493, loss 0.0144773, acc 0.984375\n",
      "2018-10-26T17:58:37.180523: step 14494, loss 7.91107e-05, acc 1\n",
      "2018-10-26T17:58:37.359047: step 14495, loss 0.00232219, acc 1\n",
      "2018-10-26T17:58:37.530588: step 14496, loss 8.62384e-07, acc 1\n",
      "2018-10-26T17:58:37.716092: step 14497, loss 5.11919e-05, acc 1\n",
      "2018-10-26T17:58:37.898605: step 14498, loss 8.77992e-06, acc 1\n",
      "2018-10-26T17:58:38.075134: step 14499, loss 1.23721e-05, acc 1\n",
      "2018-10-26T17:58:38.236702: step 14500, loss 6.6436e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:58:38.704452: step 14500, loss 5.10421, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14500\n",
      "\n",
      "2018-10-26T17:58:39.092443: step 14501, loss 1.08404e-06, acc 1\n",
      "2018-10-26T17:58:39.271963: step 14502, loss 0.000907232, acc 1\n",
      "2018-10-26T17:58:39.441510: step 14503, loss 6.12807e-07, acc 1\n",
      "2018-10-26T17:58:39.600087: step 14504, loss 7.69927e-06, acc 1\n",
      "2018-10-26T17:58:39.792572: step 14505, loss 0.000116678, acc 1\n",
      "2018-10-26T17:58:40.033928: step 14506, loss 2.45868e-07, acc 1\n",
      "2018-10-26T17:58:40.200482: step 14507, loss 3.05831e-06, acc 1\n",
      "2018-10-26T17:58:40.379005: step 14508, loss 4.33936e-05, acc 1\n",
      "2018-10-26T17:58:40.544563: step 14509, loss 0.000106229, acc 1\n",
      "2018-10-26T17:58:40.714110: step 14510, loss 7.37597e-07, acc 1\n",
      "2018-10-26T17:58:40.877673: step 14511, loss 8.17393e-05, acc 1\n",
      "2018-10-26T17:58:41.051209: step 14512, loss 1.7421e-05, acc 1\n",
      "2018-10-26T17:58:41.212778: step 14513, loss 7.18973e-07, acc 1\n",
      "2018-10-26T17:58:41.391301: step 14514, loss 5.21538e-07, acc 1\n",
      "2018-10-26T17:58:41.551872: step 14515, loss 4.21666e-06, acc 1\n",
      "2018-10-26T17:58:41.722416: step 14516, loss 3.33013e-05, acc 1\n",
      "2018-10-26T17:58:41.896951: step 14517, loss 3.35273e-07, acc 1\n",
      "2018-10-26T17:58:42.072481: step 14518, loss 2.08464e-05, acc 1\n",
      "2018-10-26T17:58:42.237041: step 14519, loss 3.7847e-06, acc 1\n",
      "2018-10-26T17:58:42.408582: step 14520, loss 1.10124e-05, acc 1\n",
      "2018-10-26T17:58:42.571148: step 14521, loss 3.07874e-06, acc 1\n",
      "2018-10-26T17:58:42.743687: step 14522, loss 0.000202695, acc 1\n",
      "2018-10-26T17:58:42.911239: step 14523, loss 3.59834e-06, acc 1\n",
      "2018-10-26T17:58:43.085772: step 14524, loss 4.98978e-05, acc 1\n",
      "2018-10-26T17:58:43.245347: step 14525, loss 5.72474e-06, acc 1\n",
      "2018-10-26T17:58:43.416889: step 14526, loss 7.0035e-07, acc 1\n",
      "2018-10-26T17:58:43.585438: step 14527, loss 4.064e-06, acc 1\n",
      "2018-10-26T17:58:43.760969: step 14528, loss 4.97319e-07, acc 1\n",
      "2018-10-26T17:58:43.922538: step 14529, loss 0.00855778, acc 1\n",
      "2018-10-26T17:58:44.098070: step 14530, loss 0.000194772, acc 1\n",
      "2018-10-26T17:58:44.265622: step 14531, loss 0.000114286, acc 1\n",
      "2018-10-26T17:58:44.438160: step 14532, loss 4.59447e-05, acc 1\n",
      "2018-10-26T17:58:44.598731: step 14533, loss 9.68554e-07, acc 1\n",
      "2018-10-26T17:58:44.781243: step 14534, loss 6.02319e-06, acc 1\n",
      "2018-10-26T17:58:44.947797: step 14535, loss 9.70514e-06, acc 1\n",
      "2018-10-26T17:58:45.116347: step 14536, loss 0.000244849, acc 1\n",
      "2018-10-26T17:58:45.276919: step 14537, loss 2.69654e-05, acc 1\n",
      "2018-10-26T17:58:45.463421: step 14538, loss 6.01629e-07, acc 1\n",
      "2018-10-26T17:58:45.623991: step 14539, loss 4.54737e-05, acc 1\n",
      "2018-10-26T17:58:45.796530: step 14540, loss 9.49948e-08, acc 1\n",
      "2018-10-26T17:58:45.966077: step 14541, loss 3.16649e-07, acc 1\n",
      "2018-10-26T17:58:46.140611: step 14542, loss 0.00267672, acc 1\n",
      "2018-10-26T17:58:46.302180: step 14543, loss 0.000844814, acc 1\n",
      "2018-10-26T17:58:46.479706: step 14544, loss 1.58325e-07, acc 1\n",
      "2018-10-26T17:58:46.652257: step 14545, loss 1.16039e-06, acc 1\n",
      "2018-10-26T17:58:46.820794: step 14546, loss 1.65195e-05, acc 1\n",
      "2018-10-26T17:58:46.984357: step 14547, loss 0.000141223, acc 1\n",
      "2018-10-26T17:58:47.159888: step 14548, loss 2.1717e-06, acc 1\n",
      "2018-10-26T17:58:47.323451: step 14549, loss 9.69038e-06, acc 1\n",
      "2018-10-26T17:58:47.488011: step 14550, loss 2.6086e-06, acc 1\n",
      "2018-10-26T17:58:47.648582: step 14551, loss 0.000484762, acc 1\n",
      "2018-10-26T17:58:47.826107: step 14552, loss 0.000157566, acc 1\n",
      "2018-10-26T17:58:47.991666: step 14553, loss 8.62385e-07, acc 1\n",
      "2018-10-26T17:58:48.165209: step 14554, loss 0.00079172, acc 1\n",
      "2018-10-26T17:58:48.323777: step 14555, loss 0.000290142, acc 1\n",
      "2018-10-26T17:58:48.498311: step 14556, loss 4.85703e-06, acc 1\n",
      "2018-10-26T17:58:48.655891: step 14557, loss 7.01874e-06, acc 1\n",
      "2018-10-26T17:58:48.846381: step 14558, loss 6.33299e-08, acc 1\n",
      "2018-10-26T17:58:49.035876: step 14559, loss 5.44505e-05, acc 1\n",
      "2018-10-26T17:58:49.267258: step 14560, loss 9.5552e-07, acc 1\n",
      "2018-10-26T17:58:49.429823: step 14561, loss 1.21072e-07, acc 1\n",
      "2018-10-26T17:58:49.616325: step 14562, loss 3.84415e-06, acc 1\n",
      "2018-10-26T17:58:49.777893: step 14563, loss 1.50496e-06, acc 1\n",
      "2018-10-26T17:58:49.974368: step 14564, loss 3.68958e-06, acc 1\n",
      "2018-10-26T17:58:50.134939: step 14565, loss 4.90148e-05, acc 1\n",
      "2018-10-26T17:58:50.308476: step 14566, loss 4.40496e-06, acc 1\n",
      "2018-10-26T17:58:50.470044: step 14567, loss 0.000320956, acc 1\n",
      "2018-10-26T17:58:50.651558: step 14568, loss 3.07336e-07, acc 1\n",
      "2018-10-26T17:58:50.811132: step 14569, loss 2.18362e-05, acc 1\n",
      "2018-10-26T17:58:50.988658: step 14570, loss 1.82863e-05, acc 1\n",
      "2018-10-26T17:58:51.176156: step 14571, loss 1.32247e-07, acc 1\n",
      "2018-10-26T17:58:51.357671: step 14572, loss 0, acc 1\n",
      "2018-10-26T17:58:51.534200: step 14573, loss 1.19044e-05, acc 1\n",
      "2018-10-26T17:58:51.710729: step 14574, loss 4.6263e-06, acc 1\n",
      "2018-10-26T17:58:51.869312: step 14575, loss 2.89143e-05, acc 1\n",
      "2018-10-26T17:58:52.044836: step 14576, loss 1.45841e-06, acc 1\n",
      "2018-10-26T17:58:52.206404: step 14577, loss 6.91987e-05, acc 1\n",
      "2018-10-26T17:58:52.391909: step 14578, loss 5.54438e-05, acc 1\n",
      "2018-10-26T17:58:52.561455: step 14579, loss 7.73411e-06, acc 1\n",
      "2018-10-26T17:58:52.735989: step 14580, loss 9.14904e-06, acc 1\n",
      "2018-10-26T17:58:52.899552: step 14581, loss 6.93329e-06, acc 1\n",
      "2018-10-26T17:58:53.080069: step 14582, loss 0.00050884, acc 1\n",
      "2018-10-26T17:58:53.254603: step 14583, loss 2.95762e-06, acc 1\n",
      "2018-10-26T17:58:53.429137: step 14584, loss 3.35803e-06, acc 1\n",
      "2018-10-26T17:58:53.596690: step 14585, loss 9.80568e-06, acc 1\n",
      "2018-10-26T17:58:53.773218: step 14586, loss 7.46268e-06, acc 1\n",
      "2018-10-26T17:58:53.930797: step 14587, loss 4.10491e-06, acc 1\n",
      "2018-10-26T17:58:54.106328: step 14588, loss 2.7567e-07, acc 1\n",
      "2018-10-26T17:58:54.268893: step 14589, loss 1.26795e-05, acc 1\n",
      "2018-10-26T17:58:54.443427: step 14590, loss 1.02834e-05, acc 1\n",
      "2018-10-26T17:58:54.606991: step 14591, loss 4.71864e-05, acc 1\n",
      "2018-10-26T17:58:54.783518: step 14592, loss 1.50874e-07, acc 1\n",
      "2018-10-26T17:58:54.965033: step 14593, loss 3.15414e-05, acc 1\n",
      "2018-10-26T17:58:55.142560: step 14594, loss 1.17344e-06, acc 1\n",
      "2018-10-26T17:58:55.316096: step 14595, loss 0.00790261, acc 1\n",
      "2018-10-26T17:58:55.482651: step 14596, loss 8.71697e-07, acc 1\n",
      "2018-10-26T17:58:55.656210: step 14597, loss 2.56818e-05, acc 1\n",
      "2018-10-26T17:58:55.821745: step 14598, loss 1.49011e-07, acc 1\n",
      "2018-10-26T17:58:55.996278: step 14599, loss 1.53478e-06, acc 1\n",
      "2018-10-26T17:58:56.169815: step 14600, loss 5.74216e-06, acc 1\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:58:56.606648: step 14600, loss 5.13964, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14600\n",
      "\n",
      "2018-10-26T17:58:56.957042: step 14601, loss 4.83514e-06, acc 1\n",
      "2018-10-26T17:58:57.117612: step 14602, loss 1.42673e-06, acc 1\n",
      "2018-10-26T17:58:57.283171: step 14603, loss 0.00254856, acc 1\n",
      "2018-10-26T17:58:57.441747: step 14604, loss 1.35226e-06, acc 1\n",
      "2018-10-26T17:58:57.629246: step 14605, loss 0.000211479, acc 1\n",
      "2018-10-26T17:58:57.873594: step 14606, loss 7.36553e-05, acc 1\n",
      "2018-10-26T17:58:58.037156: step 14607, loss 0.000516185, acc 1\n",
      "2018-10-26T17:58:58.210692: step 14608, loss 2.57969e-06, acc 1\n",
      "2018-10-26T17:58:58.374255: step 14609, loss 1.03957e-05, acc 1\n",
      "2018-10-26T17:58:58.550784: step 14610, loss 0.000136812, acc 1\n",
      "2018-10-26T17:58:58.712352: step 14611, loss 5.30614e-05, acc 1\n",
      "2018-10-26T17:58:58.876913: step 14612, loss 0.000448808, acc 1\n",
      "2018-10-26T17:58:59.038481: step 14613, loss 6.03406e-05, acc 1\n",
      "2018-10-26T17:58:59.215009: step 14614, loss 0.000258368, acc 1\n",
      "2018-10-26T17:58:59.379569: step 14615, loss 5.86089e-05, acc 1\n",
      "2018-10-26T17:58:59.551111: step 14616, loss 2.45868e-07, acc 1\n",
      "2018-10-26T17:58:59.713676: step 14617, loss 1.05797e-06, acc 1\n",
      "2018-10-26T17:58:59.891231: step 14618, loss 1.21072e-07, acc 1\n",
      "2018-10-26T17:59:00.062744: step 14619, loss 8.75995e-06, acc 1\n",
      "2018-10-26T17:59:00.235283: step 14620, loss 4.37176e-05, acc 1\n",
      "2018-10-26T17:59:00.403833: step 14621, loss 0.000677559, acc 1\n",
      "2018-10-26T17:59:00.575374: step 14622, loss 7.2615e-06, acc 1\n",
      "2018-10-26T17:59:00.737940: step 14623, loss 2.84552e-05, acc 1\n",
      "2018-10-26T17:59:00.905492: step 14624, loss 9.30872e-06, acc 1\n",
      "2018-10-26T17:59:01.067061: step 14625, loss 2.48088e-06, acc 1\n",
      "2018-10-26T17:59:01.244586: step 14626, loss 2.84178e-05, acc 1\n",
      "2018-10-26T17:59:01.402166: step 14627, loss 0.00030435, acc 1\n",
      "2018-10-26T17:59:01.579692: step 14628, loss 0.00012761, acc 1\n",
      "2018-10-26T17:59:01.741258: step 14629, loss 7.20832e-07, acc 1\n",
      "2018-10-26T17:59:01.912800: step 14630, loss 9.12695e-08, acc 1\n",
      "2018-10-26T17:59:02.072375: step 14631, loss 1.49012e-08, acc 1\n",
      "2018-10-26T17:59:02.244914: step 14632, loss 5.95825e-06, acc 1\n",
      "2018-10-26T17:59:02.402493: step 14633, loss 4.53667e-05, acc 1\n",
      "2018-10-26T17:59:02.576030: step 14634, loss 0.00144345, acc 1\n",
      "2018-10-26T17:59:02.737597: step 14635, loss 1.25655e-05, acc 1\n",
      "2018-10-26T17:59:02.909139: step 14636, loss 5.25263e-07, acc 1\n",
      "2018-10-26T17:59:03.073700: step 14637, loss 1.97778e-05, acc 1\n",
      "2018-10-26T17:59:03.243245: step 14638, loss 0.000582835, acc 1\n",
      "2018-10-26T17:59:03.408803: step 14639, loss 0.0860554, acc 0.984375\n",
      "2018-10-26T17:59:03.593327: step 14640, loss 2.94184e-05, acc 1\n",
      "2018-10-26T17:59:03.759866: step 14641, loss 8.61819e-06, acc 1\n",
      "2018-10-26T17:59:03.931407: step 14642, loss 0.00735851, acc 1\n",
      "2018-10-26T17:59:04.093974: step 14643, loss 6.08934e-05, acc 1\n",
      "2018-10-26T17:59:04.266512: step 14644, loss 4.10291e-06, acc 1\n",
      "2018-10-26T17:59:04.428081: step 14645, loss 9.29721e-05, acc 1\n",
      "2018-10-26T17:59:04.598625: step 14646, loss 5.61846e-05, acc 1\n",
      "2018-10-26T17:59:04.772631: step 14647, loss 0.000123114, acc 1\n",
      "2018-10-26T17:59:04.955143: step 14648, loss 3.92443e-06, acc 1\n",
      "2018-10-26T17:59:05.117709: step 14649, loss 9.63738e-05, acc 1\n",
      "2018-10-26T17:59:05.285261: step 14650, loss 1.62978e-06, acc 1\n",
      "2018-10-26T17:59:05.454808: step 14651, loss 5.0983e-05, acc 1\n",
      "2018-10-26T17:59:05.631351: step 14652, loss 9.14395e-06, acc 1\n",
      "2018-10-26T17:59:05.804875: step 14653, loss 0.00177862, acc 1\n",
      "2018-10-26T17:59:05.979406: step 14654, loss 2.69186e-05, acc 1\n",
      "2018-10-26T17:59:06.137983: step 14655, loss 6.26624e-05, acc 1\n",
      "2018-10-26T17:59:06.309525: step 14656, loss 2.08049e-06, acc 1\n",
      "2018-10-26T17:59:06.476080: step 14657, loss 7.26419e-07, acc 1\n",
      "2018-10-26T17:59:06.655600: step 14658, loss 9.01135e-06, acc 1\n",
      "2018-10-26T17:59:06.818166: step 14659, loss 0.00016172, acc 1\n",
      "2018-10-26T17:59:06.996689: step 14660, loss 1.69498e-06, acc 1\n",
      "2018-10-26T17:59:07.158257: step 14661, loss 4.16806e-05, acc 1\n",
      "2018-10-26T17:59:07.336780: step 14662, loss 1.65546e-05, acc 1\n",
      "2018-10-26T17:59:07.499345: step 14663, loss 3.00604e-06, acc 1\n",
      "2018-10-26T17:59:07.673879: step 14664, loss 9.44346e-07, acc 1\n",
      "2018-10-26T17:59:07.837443: step 14665, loss 1.42633e-05, acc 1\n",
      "2018-10-26T17:59:08.015965: step 14666, loss 1.43604e-06, acc 1\n",
      "2018-10-26T17:59:08.178531: step 14667, loss 3.90253e-05, acc 1\n",
      "2018-10-26T17:59:08.354063: step 14668, loss 1.43607e-06, acc 1\n",
      "2018-10-26T17:59:08.516628: step 14669, loss 8.27784e-06, acc 1\n",
      "2018-10-26T17:59:08.690164: step 14670, loss 4.55113e-05, acc 1\n",
      "2018-10-26T17:59:08.864698: step 14671, loss 0.00343623, acc 1\n",
      "2018-10-26T17:59:09.037237: step 14672, loss 3.5517e-05, acc 1\n",
      "2018-10-26T17:59:09.203791: step 14673, loss 1.15668e-06, acc 1\n",
      "2018-10-26T17:59:09.385321: step 14674, loss 2.62632e-07, acc 1\n",
      "2018-10-26T17:59:09.547873: step 14675, loss 5.58794e-09, acc 1\n",
      "2018-10-26T17:59:09.727393: step 14676, loss 9.12932e-06, acc 1\n",
      "2018-10-26T17:59:09.887963: step 14677, loss 0.000247745, acc 1\n",
      "2018-10-26T17:59:10.069479: step 14678, loss 0.0004389, acc 1\n",
      "2018-10-26T17:59:10.234039: step 14679, loss 0.000250806, acc 1\n",
      "2018-10-26T17:59:10.409571: step 14680, loss 4.74175e-05, acc 1\n",
      "2018-10-26T17:59:10.569144: step 14681, loss 2.91518e-05, acc 1\n",
      "2018-10-26T17:59:10.748664: step 14682, loss 3.39353e-05, acc 1\n",
      "2018-10-26T17:59:10.923198: step 14683, loss 0.000130139, acc 1\n",
      "2018-10-26T17:59:11.102718: step 14684, loss 2.16241e-06, acc 1\n",
      "2018-10-26T17:59:11.266281: step 14685, loss 0.000512919, acc 1\n",
      "2018-10-26T17:59:11.446800: step 14686, loss 0.000279329, acc 1\n",
      "2018-10-26T17:59:11.620336: step 14687, loss 2.50892e-06, acc 1\n",
      "2018-10-26T17:59:11.789882: step 14688, loss 7.21322e-06, acc 1\n",
      "2018-10-26T17:59:11.962422: step 14689, loss 0.00178086, acc 1\n",
      "2018-10-26T17:59:12.122992: step 14690, loss 8.71983e-05, acc 1\n",
      "2018-10-26T17:59:12.299521: step 14691, loss 7.43186e-07, acc 1\n",
      "2018-10-26T17:59:12.466076: step 14692, loss 0.000132771, acc 1\n",
      "2018-10-26T17:59:12.642603: step 14693, loss 2.36554e-07, acc 1\n",
      "2018-10-26T17:59:12.802177: step 14694, loss 1.34293e-06, acc 1\n",
      "2018-10-26T17:59:12.982695: step 14695, loss 3.98931e-06, acc 1\n",
      "2018-10-26T17:59:13.141272: step 14696, loss 9.64297e-05, acc 1\n",
      "2018-10-26T17:59:13.314807: step 14697, loss 1.24481e-05, acc 1\n",
      "2018-10-26T17:59:13.481363: step 14698, loss 8.24232e-06, acc 1\n",
      "2018-10-26T17:59:13.661881: step 14699, loss 1.60185e-05, acc 1\n",
      "2018-10-26T17:59:13.818462: step 14700, loss 1.78015e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:59:14.249311: step 14700, loss 5.12372, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14700\n",
      "\n",
      "2018-10-26T17:59:14.594639: step 14701, loss 0.00015773, acc 1\n",
      "2018-10-26T17:59:14.762191: step 14702, loss 1.71151e-05, acc 1\n",
      "2018-10-26T17:59:14.935726: step 14703, loss 2.13264e-06, acc 1\n",
      "2018-10-26T17:59:15.109295: step 14704, loss 1.41143e-05, acc 1\n",
      "2018-10-26T17:59:15.308730: step 14705, loss 6.02276e-06, acc 1\n",
      "2018-10-26T17:59:15.549087: step 14706, loss 1.66143e-06, acc 1\n",
      "2018-10-26T17:59:15.718635: step 14707, loss 2.19969e-06, acc 1\n",
      "2018-10-26T17:59:15.903142: step 14708, loss 8.94059e-07, acc 1\n",
      "2018-10-26T17:59:16.070695: step 14709, loss 1.83477e-05, acc 1\n",
      "2018-10-26T17:59:16.259190: step 14710, loss 4.67518e-07, acc 1\n",
      "2018-10-26T17:59:16.490575: step 14711, loss 7.11054e-05, acc 1\n",
      "2018-10-26T17:59:16.678073: step 14712, loss 4.33954e-06, acc 1\n",
      "2018-10-26T17:59:16.884521: step 14713, loss 4.1888e-06, acc 1\n",
      "2018-10-26T17:59:17.091966: step 14714, loss 5.11073e-06, acc 1\n",
      "2018-10-26T17:59:17.283453: step 14715, loss 6.57935e-06, acc 1\n",
      "2018-10-26T17:59:17.517828: step 14716, loss 4.41442e-07, acc 1\n",
      "2018-10-26T17:59:17.758186: step 14717, loss 3.50947e-05, acc 1\n",
      "2018-10-26T17:59:18.013505: step 14718, loss 0.000147453, acc 1\n",
      "2018-10-26T17:59:18.295750: step 14719, loss 1.99302e-07, acc 1\n",
      "2018-10-26T17:59:18.530124: step 14720, loss 3.00604e-06, acc 1\n",
      "2018-10-26T17:59:18.795416: step 14721, loss 0.00029569, acc 1\n",
      "2018-10-26T17:59:19.048738: step 14722, loss 6.46228e-06, acc 1\n",
      "2018-10-26T17:59:19.302061: step 14723, loss 1.05092e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:59:19.539427: step 14724, loss 7.49744e-06, acc 1\n",
      "2018-10-26T17:59:19.799732: step 14725, loss 7.27131e-06, acc 1\n",
      "2018-10-26T17:59:20.075993: step 14726, loss 3.95586e-06, acc 1\n",
      "2018-10-26T17:59:20.320341: step 14727, loss 6.7836e-05, acc 1\n",
      "2018-10-26T17:59:20.559701: step 14728, loss 5.71411e-06, acc 1\n",
      "2018-10-26T17:59:20.811029: step 14729, loss 4.99503e-06, acc 1\n",
      "2018-10-26T17:59:21.004512: step 14730, loss 3.24099e-07, acc 1\n",
      "2018-10-26T17:59:21.181039: step 14731, loss 6.14672e-08, acc 1\n",
      "2018-10-26T17:59:21.357568: step 14732, loss 5.60578e-06, acc 1\n",
      "2018-10-26T17:59:21.528113: step 14733, loss 6.5671e-06, acc 1\n",
      "2018-10-26T17:59:21.700651: step 14734, loss 5.36212e-06, acc 1\n",
      "2018-10-26T17:59:21.864215: step 14735, loss 1.58229e-05, acc 1\n",
      "2018-10-26T17:59:22.034758: step 14736, loss 4.62573e-05, acc 1\n",
      "2018-10-26T17:59:22.200317: step 14737, loss 3.28912e-06, acc 1\n",
      "2018-10-26T17:59:22.373853: step 14738, loss 6.70543e-07, acc 1\n",
      "2018-10-26T17:59:22.536419: step 14739, loss 8.86554e-06, acc 1\n",
      "2018-10-26T17:59:22.707961: step 14740, loss 1.24454e-05, acc 1\n",
      "2018-10-26T17:59:22.873518: step 14741, loss 0.000127242, acc 1\n",
      "2018-10-26T17:59:23.052041: step 14742, loss 2.04891e-08, acc 1\n",
      "2018-10-26T17:59:23.216601: step 14743, loss 1.32248e-07, acc 1\n",
      "2018-10-26T17:59:23.384154: step 14744, loss 7.87187e-06, acc 1\n",
      "2018-10-26T17:59:23.554697: step 14745, loss 3.56155e-05, acc 1\n",
      "2018-10-26T17:59:23.724245: step 14746, loss 0.000117019, acc 1\n",
      "2018-10-26T17:59:23.888806: step 14747, loss 6.29893e-05, acc 1\n",
      "2018-10-26T17:59:24.059350: step 14748, loss 7.07804e-08, acc 1\n",
      "2018-10-26T17:59:24.228897: step 14749, loss 0.000344074, acc 1\n",
      "2018-10-26T17:59:24.399441: step 14750, loss 2.05162e-05, acc 1\n",
      "2018-10-26T17:59:24.566993: step 14751, loss 4.78697e-07, acc 1\n",
      "2018-10-26T17:59:24.739532: step 14752, loss 4.11883e-05, acc 1\n",
      "2018-10-26T17:59:24.907085: step 14753, loss 9.81587e-07, acc 1\n",
      "2018-10-26T17:59:25.078627: step 14754, loss 0.000579258, acc 1\n",
      "2018-10-26T17:59:25.241192: step 14755, loss 4.61194e-05, acc 1\n",
      "2018-10-26T17:59:25.412733: step 14756, loss 8.12104e-07, acc 1\n",
      "2018-10-26T17:59:25.579310: step 14757, loss 0.000228068, acc 1\n",
      "2018-10-26T17:59:25.747846: step 14758, loss 7.59952e-07, acc 1\n",
      "2018-10-26T17:59:25.914393: step 14759, loss 1.71881e-05, acc 1\n",
      "2018-10-26T17:59:26.087929: step 14760, loss 6.51925e-08, acc 1\n",
      "2018-10-26T17:59:26.246506: step 14761, loss 7.20204e-06, acc 1\n",
      "2018-10-26T17:59:26.417051: step 14762, loss 1.49011e-07, acc 1\n",
      "2018-10-26T17:59:26.582607: step 14763, loss 9.68573e-08, acc 1\n",
      "2018-10-26T17:59:26.755147: step 14764, loss 0.00026006, acc 1\n",
      "2018-10-26T17:59:26.924693: step 14765, loss 9.98423e-05, acc 1\n",
      "2018-10-26T17:59:27.099227: step 14766, loss 0.000138659, acc 1\n",
      "2018-10-26T17:59:27.261794: step 14767, loss 6.19554e-05, acc 1\n",
      "2018-10-26T17:59:27.442314: step 14768, loss 0.0034397, acc 1\n",
      "2018-10-26T17:59:27.607869: step 14769, loss 1.49633e-05, acc 1\n",
      "2018-10-26T17:59:27.777416: step 14770, loss 1.67829e-05, acc 1\n",
      "2018-10-26T17:59:27.942974: step 14771, loss 1.10975e-05, acc 1\n",
      "2018-10-26T17:59:28.113518: step 14772, loss 3.79957e-06, acc 1\n",
      "2018-10-26T17:59:28.272094: step 14773, loss 5.53133e-06, acc 1\n",
      "2018-10-26T17:59:28.444633: step 14774, loss 2.34693e-07, acc 1\n",
      "2018-10-26T17:59:28.620165: step 14775, loss 6.88206e-05, acc 1\n",
      "2018-10-26T17:59:28.792703: step 14776, loss 2.24722e-05, acc 1\n",
      "2018-10-26T17:59:28.960255: step 14777, loss 2.99299e-06, acc 1\n",
      "2018-10-26T17:59:29.123819: step 14778, loss 1.28892e-06, acc 1\n",
      "2018-10-26T17:59:29.308325: step 14779, loss 3.4214e-06, acc 1\n",
      "2018-10-26T17:59:29.471889: step 14780, loss 2.50515e-06, acc 1\n",
      "2018-10-26T17:59:29.640438: step 14781, loss 3.40455e-06, acc 1\n",
      "2018-10-26T17:59:29.804001: step 14782, loss 5.03783e-06, acc 1\n",
      "2018-10-26T17:59:29.975543: step 14783, loss 7.12907e-06, acc 1\n",
      "2018-10-26T17:59:30.135117: step 14784, loss 0.000110797, acc 1\n",
      "2018-10-26T17:59:30.305661: step 14785, loss 4.41443e-07, acc 1\n",
      "2018-10-26T17:59:30.473213: step 14786, loss 0.00110524, acc 1\n",
      "2018-10-26T17:59:30.645752: step 14787, loss 4.05819e-06, acc 1\n",
      "2018-10-26T17:59:30.804328: step 14788, loss 0.000280982, acc 1\n",
      "2018-10-26T17:59:30.974872: step 14789, loss 1.04119e-06, acc 1\n",
      "2018-10-26T17:59:31.133449: step 14790, loss 4.97269e-06, acc 1\n",
      "2018-10-26T17:59:31.313968: step 14791, loss 5.37885e-06, acc 1\n",
      "2018-10-26T17:59:31.479524: step 14792, loss 0.00282091, acc 1\n",
      "2018-10-26T17:59:31.650069: step 14793, loss 0.000139626, acc 1\n",
      "2018-10-26T17:59:31.818618: step 14794, loss 3.96741e-07, acc 1\n",
      "2018-10-26T17:59:31.992155: step 14795, loss 1.3869e-05, acc 1\n",
      "2018-10-26T17:59:32.153723: step 14796, loss 1.76756e-06, acc 1\n",
      "2018-10-26T17:59:32.324267: step 14797, loss 8.94068e-08, acc 1\n",
      "2018-10-26T17:59:32.487831: step 14798, loss 8.51211e-07, acc 1\n",
      "2018-10-26T17:59:32.658375: step 14799, loss 6.8152e-05, acc 1\n",
      "2018-10-26T17:59:32.820940: step 14800, loss 8.16965e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:59:33.244808: step 14800, loss 5.14898, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14800\n",
      "\n",
      "2018-10-26T17:59:33.612972: step 14801, loss 4.73406e-05, acc 1\n",
      "2018-10-26T17:59:33.783515: step 14802, loss 1.39137e-06, acc 1\n",
      "2018-10-26T17:59:33.945084: step 14803, loss 0.0127775, acc 0.984375\n",
      "2018-10-26T17:59:34.116625: step 14804, loss 1.166e-06, acc 1\n",
      "2018-10-26T17:59:34.294150: step 14805, loss 2.10473e-06, acc 1\n",
      "2018-10-26T17:59:34.549468: step 14806, loss 2.20544e-05, acc 1\n",
      "2018-10-26T17:59:34.710039: step 14807, loss 2.33013e-06, acc 1\n",
      "2018-10-26T17:59:34.886568: step 14808, loss 0.000169661, acc 1\n",
      "2018-10-26T17:59:35.047139: step 14809, loss 2.58102e-05, acc 1\n",
      "2018-10-26T17:59:35.225661: step 14810, loss 8.8672e-06, acc 1\n",
      "2018-10-26T17:59:35.403187: step 14811, loss 9.38994e-05, acc 1\n",
      "2018-10-26T17:59:35.579715: step 14812, loss 0.000373502, acc 1\n",
      "2018-10-26T17:59:35.756244: step 14813, loss 0.000226735, acc 1\n",
      "2018-10-26T17:59:35.917813: step 14814, loss 2.14203e-07, acc 1\n",
      "2018-10-26T17:59:36.098330: step 14815, loss 1.35133e-05, acc 1\n",
      "2018-10-26T17:59:36.289819: step 14816, loss 0.0852627, acc 0.984375\n",
      "2018-10-26T17:59:36.492307: step 14817, loss 0.00608135, acc 1\n",
      "2018-10-26T17:59:36.721666: step 14818, loss 3.90178e-06, acc 1\n",
      "2018-10-26T17:59:36.958061: step 14819, loss 0.000165304, acc 1\n",
      "2018-10-26T17:59:37.208363: step 14820, loss 3.23155e-06, acc 1\n",
      "2018-10-26T17:59:37.456701: step 14821, loss 0.000167332, acc 1\n",
      "2018-10-26T17:59:37.662152: step 14822, loss 2.23069e-05, acc 1\n",
      "2018-10-26T17:59:37.906499: step 14823, loss 0.000122754, acc 1\n",
      "2018-10-26T17:59:38.086019: step 14824, loss 0.00673671, acc 1\n",
      "2018-10-26T17:59:38.306430: step 14825, loss 1.41942e-05, acc 1\n",
      "2018-10-26T17:59:38.528838: step 14826, loss 0.00700498, acc 1\n",
      "2018-10-26T17:59:38.699380: step 14827, loss 4.72115e-06, acc 1\n",
      "2018-10-26T17:59:38.932757: step 14828, loss 1.21069e-06, acc 1\n",
      "2018-10-26T17:59:39.118261: step 14829, loss 1.24423e-06, acc 1\n",
      "2018-10-26T17:59:39.348645: step 14830, loss 7.35298e-06, acc 1\n",
      "2018-10-26T17:59:39.573046: step 14831, loss 7.73562e-06, acc 1\n",
      "2018-10-26T17:59:39.763536: step 14832, loss 0.0252601, acc 0.984375\n",
      "2018-10-26T17:59:39.991927: step 14833, loss 0.00079067, acc 1\n",
      "2018-10-26T17:59:40.185410: step 14834, loss 1.99675e-05, acc 1\n",
      "2018-10-26T17:59:40.480621: step 14835, loss 0.000123466, acc 1\n",
      "2018-10-26T17:59:40.710008: step 14836, loss 6.89772e-06, acc 1\n",
      "2018-10-26T17:59:40.950366: step 14837, loss 7.45058e-09, acc 1\n",
      "2018-10-26T17:59:41.183743: step 14838, loss 8.94067e-08, acc 1\n",
      "2018-10-26T17:59:41.427097: step 14839, loss 0.00127661, acc 1\n",
      "2018-10-26T17:59:41.612596: step 14840, loss 1.34807e-05, acc 1\n",
      "2018-10-26T17:59:41.812063: step 14841, loss 0.00737871, acc 1\n",
      "2018-10-26T17:59:41.998565: step 14842, loss 0.000166236, acc 1\n",
      "2018-10-26T17:59:42.205015: step 14843, loss 0.000164451, acc 1\n",
      "2018-10-26T17:59:42.390518: step 14844, loss 7.60729e-06, acc 1\n",
      "2018-10-26T17:59:42.576023: step 14845, loss 5.63181e-06, acc 1\n",
      "2018-10-26T17:59:42.801420: step 14846, loss 5.66241e-07, acc 1\n",
      "2018-10-26T17:59:42.968973: step 14847, loss 2.38031e-06, acc 1\n",
      "2018-10-26T17:59:43.143506: step 14848, loss 1.54225e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T17:59:43.338984: step 14849, loss 5.58793e-08, acc 1\n",
      "2018-10-26T17:59:43.514515: step 14850, loss 4.51382e-06, acc 1\n",
      "2018-10-26T17:59:43.690046: step 14851, loss 1.55971e-05, acc 1\n",
      "2018-10-26T17:59:43.850617: step 14852, loss 1.30385e-08, acc 1\n",
      "2018-10-26T17:59:44.054073: step 14853, loss 4.33992e-07, acc 1\n",
      "2018-10-26T17:59:44.221625: step 14854, loss 2.28903e-06, acc 1\n",
      "2018-10-26T17:59:44.395162: step 14855, loss 4.66912e-06, acc 1\n",
      "2018-10-26T17:59:44.565707: step 14856, loss 0.00109269, acc 1\n",
      "2018-10-26T17:59:44.773152: step 14857, loss 1.6368e-05, acc 1\n",
      "2018-10-26T17:59:44.939708: step 14858, loss 7.33947e-05, acc 1\n",
      "2018-10-26T17:59:45.114241: step 14859, loss 0.000415771, acc 1\n",
      "2018-10-26T17:59:45.279799: step 14860, loss 0.000127032, acc 1\n",
      "2018-10-26T17:59:45.466316: step 14861, loss 1.13059e-06, acc 1\n",
      "2018-10-26T17:59:45.639837: step 14862, loss 3.26463e-05, acc 1\n",
      "2018-10-26T17:59:45.821352: step 14863, loss 0.000161605, acc 1\n",
      "2018-10-26T17:59:45.984915: step 14864, loss 0.000130336, acc 1\n",
      "2018-10-26T17:59:46.162440: step 14865, loss 0.00804198, acc 1\n",
      "2018-10-26T17:59:46.328996: step 14866, loss 1.11759e-07, acc 1\n",
      "2018-10-26T17:59:46.500537: step 14867, loss 0.000125, acc 1\n",
      "2018-10-26T17:59:46.664100: step 14868, loss 2.25379e-07, acc 1\n",
      "2018-10-26T17:59:46.833647: step 14869, loss 0.000257075, acc 1\n",
      "2018-10-26T17:59:46.996212: step 14870, loss 2.92433e-07, acc 1\n",
      "2018-10-26T17:59:47.172740: step 14871, loss 9.08947e-07, acc 1\n",
      "2018-10-26T17:59:47.338299: step 14872, loss 0.00573728, acc 1\n",
      "2018-10-26T17:59:47.515825: step 14873, loss 2.96485e-05, acc 1\n",
      "2018-10-26T17:59:47.676401: step 14874, loss 0.000101818, acc 1\n",
      "2018-10-26T17:59:47.846940: step 14875, loss 1.32248e-07, acc 1\n",
      "2018-10-26T17:59:48.010503: step 14876, loss 0.00100423, acc 1\n",
      "2018-10-26T17:59:48.180050: step 14877, loss 0.000435049, acc 1\n",
      "2018-10-26T17:59:48.349596: step 14878, loss 4.3028e-05, acc 1\n",
      "2018-10-26T17:59:48.520140: step 14879, loss 1.65775e-07, acc 1\n",
      "2018-10-26T17:59:48.682706: step 14880, loss 9.87201e-08, acc 1\n",
      "2018-10-26T17:59:48.854248: step 14881, loss 1.19209e-07, acc 1\n",
      "2018-10-26T17:59:49.021801: step 14882, loss 1.2889e-06, acc 1\n",
      "2018-10-26T17:59:49.192344: step 14883, loss 9.16165e-06, acc 1\n",
      "2018-10-26T17:59:49.400788: step 14884, loss 2.22113e-05, acc 1\n",
      "2018-10-26T17:59:49.574325: step 14885, loss 4.29861e-06, acc 1\n",
      "2018-10-26T17:59:49.750852: step 14886, loss 2.03767e-06, acc 1\n",
      "2018-10-26T17:59:49.916410: step 14887, loss 6.89821e-05, acc 1\n",
      "2018-10-26T17:59:50.091941: step 14888, loss 4.60126e-05, acc 1\n",
      "2018-10-26T17:59:50.253510: step 14889, loss 2.32815e-06, acc 1\n",
      "2018-10-26T17:59:50.423057: step 14890, loss 6.05058e-06, acc 1\n",
      "2018-10-26T17:59:50.590609: step 14891, loss 1.08031e-06, acc 1\n",
      "2018-10-26T17:59:50.769131: step 14892, loss 2.64849e-06, acc 1\n",
      "2018-10-26T17:59:50.935687: step 14893, loss 1.63912e-07, acc 1\n",
      "2018-10-26T17:59:51.104236: step 14894, loss 4.84938e-05, acc 1\n",
      "2018-10-26T17:59:51.265805: step 14895, loss 4.87761e-06, acc 1\n",
      "2018-10-26T17:59:51.440338: step 14896, loss 1.32247e-07, acc 1\n",
      "2018-10-26T17:59:51.609885: step 14897, loss 0.00031198, acc 1\n",
      "2018-10-26T17:59:51.784419: step 14898, loss 7.54359e-07, acc 1\n",
      "2018-10-26T17:59:51.950974: step 14899, loss 1.71917e-06, acc 1\n",
      "2018-10-26T17:59:52.127503: step 14900, loss 0.0125015, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T17:59:52.577301: step 14900, loss 5.17106, acc 0.704503\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-14900\n",
      "\n",
      "2018-10-26T17:59:52.914400: step 14901, loss 1.01001e-05, acc 1\n",
      "2018-10-26T17:59:53.075968: step 14902, loss 2.76402e-06, acc 1\n",
      "2018-10-26T17:59:53.254491: step 14903, loss 1.86264e-07, acc 1\n",
      "2018-10-26T17:59:53.417057: step 14904, loss 9.12695e-08, acc 1\n",
      "2018-10-26T17:59:53.602563: step 14905, loss 1.87005e-06, acc 1\n",
      "2018-10-26T17:59:53.843917: step 14906, loss 0.0638913, acc 0.984375\n",
      "2018-10-26T17:59:54.002493: step 14907, loss 1.41741e-06, acc 1\n",
      "2018-10-26T17:59:54.176029: step 14908, loss 9.79094e-06, acc 1\n",
      "2018-10-26T17:59:54.362530: step 14909, loss 0.00568803, acc 1\n",
      "2018-10-26T17:59:54.542051: step 14910, loss 7.76716e-07, acc 1\n",
      "2018-10-26T17:59:54.713593: step 14911, loss 1.20114e-05, acc 1\n",
      "2018-10-26T17:59:54.891119: step 14912, loss 5.98957e-06, acc 1\n",
      "2018-10-26T17:59:55.059668: step 14913, loss 3.78115e-07, acc 1\n",
      "2018-10-26T17:59:55.231210: step 14914, loss 5.08627e-06, acc 1\n",
      "2018-10-26T17:59:55.396768: step 14915, loss 0.000870612, acc 1\n",
      "2018-10-26T17:59:55.570304: step 14916, loss 0.00211818, acc 1\n",
      "2018-10-26T17:59:55.729878: step 14917, loss 0.00192131, acc 1\n",
      "2018-10-26T17:59:55.910411: step 14918, loss 2.89343e-05, acc 1\n",
      "2018-10-26T17:59:56.077948: step 14919, loss 3.09198e-07, acc 1\n",
      "2018-10-26T17:59:56.251484: step 14920, loss 5.08606e-06, acc 1\n",
      "2018-10-26T17:59:56.412055: step 14921, loss 6.07544e-06, acc 1\n",
      "2018-10-26T17:59:56.586589: step 14922, loss 0.0617921, acc 0.984375\n",
      "2018-10-26T17:59:56.750151: step 14923, loss 6.23262e-05, acc 1\n",
      "2018-10-26T17:59:56.925682: step 14924, loss 3.116e-06, acc 1\n",
      "2018-10-26T17:59:57.087251: step 14925, loss 0.00189048, acc 1\n",
      "2018-10-26T17:59:57.267768: step 14926, loss 1.51842e-05, acc 1\n",
      "2018-10-26T17:59:57.428339: step 14927, loss 4.93725e-06, acc 1\n",
      "2018-10-26T17:59:57.602873: step 14928, loss 2.63925e-06, acc 1\n",
      "2018-10-26T17:59:57.767434: step 14929, loss 3.18763e-05, acc 1\n",
      "2018-10-26T17:59:57.948949: step 14930, loss 1.48141e-05, acc 1\n",
      "2018-10-26T17:59:58.109520: step 14931, loss 1.93149e-05, acc 1\n",
      "2018-10-26T17:59:58.293030: step 14932, loss 0.00052367, acc 1\n",
      "2018-10-26T17:59:58.456592: step 14933, loss 6.77111e-05, acc 1\n",
      "2018-10-26T17:59:58.631126: step 14934, loss 4.01365e-06, acc 1\n",
      "2018-10-26T17:59:58.791697: step 14935, loss 0.000170249, acc 1\n",
      "2018-10-26T17:59:58.971217: step 14936, loss 0.000195222, acc 1\n",
      "2018-10-26T17:59:59.149741: step 14937, loss 0.000111196, acc 1\n",
      "2018-10-26T17:59:59.323277: step 14938, loss 9.94628e-07, acc 1\n",
      "2018-10-26T17:59:59.490829: step 14939, loss 0.000983187, acc 1\n",
      "2018-10-26T17:59:59.665362: step 14940, loss 2.96889e-06, acc 1\n",
      "2018-10-26T17:59:59.828926: step 14941, loss 2.93526e-06, acc 1\n",
      "2018-10-26T18:00:00.021411: step 14942, loss 1.57124e-05, acc 1\n",
      "2018-10-26T18:00:00.189960: step 14943, loss 0.000391345, acc 1\n",
      "2018-10-26T18:00:00.363497: step 14944, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:00:00.526063: step 14945, loss 6.75521e-05, acc 1\n",
      "2018-10-26T18:00:00.709573: step 14946, loss 1.53662e-06, acc 1\n",
      "2018-10-26T18:00:00.870144: step 14947, loss 8.22723e-06, acc 1\n",
      "2018-10-26T18:00:01.042683: step 14948, loss 4.17229e-07, acc 1\n",
      "2018-10-26T18:00:01.208240: step 14949, loss 1.01699e-06, acc 1\n",
      "2018-10-26T18:00:01.381777: step 14950, loss 6.74265e-07, acc 1\n",
      "2018-10-26T18:00:01.551350: step 14951, loss 5.29648e-05, acc 1\n",
      "2018-10-26T18:00:01.725857: step 14952, loss 0.00747227, acc 1\n",
      "2018-10-26T18:00:01.887426: step 14953, loss 8.59969e-06, acc 1\n",
      "2018-10-26T18:00:02.072931: step 14954, loss 1.14132e-05, acc 1\n",
      "2018-10-26T18:00:02.238487: step 14955, loss 0.00064865, acc 1\n",
      "2018-10-26T18:00:02.412024: step 14956, loss 0.00281942, acc 1\n",
      "2018-10-26T18:00:02.572595: step 14957, loss 6.95962e-06, acc 1\n",
      "2018-10-26T18:00:02.748125: step 14958, loss 0.00195909, acc 1\n",
      "2018-10-26T18:00:02.916676: step 14959, loss 8.98681e-05, acc 1\n",
      "2018-10-26T18:00:03.097193: step 14960, loss 7.27272e-05, acc 1\n",
      "2018-10-26T18:00:03.260756: step 14961, loss 1.57391e-06, acc 1\n",
      "2018-10-26T18:00:03.432299: step 14962, loss 0.000924868, acc 1\n",
      "2018-10-26T18:00:03.599850: step 14963, loss 2.1047e-06, acc 1\n",
      "2018-10-26T18:00:03.772389: step 14964, loss 2.88412e-05, acc 1\n",
      "2018-10-26T18:00:03.934955: step 14965, loss 4.38653e-05, acc 1\n",
      "2018-10-26T18:00:04.119461: step 14966, loss 8.89482e-06, acc 1\n",
      "2018-10-26T18:00:04.292001: step 14967, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:00:04.471521: step 14968, loss 0.000934639, acc 1\n",
      "2018-10-26T18:00:04.635084: step 14969, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:00:04.816599: step 14970, loss 3.25564e-05, acc 1\n",
      "2018-10-26T18:00:04.979165: step 14971, loss 1.69973e-05, acc 1\n",
      "2018-10-26T18:00:05.153699: step 14972, loss 6.51069e-06, acc 1\n",
      "2018-10-26T18:00:05.320253: step 14973, loss 1.2666e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:00:05.496782: step 14974, loss 3.40863e-07, acc 1\n",
      "2018-10-26T18:00:05.661343: step 14975, loss 0.00106701, acc 1\n",
      "2018-10-26T18:00:05.829891: step 14976, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:00:06.000436: step 14977, loss 1.01004e-05, acc 1\n",
      "2018-10-26T18:00:06.173973: step 14978, loss 9.27304e-05, acc 1\n",
      "2018-10-26T18:00:06.342522: step 14979, loss 0.00416106, acc 1\n",
      "2018-10-26T18:00:06.515061: step 14980, loss 1.83531e-05, acc 1\n",
      "2018-10-26T18:00:06.673638: step 14981, loss 3.94653e-06, acc 1\n",
      "2018-10-26T18:00:06.851162: step 14982, loss 0.000365499, acc 1\n",
      "2018-10-26T18:00:07.009740: step 14983, loss 0.000154546, acc 1\n",
      "2018-10-26T18:00:07.186268: step 14984, loss 8.50369e-06, acc 1\n",
      "2018-10-26T18:00:07.346839: step 14985, loss 9.01527e-06, acc 1\n",
      "2018-10-26T18:00:07.518381: step 14986, loss 0.00611632, acc 1\n",
      "2018-10-26T18:00:07.694909: step 14987, loss 0.00532078, acc 1\n",
      "2018-10-26T18:00:07.863464: step 14988, loss 0.00282721, acc 1\n",
      "2018-10-26T18:00:08.035000: step 14989, loss 9.8417e-05, acc 1\n",
      "2018-10-26T18:00:08.206542: step 14990, loss 3.61926e-05, acc 1\n",
      "2018-10-26T18:00:08.372100: step 14991, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:00:08.545636: step 14992, loss 0.00687789, acc 1\n",
      "2018-10-26T18:00:08.716013: step 14993, loss 6.01626e-07, acc 1\n",
      "2018-10-26T18:00:08.887555: step 14994, loss 0.000964105, acc 1\n",
      "2018-10-26T18:00:09.054110: step 14995, loss 9.68574e-08, acc 1\n",
      "2018-10-26T18:00:09.234627: step 14996, loss 1.67886e-05, acc 1\n",
      "2018-10-26T18:00:09.396196: step 14997, loss 1.30321e-05, acc 1\n",
      "2018-10-26T18:00:09.571727: step 14998, loss 0.000257663, acc 1\n",
      "2018-10-26T18:00:09.735290: step 14999, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:00:09.904836: step 15000, loss 2.31265e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:00:10.332693: step 15000, loss 5.21047, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15000\n",
      "\n",
      "2018-10-26T18:00:10.701959: step 15001, loss 3.38849e-05, acc 1\n",
      "2018-10-26T18:00:10.870507: step 15002, loss 0.000425715, acc 1\n",
      "2018-10-26T18:00:11.049031: step 15003, loss 0.000187048, acc 1\n",
      "2018-10-26T18:00:11.208603: step 15004, loss 1.4275e-05, acc 1\n",
      "2018-10-26T18:00:11.406076: step 15005, loss 7.95898e-06, acc 1\n",
      "2018-10-26T18:00:11.641446: step 15006, loss 1.83648e-06, acc 1\n",
      "2018-10-26T18:00:11.809997: step 15007, loss 0.00556527, acc 1\n",
      "2018-10-26T18:00:11.989517: step 15008, loss 2.88415e-05, acc 1\n",
      "2018-10-26T18:00:12.157068: step 15009, loss 9.77874e-07, acc 1\n",
      "2018-10-26T18:00:12.327613: step 15010, loss 0.000101104, acc 1\n",
      "2018-10-26T18:00:12.492174: step 15011, loss 0.148566, acc 0.984375\n",
      "2018-10-26T18:00:12.666707: step 15012, loss 3.14955e-06, acc 1\n",
      "2018-10-26T18:00:12.828277: step 15013, loss 1.83651e-06, acc 1\n",
      "2018-10-26T18:00:13.005802: step 15014, loss 0.000106704, acc 1\n",
      "2018-10-26T18:00:13.178340: step 15015, loss 1.83441e-05, acc 1\n",
      "2018-10-26T18:00:13.345893: step 15016, loss 0.00095074, acc 1\n",
      "2018-10-26T18:00:13.511450: step 15017, loss 1.76575e-06, acc 1\n",
      "2018-10-26T18:00:13.681995: step 15018, loss 2.61911e-05, acc 1\n",
      "2018-10-26T18:00:13.854533: step 15019, loss 4.63794e-07, acc 1\n",
      "2018-10-26T18:00:14.032060: step 15020, loss 7.39493e-05, acc 1\n",
      "2018-10-26T18:00:14.192631: step 15021, loss 7.26667e-06, acc 1\n",
      "2018-10-26T18:00:14.368161: step 15022, loss 1.93337e-06, acc 1\n",
      "2018-10-26T18:00:14.538705: step 15023, loss 0.00108148, acc 1\n",
      "2018-10-26T18:00:14.708252: step 15024, loss 5.17735e-06, acc 1\n",
      "2018-10-26T18:00:14.870818: step 15025, loss 1.51054e-06, acc 1\n",
      "2018-10-26T18:00:15.055327: step 15026, loss 1.18649e-06, acc 1\n",
      "2018-10-26T18:00:15.225872: step 15027, loss 1.72477e-06, acc 1\n",
      "2018-10-26T18:00:15.403396: step 15028, loss 0.000102613, acc 1\n",
      "2018-10-26T18:00:15.567956: step 15029, loss 1.55712e-06, acc 1\n",
      "2018-10-26T18:00:15.743487: step 15030, loss 2.11216e-06, acc 1\n",
      "2018-10-26T18:00:15.918021: step 15031, loss 0.101337, acc 0.984375\n",
      "2018-10-26T18:00:16.105519: step 15032, loss 0.000122854, acc 1\n",
      "2018-10-26T18:00:16.287034: step 15033, loss 4.9157e-05, acc 1\n",
      "2018-10-26T18:00:16.487498: step 15034, loss 1.09148e-06, acc 1\n",
      "2018-10-26T18:00:16.667020: step 15035, loss 0.00129719, acc 1\n",
      "2018-10-26T18:00:16.865489: step 15036, loss 9.09167e-06, acc 1\n",
      "2018-10-26T18:00:17.057974: step 15037, loss 7.19822e-06, acc 1\n",
      "2018-10-26T18:00:17.252455: step 15038, loss 0.000258587, acc 1\n",
      "2018-10-26T18:00:17.425991: step 15039, loss 0.00868213, acc 1\n",
      "2018-10-26T18:00:17.602519: step 15040, loss 0.000122931, acc 1\n",
      "2018-10-26T18:00:17.764089: step 15041, loss 2.23381e-05, acc 1\n",
      "2018-10-26T18:00:17.939619: step 15042, loss 3.57624e-07, acc 1\n",
      "2018-10-26T18:00:18.100192: step 15043, loss 4.7472e-06, acc 1\n",
      "2018-10-26T18:00:18.281706: step 15044, loss 5.34676e-06, acc 1\n",
      "2018-10-26T18:00:18.441279: step 15045, loss 2.12249e-05, acc 1\n",
      "2018-10-26T18:00:18.619801: step 15046, loss 0.00205535, acc 1\n",
      "2018-10-26T18:00:18.780373: step 15047, loss 1.47148e-07, acc 1\n",
      "2018-10-26T18:00:18.962885: step 15048, loss 1.40624e-06, acc 1\n",
      "2018-10-26T18:00:19.125451: step 15049, loss 0.000334053, acc 1\n",
      "2018-10-26T18:00:19.298987: step 15050, loss 0.00121191, acc 1\n",
      "2018-10-26T18:00:19.459558: step 15051, loss 1.57389e-06, acc 1\n",
      "2018-10-26T18:00:19.636086: step 15052, loss 0.00153767, acc 1\n",
      "2018-10-26T18:00:19.801644: step 15053, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:00:19.979170: step 15054, loss 0.000178968, acc 1\n",
      "2018-10-26T18:00:20.142733: step 15055, loss 1.63762e-05, acc 1\n",
      "2018-10-26T18:00:20.316269: step 15056, loss 1.1727e-05, acc 1\n",
      "2018-10-26T18:00:20.478835: step 15057, loss 0.00714154, acc 1\n",
      "2018-10-26T18:00:20.653368: step 15058, loss 2.84777e-05, acc 1\n",
      "2018-10-26T18:00:20.821918: step 15059, loss 0.000137149, acc 1\n",
      "2018-10-26T18:00:20.999450: step 15060, loss 6.31193e-05, acc 1\n",
      "2018-10-26T18:00:21.158020: step 15061, loss 9.89186e-05, acc 1\n",
      "2018-10-26T18:00:21.331555: step 15062, loss 9.54977e-06, acc 1\n",
      "2018-10-26T18:00:21.504096: step 15063, loss 0.000302189, acc 1\n",
      "2018-10-26T18:00:21.677631: step 15064, loss 0.00267916, acc 1\n",
      "2018-10-26T18:00:21.855159: step 15065, loss 0.00357204, acc 1\n",
      "2018-10-26T18:00:22.025702: step 15066, loss 1.30498e-05, acc 1\n",
      "2018-10-26T18:00:22.204226: step 15067, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:00:22.376764: step 15068, loss 0.000555973, acc 1\n",
      "2018-10-26T18:00:22.558280: step 15069, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:00:22.722839: step 15070, loss 6.2052e-06, acc 1\n",
      "2018-10-26T18:00:22.899369: step 15071, loss 8.34508e-06, acc 1\n",
      "2018-10-26T18:00:23.065923: step 15072, loss 1.06999e-05, acc 1\n",
      "2018-10-26T18:00:23.251428: step 15073, loss 0.00320998, acc 1\n",
      "2018-10-26T18:00:23.414990: step 15074, loss 2.05629e-06, acc 1\n",
      "2018-10-26T18:00:23.601491: step 15075, loss 7.32495e-05, acc 1\n",
      "2018-10-26T18:00:23.774030: step 15076, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:00:23.948565: step 15077, loss 2.52751e-06, acc 1\n",
      "2018-10-26T18:00:24.112127: step 15078, loss 3.39912e-06, acc 1\n",
      "2018-10-26T18:00:24.293642: step 15079, loss 3.08835e-05, acc 1\n",
      "2018-10-26T18:00:24.459200: step 15080, loss 1.19765e-06, acc 1\n",
      "2018-10-26T18:00:24.637722: step 15081, loss 6.07964e-05, acc 1\n",
      "2018-10-26T18:00:24.803281: step 15082, loss 7.8716e-06, acc 1\n",
      "2018-10-26T18:00:24.980806: step 15083, loss 9.872e-08, acc 1\n",
      "2018-10-26T18:00:25.143372: step 15084, loss 0.000347384, acc 1\n",
      "2018-10-26T18:00:25.321895: step 15085, loss 3.24099e-07, acc 1\n",
      "2018-10-26T18:00:25.481478: step 15086, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:00:25.666972: step 15087, loss 4.99511e-06, acc 1\n",
      "2018-10-26T18:00:25.831533: step 15088, loss 0.000257657, acc 1\n",
      "2018-10-26T18:00:26.007065: step 15089, loss 2.14563e-06, acc 1\n",
      "2018-10-26T18:00:26.167635: step 15090, loss 1.06171e-07, acc 1\n",
      "2018-10-26T18:00:26.341171: step 15091, loss 0.00166902, acc 1\n",
      "2018-10-26T18:00:26.512714: step 15092, loss 5.31881e-06, acc 1\n",
      "2018-10-26T18:00:26.695225: step 15093, loss 0.000191613, acc 1\n",
      "2018-10-26T18:00:26.856794: step 15094, loss 0.011083, acc 0.984375\n",
      "2018-10-26T18:00:27.032325: step 15095, loss 0.000130253, acc 1\n",
      "2018-10-26T18:00:27.194890: step 15096, loss 7.67677e-05, acc 1\n",
      "2018-10-26T18:00:27.367470: step 15097, loss 0.000173832, acc 1\n",
      "2018-10-26T18:00:27.536976: step 15098, loss 0.0187288, acc 0.984375\n",
      "2018-10-26T18:00:27.713504: step 15099, loss 8.51218e-07, acc 1\n",
      "2018-10-26T18:00:27.875074: step 15100, loss 7.74843e-07, acc 1\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:00:28.313900: step 15100, loss 5.32034, acc 0.704503\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15100\n",
      "\n",
      "2018-10-26T18:00:28.675017: step 15101, loss 0.00011046, acc 1\n",
      "2018-10-26T18:00:28.840575: step 15102, loss 4.88208e-05, acc 1\n",
      "2018-10-26T18:00:29.019098: step 15103, loss 1.99142e-05, acc 1\n",
      "2018-10-26T18:00:29.179668: step 15104, loss 5.4751e-05, acc 1\n",
      "2018-10-26T18:00:29.364175: step 15105, loss 3.38989e-06, acc 1\n",
      "2018-10-26T18:00:29.604534: step 15106, loss 9.18398e-06, acc 1\n",
      "2018-10-26T18:00:29.769093: step 15107, loss 1.5953e-05, acc 1\n",
      "2018-10-26T18:00:29.943628: step 15108, loss 0.00119148, acc 1\n",
      "2018-10-26T18:00:30.102204: step 15109, loss 0.000239044, acc 1\n",
      "2018-10-26T18:00:30.273745: step 15110, loss 0.0228302, acc 0.984375\n",
      "2018-10-26T18:00:30.433319: step 15111, loss 7.00062e-06, acc 1\n",
      "2018-10-26T18:00:30.611842: step 15112, loss 2.93332e-05, acc 1\n",
      "2018-10-26T18:00:30.774407: step 15113, loss 0.0338073, acc 0.984375\n",
      "2018-10-26T18:00:30.948942: step 15114, loss 3.40407e-05, acc 1\n",
      "2018-10-26T18:00:31.122478: step 15115, loss 0.00100471, acc 1\n",
      "2018-10-26T18:00:31.296014: step 15116, loss 0.000130455, acc 1\n",
      "2018-10-26T18:00:31.462569: step 15117, loss 9.87186e-07, acc 1\n",
      "2018-10-26T18:00:31.631119: step 15118, loss 5.35655e-05, acc 1\n",
      "2018-10-26T18:00:31.797674: step 15119, loss 8.73298e-05, acc 1\n",
      "2018-10-26T18:00:31.970212: step 15120, loss 0.000175646, acc 1\n",
      "2018-10-26T18:00:32.132778: step 15121, loss 0.000313333, acc 1\n",
      "2018-10-26T18:00:32.306316: step 15122, loss 2.257e-05, acc 1\n",
      "2018-10-26T18:00:32.469877: step 15123, loss 7.65386e-05, acc 1\n",
      "2018-10-26T18:00:32.639425: step 15124, loss 3.18222e-05, acc 1\n",
      "2018-10-26T18:00:32.801991: step 15125, loss 0.00183679, acc 1\n",
      "2018-10-26T18:00:32.978518: step 15126, loss 1.39883e-06, acc 1\n",
      "2018-10-26T18:00:33.143079: step 15127, loss 0.0078961, acc 1\n",
      "2018-10-26T18:00:33.315618: step 15128, loss 0.00333424, acc 1\n",
      "2018-10-26T18:00:33.481176: step 15129, loss 0.000898983, acc 1\n",
      "2018-10-26T18:00:33.651720: step 15130, loss 9.00977e-06, acc 1\n",
      "2018-10-26T18:00:33.810296: step 15131, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:00:33.991811: step 15132, loss 3.78113e-07, acc 1\n",
      "2018-10-26T18:00:34.154377: step 15133, loss 8.00077e-06, acc 1\n",
      "2018-10-26T18:00:34.321929: step 15134, loss 6.40675e-06, acc 1\n",
      "2018-10-26T18:00:34.484495: step 15135, loss 1.52011e-05, acc 1\n",
      "2018-10-26T18:00:34.661023: step 15136, loss 8.74159e-05, acc 1\n",
      "2018-10-26T18:00:34.823589: step 15137, loss 0.000906407, acc 1\n",
      "2018-10-26T18:00:34.996127: step 15138, loss 4.56344e-07, acc 1\n",
      "2018-10-26T18:00:35.160688: step 15139, loss 0.000475211, acc 1\n",
      "2018-10-26T18:00:35.341206: step 15140, loss 0.000401376, acc 1\n",
      "2018-10-26T18:00:35.509756: step 15141, loss 4.69336e-06, acc 1\n",
      "2018-10-26T18:00:35.690273: step 15142, loss 4.92442e-06, acc 1\n",
      "2018-10-26T18:00:35.857826: step 15143, loss 0.0012264, acc 1\n",
      "2018-10-26T18:00:36.031361: step 15144, loss 3.05277e-06, acc 1\n",
      "2018-10-26T18:00:36.191933: step 15145, loss 1.21812e-06, acc 1\n",
      "2018-10-26T18:00:36.365469: step 15146, loss 0.00326344, acc 1\n",
      "2018-10-26T18:00:36.528035: step 15147, loss 5.49613e-06, acc 1\n",
      "2018-10-26T18:00:36.701571: step 15148, loss 8.03932e-06, acc 1\n",
      "2018-10-26T18:00:36.866132: step 15149, loss 0.000105695, acc 1\n",
      "2018-10-26T18:00:37.035678: step 15150, loss 2.34709e-05, acc 1\n",
      "2018-10-26T18:00:37.202234: step 15151, loss 0.000119096, acc 1\n",
      "2018-10-26T18:00:37.378761: step 15152, loss 1.93311e-05, acc 1\n",
      "2018-10-26T18:00:37.549306: step 15153, loss 3.90208e-06, acc 1\n",
      "2018-10-26T18:00:37.717856: step 15154, loss 1.02003e-05, acc 1\n",
      "2018-10-26T18:00:37.888401: step 15155, loss 2.68886e-05, acc 1\n",
      "2018-10-26T18:00:38.064929: step 15156, loss 3.48313e-07, acc 1\n",
      "2018-10-26T18:00:38.229488: step 15157, loss 3.02596e-05, acc 1\n",
      "2018-10-26T18:00:38.403026: step 15158, loss 3.66361e-06, acc 1\n",
      "2018-10-26T18:00:38.568583: step 15159, loss 6.68048e-06, acc 1\n",
      "2018-10-26T18:00:38.741122: step 15160, loss 2.40108e-05, acc 1\n",
      "2018-10-26T18:00:38.912663: step 15161, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:00:39.090189: step 15162, loss 0.000726496, acc 1\n",
      "2018-10-26T18:00:39.254750: step 15163, loss 1.60739e-06, acc 1\n",
      "2018-10-26T18:00:39.431278: step 15164, loss 8.60663e-06, acc 1\n",
      "2018-10-26T18:00:39.600825: step 15165, loss 2.35546e-05, acc 1\n",
      "2018-10-26T18:00:39.781342: step 15166, loss 2.6467e-06, acc 1\n",
      "2018-10-26T18:00:39.950889: step 15167, loss 4.65447e-06, acc 1\n",
      "2018-10-26T18:00:40.129413: step 15168, loss 6.12356e-06, acc 1\n",
      "2018-10-26T18:00:40.296964: step 15169, loss 1.41372e-05, acc 1\n",
      "2018-10-26T18:00:40.471499: step 15170, loss 1.77902e-05, acc 1\n",
      "2018-10-26T18:00:40.630075: step 15171, loss 7.45058e-08, acc 1\n",
      "2018-10-26T18:00:40.807601: step 15172, loss 5.4231e-06, acc 1\n",
      "2018-10-26T18:00:40.975153: step 15173, loss 3.20004e-05, acc 1\n",
      "2018-10-26T18:00:41.157665: step 15174, loss 1.82659e-05, acc 1\n",
      "2018-10-26T18:00:41.323224: step 15175, loss 6.75817e-05, acc 1\n",
      "2018-10-26T18:00:41.497756: step 15176, loss 0.000360539, acc 1\n",
      "2018-10-26T18:00:41.660322: step 15177, loss 2.02427e-05, acc 1\n",
      "2018-10-26T18:00:41.840840: step 15178, loss 4.84284e-07, acc 1\n",
      "2018-10-26T18:00:42.023352: step 15179, loss 2.14755e-06, acc 1\n",
      "2018-10-26T18:00:42.251741: step 15180, loss 3.96881e-06, acc 1\n",
      "2018-10-26T18:00:42.454202: step 15181, loss 2.37094e-05, acc 1\n",
      "2018-10-26T18:00:42.706527: step 15182, loss 3.58721e-06, acc 1\n",
      "2018-10-26T18:00:42.926951: step 15183, loss 2.05635e-05, acc 1\n",
      "2018-10-26T18:00:43.151338: step 15184, loss 0.000179836, acc 1\n",
      "2018-10-26T18:00:43.377734: step 15185, loss 2.40263e-06, acc 1\n",
      "2018-10-26T18:00:43.597147: step 15186, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:00:43.822546: step 15187, loss 6.7246e-06, acc 1\n",
      "2018-10-26T18:00:43.993089: step 15188, loss 0.000109237, acc 1\n",
      "2018-10-26T18:00:44.242423: step 15189, loss 1.82665e-05, acc 1\n",
      "2018-10-26T18:00:44.467821: step 15190, loss 8.41903e-07, acc 1\n",
      "2018-10-26T18:00:44.686237: step 15191, loss 1.47146e-06, acc 1\n",
      "2018-10-26T18:00:44.898669: step 15192, loss 0.000276347, acc 1\n",
      "2018-10-26T18:00:45.145011: step 15193, loss 2.80994e-05, acc 1\n",
      "2018-10-26T18:00:45.370409: step 15194, loss 1.25548e-05, acc 1\n",
      "2018-10-26T18:00:45.572869: step 15195, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:00:45.810234: step 15196, loss 1.20882e-06, acc 1\n",
      "2018-10-26T18:00:46.015685: step 15197, loss 0.000111868, acc 1\n",
      "2018-10-26T18:00:46.240086: step 15198, loss 4.95592e-06, acc 1\n",
      "2018-10-26T18:00:46.459500: step 15199, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:00:46.636028: step 15200, loss 1.57945e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:00:47.208498: step 15200, loss 5.2685, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15200\n",
      "\n",
      "2018-10-26T18:00:47.583846: step 15201, loss 1.26593e-05, acc 1\n",
      "2018-10-26T18:00:47.755388: step 15202, loss 9.43149e-06, acc 1\n",
      "2018-10-26T18:00:47.953858: step 15203, loss 3.48314e-07, acc 1\n",
      "2018-10-26T18:00:48.129389: step 15204, loss 0.0040996, acc 1\n",
      "2018-10-26T18:00:48.333842: step 15205, loss 2.59997e-05, acc 1\n",
      "2018-10-26T18:00:48.590158: step 15206, loss 2.4772e-06, acc 1\n",
      "2018-10-26T18:00:48.752723: step 15207, loss 0.000222244, acc 1\n",
      "2018-10-26T18:00:48.955182: step 15208, loss 1.19355e-05, acc 1\n",
      "2018-10-26T18:00:49.125727: step 15209, loss 4.09094e-05, acc 1\n",
      "2018-10-26T18:00:49.306245: step 15210, loss 0.00114698, acc 1\n",
      "2018-10-26T18:00:49.469807: step 15211, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:00:49.680245: step 15212, loss 8.19563e-08, acc 1\n",
      "2018-10-26T18:00:49.852784: step 15213, loss 2.178e-05, acc 1\n",
      "2018-10-26T18:00:50.029313: step 15214, loss 2.24055e-05, acc 1\n",
      "2018-10-26T18:00:50.208832: step 15215, loss 1.26611e-05, acc 1\n",
      "2018-10-26T18:00:50.408299: step 15216, loss 4.22025e-06, acc 1\n",
      "2018-10-26T18:00:50.578844: step 15217, loss 1.70378e-05, acc 1\n",
      "2018-10-26T18:00:50.753379: step 15218, loss 2.35447e-05, acc 1\n",
      "2018-10-26T18:00:50.936887: step 15219, loss 0.000161598, acc 1\n",
      "2018-10-26T18:00:51.132366: step 15220, loss 3.92259e-05, acc 1\n",
      "2018-10-26T18:00:51.308894: step 15221, loss 5.80093e-05, acc 1\n",
      "2018-10-26T18:00:51.498388: step 15222, loss 4.97445e-06, acc 1\n",
      "2018-10-26T18:00:51.716804: step 15223, loss 5.24088e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:00:51.944197: step 15224, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:00:52.119727: step 15225, loss 0.00023928, acc 1\n",
      "2018-10-26T18:00:52.293263: step 15226, loss 1.59253e-06, acc 1\n",
      "2018-10-26T18:00:52.461813: step 15227, loss 9.10808e-07, acc 1\n",
      "2018-10-26T18:00:52.623382: step 15228, loss 1.65585e-06, acc 1\n",
      "2018-10-26T18:00:52.796918: step 15229, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:00:52.960481: step 15230, loss 1.80295e-06, acc 1\n",
      "2018-10-26T18:00:53.132023: step 15231, loss 2.4251e-06, acc 1\n",
      "2018-10-26T18:00:53.299575: step 15232, loss 2.25932e-06, acc 1\n",
      "2018-10-26T18:00:53.483084: step 15233, loss 1.81074e-05, acc 1\n",
      "2018-10-26T18:00:53.647645: step 15234, loss 1.02449e-05, acc 1\n",
      "2018-10-26T18:00:53.822179: step 15235, loss 4.28404e-07, acc 1\n",
      "2018-10-26T18:00:53.988734: step 15236, loss 0.000731371, acc 1\n",
      "2018-10-26T18:00:54.166259: step 15237, loss 1.26285e-06, acc 1\n",
      "2018-10-26T18:00:54.337801: step 15238, loss 0.0594263, acc 0.984375\n",
      "2018-10-26T18:00:54.509343: step 15239, loss 9.927e-06, acc 1\n",
      "2018-10-26T18:00:54.672906: step 15240, loss 2.14203e-07, acc 1\n",
      "2018-10-26T18:00:54.842453: step 15241, loss 0.00099448, acc 1\n",
      "2018-10-26T18:00:55.010005: step 15242, loss 0.000483632, acc 1\n",
      "2018-10-26T18:00:55.181547: step 15243, loss 1.86141e-05, acc 1\n",
      "2018-10-26T18:00:55.349098: step 15244, loss 0.000313444, acc 1\n",
      "2018-10-26T18:00:55.517649: step 15245, loss 6.41982e-05, acc 1\n",
      "2018-10-26T18:00:55.682209: step 15246, loss 1.60215e-05, acc 1\n",
      "2018-10-26T18:00:55.855745: step 15247, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:00:56.023298: step 15248, loss 1.28339e-05, acc 1\n",
      "2018-10-26T18:00:56.202818: step 15249, loss 4.11641e-07, acc 1\n",
      "2018-10-26T18:00:56.363388: step 15250, loss 4.87986e-06, acc 1\n",
      "2018-10-26T18:00:56.539917: step 15251, loss 2.27789e-06, acc 1\n",
      "2018-10-26T18:00:56.705475: step 15252, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:00:56.879011: step 15253, loss 1.72665e-06, acc 1\n",
      "2018-10-26T18:00:57.040580: step 15254, loss 0.00261181, acc 1\n",
      "2018-10-26T18:00:57.210127: step 15255, loss 1.89989e-07, acc 1\n",
      "2018-10-26T18:00:57.377679: step 15256, loss 0.000109817, acc 1\n",
      "2018-10-26T18:00:57.551215: step 15257, loss 1.39507e-06, acc 1\n",
      "2018-10-26T18:00:57.717770: step 15258, loss 0.000530014, acc 1\n",
      "2018-10-26T18:00:57.893302: step 15259, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:00:58.053872: step 15260, loss 4.03253e-05, acc 1\n",
      "2018-10-26T18:00:58.223448: step 15261, loss 0.00370881, acc 1\n",
      "2018-10-26T18:00:58.390971: step 15262, loss 0.0336869, acc 0.984375\n",
      "2018-10-26T18:00:58.562513: step 15263, loss 6.20311e-05, acc 1\n",
      "2018-10-26T18:00:58.727073: step 15264, loss 0.000189921, acc 1\n",
      "2018-10-26T18:00:58.899613: step 15265, loss 0.000514817, acc 1\n",
      "2018-10-26T18:00:59.063176: step 15266, loss 0.00048512, acc 1\n",
      "2018-10-26T18:00:59.237709: step 15267, loss 0.000179878, acc 1\n",
      "2018-10-26T18:00:59.408254: step 15268, loss 2.43854e-05, acc 1\n",
      "2018-10-26T18:00:59.581790: step 15269, loss 9.01511e-07, acc 1\n",
      "2018-10-26T18:00:59.750340: step 15270, loss 2.38223e-06, acc 1\n",
      "2018-10-26T18:00:59.928862: step 15271, loss 6.80178e-06, acc 1\n",
      "2018-10-26T18:01:00.108383: step 15272, loss 1.61619e-05, acc 1\n",
      "2018-10-26T18:01:00.277930: step 15273, loss 9.86483e-06, acc 1\n",
      "2018-10-26T18:01:00.454462: step 15274, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:01:00.617023: step 15275, loss 1.77134e-06, acc 1\n",
      "2018-10-26T18:01:00.788566: step 15276, loss 3.05268e-06, acc 1\n",
      "2018-10-26T18:01:00.958112: step 15277, loss 4.92639e-05, acc 1\n",
      "2018-10-26T18:01:01.128658: step 15278, loss 3.30972e-06, acc 1\n",
      "2018-10-26T18:01:01.295212: step 15279, loss 1.29638e-06, acc 1\n",
      "2018-10-26T18:01:01.473734: step 15280, loss 0.000532047, acc 1\n",
      "2018-10-26T18:01:01.635303: step 15281, loss 4.59437e-05, acc 1\n",
      "2018-10-26T18:01:01.806845: step 15282, loss 0.00019005, acc 1\n",
      "2018-10-26T18:01:01.971405: step 15283, loss 0.0338553, acc 0.984375\n",
      "2018-10-26T18:01:02.139955: step 15284, loss 0.000571111, acc 1\n",
      "2018-10-26T18:01:02.303518: step 15285, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:01:02.478051: step 15286, loss 0.000124308, acc 1\n",
      "2018-10-26T18:01:02.645604: step 15287, loss 0.00137463, acc 1\n",
      "2018-10-26T18:01:02.814154: step 15288, loss 6.47917e-05, acc 1\n",
      "2018-10-26T18:01:02.982703: step 15289, loss 3.8785e-05, acc 1\n",
      "2018-10-26T18:01:03.155241: step 15290, loss 0.003199, acc 1\n",
      "2018-10-26T18:01:03.322794: step 15291, loss 0.00014167, acc 1\n",
      "2018-10-26T18:01:03.502315: step 15292, loss 1.28399e-05, acc 1\n",
      "2018-10-26T18:01:03.669866: step 15293, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:01:03.846395: step 15294, loss 0.00616156, acc 1\n",
      "2018-10-26T18:01:04.013948: step 15295, loss 0.000639953, acc 1\n",
      "2018-10-26T18:01:04.182512: step 15296, loss 0.0190464, acc 0.984375\n",
      "2018-10-26T18:01:04.343068: step 15297, loss 0.000205155, acc 1\n",
      "2018-10-26T18:01:04.517601: step 15298, loss 1.74154e-06, acc 1\n",
      "2018-10-26T18:01:04.688146: step 15299, loss 0.00150966, acc 1\n",
      "2018-10-26T18:01:04.856696: step 15300, loss 6.12448e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:01:05.288542: step 15300, loss 5.32211, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15300\n",
      "\n",
      "2018-10-26T18:01:05.665267: step 15301, loss 7.93476e-07, acc 1\n",
      "2018-10-26T18:01:05.838804: step 15302, loss 5.35174e-05, acc 1\n",
      "2018-10-26T18:01:06.008351: step 15303, loss 8.47424e-06, acc 1\n",
      "2018-10-26T18:01:06.193855: step 15304, loss 6.05064e-06, acc 1\n",
      "2018-10-26T18:01:06.379359: step 15305, loss 2.33675e-05, acc 1\n",
      "2018-10-26T18:01:06.631684: step 15306, loss 1.05982e-06, acc 1\n",
      "2018-10-26T18:01:06.800234: step 15307, loss 5.41752e-05, acc 1\n",
      "2018-10-26T18:01:06.979755: step 15308, loss 2.24442e-06, acc 1\n",
      "2018-10-26T18:01:07.140326: step 15309, loss 3.8184e-07, acc 1\n",
      "2018-10-26T18:01:07.312864: step 15310, loss 0.000740604, acc 1\n",
      "2018-10-26T18:01:07.480417: step 15311, loss 0.000122074, acc 1\n",
      "2018-10-26T18:01:07.663926: step 15312, loss 0.0003449, acc 1\n",
      "2018-10-26T18:01:07.829495: step 15313, loss 0.0196466, acc 0.984375\n",
      "2018-10-26T18:01:08.002023: step 15314, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:01:08.172567: step 15315, loss 3.89803e-06, acc 1\n",
      "2018-10-26T18:01:08.349096: step 15316, loss 1.96982e-05, acc 1\n",
      "2018-10-26T18:01:08.513656: step 15317, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:01:08.689187: step 15318, loss 1.18696e-05, acc 1\n",
      "2018-10-26T18:01:08.911594: step 15319, loss 0.000331517, acc 1\n",
      "2018-10-26T18:01:09.117044: step 15320, loss 1.99076e-05, acc 1\n",
      "2018-10-26T18:01:09.293572: step 15321, loss 3.49788e-06, acc 1\n",
      "2018-10-26T18:01:09.492043: step 15322, loss 0.0533716, acc 0.984375\n",
      "2018-10-26T18:01:09.674555: step 15323, loss 0.00242228, acc 1\n",
      "2018-10-26T18:01:09.849089: step 15324, loss 0.000229643, acc 1\n",
      "2018-10-26T18:01:10.017639: step 15325, loss 4.99999e-05, acc 1\n",
      "2018-10-26T18:01:10.194167: step 15326, loss 0.0672514, acc 0.984375\n",
      "2018-10-26T18:01:10.360721: step 15327, loss 4.31895e-05, acc 1\n",
      "2018-10-26T18:01:10.537250: step 15328, loss 1.46957e-06, acc 1\n",
      "2018-10-26T18:01:10.707794: step 15329, loss 6.32127e-06, acc 1\n",
      "2018-10-26T18:01:10.882327: step 15330, loss 2.42255e-05, acc 1\n",
      "2018-10-26T18:01:11.041901: step 15331, loss 2.79011e-06, acc 1\n",
      "2018-10-26T18:01:11.235384: step 15332, loss 6.76324e-05, acc 1\n",
      "2018-10-26T18:01:11.399944: step 15333, loss 1.96589e-05, acc 1\n",
      "2018-10-26T18:01:11.580463: step 15334, loss 6.5378e-07, acc 1\n",
      "2018-10-26T18:01:11.751007: step 15335, loss 2.1234e-07, acc 1\n",
      "2018-10-26T18:01:11.925540: step 15336, loss 1.28147e-06, acc 1\n",
      "2018-10-26T18:01:12.091110: step 15337, loss 0.00917543, acc 1\n",
      "2018-10-26T18:01:12.264634: step 15338, loss 5.33953e-05, acc 1\n",
      "2018-10-26T18:01:12.429194: step 15339, loss 2.51071e-06, acc 1\n",
      "2018-10-26T18:01:12.619686: step 15340, loss 7.43052e-06, acc 1\n",
      "2018-10-26T18:01:12.798209: step 15341, loss 3.09807e-05, acc 1\n",
      "2018-10-26T18:01:12.977729: step 15342, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:01:13.136305: step 15343, loss 2.34692e-07, acc 1\n",
      "2018-10-26T18:01:13.312835: step 15344, loss 0.000655043, acc 1\n",
      "2018-10-26T18:01:13.482381: step 15345, loss 0.000145805, acc 1\n",
      "2018-10-26T18:01:13.658910: step 15346, loss 2.70862e-05, acc 1\n",
      "2018-10-26T18:01:13.827458: step 15347, loss 1.88899e-05, acc 1\n",
      "2018-10-26T18:01:14.006979: step 15348, loss 2.5833e-06, acc 1\n",
      "2018-10-26T18:01:14.170542: step 15349, loss 1.58877e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:01:14.347070: step 15350, loss 3.01161e-06, acc 1\n",
      "2018-10-26T18:01:14.535567: step 15351, loss 1.41184e-06, acc 1\n",
      "2018-10-26T18:01:14.721072: step 15352, loss 0.000204262, acc 1\n",
      "2018-10-26T18:01:14.896602: step 15353, loss 2.53471e-05, acc 1\n",
      "2018-10-26T18:01:15.070139: step 15354, loss 0.138632, acc 0.984375\n",
      "2018-10-26T18:01:15.232704: step 15355, loss 2.12554e-05, acc 1\n",
      "2018-10-26T18:01:15.405243: step 15356, loss 8.66106e-07, acc 1\n",
      "2018-10-26T18:01:15.569803: step 15357, loss 0.00107919, acc 1\n",
      "2018-10-26T18:01:15.742342: step 15358, loss 1.02815e-06, acc 1\n",
      "2018-10-26T18:01:15.915880: step 15359, loss 0.000228872, acc 1\n",
      "2018-10-26T18:01:16.083433: step 15360, loss 9.38756e-06, acc 1\n",
      "2018-10-26T18:01:16.256968: step 15361, loss 1.99856e-06, acc 1\n",
      "2018-10-26T18:01:16.421527: step 15362, loss 4.29786e-05, acc 1\n",
      "2018-10-26T18:01:16.595065: step 15363, loss 0.000239091, acc 1\n",
      "2018-10-26T18:01:16.765609: step 15364, loss 3.10102e-06, acc 1\n",
      "2018-10-26T18:01:16.941141: step 15365, loss 1.19951e-06, acc 1\n",
      "2018-10-26T18:01:17.104703: step 15366, loss 4.86263e-06, acc 1\n",
      "2018-10-26T18:01:17.281231: step 15367, loss 0.00023308, acc 1\n",
      "2018-10-26T18:01:17.447786: step 15368, loss 0.00016535, acc 1\n",
      "2018-10-26T18:01:17.621323: step 15369, loss 8.41508e-05, acc 1\n",
      "2018-10-26T18:01:17.795855: step 15370, loss 0.00210085, acc 1\n",
      "2018-10-26T18:01:17.967397: step 15371, loss 5.17019e-05, acc 1\n",
      "2018-10-26T18:01:18.131958: step 15372, loss 0.000850966, acc 1\n",
      "2018-10-26T18:01:18.304528: step 15373, loss 6.25408e-05, acc 1\n",
      "2018-10-26T18:01:18.469057: step 15374, loss 0.000982883, acc 1\n",
      "2018-10-26T18:01:18.639601: step 15375, loss 0.0231366, acc 0.984375\n",
      "2018-10-26T18:01:18.805160: step 15376, loss 2.87537e-05, acc 1\n",
      "2018-10-26T18:01:18.985677: step 15377, loss 0.000115471, acc 1\n",
      "2018-10-26T18:01:19.149240: step 15378, loss 0.000350092, acc 1\n",
      "2018-10-26T18:01:19.325769: step 15379, loss 0.000451592, acc 1\n",
      "2018-10-26T18:01:19.491327: step 15380, loss 6.79562e-06, acc 1\n",
      "2018-10-26T18:01:19.668852: step 15381, loss 7.16084e-06, acc 1\n",
      "2018-10-26T18:01:19.830420: step 15382, loss 0.000466476, acc 1\n",
      "2018-10-26T18:01:20.016921: step 15383, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:01:20.175498: step 15384, loss 0.000657933, acc 1\n",
      "2018-10-26T18:01:20.348037: step 15385, loss 1.97059e-06, acc 1\n",
      "2018-10-26T18:01:20.514593: step 15386, loss 2.27601e-06, acc 1\n",
      "2018-10-26T18:01:20.687131: step 15387, loss 6.62594e-05, acc 1\n",
      "2018-10-26T18:01:20.854683: step 15388, loss 7.63675e-07, acc 1\n",
      "2018-10-26T18:01:21.031211: step 15389, loss 1.00495e-05, acc 1\n",
      "2018-10-26T18:01:21.192780: step 15390, loss 4.44365e-06, acc 1\n",
      "2018-10-26T18:01:21.364322: step 15391, loss 0.000106267, acc 1\n",
      "2018-10-26T18:01:21.554813: step 15392, loss 0.00012053, acc 1\n",
      "2018-10-26T18:01:21.779214: step 15393, loss 0.00691734, acc 1\n",
      "2018-10-26T18:01:21.971700: step 15394, loss 1.55527e-06, acc 1\n",
      "2018-10-26T18:01:22.171167: step 15395, loss 5.90185e-06, acc 1\n",
      "2018-10-26T18:01:22.350686: step 15396, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:01:22.522227: step 15397, loss 4.74313e-05, acc 1\n",
      "2018-10-26T18:01:22.696761: step 15398, loss 2.00782e-06, acc 1\n",
      "2018-10-26T18:01:22.884261: step 15399, loss 0.000105961, acc 1\n",
      "2018-10-26T18:01:23.071761: step 15400, loss 5.5941e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:01:23.518567: step 15400, loss 5.36367, acc 0.699812\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15400\n",
      "\n",
      "2018-10-26T18:01:23.881955: step 15401, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:01:24.059479: step 15402, loss 6.72845e-06, acc 1\n",
      "2018-10-26T18:01:24.223042: step 15403, loss 3.18149e-05, acc 1\n",
      "2018-10-26T18:01:24.397575: step 15404, loss 2.89891e-05, acc 1\n",
      "2018-10-26T18:01:24.583080: step 15405, loss 7.79077e-06, acc 1\n",
      "2018-10-26T18:01:24.828425: step 15406, loss 2.86449e-06, acc 1\n",
      "2018-10-26T18:01:24.992985: step 15407, loss 2.22768e-06, acc 1\n",
      "2018-10-26T18:01:25.168515: step 15408, loss 1.77503e-06, acc 1\n",
      "2018-10-26T18:01:25.333077: step 15409, loss 3.03668e-05, acc 1\n",
      "2018-10-26T18:01:25.514590: step 15410, loss 2.90571e-07, acc 1\n",
      "2018-10-26T18:01:25.677156: step 15411, loss 2.78817e-06, acc 1\n",
      "2018-10-26T18:01:25.853685: step 15412, loss 2.19222e-06, acc 1\n",
      "2018-10-26T18:01:26.021237: step 15413, loss 9.21481e-06, acc 1\n",
      "2018-10-26T18:01:26.201755: step 15414, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:01:26.366316: step 15415, loss 5.65077e-06, acc 1\n",
      "2018-10-26T18:01:26.541847: step 15416, loss 3.94744e-05, acc 1\n",
      "2018-10-26T18:01:26.708402: step 15417, loss 2.76896e-05, acc 1\n",
      "2018-10-26T18:01:26.884929: step 15418, loss 7.66568e-06, acc 1\n",
      "2018-10-26T18:01:27.046497: step 15419, loss 4.5702e-05, acc 1\n",
      "2018-10-26T18:01:27.224024: step 15420, loss 2.17164e-05, acc 1\n",
      "2018-10-26T18:01:27.391576: step 15421, loss 1.52357e-06, acc 1\n",
      "2018-10-26T18:01:27.565115: step 15422, loss 1.10002e-05, acc 1\n",
      "2018-10-26T18:01:27.727678: step 15423, loss 6.40865e-06, acc 1\n",
      "2018-10-26T18:01:27.914179: step 15424, loss 9.64281e-06, acc 1\n",
      "2018-10-26T18:01:28.095695: step 15425, loss 4.2314e-06, acc 1\n",
      "2018-10-26T18:01:28.294165: step 15426, loss 0.000585051, acc 1\n",
      "2018-10-26T18:01:28.483659: step 15427, loss 9.87201e-08, acc 1\n",
      "2018-10-26T18:01:28.686119: step 15428, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:01:28.864640: step 15429, loss 5.01046e-07, acc 1\n",
      "2018-10-26T18:01:29.045161: step 15430, loss 0.000328533, acc 1\n",
      "2018-10-26T18:01:29.207723: step 15431, loss 8.94069e-08, acc 1\n",
      "2018-10-26T18:01:29.382258: step 15432, loss 0.10641, acc 0.984375\n",
      "2018-10-26T18:01:29.548813: step 15433, loss 1.47299e-05, acc 1\n",
      "2018-10-26T18:01:29.727336: step 15434, loss 0.00120441, acc 1\n",
      "2018-10-26T18:01:29.891896: step 15435, loss 0.000117066, acc 1\n",
      "2018-10-26T18:01:30.065432: step 15436, loss 4.17566e-06, acc 1\n",
      "2018-10-26T18:01:30.231987: step 15437, loss 4.01007e-06, acc 1\n",
      "2018-10-26T18:01:30.418510: step 15438, loss 0.000181832, acc 1\n",
      "2018-10-26T18:01:30.579060: step 15439, loss 1.59623e-06, acc 1\n",
      "2018-10-26T18:01:30.752596: step 15440, loss 9.02649e-05, acc 1\n",
      "2018-10-26T18:01:30.921145: step 15441, loss 2.49026e-06, acc 1\n",
      "2018-10-26T18:01:31.099669: step 15442, loss 0.00263914, acc 1\n",
      "2018-10-26T18:01:31.262235: step 15443, loss 0.000570257, acc 1\n",
      "2018-10-26T18:01:31.438763: step 15444, loss 8.45288e-05, acc 1\n",
      "2018-10-26T18:01:31.603323: step 15445, loss 3.63214e-07, acc 1\n",
      "2018-10-26T18:01:31.776859: step 15446, loss 2.27046e-06, acc 1\n",
      "2018-10-26T18:01:31.948402: step 15447, loss 4.74969e-07, acc 1\n",
      "2018-10-26T18:01:32.123933: step 15448, loss 9.56431e-05, acc 1\n",
      "2018-10-26T18:01:32.292482: step 15449, loss 0.000232019, acc 1\n",
      "2018-10-26T18:01:32.475991: step 15450, loss 0.000134001, acc 1\n",
      "2018-10-26T18:01:32.640559: step 15451, loss 9.0522e-07, acc 1\n",
      "2018-10-26T18:01:32.815085: step 15452, loss 3.36735e-06, acc 1\n",
      "2018-10-26T18:01:32.985630: step 15453, loss 5.31526e-06, acc 1\n",
      "2018-10-26T18:01:33.165150: step 15454, loss 2.34501e-06, acc 1\n",
      "2018-10-26T18:01:33.328713: step 15455, loss 6.37144e-06, acc 1\n",
      "2018-10-26T18:01:33.513220: step 15456, loss 1.55547e-05, acc 1\n",
      "2018-10-26T18:01:33.684762: step 15457, loss 0.0232425, acc 0.984375\n",
      "2018-10-26T18:01:33.859295: step 15458, loss 2.75667e-06, acc 1\n",
      "2018-10-26T18:01:34.021861: step 15459, loss 3.86226e-05, acc 1\n",
      "2018-10-26T18:01:34.200384: step 15460, loss 1.56644e-06, acc 1\n",
      "2018-10-26T18:01:34.363947: step 15461, loss 0.000196114, acc 1\n",
      "2018-10-26T18:01:34.547457: step 15462, loss 2.53299e-06, acc 1\n",
      "2018-10-26T18:01:34.709025: step 15463, loss 0.000259627, acc 1\n",
      "2018-10-26T18:01:34.891537: step 15464, loss 0.00292535, acc 1\n",
      "2018-10-26T18:01:35.066072: step 15465, loss 5.56923e-07, acc 1\n",
      "2018-10-26T18:01:35.239607: step 15466, loss 0.000281418, acc 1\n",
      "2018-10-26T18:01:35.406164: step 15467, loss 6.29492e-06, acc 1\n",
      "2018-10-26T18:01:35.590670: step 15468, loss 3.46449e-07, acc 1\n",
      "2018-10-26T18:01:35.755230: step 15469, loss 1.26471e-06, acc 1\n",
      "2018-10-26T18:01:35.942729: step 15470, loss 0.00413815, acc 1\n",
      "2018-10-26T18:01:36.104296: step 15471, loss 0.000345012, acc 1\n",
      "2018-10-26T18:01:36.286809: step 15472, loss 1.76951e-07, acc 1\n",
      "2018-10-26T18:01:36.451370: step 15473, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:01:36.633882: step 15474, loss 5.08494e-07, acc 1\n",
      "2018-10-26T18:01:36.799440: step 15475, loss 0.000422936, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:01:36.972976: step 15476, loss 8.48211e-06, acc 1\n",
      "2018-10-26T18:01:37.141531: step 15477, loss 1.62873e-05, acc 1\n",
      "2018-10-26T18:01:37.318055: step 15478, loss 1.39878e-06, acc 1\n",
      "2018-10-26T18:01:37.479623: step 15479, loss 0.000116117, acc 1\n",
      "2018-10-26T18:01:37.657148: step 15480, loss 1.2405e-06, acc 1\n",
      "2018-10-26T18:01:37.820711: step 15481, loss 3.59835e-06, acc 1\n",
      "2018-10-26T18:01:37.991256: step 15482, loss 1.51054e-06, acc 1\n",
      "2018-10-26T18:01:38.154819: step 15483, loss 1.41558e-06, acc 1\n",
      "2018-10-26T18:01:38.329352: step 15484, loss 2.55914e-06, acc 1\n",
      "2018-10-26T18:01:38.490920: step 15485, loss 0.000299322, acc 1\n",
      "2018-10-26T18:01:38.665454: step 15486, loss 0.000407003, acc 1\n",
      "2018-10-26T18:01:38.829017: step 15487, loss 1.84769e-06, acc 1\n",
      "2018-10-26T18:01:38.998564: step 15488, loss 0.017061, acc 0.984375\n",
      "2018-10-26T18:01:39.164122: step 15489, loss 6.81952e-06, acc 1\n",
      "2018-10-26T18:01:39.335663: step 15490, loss 2.31518e-06, acc 1\n",
      "2018-10-26T18:01:39.501221: step 15491, loss 5.43827e-06, acc 1\n",
      "2018-10-26T18:01:39.682736: step 15492, loss 1.16037e-05, acc 1\n",
      "2018-10-26T18:01:39.846299: step 15493, loss 3.53302e-05, acc 1\n",
      "2018-10-26T18:01:40.029809: step 15494, loss 4.3161e-05, acc 1\n",
      "2018-10-26T18:01:40.193373: step 15495, loss 5.3271e-07, acc 1\n",
      "2018-10-26T18:01:40.364913: step 15496, loss 0.00955565, acc 1\n",
      "2018-10-26T18:01:40.541442: step 15497, loss 2.89242e-06, acc 1\n",
      "2018-10-26T18:01:40.711986: step 15498, loss 0, acc 1\n",
      "2018-10-26T18:01:40.882531: step 15499, loss 0.000120532, acc 1\n",
      "2018-10-26T18:01:41.043101: step 15500, loss 0.0236269, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:01:41.475945: step 15500, loss 5.47492, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15500\n",
      "\n",
      "2018-10-26T18:01:41.801986: step 15501, loss 5.71005e-05, acc 1\n",
      "2018-10-26T18:01:41.971533: step 15502, loss 5.62646e-06, acc 1\n",
      "2018-10-26T18:01:42.144072: step 15503, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:01:42.305640: step 15504, loss 7.62776e-06, acc 1\n",
      "2018-10-26T18:01:42.477182: step 15505, loss 4.61895e-05, acc 1\n",
      "2018-10-26T18:01:42.737487: step 15506, loss 4.32455e-05, acc 1\n",
      "2018-10-26T18:01:42.907034: step 15507, loss 0.00047484, acc 1\n",
      "2018-10-26T18:01:43.083561: step 15508, loss 1.83653e-06, acc 1\n",
      "2018-10-26T18:01:43.249120: step 15509, loss 0.00308485, acc 1\n",
      "2018-10-26T18:01:43.417669: step 15510, loss 9.98853e-05, acc 1\n",
      "2018-10-26T18:01:43.584224: step 15511, loss 0.00142092, acc 1\n",
      "2018-10-26T18:01:43.756763: step 15512, loss 1.19205e-05, acc 1\n",
      "2018-10-26T18:01:43.929301: step 15513, loss 1.5385e-06, acc 1\n",
      "2018-10-26T18:01:44.108822: step 15514, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:01:44.277372: step 15515, loss 4.39368e-06, acc 1\n",
      "2018-10-26T18:01:44.445922: step 15516, loss 3.16961e-05, acc 1\n",
      "2018-10-26T18:01:44.609484: step 15517, loss 0.0419144, acc 0.984375\n",
      "2018-10-26T18:01:44.785016: step 15518, loss 0.000247496, acc 1\n",
      "2018-10-26T18:01:44.951570: step 15519, loss 7.14256e-06, acc 1\n",
      "2018-10-26T18:01:45.130094: step 15520, loss 8.81393e-05, acc 1\n",
      "2018-10-26T18:01:45.287672: step 15521, loss 0.000144634, acc 1\n",
      "2018-10-26T18:01:45.462206: step 15522, loss 0.000939241, acc 1\n",
      "2018-10-26T18:01:45.628761: step 15523, loss 9.1454e-07, acc 1\n",
      "2018-10-26T18:01:45.807285: step 15524, loss 3.78114e-07, acc 1\n",
      "2018-10-26T18:01:45.971844: step 15525, loss 0.000166485, acc 1\n",
      "2018-10-26T18:01:46.157349: step 15526, loss 0.0330064, acc 0.984375\n",
      "2018-10-26T18:01:46.323904: step 15527, loss 0.00345979, acc 1\n",
      "2018-10-26T18:01:46.497441: step 15528, loss 4.93596e-07, acc 1\n",
      "2018-10-26T18:01:46.666987: step 15529, loss 8.58103e-05, acc 1\n",
      "2018-10-26T18:01:46.841521: step 15530, loss 3.17749e-06, acc 1\n",
      "2018-10-26T18:01:47.009076: step 15531, loss 0.000274053, acc 1\n",
      "2018-10-26T18:01:47.185602: step 15532, loss 0.00126613, acc 1\n",
      "2018-10-26T18:01:47.355149: step 15533, loss 1.07596e-05, acc 1\n",
      "2018-10-26T18:01:47.527687: step 15534, loss 0.0001806, acc 1\n",
      "2018-10-26T18:01:47.693245: step 15535, loss 4.07917e-07, acc 1\n",
      "2018-10-26T18:01:47.868776: step 15536, loss 1.36716e-06, acc 1\n",
      "2018-10-26T18:01:48.108137: step 15537, loss 0.000165059, acc 1\n",
      "2018-10-26T18:01:48.339519: step 15538, loss 1.24511e-05, acc 1\n",
      "2018-10-26T18:01:48.518041: step 15539, loss 1.21442e-06, acc 1\n",
      "2018-10-26T18:01:48.774356: step 15540, loss 1.23892e-05, acc 1\n",
      "2018-10-26T18:01:49.005754: step 15541, loss 0.000226197, acc 1\n",
      "2018-10-26T18:01:49.210193: step 15542, loss 4.788e-05, acc 1\n",
      "2018-10-26T18:01:49.442571: step 15543, loss 8.11162e-06, acc 1\n",
      "2018-10-26T18:01:49.663980: step 15544, loss 1.26659e-05, acc 1\n",
      "2018-10-26T18:01:49.885389: step 15545, loss 1.34108e-06, acc 1\n",
      "2018-10-26T18:01:50.126743: step 15546, loss 0.001384, acc 1\n",
      "2018-10-26T18:01:50.296290: step 15547, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:01:50.514707: step 15548, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:01:50.708189: step 15549, loss 0.000596131, acc 1\n",
      "2018-10-26T18:01:50.929598: step 15550, loss 7.69265e-07, acc 1\n",
      "2018-10-26T18:01:51.150009: step 15551, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:01:51.374410: step 15552, loss 7.17102e-07, acc 1\n",
      "2018-10-26T18:01:51.584847: step 15553, loss 0.000155365, acc 1\n",
      "2018-10-26T18:01:51.791296: step 15554, loss 7.14467e-05, acc 1\n",
      "2018-10-26T18:01:52.034646: step 15555, loss 8.10239e-07, acc 1\n",
      "2018-10-26T18:01:52.240097: step 15556, loss 4.01089e-05, acc 1\n",
      "2018-10-26T18:01:52.424603: step 15557, loss 0.000211718, acc 1\n",
      "2018-10-26T18:01:52.674935: step 15558, loss 0.000386306, acc 1\n",
      "2018-10-26T18:01:52.882381: step 15559, loss 5.55064e-07, acc 1\n",
      "2018-10-26T18:01:53.109773: step 15560, loss 1.1306e-06, acc 1\n",
      "2018-10-26T18:01:53.298269: step 15561, loss 0.000126635, acc 1\n",
      "2018-10-26T18:01:53.535635: step 15562, loss 6.37644e-06, acc 1\n",
      "2018-10-26T18:01:53.728121: step 15563, loss 3.57178e-05, acc 1\n",
      "2018-10-26T18:01:53.917629: step 15564, loss 0.000296284, acc 1\n",
      "2018-10-26T18:01:54.112095: step 15565, loss 4.74021e-05, acc 1\n",
      "2018-10-26T18:01:54.301595: step 15566, loss 9.42983e-05, acc 1\n",
      "2018-10-26T18:01:54.483103: step 15567, loss 4.09365e-06, acc 1\n",
      "2018-10-26T18:01:54.676617: step 15568, loss 2.49256e-05, acc 1\n",
      "2018-10-26T18:01:54.853115: step 15569, loss 0.000566997, acc 1\n",
      "2018-10-26T18:01:55.058566: step 15570, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:01:55.230108: step 15571, loss 5.15336e-06, acc 1\n",
      "2018-10-26T18:01:55.405639: step 15572, loss 1.62049e-07, acc 1\n",
      "2018-10-26T18:01:55.599121: step 15573, loss 6.74347e-06, acc 1\n",
      "2018-10-26T18:01:55.776648: step 15574, loss 9.36884e-07, acc 1\n",
      "2018-10-26T18:01:55.953175: step 15575, loss 0.00079527, acc 1\n",
      "2018-10-26T18:01:56.153640: step 15576, loss 0.00111384, acc 1\n",
      "2018-10-26T18:01:56.319198: step 15577, loss 1.125e-06, acc 1\n",
      "2018-10-26T18:01:56.497721: step 15578, loss 7.98612e-06, acc 1\n",
      "2018-10-26T18:01:56.660287: step 15579, loss 0, acc 1\n",
      "2018-10-26T18:01:56.887678: step 15580, loss 0.000228699, acc 1\n",
      "2018-10-26T18:01:57.051242: step 15581, loss 2.22204e-06, acc 1\n",
      "2018-10-26T18:01:57.230762: step 15582, loss 0.000285176, acc 1\n",
      "2018-10-26T18:01:57.394326: step 15583, loss 6.81202e-05, acc 1\n",
      "2018-10-26T18:01:57.599777: step 15584, loss 3.44588e-07, acc 1\n",
      "2018-10-26T18:01:57.770321: step 15585, loss 3.79396e-05, acc 1\n",
      "2018-10-26T18:01:57.950839: step 15586, loss 0.000128752, acc 1\n",
      "2018-10-26T18:01:58.114402: step 15587, loss 1.28891e-06, acc 1\n",
      "2018-10-26T18:01:58.293921: step 15588, loss 0.00276279, acc 1\n",
      "2018-10-26T18:01:58.458482: step 15589, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:01:58.635011: step 15590, loss 2.77211e-05, acc 1\n",
      "2018-10-26T18:01:58.797576: step 15591, loss 7.24036e-06, acc 1\n",
      "2018-10-26T18:01:58.971113: step 15592, loss 1.97992e-06, acc 1\n",
      "2018-10-26T18:01:59.133678: step 15593, loss 7.91605e-07, acc 1\n",
      "2018-10-26T18:01:59.310213: step 15594, loss 3.21276e-06, acc 1\n",
      "2018-10-26T18:01:59.492719: step 15595, loss 4.17941e-06, acc 1\n",
      "2018-10-26T18:01:59.668250: step 15596, loss 5.1967e-07, acc 1\n",
      "2018-10-26T18:01:59.836800: step 15597, loss 0.00130615, acc 1\n",
      "2018-10-26T18:02:00.018316: step 15598, loss 0.00015975, acc 1\n",
      "2018-10-26T18:02:00.186864: step 15599, loss 2.96335e-06, acc 1\n",
      "2018-10-26T18:02:00.364389: step 15600, loss 0.00850762, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:02:00.807207: step 15600, loss 5.42731, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15600\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:02:01.160656: step 15601, loss 9.01106e-06, acc 1\n",
      "2018-10-26T18:02:01.331171: step 15602, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:02:01.507700: step 15603, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:02:01.679241: step 15604, loss 2.87978e-05, acc 1\n",
      "2018-10-26T18:02:01.868736: step 15605, loss 0.000344717, acc 1\n",
      "2018-10-26T18:02:02.105104: step 15606, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:02:02.282629: step 15607, loss 1.86406e-05, acc 1\n",
      "2018-10-26T18:02:02.461153: step 15608, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:02:02.626710: step 15609, loss 1.49007e-06, acc 1\n",
      "2018-10-26T18:02:02.800246: step 15610, loss 1.23264e-05, acc 1\n",
      "2018-10-26T18:02:02.968796: step 15611, loss 8.97765e-06, acc 1\n",
      "2018-10-26T18:02:03.144327: step 15612, loss 3.42455e-05, acc 1\n",
      "2018-10-26T18:02:03.307890: step 15613, loss 1.87912e-05, acc 1\n",
      "2018-10-26T18:02:03.484418: step 15614, loss 5.36917e-06, acc 1\n",
      "2018-10-26T18:02:03.651971: step 15615, loss 5.2649e-06, acc 1\n",
      "2018-10-26T18:02:03.823513: step 15616, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:02:03.995054: step 15617, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:02:04.169587: step 15618, loss 0.000108015, acc 1\n",
      "2018-10-26T18:02:04.330159: step 15619, loss 5.12527e-06, acc 1\n",
      "2018-10-26T18:02:04.502697: step 15620, loss 4.86629e-05, acc 1\n",
      "2018-10-26T18:02:04.662271: step 15621, loss 0.00020671, acc 1\n",
      "2018-10-26T18:02:04.832816: step 15622, loss 0.000148143, acc 1\n",
      "2018-10-26T18:02:05.004357: step 15623, loss 0.00564141, acc 1\n",
      "2018-10-26T18:02:05.176896: step 15624, loss 8.37274e-05, acc 1\n",
      "2018-10-26T18:02:05.343452: step 15625, loss 0.000130348, acc 1\n",
      "2018-10-26T18:02:05.513995: step 15626, loss 1.9687e-06, acc 1\n",
      "2018-10-26T18:02:05.680551: step 15627, loss 2.37852e-06, acc 1\n",
      "2018-10-26T18:02:05.855084: step 15628, loss 1.89599e-05, acc 1\n",
      "2018-10-26T18:02:06.018647: step 15629, loss 8.49353e-07, acc 1\n",
      "2018-10-26T18:02:06.202157: step 15630, loss 4.59485e-06, acc 1\n",
      "2018-10-26T18:02:06.367714: step 15631, loss 0.00120819, acc 1\n",
      "2018-10-26T18:02:06.538259: step 15632, loss 4.28193e-06, acc 1\n",
      "2018-10-26T18:02:06.699827: step 15633, loss 4.54482e-07, acc 1\n",
      "2018-10-26T18:02:06.879347: step 15634, loss 1.1992e-05, acc 1\n",
      "2018-10-26T18:02:07.039918: step 15635, loss 1.34703e-05, acc 1\n",
      "2018-10-26T18:02:07.213455: step 15636, loss 9.14228e-06, acc 1\n",
      "2018-10-26T18:02:07.377018: step 15637, loss 4.34822e-05, acc 1\n",
      "2018-10-26T18:02:07.556538: step 15638, loss 1.85326e-06, acc 1\n",
      "2018-10-26T18:02:07.715114: step 15639, loss 9.11444e-06, acc 1\n",
      "2018-10-26T18:02:07.888651: step 15640, loss 1.24834e-05, acc 1\n",
      "2018-10-26T18:02:08.052214: step 15641, loss 2.10841e-06, acc 1\n",
      "2018-10-26T18:02:08.226747: step 15642, loss 7.868e-05, acc 1\n",
      "2018-10-26T18:02:08.386321: step 15643, loss 4.22234e-06, acc 1\n",
      "2018-10-26T18:02:08.560855: step 15644, loss 0.000131191, acc 1\n",
      "2018-10-26T18:02:08.730435: step 15645, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:02:08.912947: step 15646, loss 0.000321998, acc 1\n",
      "2018-10-26T18:02:09.080499: step 15647, loss 2.26262e-05, acc 1\n",
      "2018-10-26T18:02:09.254036: step 15648, loss 0.000303446, acc 1\n",
      "2018-10-26T18:02:09.415605: step 15649, loss 0.000445591, acc 1\n",
      "2018-10-26T18:02:09.596122: step 15650, loss 1.59993e-06, acc 1\n",
      "2018-10-26T18:02:09.765669: step 15651, loss 0.000108175, acc 1\n",
      "2018-10-26T18:02:09.938208: step 15652, loss 1.05797e-06, acc 1\n",
      "2018-10-26T18:02:10.107755: step 15653, loss 1.64376e-05, acc 1\n",
      "2018-10-26T18:02:10.286278: step 15654, loss 1.3611e-05, acc 1\n",
      "2018-10-26T18:02:10.452833: step 15655, loss 0.00121334, acc 1\n",
      "2018-10-26T18:02:10.627366: step 15656, loss 3.01979e-05, acc 1\n",
      "2018-10-26T18:02:10.790929: step 15657, loss 1.11943e-06, acc 1\n",
      "2018-10-26T18:02:10.965462: step 15658, loss 5.02496e-06, acc 1\n",
      "2018-10-26T18:02:11.136007: step 15659, loss 4.19092e-07, acc 1\n",
      "2018-10-26T18:02:11.307549: step 15660, loss 1.27029e-06, acc 1\n",
      "2018-10-26T18:02:11.473106: step 15661, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:02:11.646643: step 15662, loss 2.65782e-06, acc 1\n",
      "2018-10-26T18:02:11.812202: step 15663, loss 1.07846e-06, acc 1\n",
      "2018-10-26T18:02:11.987732: step 15664, loss 3.51922e-05, acc 1\n",
      "2018-10-26T18:02:12.159273: step 15665, loss 3.55104e-05, acc 1\n",
      "2018-10-26T18:02:12.333808: step 15666, loss 1.32773e-05, acc 1\n",
      "2018-10-26T18:02:12.503354: step 15667, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:02:12.685867: step 15668, loss 8.12174e-05, acc 1\n",
      "2018-10-26T18:02:12.849429: step 15669, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:02:13.023963: step 15670, loss 9.36894e-07, acc 1\n",
      "2018-10-26T18:02:13.191515: step 15671, loss 1.39427e-05, acc 1\n",
      "2018-10-26T18:02:13.364055: step 15672, loss 3.2813e-05, acc 1\n",
      "2018-10-26T18:02:13.534599: step 15673, loss 5.72874e-06, acc 1\n",
      "2018-10-26T18:02:13.707138: step 15674, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:02:13.870701: step 15675, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:02:14.046232: step 15676, loss 0.000393128, acc 1\n",
      "2018-10-26T18:02:14.209794: step 15677, loss 1.6018e-06, acc 1\n",
      "2018-10-26T18:02:14.383333: step 15678, loss 5.97677e-05, acc 1\n",
      "2018-10-26T18:02:14.547892: step 15679, loss 0.000116097, acc 1\n",
      "2018-10-26T18:02:14.722425: step 15680, loss 1.70429e-06, acc 1\n",
      "2018-10-26T18:02:14.892969: step 15681, loss 0.000232521, acc 1\n",
      "2018-10-26T18:02:15.083462: step 15682, loss 0.0168732, acc 0.984375\n",
      "2018-10-26T18:02:15.251013: step 15683, loss 4.29535e-05, acc 1\n",
      "2018-10-26T18:02:15.434522: step 15684, loss 0.00113607, acc 1\n",
      "2018-10-26T18:02:15.598085: step 15685, loss 7.44441e-05, acc 1\n",
      "2018-10-26T18:02:15.782593: step 15686, loss 1.1805e-05, acc 1\n",
      "2018-10-26T18:02:15.956128: step 15687, loss 7.49039e-06, acc 1\n",
      "2018-10-26T18:02:16.127670: step 15688, loss 8.74071e-05, acc 1\n",
      "2018-10-26T18:02:16.296220: step 15689, loss 9.05127e-05, acc 1\n",
      "2018-10-26T18:02:16.473746: step 15690, loss 1.47149e-07, acc 1\n",
      "2018-10-26T18:02:16.640301: step 15691, loss 9.24229e-05, acc 1\n",
      "2018-10-26T18:02:16.814834: step 15692, loss 7.51589e-06, acc 1\n",
      "2018-10-26T18:02:16.981389: step 15693, loss 0.000498768, acc 1\n",
      "2018-10-26T18:02:17.157918: step 15694, loss 0.00676054, acc 1\n",
      "2018-10-26T18:02:17.323476: step 15695, loss 3.99508e-06, acc 1\n",
      "2018-10-26T18:02:17.498008: step 15696, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:02:17.665562: step 15697, loss 2.64156e-05, acc 1\n",
      "2018-10-26T18:02:17.840097: step 15698, loss 9.67595e-05, acc 1\n",
      "2018-10-26T18:02:18.012634: step 15699, loss 4.80304e-06, acc 1\n",
      "2018-10-26T18:02:18.190160: step 15700, loss 7.41316e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:02:18.631979: step 15700, loss 5.46878, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15700\n",
      "\n",
      "2018-10-26T18:02:18.949949: step 15701, loss 2.59904e-05, acc 1\n",
      "2018-10-26T18:02:19.119496: step 15702, loss 0.000189629, acc 1\n",
      "2018-10-26T18:02:19.295027: step 15703, loss 4.31527e-06, acc 1\n",
      "2018-10-26T18:02:19.459586: step 15704, loss 5.33625e-05, acc 1\n",
      "2018-10-26T18:02:19.658058: step 15705, loss 3.91151e-07, acc 1\n",
      "2018-10-26T18:02:19.905396: step 15706, loss 1.95568e-06, acc 1\n",
      "2018-10-26T18:02:20.082921: step 15707, loss 0.000172716, acc 1\n",
      "2018-10-26T18:02:20.267429: step 15708, loss 0.000210505, acc 1\n",
      "2018-10-26T18:02:20.427002: step 15709, loss 6.03017e-06, acc 1\n",
      "2018-10-26T18:02:20.612506: step 15710, loss 3.21426e-05, acc 1\n",
      "2018-10-26T18:02:20.786044: step 15711, loss 0.00787806, acc 1\n",
      "2018-10-26T18:02:20.961574: step 15712, loss 1.82452e-05, acc 1\n",
      "2018-10-26T18:02:21.123142: step 15713, loss 0.00211, acc 1\n",
      "2018-10-26T18:02:21.302663: step 15714, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:02:21.466225: step 15715, loss 3.14786e-07, acc 1\n",
      "2018-10-26T18:02:21.647740: step 15716, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:02:21.819282: step 15717, loss 1.54434e-05, acc 1\n",
      "2018-10-26T18:02:22.004786: step 15718, loss 5.04773e-07, acc 1\n",
      "2018-10-26T18:02:22.168350: step 15719, loss 1.88952e-05, acc 1\n",
      "2018-10-26T18:02:22.343880: step 15720, loss 4.25313e-05, acc 1\n",
      "2018-10-26T18:02:22.509438: step 15721, loss 8.56205e-05, acc 1\n",
      "2018-10-26T18:02:22.685967: step 15722, loss 4.29669e-06, acc 1\n",
      "2018-10-26T18:02:22.852521: step 15723, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:02:23.029050: step 15724, loss 0.00065467, acc 1\n",
      "2018-10-26T18:02:23.194607: step 15725, loss 1.78707e-05, acc 1\n",
      "2018-10-26T18:02:23.374128: step 15726, loss 0.00471912, acc 1\n",
      "2018-10-26T18:02:23.538701: step 15727, loss 0.0054536, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:02:23.718209: step 15728, loss 8.03798e-06, acc 1\n",
      "2018-10-26T18:02:23.881772: step 15729, loss 1.22645e-05, acc 1\n",
      "2018-10-26T18:02:24.051319: step 15730, loss 0.00128462, acc 1\n",
      "2018-10-26T18:02:24.215879: step 15731, loss 1.39136e-06, acc 1\n",
      "2018-10-26T18:02:24.388418: step 15732, loss 0.0008997, acc 1\n",
      "2018-10-26T18:02:24.552977: step 15733, loss 0.000210121, acc 1\n",
      "2018-10-26T18:02:24.722525: step 15734, loss 1.78582e-05, acc 1\n",
      "2018-10-26T18:02:24.888082: step 15735, loss 1.11465e-05, acc 1\n",
      "2018-10-26T18:02:25.060621: step 15736, loss 0.00167234, acc 1\n",
      "2018-10-26T18:02:25.226180: step 15737, loss 7.07793e-07, acc 1\n",
      "2018-10-26T18:02:25.397722: step 15738, loss 8.29565e-05, acc 1\n",
      "2018-10-26T18:02:25.563279: step 15739, loss 1.7422e-05, acc 1\n",
      "2018-10-26T18:02:25.737813: step 15740, loss 4.04598e-05, acc 1\n",
      "2018-10-26T18:02:25.902373: step 15741, loss 0.00145401, acc 1\n",
      "2018-10-26T18:02:26.070923: step 15742, loss 8.3819e-08, acc 1\n",
      "2018-10-26T18:02:26.237477: step 15743, loss 3.87428e-07, acc 1\n",
      "2018-10-26T18:02:26.413008: step 15744, loss 3.15232e-05, acc 1\n",
      "2018-10-26T18:02:26.580561: step 15745, loss 0.000193317, acc 1\n",
      "2018-10-26T18:02:26.758086: step 15746, loss 9.81599e-07, acc 1\n",
      "2018-10-26T18:02:26.931623: step 15747, loss 0.000617518, acc 1\n",
      "2018-10-26T18:02:27.102167: step 15748, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:02:27.271714: step 15749, loss 8.79146e-07, acc 1\n",
      "2018-10-26T18:02:27.431288: step 15750, loss 1.19209e-08, acc 1\n",
      "2018-10-26T18:02:27.607816: step 15751, loss 3.60406e-06, acc 1\n",
      "2018-10-26T18:02:27.781353: step 15752, loss 5.41868e-05, acc 1\n",
      "2018-10-26T18:02:27.956884: step 15753, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:02:28.119449: step 15754, loss 4.22816e-07, acc 1\n",
      "2018-10-26T18:02:28.301961: step 15755, loss 7.40983e-06, acc 1\n",
      "2018-10-26T18:02:28.466522: step 15756, loss 1.3634e-06, acc 1\n",
      "2018-10-26T18:02:28.644047: step 15757, loss 5.08796e-06, acc 1\n",
      "2018-10-26T18:02:28.822570: step 15758, loss 2.59277e-05, acc 1\n",
      "2018-10-26T18:02:28.994111: step 15759, loss 3.53679e-05, acc 1\n",
      "2018-10-26T18:02:29.169643: step 15760, loss 9.48065e-07, acc 1\n",
      "2018-10-26T18:02:29.332209: step 15761, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:02:29.507740: step 15762, loss 2.96509e-06, acc 1\n",
      "2018-10-26T18:02:29.672300: step 15763, loss 4.73607e-06, acc 1\n",
      "2018-10-26T18:02:29.850823: step 15764, loss 4.35857e-07, acc 1\n",
      "2018-10-26T18:02:30.014386: step 15765, loss 5.36039e-06, acc 1\n",
      "2018-10-26T18:02:30.190915: step 15766, loss 4.17228e-07, acc 1\n",
      "2018-10-26T18:02:30.356473: step 15767, loss 6.74274e-07, acc 1\n",
      "2018-10-26T18:02:30.536990: step 15768, loss 2.54709e-05, acc 1\n",
      "2018-10-26T18:02:30.698558: step 15769, loss 8.0443e-05, acc 1\n",
      "2018-10-26T18:02:30.868105: step 15770, loss 5.79278e-07, acc 1\n",
      "2018-10-26T18:02:31.033663: step 15771, loss 4.78526e-05, acc 1\n",
      "2018-10-26T18:02:31.220164: step 15772, loss 2.06753e-07, acc 1\n",
      "2018-10-26T18:02:31.382730: step 15773, loss 1.58374e-05, acc 1\n",
      "2018-10-26T18:02:31.564245: step 15774, loss 2.38101e-05, acc 1\n",
      "2018-10-26T18:02:31.725813: step 15775, loss 5.6016e-05, acc 1\n",
      "2018-10-26T18:02:31.904336: step 15776, loss 0.0205251, acc 0.984375\n",
      "2018-10-26T18:02:32.067899: step 15777, loss 3.63212e-07, acc 1\n",
      "2018-10-26T18:02:32.244428: step 15778, loss 2.60936e-06, acc 1\n",
      "2018-10-26T18:02:32.404999: step 15779, loss 1.32357e-05, acc 1\n",
      "2018-10-26T18:02:32.589506: step 15780, loss 0.000287848, acc 1\n",
      "2018-10-26T18:02:32.754066: step 15781, loss 3.16068e-06, acc 1\n",
      "2018-10-26T18:02:32.934584: step 15782, loss 8.27109e-05, acc 1\n",
      "2018-10-26T18:02:33.101139: step 15783, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:02:33.272681: step 15784, loss 2.43041e-05, acc 1\n",
      "2018-10-26T18:02:33.435246: step 15785, loss 1.42116e-06, acc 1\n",
      "2018-10-26T18:02:33.609779: step 15786, loss 6.67137e-06, acc 1\n",
      "2018-10-26T18:02:33.781321: step 15787, loss 1.60556e-06, acc 1\n",
      "2018-10-26T18:02:33.959844: step 15788, loss 1.90543e-06, acc 1\n",
      "2018-10-26T18:02:34.129392: step 15789, loss 1.4641e-05, acc 1\n",
      "2018-10-26T18:02:34.299935: step 15790, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:02:34.460507: step 15791, loss 5.28984e-07, acc 1\n",
      "2018-10-26T18:02:34.633046: step 15792, loss 3.71661e-05, acc 1\n",
      "2018-10-26T18:02:34.795611: step 15793, loss 9.88084e-05, acc 1\n",
      "2018-10-26T18:02:34.963164: step 15794, loss 5.35803e-05, acc 1\n",
      "2018-10-26T18:02:35.126726: step 15795, loss 0.000180418, acc 1\n",
      "2018-10-26T18:02:35.295276: step 15796, loss 1.29254e-05, acc 1\n",
      "2018-10-26T18:02:35.462829: step 15797, loss 0.00425601, acc 1\n",
      "2018-10-26T18:02:35.641351: step 15798, loss 4.15368e-07, acc 1\n",
      "2018-10-26T18:02:35.812893: step 15799, loss 2.3283e-07, acc 1\n",
      "2018-10-26T18:02:35.993413: step 15800, loss 8.42073e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:02:36.431241: step 15800, loss 5.69825, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15800\n",
      "\n",
      "2018-10-26T18:02:36.807839: step 15801, loss 5.59517e-05, acc 1\n",
      "2018-10-26T18:02:36.992346: step 15802, loss 0.00216857, acc 1\n",
      "2018-10-26T18:02:37.179844: step 15803, loss 1.34687e-05, acc 1\n",
      "2018-10-26T18:02:37.355376: step 15804, loss 6.37019e-07, acc 1\n",
      "2018-10-26T18:02:37.635627: step 15805, loss 0.000206009, acc 1\n",
      "2018-10-26T18:02:37.808166: step 15806, loss 4.65057e-06, acc 1\n",
      "2018-10-26T18:02:37.989680: step 15807, loss 7.74851e-07, acc 1\n",
      "2018-10-26T18:02:38.168203: step 15808, loss 8.83311e-06, acc 1\n",
      "2018-10-26T18:02:38.354706: step 15809, loss 2.06753e-07, acc 1\n",
      "2018-10-26T18:02:38.550183: step 15810, loss 0.000504976, acc 1\n",
      "2018-10-26T18:02:38.767603: step 15811, loss 5.61732e-05, acc 1\n",
      "2018-10-26T18:02:38.971062: step 15812, loss 0.000383066, acc 1\n",
      "2018-10-26T18:02:39.147588: step 15813, loss 0.000333326, acc 1\n",
      "2018-10-26T18:02:39.331097: step 15814, loss 2.48088e-05, acc 1\n",
      "2018-10-26T18:02:39.502638: step 15815, loss 0.0226119, acc 0.984375\n",
      "2018-10-26T18:02:39.686148: step 15816, loss 1.11759e-07, acc 1\n",
      "2018-10-26T18:02:39.849711: step 15817, loss 1.94973e-05, acc 1\n",
      "2018-10-26T18:02:40.027237: step 15818, loss 6.84036e-06, acc 1\n",
      "2018-10-26T18:02:40.189803: step 15819, loss 4.34369e-05, acc 1\n",
      "2018-10-26T18:02:40.363339: step 15820, loss 3.22672e-05, acc 1\n",
      "2018-10-26T18:02:40.525905: step 15821, loss 0.000104689, acc 1\n",
      "2018-10-26T18:02:40.707420: step 15822, loss 2.19597e-06, acc 1\n",
      "2018-10-26T18:02:40.887937: step 15823, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:02:41.062471: step 15824, loss 2.17039e-05, acc 1\n",
      "2018-10-26T18:02:41.243986: step 15825, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:02:41.407549: step 15826, loss 5.32412e-05, acc 1\n",
      "2018-10-26T18:02:41.587070: step 15827, loss 1.40439e-06, acc 1\n",
      "2018-10-26T18:02:41.747640: step 15828, loss 5.58789e-07, acc 1\n",
      "2018-10-26T18:02:41.925166: step 15829, loss 8.19133e-06, acc 1\n",
      "2018-10-26T18:02:42.087732: step 15830, loss 2.66718e-06, acc 1\n",
      "2018-10-26T18:02:42.263263: step 15831, loss 0.000195105, acc 1\n",
      "2018-10-26T18:02:42.434804: step 15832, loss 4.69383e-07, acc 1\n",
      "2018-10-26T18:02:42.610336: step 15833, loss 7.89754e-05, acc 1\n",
      "2018-10-26T18:02:42.773899: step 15834, loss 1.36527e-06, acc 1\n",
      "2018-10-26T18:02:42.955414: step 15835, loss 6.49928e-06, acc 1\n",
      "2018-10-26T18:02:43.118976: step 15836, loss 0.00113208, acc 1\n",
      "2018-10-26T18:02:43.295505: step 15837, loss 0.00043567, acc 1\n",
      "2018-10-26T18:02:43.459068: step 15838, loss 2.03017e-06, acc 1\n",
      "2018-10-26T18:02:43.631607: step 15839, loss 0.000122635, acc 1\n",
      "2018-10-26T18:02:43.801154: step 15840, loss 1.39694e-06, acc 1\n",
      "2018-10-26T18:02:43.979696: step 15841, loss 3.25556e-05, acc 1\n",
      "2018-10-26T18:02:44.142242: step 15842, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:02:44.330739: step 15843, loss 2.3206e-05, acc 1\n",
      "2018-10-26T18:02:44.497294: step 15844, loss 4.16253e-05, acc 1\n",
      "2018-10-26T18:02:44.671828: step 15845, loss 4.73262e-05, acc 1\n",
      "2018-10-26T18:02:44.836387: step 15846, loss 0.00016654, acc 1\n",
      "2018-10-26T18:02:45.012916: step 15847, loss 0.000719708, acc 1\n",
      "2018-10-26T18:02:45.178474: step 15848, loss 2.59475e-05, acc 1\n",
      "2018-10-26T18:02:45.363978: step 15849, loss 8.40635e-06, acc 1\n",
      "2018-10-26T18:02:45.537515: step 15850, loss 1.84578e-05, acc 1\n",
      "2018-10-26T18:02:45.711050: step 15851, loss 1.46959e-06, acc 1\n",
      "2018-10-26T18:02:45.874614: step 15852, loss 3.00052e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:02:46.050145: step 15853, loss 3.13589e-05, acc 1\n",
      "2018-10-26T18:02:46.218694: step 15854, loss 1.66746e-05, acc 1\n",
      "2018-10-26T18:02:46.397217: step 15855, loss 0.00037162, acc 1\n",
      "2018-10-26T18:02:46.562774: step 15856, loss 3.11715e-05, acc 1\n",
      "2018-10-26T18:02:46.731325: step 15857, loss 5.69479e-05, acc 1\n",
      "2018-10-26T18:02:46.894888: step 15858, loss 2.21963e-05, acc 1\n",
      "2018-10-26T18:02:47.069422: step 15859, loss 2.26785e-05, acc 1\n",
      "2018-10-26T18:02:47.231987: step 15860, loss 0.000274198, acc 1\n",
      "2018-10-26T18:02:47.412505: step 15861, loss 1.20805e-05, acc 1\n",
      "2018-10-26T18:02:47.580057: step 15862, loss 0.000414063, acc 1\n",
      "2018-10-26T18:02:47.748607: step 15863, loss 2.66356e-07, acc 1\n",
      "2018-10-26T18:02:47.916159: step 15864, loss 0.00047247, acc 1\n",
      "2018-10-26T18:02:48.088698: step 15865, loss 0.000101009, acc 1\n",
      "2018-10-26T18:02:48.248272: step 15866, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:02:48.427792: step 15867, loss 7.45049e-07, acc 1\n",
      "2018-10-26T18:02:48.594347: step 15868, loss 2.60768e-07, acc 1\n",
      "2018-10-26T18:02:48.772870: step 15869, loss 0.0110917, acc 0.984375\n",
      "2018-10-26T18:02:48.935436: step 15870, loss 7.02618e-06, acc 1\n",
      "2018-10-26T18:02:49.117948: step 15871, loss 8.56815e-08, acc 1\n",
      "2018-10-26T18:02:49.279516: step 15872, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:02:49.450061: step 15873, loss 0.000109769, acc 1\n",
      "2018-10-26T18:02:49.613624: step 15874, loss 6.35409e-06, acc 1\n",
      "2018-10-26T18:02:49.788158: step 15875, loss 5.71822e-07, acc 1\n",
      "2018-10-26T18:02:49.950724: step 15876, loss 7.99154e-06, acc 1\n",
      "2018-10-26T18:02:50.122265: step 15877, loss 5.21878e-05, acc 1\n",
      "2018-10-26T18:02:50.285828: step 15878, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:02:50.468340: step 15879, loss 1.34574e-05, acc 1\n",
      "2018-10-26T18:02:50.633898: step 15880, loss 5.34495e-06, acc 1\n",
      "2018-10-26T18:02:50.811423: step 15881, loss 2.18295e-06, acc 1\n",
      "2018-10-26T18:02:50.983962: step 15882, loss 9.49947e-08, acc 1\n",
      "2018-10-26T18:02:51.167473: step 15883, loss 2.00409e-06, acc 1\n",
      "2018-10-26T18:02:51.331035: step 15884, loss 1.17715e-06, acc 1\n",
      "2018-10-26T18:02:51.503574: step 15885, loss 3.82925e-06, acc 1\n",
      "2018-10-26T18:02:51.674118: step 15886, loss 4.71246e-07, acc 1\n",
      "2018-10-26T18:02:51.843665: step 15887, loss 7.0352e-05, acc 1\n",
      "2018-10-26T18:02:52.006231: step 15888, loss 1.5925e-06, acc 1\n",
      "2018-10-26T18:02:52.177773: step 15889, loss 4.09563e-05, acc 1\n",
      "2018-10-26T18:02:52.342333: step 15890, loss 2.06753e-07, acc 1\n",
      "2018-10-26T18:02:52.518861: step 15891, loss 2.79396e-07, acc 1\n",
      "2018-10-26T18:02:52.686414: step 15892, loss 9.33523e-05, acc 1\n",
      "2018-10-26T18:02:52.859950: step 15893, loss 7.13874e-06, acc 1\n",
      "2018-10-26T18:02:53.075375: step 15894, loss 0.0006829, acc 1\n",
      "2018-10-26T18:02:53.265866: step 15895, loss 6.68676e-07, acc 1\n",
      "2018-10-26T18:02:53.440399: step 15896, loss 3.23359e-05, acc 1\n",
      "2018-10-26T18:02:53.647844: step 15897, loss 3.29686e-07, acc 1\n",
      "2018-10-26T18:02:53.873244: step 15898, loss 4.42156e-06, acc 1\n",
      "2018-10-26T18:02:54.087670: step 15899, loss 0.000104539, acc 1\n",
      "2018-10-26T18:02:54.294118: step 15900, loss 2.744e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:02:54.812733: step 15900, loss 5.50604, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-15900\n",
      "\n",
      "2018-10-26T18:02:55.237598: step 15901, loss 9.96501e-07, acc 1\n",
      "2018-10-26T18:02:55.471972: step 15902, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:02:55.677424: step 15903, loss 2.48471e-06, acc 1\n",
      "2018-10-26T18:02:55.924762: step 15904, loss 1.48074e-06, acc 1\n",
      "2018-10-26T18:02:56.218975: step 15905, loss 7.72988e-07, acc 1\n",
      "2018-10-26T18:02:56.418442: step 15906, loss 5.85533e-05, acc 1\n",
      "2018-10-26T18:02:56.648827: step 15907, loss 2.86277e-06, acc 1\n",
      "2018-10-26T18:02:56.871234: step 15908, loss 4.237e-06, acc 1\n",
      "2018-10-26T18:02:57.031804: step 15909, loss 0.000234444, acc 1\n",
      "2018-10-26T18:02:57.267175: step 15910, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:02:57.461655: step 15911, loss 0.00479615, acc 1\n",
      "2018-10-26T18:02:57.667106: step 15912, loss 6.03491e-07, acc 1\n",
      "2018-10-26T18:02:57.859591: step 15913, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:02:58.101944: step 15914, loss 0.000129016, acc 1\n",
      "2018-10-26T18:02:58.324350: step 15915, loss 1.32398e-05, acc 1\n",
      "2018-10-26T18:02:58.494894: step 15916, loss 1.28452e-05, acc 1\n",
      "2018-10-26T18:02:58.665438: step 15917, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:02:58.885849: step 15918, loss 1.40253e-06, acc 1\n",
      "2018-10-26T18:02:59.076340: step 15919, loss 4.09327e-05, acc 1\n",
      "2018-10-26T18:02:59.262843: step 15920, loss 3.96741e-07, acc 1\n",
      "2018-10-26T18:02:59.468295: step 15921, loss 0.00699662, acc 1\n",
      "2018-10-26T18:02:59.683718: step 15922, loss 4.29855e-06, acc 1\n",
      "2018-10-26T18:02:59.903131: step 15923, loss 3.15142e-06, acc 1\n",
      "2018-10-26T18:03:00.086641: step 15924, loss 2.19964e-05, acc 1\n",
      "2018-10-26T18:03:00.297078: step 15925, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:03:00.461640: step 15926, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:03:00.644151: step 15927, loss 1.93715e-07, acc 1\n",
      "2018-10-26T18:03:00.851597: step 15928, loss 3.16647e-07, acc 1\n",
      "2018-10-26T18:03:01.025134: step 15929, loss 2.85146e-06, acc 1\n",
      "2018-10-26T18:03:01.204655: step 15930, loss 0.000220439, acc 1\n",
      "2018-10-26T18:03:01.409108: step 15931, loss 2.0246e-06, acc 1\n",
      "2018-10-26T18:03:01.580650: step 15932, loss 1.41374e-05, acc 1\n",
      "2018-10-26T18:03:01.761167: step 15933, loss 1.61484e-06, acc 1\n",
      "2018-10-26T18:03:01.925727: step 15934, loss 7.51244e-06, acc 1\n",
      "2018-10-26T18:03:02.137163: step 15935, loss 1.38016e-06, acc 1\n",
      "2018-10-26T18:03:02.300726: step 15936, loss 1.66173e-05, acc 1\n",
      "2018-10-26T18:03:02.480245: step 15937, loss 2.27241e-07, acc 1\n",
      "2018-10-26T18:03:02.643809: step 15938, loss 1.12047e-05, acc 1\n",
      "2018-10-26T18:03:02.857238: step 15939, loss 0.000214478, acc 1\n",
      "2018-10-26T18:03:03.019805: step 15940, loss 5.53201e-07, acc 1\n",
      "2018-10-26T18:03:03.194338: step 15941, loss 4.53176e-05, acc 1\n",
      "2018-10-26T18:03:03.353912: step 15942, loss 1.53105e-06, acc 1\n",
      "2018-10-26T18:03:03.558365: step 15943, loss 0.000124426, acc 1\n",
      "2018-10-26T18:03:03.739880: step 15944, loss 3.58544e-06, acc 1\n",
      "2018-10-26T18:03:03.913417: step 15945, loss 2.22021e-06, acc 1\n",
      "2018-10-26T18:03:04.075982: step 15946, loss 0.000119216, acc 1\n",
      "2018-10-26T18:03:04.255504: step 15947, loss 9.29015e-06, acc 1\n",
      "2018-10-26T18:03:04.422058: step 15948, loss 2.18832e-05, acc 1\n",
      "2018-10-26T18:03:04.599583: step 15949, loss 3.14583e-06, acc 1\n",
      "2018-10-26T18:03:04.772122: step 15950, loss 2.21654e-07, acc 1\n",
      "2018-10-26T18:03:04.954634: step 15951, loss 2.68202e-06, acc 1\n",
      "2018-10-26T18:03:05.126176: step 15952, loss 1.50681e-06, acc 1\n",
      "2018-10-26T18:03:05.299712: step 15953, loss 0.000112068, acc 1\n",
      "2018-10-26T18:03:05.463276: step 15954, loss 4.80317e-06, acc 1\n",
      "2018-10-26T18:03:05.647801: step 15955, loss 0.000485294, acc 1\n",
      "2018-10-26T18:03:05.829298: step 15956, loss 2.42053e-05, acc 1\n",
      "2018-10-26T18:03:06.003831: step 15957, loss 0.000178635, acc 1\n",
      "2018-10-26T18:03:06.181356: step 15958, loss 0.000539788, acc 1\n",
      "2018-10-26T18:03:06.354893: step 15959, loss 1.05051e-06, acc 1\n",
      "2018-10-26T18:03:06.531422: step 15960, loss 2.01165e-07, acc 1\n",
      "2018-10-26T18:03:06.705955: step 15961, loss 6.35121e-05, acc 1\n",
      "2018-10-26T18:03:06.891460: step 15962, loss 0.00182791, acc 1\n",
      "2018-10-26T18:03:07.054025: step 15963, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:03:07.231551: step 15964, loss 1.06542e-06, acc 1\n",
      "2018-10-26T18:03:07.393120: step 15965, loss 1.94902e-05, acc 1\n",
      "2018-10-26T18:03:07.576630: step 15966, loss 1.38303e-05, acc 1\n",
      "2018-10-26T18:03:07.746176: step 15967, loss 6.80266e-05, acc 1\n",
      "2018-10-26T18:03:07.923702: step 15968, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:03:08.086267: step 15969, loss 5.67465e-06, acc 1\n",
      "2018-10-26T18:03:08.268780: step 15970, loss 5.46415e-06, acc 1\n",
      "2018-10-26T18:03:08.429350: step 15971, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:03:08.605891: step 15972, loss 0.000311608, acc 1\n",
      "2018-10-26T18:03:08.781452: step 15973, loss 0, acc 1\n",
      "2018-10-26T18:03:08.965917: step 15974, loss 0.000104895, acc 1\n",
      "2018-10-26T18:03:09.129480: step 15975, loss 0.00212103, acc 1\n",
      "2018-10-26T18:03:09.307006: step 15976, loss 2.18853e-06, acc 1\n",
      "2018-10-26T18:03:09.475556: step 15977, loss 1.13607e-05, acc 1\n",
      "2018-10-26T18:03:09.662057: step 15978, loss 1.07429e-05, acc 1\n",
      "2018-10-26T18:03:09.827615: step 15979, loss 1.88716e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:03:10.010127: step 15980, loss 1.54316e-05, acc 1\n",
      "2018-10-26T18:03:10.168703: step 15981, loss 1.6279e-06, acc 1\n",
      "2018-10-26T18:03:10.346229: step 15982, loss 2.27242e-07, acc 1\n",
      "2018-10-26T18:03:10.515776: step 15983, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:03:10.694300: step 15984, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:03:10.857862: step 15985, loss 7.56222e-07, acc 1\n",
      "2018-10-26T18:03:11.038381: step 15986, loss 0.000121741, acc 1\n",
      "2018-10-26T18:03:11.197953: step 15987, loss 2.81797e-06, acc 1\n",
      "2018-10-26T18:03:11.367500: step 15988, loss 1.40923e-05, acc 1\n",
      "2018-10-26T18:03:11.529069: step 15989, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:03:11.698616: step 15990, loss 0.000104834, acc 1\n",
      "2018-10-26T18:03:11.866168: step 15991, loss 0.0254149, acc 0.984375\n",
      "2018-10-26T18:03:12.042697: step 15992, loss 8.87263e-06, acc 1\n",
      "2018-10-26T18:03:12.205262: step 15993, loss 1.88127e-07, acc 1\n",
      "2018-10-26T18:03:12.387774: step 15994, loss 8.24657e-06, acc 1\n",
      "2018-10-26T18:03:12.555327: step 15995, loss 3.91196e-05, acc 1\n",
      "2018-10-26T18:03:12.728863: step 15996, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:03:12.895418: step 15997, loss 1.55526e-06, acc 1\n",
      "2018-10-26T18:03:13.075936: step 15998, loss 1.94262e-06, acc 1\n",
      "2018-10-26T18:03:13.237503: step 15999, loss 6.48264e-06, acc 1\n",
      "2018-10-26T18:03:13.414032: step 16000, loss 1.16972e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:03:13.848870: step 16000, loss 5.51464, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16000\n",
      "\n",
      "2018-10-26T18:03:14.209357: step 16001, loss 7.58764e-05, acc 1\n",
      "2018-10-26T18:03:14.376909: step 16002, loss 1.04768e-05, acc 1\n",
      "2018-10-26T18:03:14.552440: step 16003, loss 0.000294721, acc 1\n",
      "2018-10-26T18:03:14.721988: step 16004, loss 1.20324e-06, acc 1\n",
      "2018-10-26T18:03:14.918463: step 16005, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:03:15.163807: step 16006, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:03:15.325375: step 16007, loss 5.60083e-05, acc 1\n",
      "2018-10-26T18:03:15.503898: step 16008, loss 5.94278e-06, acc 1\n",
      "2018-10-26T18:03:15.675440: step 16009, loss 1.5361e-05, acc 1\n",
      "2018-10-26T18:03:15.856955: step 16010, loss 2.30913e-05, acc 1\n",
      "2018-10-26T18:03:16.022512: step 16011, loss 6.51276e-06, acc 1\n",
      "2018-10-26T18:03:16.203032: step 16012, loss 1.5869e-06, acc 1\n",
      "2018-10-26T18:03:16.368588: step 16013, loss 2.27242e-07, acc 1\n",
      "2018-10-26T18:03:16.558082: step 16014, loss 1.18397e-05, acc 1\n",
      "2018-10-26T18:03:16.720647: step 16015, loss 2.37469e-06, acc 1\n",
      "2018-10-26T18:03:16.897175: step 16016, loss 0.000157632, acc 1\n",
      "2018-10-26T18:03:17.069714: step 16017, loss 0.00406248, acc 1\n",
      "2018-10-26T18:03:17.247240: step 16018, loss 5.70017e-05, acc 1\n",
      "2018-10-26T18:03:17.413795: step 16019, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:03:17.585336: step 16020, loss 6.41633e-06, acc 1\n",
      "2018-10-26T18:03:17.755881: step 16021, loss 2.00973e-06, acc 1\n",
      "2018-10-26T18:03:17.926425: step 16022, loss 1.746e-05, acc 1\n",
      "2018-10-26T18:03:18.099962: step 16023, loss 2.56758e-05, acc 1\n",
      "2018-10-26T18:03:18.285468: step 16024, loss 5.49476e-07, acc 1\n",
      "2018-10-26T18:03:18.450027: step 16025, loss 4.09191e-06, acc 1\n",
      "2018-10-26T18:03:18.623563: step 16026, loss 5.27122e-07, acc 1\n",
      "2018-10-26T18:03:18.789120: step 16027, loss 7.78348e-05, acc 1\n",
      "2018-10-26T18:03:18.972630: step 16028, loss 0.000411876, acc 1\n",
      "2018-10-26T18:03:19.136193: step 16029, loss 3.25023e-05, acc 1\n",
      "2018-10-26T18:03:19.317709: step 16030, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:03:19.479277: step 16031, loss 4.70842e-06, acc 1\n",
      "2018-10-26T18:03:19.656820: step 16032, loss 0.000230414, acc 1\n",
      "2018-10-26T18:03:19.818371: step 16033, loss 0.000132588, acc 1\n",
      "2018-10-26T18:03:19.991907: step 16034, loss 1.90902e-05, acc 1\n",
      "2018-10-26T18:03:20.155469: step 16035, loss 1.45283e-06, acc 1\n",
      "2018-10-26T18:03:20.333993: step 16036, loss 9.99604e-05, acc 1\n",
      "2018-10-26T18:03:20.497556: step 16037, loss 5.83001e-07, acc 1\n",
      "2018-10-26T18:03:20.685055: step 16038, loss 1.34576e-05, acc 1\n",
      "2018-10-26T18:03:20.852607: step 16039, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:03:21.025148: step 16040, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:03:21.194693: step 16041, loss 3.77008e-05, acc 1\n",
      "2018-10-26T18:03:21.380198: step 16042, loss 6.87352e-06, acc 1\n",
      "2018-10-26T18:03:21.543761: step 16043, loss 0.0590551, acc 0.984375\n",
      "2018-10-26T18:03:21.717297: step 16044, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:03:21.888839: step 16045, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:03:22.064371: step 16046, loss 5.01016e-06, acc 1\n",
      "2018-10-26T18:03:22.224941: step 16047, loss 7.66479e-06, acc 1\n",
      "2018-10-26T18:03:22.403464: step 16048, loss 7.28913e-06, acc 1\n",
      "2018-10-26T18:03:22.573010: step 16049, loss 5.60605e-06, acc 1\n",
      "2018-10-26T18:03:22.744553: step 16050, loss 0.000325432, acc 1\n",
      "2018-10-26T18:03:22.912105: step 16051, loss 1.26285e-06, acc 1\n",
      "2018-10-26T18:03:23.087636: step 16052, loss 5.64377e-07, acc 1\n",
      "2018-10-26T18:03:23.258180: step 16053, loss 6.55098e-05, acc 1\n",
      "2018-10-26T18:03:23.437700: step 16054, loss 2.32415e-05, acc 1\n",
      "2018-10-26T18:03:23.604255: step 16055, loss 0.0225055, acc 0.984375\n",
      "2018-10-26T18:03:23.784773: step 16056, loss 0.000736518, acc 1\n",
      "2018-10-26T18:03:23.955318: step 16057, loss 0.000207304, acc 1\n",
      "2018-10-26T18:03:24.130848: step 16058, loss 3.91255e-05, acc 1\n",
      "2018-10-26T18:03:24.292416: step 16059, loss 2.38387e-05, acc 1\n",
      "2018-10-26T18:03:24.467948: step 16060, loss 2.06369e-06, acc 1\n",
      "2018-10-26T18:03:24.630514: step 16061, loss 0.000156999, acc 1\n",
      "2018-10-26T18:03:24.809037: step 16062, loss 0.0471965, acc 0.984375\n",
      "2018-10-26T18:03:24.974594: step 16063, loss 4.78953e-05, acc 1\n",
      "2018-10-26T18:03:25.151123: step 16064, loss 2.64495e-07, acc 1\n",
      "2018-10-26T18:03:25.311723: step 16065, loss 0.000288773, acc 1\n",
      "2018-10-26T18:03:25.489219: step 16066, loss 2.94295e-07, acc 1\n",
      "2018-10-26T18:03:25.652782: step 16067, loss 1.41267e-05, acc 1\n",
      "2018-10-26T18:03:25.828313: step 16068, loss 7.38111e-05, acc 1\n",
      "2018-10-26T18:03:25.998857: step 16069, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:03:26.170399: step 16070, loss 0.00122405, acc 1\n",
      "2018-10-26T18:03:26.338948: step 16071, loss 9.41e-05, acc 1\n",
      "2018-10-26T18:03:26.510490: step 16072, loss 0.00114887, acc 1\n",
      "2018-10-26T18:03:26.675051: step 16073, loss 7.64719e-06, acc 1\n",
      "2018-10-26T18:03:26.849584: step 16074, loss 2.54051e-05, acc 1\n",
      "2018-10-26T18:03:27.020129: step 16075, loss 0.000227146, acc 1\n",
      "2018-10-26T18:03:27.196658: step 16076, loss 3.26006e-05, acc 1\n",
      "2018-10-26T18:03:27.364210: step 16077, loss 0.0311386, acc 0.984375\n",
      "2018-10-26T18:03:27.538743: step 16078, loss 0.000412221, acc 1\n",
      "2018-10-26T18:03:27.705298: step 16079, loss 1.42489e-06, acc 1\n",
      "2018-10-26T18:03:27.881827: step 16080, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:03:28.055364: step 16081, loss 0.0798551, acc 0.984375\n",
      "2018-10-26T18:03:28.234883: step 16082, loss 1.85327e-06, acc 1\n",
      "2018-10-26T18:03:28.395454: step 16083, loss 7.71116e-07, acc 1\n",
      "2018-10-26T18:03:28.580958: step 16084, loss 8.84735e-07, acc 1\n",
      "2018-10-26T18:03:28.758484: step 16085, loss 2.16065e-07, acc 1\n",
      "2018-10-26T18:03:28.950969: step 16086, loss 3.27352e-05, acc 1\n",
      "2018-10-26T18:03:29.144453: step 16087, loss 0.0019938, acc 1\n",
      "2018-10-26T18:03:29.325967: step 16088, loss 2.78815e-06, acc 1\n",
      "2018-10-26T18:03:29.506486: step 16089, loss 0.0033961, acc 1\n",
      "2018-10-26T18:03:29.688000: step 16090, loss 1.0647e-05, acc 1\n",
      "2018-10-26T18:03:29.867521: step 16091, loss 5.31746e-06, acc 1\n",
      "2018-10-26T18:03:30.050034: step 16092, loss 1.4491e-06, acc 1\n",
      "2018-10-26T18:03:30.215590: step 16093, loss 0.0080976, acc 1\n",
      "2018-10-26T18:03:30.395111: step 16094, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:03:30.566653: step 16095, loss 0.123181, acc 0.984375\n",
      "2018-10-26T18:03:30.759140: step 16096, loss 1.00534e-05, acc 1\n",
      "2018-10-26T18:03:30.922702: step 16097, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:03:31.110200: step 16098, loss 0.000327219, acc 1\n",
      "2018-10-26T18:03:31.279747: step 16099, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:03:31.463258: step 16100, loss 3.68586e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:03:31.908069: step 16100, loss 5.63078, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16100\n",
      "\n",
      "2018-10-26T18:03:32.271959: step 16101, loss 2.51456e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:03:32.458461: step 16102, loss 2.83121e-07, acc 1\n",
      "2018-10-26T18:03:32.623022: step 16103, loss 3.1869e-06, acc 1\n",
      "2018-10-26T18:03:32.793566: step 16104, loss 0.000136773, acc 1\n",
      "2018-10-26T18:03:32.985054: step 16105, loss 1.41561e-07, acc 1\n",
      "2018-10-26T18:03:33.221423: step 16106, loss 6.37536e-06, acc 1\n",
      "2018-10-26T18:03:33.388975: step 16107, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:03:33.563509: step 16108, loss 3.73239e-05, acc 1\n",
      "2018-10-26T18:03:33.731060: step 16109, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:03:33.905594: step 16110, loss 2.70254e-06, acc 1\n",
      "2018-10-26T18:03:34.069158: step 16111, loss 1.39041e-05, acc 1\n",
      "2018-10-26T18:03:34.241696: step 16112, loss 4.84444e-06, acc 1\n",
      "2018-10-26T18:03:34.409249: step 16113, loss 7.87936e-06, acc 1\n",
      "2018-10-26T18:03:34.589767: step 16114, loss 1.21602e-05, acc 1\n",
      "2018-10-26T18:03:34.755324: step 16115, loss 4.76834e-07, acc 1\n",
      "2018-10-26T18:03:34.932849: step 16116, loss 2.89802e-05, acc 1\n",
      "2018-10-26T18:03:35.095417: step 16117, loss 2.04102e-05, acc 1\n",
      "2018-10-26T18:03:35.268952: step 16118, loss 4.19091e-07, acc 1\n",
      "2018-10-26T18:03:35.448472: step 16119, loss 1.12686e-06, acc 1\n",
      "2018-10-26T18:03:35.628990: step 16120, loss 7.46576e-05, acc 1\n",
      "2018-10-26T18:03:35.802526: step 16121, loss 8.73574e-07, acc 1\n",
      "2018-10-26T18:03:35.967087: step 16122, loss 1.24979e-06, acc 1\n",
      "2018-10-26T18:03:36.141620: step 16123, loss 0.00115477, acc 1\n",
      "2018-10-26T18:03:36.309172: step 16124, loss 2.628e-06, acc 1\n",
      "2018-10-26T18:03:36.481712: step 16125, loss 0.000507326, acc 1\n",
      "2018-10-26T18:03:36.646272: step 16126, loss 2.68161e-05, acc 1\n",
      "2018-10-26T18:03:36.822801: step 16127, loss 4.54646e-06, acc 1\n",
      "2018-10-26T18:03:36.989356: step 16128, loss 0.000880782, acc 1\n",
      "2018-10-26T18:03:37.159900: step 16129, loss 0, acc 1\n",
      "2018-10-26T18:03:37.322465: step 16130, loss 6.59364e-07, acc 1\n",
      "2018-10-26T18:03:37.496001: step 16131, loss 0.000163994, acc 1\n",
      "2018-10-26T18:03:37.660562: step 16132, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:03:37.831107: step 16133, loss 0.00011586, acc 1\n",
      "2018-10-26T18:03:37.999655: step 16134, loss 1.22756e-05, acc 1\n",
      "2018-10-26T18:03:38.174191: step 16135, loss 1.0259e-05, acc 1\n",
      "2018-10-26T18:03:38.340745: step 16136, loss 1.81603e-06, acc 1\n",
      "2018-10-26T18:03:38.510292: step 16137, loss 0.00981079, acc 1\n",
      "2018-10-26T18:03:38.679839: step 16138, loss 4.16977e-05, acc 1\n",
      "2018-10-26T18:03:38.850382: step 16139, loss 0.000306591, acc 1\n",
      "2018-10-26T18:03:39.012949: step 16140, loss 0.00118209, acc 1\n",
      "2018-10-26T18:03:39.185487: step 16141, loss 1.45469e-06, acc 1\n",
      "2018-10-26T18:03:39.352042: step 16142, loss 3.78476e-06, acc 1\n",
      "2018-10-26T18:03:39.523584: step 16143, loss 0.0517669, acc 0.984375\n",
      "2018-10-26T18:03:39.691136: step 16144, loss 3.85563e-07, acc 1\n",
      "2018-10-26T18:03:39.861680: step 16145, loss 1.55882e-05, acc 1\n",
      "2018-10-26T18:03:40.022253: step 16146, loss 0.000367686, acc 1\n",
      "2018-10-26T18:03:40.198781: step 16147, loss 9.55509e-07, acc 1\n",
      "2018-10-26T18:03:40.360348: step 16148, loss 0.000605026, acc 1\n",
      "2018-10-26T18:03:40.538872: step 16149, loss 0.000256053, acc 1\n",
      "2018-10-26T18:03:40.702435: step 16150, loss 6.33296e-05, acc 1\n",
      "2018-10-26T18:03:40.870984: step 16151, loss 4.52619e-07, acc 1\n",
      "2018-10-26T18:03:41.036542: step 16152, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:03:41.208083: step 16153, loss 2.23517e-07, acc 1\n",
      "2018-10-26T18:03:41.369652: step 16154, loss 2.92434e-07, acc 1\n",
      "2018-10-26T18:03:41.540196: step 16155, loss 4.11643e-07, acc 1\n",
      "2018-10-26T18:03:41.713733: step 16156, loss 2.24474e-05, acc 1\n",
      "2018-10-26T18:03:41.889263: step 16157, loss 0.00011647, acc 1\n",
      "2018-10-26T18:03:42.071782: step 16158, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:03:42.232347: step 16159, loss 0.00286326, acc 1\n",
      "2018-10-26T18:03:42.403888: step 16160, loss 2.45672e-06, acc 1\n",
      "2018-10-26T18:03:42.570443: step 16161, loss 8.30718e-07, acc 1\n",
      "2018-10-26T18:03:42.746971: step 16162, loss 2.47723e-06, acc 1\n",
      "2018-10-26T18:03:42.908540: step 16163, loss 2.57044e-07, acc 1\n",
      "2018-10-26T18:03:43.085069: step 16164, loss 4.32825e-06, acc 1\n",
      "2018-10-26T18:03:43.251624: step 16165, loss 3.36934e-06, acc 1\n",
      "2018-10-26T18:03:43.426156: step 16166, loss 0.000640563, acc 1\n",
      "2018-10-26T18:03:43.595705: step 16167, loss 4.53409e-05, acc 1\n",
      "2018-10-26T18:03:43.768243: step 16168, loss 0.00093216, acc 1\n",
      "2018-10-26T18:03:43.927817: step 16169, loss 1.52918e-06, acc 1\n",
      "2018-10-26T18:03:44.107338: step 16170, loss 0.000941784, acc 1\n",
      "2018-10-26T18:03:44.271897: step 16171, loss 1.84237e-05, acc 1\n",
      "2018-10-26T18:03:44.452414: step 16172, loss 4.91734e-07, acc 1\n",
      "2018-10-26T18:03:44.619967: step 16173, loss 2.23525e-05, acc 1\n",
      "2018-10-26T18:03:44.792506: step 16174, loss 6.09073e-07, acc 1\n",
      "2018-10-26T18:03:44.956069: step 16175, loss 0.000373107, acc 1\n",
      "2018-10-26T18:03:45.131601: step 16176, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:03:45.297158: step 16177, loss 0.00338807, acc 1\n",
      "2018-10-26T18:03:45.476678: step 16178, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:03:45.644232: step 16179, loss 1.43122e-05, acc 1\n",
      "2018-10-26T18:03:45.819792: step 16180, loss 3.05268e-06, acc 1\n",
      "2018-10-26T18:03:45.983324: step 16181, loss 1.59109e-05, acc 1\n",
      "2018-10-26T18:03:46.164839: step 16182, loss 4.53082e-05, acc 1\n",
      "2018-10-26T18:03:46.330397: step 16183, loss 3.84429e-06, acc 1\n",
      "2018-10-26T18:03:46.508920: step 16184, loss 4.39524e-06, acc 1\n",
      "2018-10-26T18:03:46.673481: step 16185, loss 0.00445252, acc 1\n",
      "2018-10-26T18:03:46.852003: step 16186, loss 2.94083e-05, acc 1\n",
      "2018-10-26T18:03:47.011577: step 16187, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:03:47.197081: step 16188, loss 5.97901e-07, acc 1\n",
      "2018-10-26T18:03:47.363637: step 16189, loss 4.22589e-06, acc 1\n",
      "2018-10-26T18:03:47.542160: step 16190, loss 9.92025e-05, acc 1\n",
      "2018-10-26T18:03:47.705722: step 16191, loss 1.76101e-05, acc 1\n",
      "2018-10-26T18:03:47.883248: step 16192, loss 1.74339e-06, acc 1\n",
      "2018-10-26T18:03:48.043819: step 16193, loss 0.000134642, acc 1\n",
      "2018-10-26T18:03:48.218353: step 16194, loss 6.20929e-06, acc 1\n",
      "2018-10-26T18:03:48.380918: step 16195, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:03:48.556450: step 16196, loss 9.53653e-07, acc 1\n",
      "2018-10-26T18:03:48.727991: step 16197, loss 0.000150533, acc 1\n",
      "2018-10-26T18:03:48.903522: step 16198, loss 7.04068e-07, acc 1\n",
      "2018-10-26T18:03:49.069079: step 16199, loss 5.04249e-05, acc 1\n",
      "2018-10-26T18:03:49.237629: step 16200, loss 2.84114e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:03:49.678452: step 16200, loss 5.65002, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16200\n",
      "\n",
      "2018-10-26T18:03:50.003580: step 16201, loss 2.58186e-05, acc 1\n",
      "2018-10-26T18:03:50.177117: step 16202, loss 2.2053e-06, acc 1\n",
      "2018-10-26T18:03:50.353645: step 16203, loss 1.19765e-06, acc 1\n",
      "2018-10-26T18:03:50.520201: step 16204, loss 1.25167e-05, acc 1\n",
      "2018-10-26T18:03:50.707701: step 16205, loss 1.04119e-06, acc 1\n",
      "2018-10-26T18:03:50.960026: step 16206, loss 3.75291e-05, acc 1\n",
      "2018-10-26T18:03:51.125583: step 16207, loss 0.000453934, acc 1\n",
      "2018-10-26T18:03:51.305104: step 16208, loss 4.53881e-06, acc 1\n",
      "2018-10-26T18:03:51.472656: step 16209, loss 1.54225e-06, acc 1\n",
      "2018-10-26T18:03:51.659157: step 16210, loss 2.39901e-06, acc 1\n",
      "2018-10-26T18:03:51.823720: step 16211, loss 0.029222, acc 0.984375\n",
      "2018-10-26T18:03:51.997254: step 16212, loss 6.44736e-05, acc 1\n",
      "2018-10-26T18:03:52.163809: step 16213, loss 1.54034e-06, acc 1\n",
      "2018-10-26T18:03:52.344327: step 16214, loss 0.000394448, acc 1\n",
      "2018-10-26T18:03:52.508887: step 16215, loss 2.51456e-07, acc 1\n",
      "2018-10-26T18:03:52.686413: step 16216, loss 0.000199358, acc 1\n",
      "2018-10-26T18:03:52.854962: step 16217, loss 4.59075e-06, acc 1\n",
      "2018-10-26T18:03:53.030494: step 16218, loss 3.50232e-05, acc 1\n",
      "2018-10-26T18:03:53.197049: step 16219, loss 4.62947e-05, acc 1\n",
      "2018-10-26T18:03:53.389534: step 16220, loss 3.34144e-06, acc 1\n",
      "2018-10-26T18:03:53.554094: step 16221, loss 0.000188886, acc 1\n",
      "2018-10-26T18:03:53.727630: step 16222, loss 2.31634e-05, acc 1\n",
      "2018-10-26T18:03:53.900169: step 16223, loss 3.85563e-07, acc 1\n",
      "2018-10-26T18:03:54.083679: step 16224, loss 1.27653e-05, acc 1\n",
      "2018-10-26T18:03:54.252229: step 16225, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:03:54.435738: step 16226, loss 1.98261e-05, acc 1\n",
      "2018-10-26T18:03:54.605286: step 16227, loss 6.70551e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:03:54.795776: step 16228, loss 0.000266394, acc 1\n",
      "2018-10-26T18:03:54.964328: step 16229, loss 1.46928e-05, acc 1\n",
      "2018-10-26T18:03:55.136865: step 16230, loss 0.000285769, acc 1\n",
      "2018-10-26T18:03:55.322370: step 16231, loss 1.37273e-06, acc 1\n",
      "2018-10-26T18:03:55.494909: step 16232, loss 5.09494e-05, acc 1\n",
      "2018-10-26T18:03:55.724296: step 16233, loss 8.95818e-05, acc 1\n",
      "2018-10-26T18:03:55.902837: step 16234, loss 0.000304218, acc 1\n",
      "2018-10-26T18:03:56.079347: step 16235, loss 0.000300182, acc 1\n",
      "2018-10-26T18:03:56.254878: step 16236, loss 7.25339e-06, acc 1\n",
      "2018-10-26T18:03:56.429413: step 16237, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:03:56.606938: step 16238, loss 4.09779e-07, acc 1\n",
      "2018-10-26T18:03:56.768507: step 16239, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:03:56.944037: step 16240, loss 0.000182951, acc 1\n",
      "2018-10-26T18:03:57.110592: step 16241, loss 2.34693e-07, acc 1\n",
      "2018-10-26T18:03:57.286124: step 16242, loss 4.44893e-05, acc 1\n",
      "2018-10-26T18:03:57.455669: step 16243, loss 0.000840575, acc 1\n",
      "2018-10-26T18:03:57.632198: step 16244, loss 2.47717e-06, acc 1\n",
      "2018-10-26T18:03:57.795761: step 16245, loss 1.35223e-06, acc 1\n",
      "2018-10-26T18:03:57.971293: step 16246, loss 1.43691e-05, acc 1\n",
      "2018-10-26T18:03:58.138844: step 16247, loss 0.000352057, acc 1\n",
      "2018-10-26T18:03:58.322354: step 16248, loss 3.2111e-06, acc 1\n",
      "2018-10-26T18:03:58.491901: step 16249, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:03:58.673416: step 16250, loss 0.00153641, acc 1\n",
      "2018-10-26T18:03:58.835982: step 16251, loss 1.0114e-06, acc 1\n",
      "2018-10-26T18:03:59.009518: step 16252, loss 1.2401e-05, acc 1\n",
      "2018-10-26T18:03:59.185049: step 16253, loss 7.44541e-06, acc 1\n",
      "2018-10-26T18:03:59.408466: step 16254, loss 4.63794e-07, acc 1\n",
      "2018-10-26T18:03:59.621883: step 16255, loss 0.000206016, acc 1\n",
      "2018-10-26T18:03:59.821349: step 16256, loss 5.07362e-05, acc 1\n",
      "2018-10-26T18:04:00.065697: step 16257, loss 4.37478e-05, acc 1\n",
      "2018-10-26T18:04:00.272146: step 16258, loss 3.33372e-05, acc 1\n",
      "2018-10-26T18:04:00.472610: step 16259, loss 4.00405e-05, acc 1\n",
      "2018-10-26T18:04:00.684045: step 16260, loss 0.000168081, acc 1\n",
      "2018-10-26T18:04:00.862567: step 16261, loss 4.91147e-06, acc 1\n",
      "2018-10-26T18:04:01.095944: step 16262, loss 0.000216818, acc 1\n",
      "2018-10-26T18:04:01.270477: step 16263, loss 4.25376e-06, acc 1\n",
      "2018-10-26T18:04:01.516820: step 16264, loss 3.72077e-05, acc 1\n",
      "2018-10-26T18:04:01.733242: step 16265, loss 3.66938e-07, acc 1\n",
      "2018-10-26T18:04:01.943679: step 16266, loss 5.97902e-07, acc 1\n",
      "2018-10-26T18:04:02.179050: step 16267, loss 1.97653e-05, acc 1\n",
      "2018-10-26T18:04:02.352586: step 16268, loss 0.000370346, acc 1\n",
      "2018-10-26T18:04:02.571999: step 16269, loss 0.000486262, acc 1\n",
      "2018-10-26T18:04:02.795403: step 16270, loss 3.50176e-07, acc 1\n",
      "2018-10-26T18:04:02.970934: step 16271, loss 0.00462817, acc 1\n",
      "2018-10-26T18:04:03.191346: step 16272, loss 4.88156e-06, acc 1\n",
      "2018-10-26T18:04:03.400785: step 16273, loss 5.69866e-06, acc 1\n",
      "2018-10-26T18:04:03.619204: step 16274, loss 4.63578e-06, acc 1\n",
      "2018-10-26T18:04:03.852578: step 16275, loss 1.44909e-06, acc 1\n",
      "2018-10-26T18:04:04.064014: step 16276, loss 1.31251e-05, acc 1\n",
      "2018-10-26T18:04:04.254505: step 16277, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:04:04.470928: step 16278, loss 1.14076e-05, acc 1\n",
      "2018-10-26T18:04:04.648462: step 16279, loss 0.000435426, acc 1\n",
      "2018-10-26T18:04:04.822985: step 16280, loss 0.000114817, acc 1\n",
      "2018-10-26T18:04:04.996522: step 16281, loss 4.35834e-06, acc 1\n",
      "2018-10-26T18:04:05.212944: step 16282, loss 0.00324984, acc 1\n",
      "2018-10-26T18:04:05.422386: step 16283, loss 1.2594e-05, acc 1\n",
      "2018-10-26T18:04:05.620861: step 16284, loss 1.88127e-07, acc 1\n",
      "2018-10-26T18:04:05.812342: step 16285, loss 1.37836e-07, acc 1\n",
      "2018-10-26T18:04:06.032755: step 16286, loss 1.66703e-06, acc 1\n",
      "2018-10-26T18:04:06.215265: step 16287, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:04:06.414733: step 16288, loss 1.9445e-06, acc 1\n",
      "2018-10-26T18:04:06.597245: step 16289, loss 6.24031e-05, acc 1\n",
      "2018-10-26T18:04:06.782751: step 16290, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:04:06.976233: step 16291, loss 7.9533e-07, acc 1\n",
      "2018-10-26T18:04:07.151764: step 16292, loss 2.09395e-05, acc 1\n",
      "2018-10-26T18:04:07.331283: step 16293, loss 0.00111876, acc 1\n",
      "2018-10-26T18:04:07.540724: step 16294, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:04:07.722239: step 16295, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:04:07.909738: step 16296, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:04:08.106214: step 16297, loss 2.08616e-07, acc 1\n",
      "2018-10-26T18:04:08.291718: step 16298, loss 0.000114656, acc 1\n",
      "2018-10-26T18:04:08.464257: step 16299, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:04:08.643777: step 16300, loss 0.000596468, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:04:09.127213: step 16300, loss 5.71563, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16300\n",
      "\n",
      "2018-10-26T18:04:09.519378: step 16301, loss 9.6855e-07, acc 1\n",
      "2018-10-26T18:04:09.697901: step 16302, loss 5.47197e-06, acc 1\n",
      "2018-10-26T18:04:09.973169: step 16303, loss 2.73808e-07, acc 1\n",
      "2018-10-26T18:04:10.154681: step 16304, loss 2.70226e-05, acc 1\n",
      "2018-10-26T18:04:10.341182: step 16305, loss 6.70096e-06, acc 1\n",
      "2018-10-26T18:04:10.521701: step 16306, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:04:10.697232: step 16307, loss 2.66357e-07, acc 1\n",
      "2018-10-26T18:04:10.871766: step 16308, loss 5.50716e-06, acc 1\n",
      "2018-10-26T18:04:11.055275: step 16309, loss 6.21279e-06, acc 1\n",
      "2018-10-26T18:04:11.219835: step 16310, loss 3.66938e-07, acc 1\n",
      "2018-10-26T18:04:11.403344: step 16311, loss 5.96906e-06, acc 1\n",
      "2018-10-26T18:04:11.576882: step 16312, loss 1.71728e-06, acc 1\n",
      "2018-10-26T18:04:11.752412: step 16313, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:04:11.919964: step 16314, loss 6.15034e-05, acc 1\n",
      "2018-10-26T18:04:12.099485: step 16315, loss 0.019139, acc 0.984375\n",
      "2018-10-26T18:04:12.261053: step 16316, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:04:12.438578: step 16317, loss 3.42522e-06, acc 1\n",
      "2018-10-26T18:04:12.600146: step 16318, loss 3.23322e-06, acc 1\n",
      "2018-10-26T18:04:12.785651: step 16319, loss 0.00850721, acc 1\n",
      "2018-10-26T18:04:12.955198: step 16320, loss 6.70543e-07, acc 1\n",
      "2018-10-26T18:04:13.134719: step 16321, loss 3.2241e-06, acc 1\n",
      "2018-10-26T18:04:13.295289: step 16322, loss 7.32067e-06, acc 1\n",
      "2018-10-26T18:04:13.484783: step 16323, loss 1.05237e-06, acc 1\n",
      "2018-10-26T18:04:13.647349: step 16324, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:04:13.828863: step 16325, loss 0, acc 1\n",
      "2018-10-26T18:04:13.990433: step 16326, loss 4.99483e-06, acc 1\n",
      "2018-10-26T18:04:14.163969: step 16327, loss 2.64494e-07, acc 1\n",
      "2018-10-26T18:04:14.326534: step 16328, loss 4.20953e-07, acc 1\n",
      "2018-10-26T18:04:14.509047: step 16329, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:04:14.681586: step 16330, loss 0.000293097, acc 1\n",
      "2018-10-26T18:04:14.866092: step 16331, loss 0.00151475, acc 1\n",
      "2018-10-26T18:04:15.038632: step 16332, loss 1.67253e-05, acc 1\n",
      "2018-10-26T18:04:15.220147: step 16333, loss 9.68095e-05, acc 1\n",
      "2018-10-26T18:04:15.381714: step 16334, loss 2.4754e-05, acc 1\n",
      "2018-10-26T18:04:15.563230: step 16335, loss 5.70468e-05, acc 1\n",
      "2018-10-26T18:04:15.728788: step 16336, loss 3.14697e-05, acc 1\n",
      "2018-10-26T18:04:15.904318: step 16337, loss 0.0771166, acc 0.984375\n",
      "2018-10-26T18:04:16.071871: step 16338, loss 0.015631, acc 0.984375\n",
      "2018-10-26T18:04:16.247402: step 16339, loss 1.62388e-05, acc 1\n",
      "2018-10-26T18:04:16.417946: step 16340, loss 0.000885662, acc 1\n",
      "2018-10-26T18:04:16.604448: step 16341, loss 6.65851e-05, acc 1\n",
      "2018-10-26T18:04:16.768010: step 16342, loss 2.16365e-05, acc 1\n",
      "2018-10-26T18:04:16.949526: step 16343, loss 3.05472e-07, acc 1\n",
      "2018-10-26T18:04:17.127052: step 16344, loss 0.000379442, acc 1\n",
      "2018-10-26T18:04:17.310561: step 16345, loss 5.55063e-07, acc 1\n",
      "2018-10-26T18:04:17.478113: step 16346, loss 9.92769e-07, acc 1\n",
      "2018-10-26T18:04:17.657634: step 16347, loss 2.31147e-06, acc 1\n",
      "2018-10-26T18:04:17.828178: step 16348, loss 8.13052e-05, acc 1\n",
      "2018-10-26T18:04:18.005704: step 16349, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:04:18.168269: step 16350, loss 2.41513e-05, acc 1\n",
      "2018-10-26T18:04:18.361752: step 16351, loss 1.71402e-05, acc 1\n",
      "2018-10-26T18:04:18.534293: step 16352, loss 0.000122293, acc 1\n",
      "2018-10-26T18:04:18.713812: step 16353, loss 7.2145e-05, acc 1\n",
      "2018-10-26T18:04:18.877375: step 16354, loss 0.000288094, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:04:19.064874: step 16355, loss 8.69725e-05, acc 1\n",
      "2018-10-26T18:04:19.231429: step 16356, loss 7.02212e-07, acc 1\n",
      "2018-10-26T18:04:19.404965: step 16357, loss 2.17921e-06, acc 1\n",
      "2018-10-26T18:04:19.565536: step 16358, loss 0.0169566, acc 0.984375\n",
      "2018-10-26T18:04:19.743062: step 16359, loss 2.67844e-05, acc 1\n",
      "2018-10-26T18:04:19.915601: step 16360, loss 1.65611e-05, acc 1\n",
      "2018-10-26T18:04:20.085148: step 16361, loss 3.0769e-06, acc 1\n",
      "2018-10-26T18:04:20.249708: step 16362, loss 1.28892e-06, acc 1\n",
      "2018-10-26T18:04:20.427234: step 16363, loss 1.35487e-05, acc 1\n",
      "2018-10-26T18:04:20.604760: step 16364, loss 6.80364e-06, acc 1\n",
      "2018-10-26T18:04:20.774307: step 16365, loss 0.000822201, acc 1\n",
      "2018-10-26T18:04:20.950835: step 16366, loss 0.00540466, acc 1\n",
      "2018-10-26T18:04:21.111406: step 16367, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:04:21.284942: step 16368, loss 4.5371e-06, acc 1\n",
      "2018-10-26T18:04:21.456485: step 16369, loss 7.69362e-06, acc 1\n",
      "2018-10-26T18:04:21.626031: step 16370, loss 0.000188172, acc 1\n",
      "2018-10-26T18:04:21.792586: step 16371, loss 0.000167399, acc 1\n",
      "2018-10-26T18:04:21.970112: step 16372, loss 0.00448962, acc 1\n",
      "2018-10-26T18:04:22.133674: step 16373, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:04:22.309206: step 16374, loss 1.54058e-05, acc 1\n",
      "2018-10-26T18:04:22.477756: step 16375, loss 0.000129497, acc 1\n",
      "2018-10-26T18:04:22.653287: step 16376, loss 3.65076e-07, acc 1\n",
      "2018-10-26T18:04:22.827820: step 16377, loss 9.86072e-05, acc 1\n",
      "2018-10-26T18:04:23.005346: step 16378, loss 1.20883e-06, acc 1\n",
      "2018-10-26T18:04:23.168908: step 16379, loss 0.00294834, acc 1\n",
      "2018-10-26T18:04:23.346434: step 16380, loss 8.19796e-05, acc 1\n",
      "2018-10-26T18:04:23.516978: step 16381, loss 0.0714678, acc 0.984375\n",
      "2018-10-26T18:04:23.687523: step 16382, loss 0.000166057, acc 1\n",
      "2018-10-26T18:04:23.860061: step 16383, loss 8.17685e-07, acc 1\n",
      "2018-10-26T18:04:24.034596: step 16384, loss 6.45273e-06, acc 1\n",
      "2018-10-26T18:04:24.196163: step 16385, loss 9.38817e-05, acc 1\n",
      "2018-10-26T18:04:24.369701: step 16386, loss 9.64469e-05, acc 1\n",
      "2018-10-26T18:04:24.537252: step 16387, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:04:24.715776: step 16388, loss 3.69714e-05, acc 1\n",
      "2018-10-26T18:04:24.878341: step 16389, loss 4.09798e-05, acc 1\n",
      "2018-10-26T18:04:25.049883: step 16390, loss 0.00233602, acc 1\n",
      "2018-10-26T18:04:25.213446: step 16391, loss 0.0780671, acc 0.984375\n",
      "2018-10-26T18:04:25.386982: step 16392, loss 3.08612e-06, acc 1\n",
      "2018-10-26T18:04:25.555532: step 16393, loss 8.60334e-06, acc 1\n",
      "2018-10-26T18:04:25.726076: step 16394, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:04:25.896620: step 16395, loss 1.56087e-06, acc 1\n",
      "2018-10-26T18:04:26.068162: step 16396, loss 1.08402e-06, acc 1\n",
      "2018-10-26T18:04:26.231724: step 16397, loss 3.41394e-06, acc 1\n",
      "2018-10-26T18:04:26.401271: step 16398, loss 3.93528e-06, acc 1\n",
      "2018-10-26T18:04:26.575806: step 16399, loss 2.53319e-07, acc 1\n",
      "2018-10-26T18:04:26.761311: step 16400, loss 7.15243e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:04:27.197146: step 16400, loss 5.71329, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16400\n",
      "\n",
      "2018-10-26T18:04:27.583795: step 16401, loss 4.71189e-06, acc 1\n",
      "2018-10-26T18:04:27.748356: step 16402, loss 9.01139e-05, acc 1\n",
      "2018-10-26T18:04:27.925883: step 16403, loss 8.37514e-06, acc 1\n",
      "2018-10-26T18:04:28.093433: step 16404, loss 1.95195e-06, acc 1\n",
      "2018-10-26T18:04:28.293897: step 16405, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:04:28.533259: step 16406, loss 4.13502e-07, acc 1\n",
      "2018-10-26T18:04:28.701808: step 16407, loss 2.34529e-05, acc 1\n",
      "2018-10-26T18:04:28.877341: step 16408, loss 1.03374e-06, acc 1\n",
      "2018-10-26T18:04:29.046886: step 16409, loss 0.000386001, acc 1\n",
      "2018-10-26T18:04:29.228401: step 16410, loss 2.26305e-06, acc 1\n",
      "2018-10-26T18:04:29.396963: step 16411, loss 2.09347e-06, acc 1\n",
      "2018-10-26T18:04:29.576471: step 16412, loss 0.000760023, acc 1\n",
      "2018-10-26T18:04:29.740034: step 16413, loss 5.60477e-05, acc 1\n",
      "2018-10-26T18:04:29.914567: step 16414, loss 0.000210981, acc 1\n",
      "2018-10-26T18:04:30.078130: step 16415, loss 4.35853e-07, acc 1\n",
      "2018-10-26T18:04:30.251667: step 16416, loss 1.4733e-06, acc 1\n",
      "2018-10-26T18:04:30.426200: step 16417, loss 6.64952e-07, acc 1\n",
      "2018-10-26T18:04:30.601732: step 16418, loss 4.99503e-06, acc 1\n",
      "2018-10-26T18:04:30.765295: step 16419, loss 8.56816e-08, acc 1\n",
      "2018-10-26T18:04:30.942820: step 16420, loss 2.41574e-06, acc 1\n",
      "2018-10-26T18:04:31.103391: step 16421, loss 0.000378263, acc 1\n",
      "2018-10-26T18:04:31.287900: step 16422, loss 0.00141522, acc 1\n",
      "2018-10-26T18:04:31.456448: step 16423, loss 0.000196655, acc 1\n",
      "2018-10-26T18:04:31.637963: step 16424, loss 4.53752e-05, acc 1\n",
      "2018-10-26T18:04:31.803521: step 16425, loss 4.82388e-06, acc 1\n",
      "2018-10-26T18:04:31.987031: step 16426, loss 7.60267e-05, acc 1\n",
      "2018-10-26T18:04:32.155579: step 16427, loss 0.000554281, acc 1\n",
      "2018-10-26T18:04:32.334103: step 16428, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:04:32.504647: step 16429, loss 1.56462e-07, acc 1\n",
      "2018-10-26T18:04:32.675192: step 16430, loss 0.00675486, acc 1\n",
      "2018-10-26T18:04:32.842744: step 16431, loss 0.0583135, acc 0.984375\n",
      "2018-10-26T18:04:33.029246: step 16432, loss 0.000305519, acc 1\n",
      "2018-10-26T18:04:33.190814: step 16433, loss 0, acc 1\n",
      "2018-10-26T18:04:33.362356: step 16434, loss 2.81608e-06, acc 1\n",
      "2018-10-26T18:04:33.530917: step 16435, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:04:33.705439: step 16436, loss 1.65923e-05, acc 1\n",
      "2018-10-26T18:04:33.874986: step 16437, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:04:34.047525: step 16438, loss 2.32829e-07, acc 1\n",
      "2018-10-26T18:04:34.212085: step 16439, loss 9.08956e-07, acc 1\n",
      "2018-10-26T18:04:34.390609: step 16440, loss 6.57333e-05, acc 1\n",
      "2018-10-26T18:04:34.557163: step 16441, loss 5.84768e-06, acc 1\n",
      "2018-10-26T18:04:34.739677: step 16442, loss 0.00493436, acc 1\n",
      "2018-10-26T18:04:34.906231: step 16443, loss 3.78284e-06, acc 1\n",
      "2018-10-26T18:04:35.083757: step 16444, loss 0.000280219, acc 1\n",
      "2018-10-26T18:04:35.245324: step 16445, loss 3.91154e-07, acc 1\n",
      "2018-10-26T18:04:35.423847: step 16446, loss 0.000127906, acc 1\n",
      "2018-10-26T18:04:35.592397: step 16447, loss 0.00043036, acc 1\n",
      "2018-10-26T18:04:35.770920: step 16448, loss 4.72684e-06, acc 1\n",
      "2018-10-26T18:04:35.932489: step 16449, loss 1.62602e-06, acc 1\n",
      "2018-10-26T18:04:36.111011: step 16450, loss 3.9352e-05, acc 1\n",
      "2018-10-26T18:04:36.275571: step 16451, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:04:36.464070: step 16452, loss 3.51702e-05, acc 1\n",
      "2018-10-26T18:04:36.629625: step 16453, loss 2.28513e-05, acc 1\n",
      "2018-10-26T18:04:36.819119: step 16454, loss 0.000150796, acc 1\n",
      "2018-10-26T18:04:36.984678: step 16455, loss 6.31429e-07, acc 1\n",
      "2018-10-26T18:04:37.162203: step 16456, loss 3.40862e-07, acc 1\n",
      "2018-10-26T18:04:37.324769: step 16457, loss 1.80676e-07, acc 1\n",
      "2018-10-26T18:04:37.505288: step 16458, loss 1.07657e-06, acc 1\n",
      "2018-10-26T18:04:37.674833: step 16459, loss 2.80639e-05, acc 1\n",
      "2018-10-26T18:04:37.857345: step 16460, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:04:38.028889: step 16461, loss 7.38598e-05, acc 1\n",
      "2018-10-26T18:04:38.208407: step 16462, loss 1.06727e-06, acc 1\n",
      "2018-10-26T18:04:38.373964: step 16463, loss 2.19596e-06, acc 1\n",
      "2018-10-26T18:04:38.557475: step 16464, loss 9.84612e-05, acc 1\n",
      "2018-10-26T18:04:38.729017: step 16465, loss 5.2822e-05, acc 1\n",
      "2018-10-26T18:04:38.903550: step 16466, loss 5.41005e-06, acc 1\n",
      "2018-10-26T18:04:39.068110: step 16467, loss 3.01199e-05, acc 1\n",
      "2018-10-26T18:04:39.247631: step 16468, loss 5.40162e-07, acc 1\n",
      "2018-10-26T18:04:39.412191: step 16469, loss 0.000374303, acc 1\n",
      "2018-10-26T18:04:39.589717: step 16470, loss 1.43049e-06, acc 1\n",
      "2018-10-26T18:04:39.758267: step 16471, loss 0.00473141, acc 1\n",
      "2018-10-26T18:04:39.940779: step 16472, loss 7.43185e-07, acc 1\n",
      "2018-10-26T18:04:40.106337: step 16473, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:04:40.287851: step 16474, loss 3.45461e-05, acc 1\n",
      "2018-10-26T18:04:40.453410: step 16475, loss 0.00402719, acc 1\n",
      "2018-10-26T18:04:40.634949: step 16476, loss 3.4679e-06, acc 1\n",
      "2018-10-26T18:04:40.806466: step 16477, loss 9.29433e-07, acc 1\n",
      "2018-10-26T18:04:40.988978: step 16478, loss 3.52746e-06, acc 1\n",
      "2018-10-26T18:04:41.152542: step 16479, loss 7.04068e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:04:41.338046: step 16480, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:04:41.510585: step 16481, loss 0.000178924, acc 1\n",
      "2018-10-26T18:04:41.688110: step 16482, loss 4.84285e-07, acc 1\n",
      "2018-10-26T18:04:41.855662: step 16483, loss 0.000204918, acc 1\n",
      "2018-10-26T18:04:42.025210: step 16484, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:04:42.194757: step 16485, loss 8.29229e-06, acc 1\n",
      "2018-10-26T18:04:42.366298: step 16486, loss 2.9456e-05, acc 1\n",
      "2018-10-26T18:04:42.532853: step 16487, loss 5.07125e-06, acc 1\n",
      "2018-10-26T18:04:42.701403: step 16488, loss 6.41601e-06, acc 1\n",
      "2018-10-26T18:04:42.868959: step 16489, loss 3.31549e-07, acc 1\n",
      "2018-10-26T18:04:43.039499: step 16490, loss 1.1331e-05, acc 1\n",
      "2018-10-26T18:04:43.205057: step 16491, loss 5.75089e-06, acc 1\n",
      "2018-10-26T18:04:43.388568: step 16492, loss 4.82418e-07, acc 1\n",
      "2018-10-26T18:04:43.552131: step 16493, loss 8.7915e-07, acc 1\n",
      "2018-10-26T18:04:43.724669: step 16494, loss 1.80676e-07, acc 1\n",
      "2018-10-26T18:04:43.892221: step 16495, loss 2.83396e-05, acc 1\n",
      "2018-10-26T18:04:44.067752: step 16496, loss 6.44262e-05, acc 1\n",
      "2018-10-26T18:04:44.232313: step 16497, loss 0.00107968, acc 1\n",
      "2018-10-26T18:04:44.406846: step 16498, loss 4.58354e-06, acc 1\n",
      "2018-10-26T18:04:44.571406: step 16499, loss 2.53868e-06, acc 1\n",
      "2018-10-26T18:04:44.736964: step 16500, loss 0.000560182, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:04:45.172800: step 16500, loss 5.84528, acc 0.725141\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16500\n",
      "\n",
      "2018-10-26T18:04:45.532804: step 16501, loss 2.27988e-05, acc 1\n",
      "2018-10-26T18:04:45.707337: step 16502, loss 0, acc 1\n",
      "2018-10-26T18:04:45.877882: step 16503, loss 0.000109959, acc 1\n",
      "2018-10-26T18:04:46.042442: step 16504, loss 5.19617e-06, acc 1\n",
      "2018-10-26T18:04:46.237919: step 16505, loss 3.26861e-06, acc 1\n",
      "2018-10-26T18:04:46.472294: step 16506, loss 3.78816e-06, acc 1\n",
      "2018-10-26T18:04:46.638848: step 16507, loss 2.36361e-06, acc 1\n",
      "2018-10-26T18:04:46.811388: step 16508, loss 1.11759e-07, acc 1\n",
      "2018-10-26T18:04:46.978940: step 16509, loss 0.000209363, acc 1\n",
      "2018-10-26T18:04:47.151478: step 16510, loss 0.000187323, acc 1\n",
      "2018-10-26T18:04:47.321030: step 16511, loss 3.65075e-07, acc 1\n",
      "2018-10-26T18:04:47.498552: step 16512, loss 0.00409756, acc 1\n",
      "2018-10-26T18:04:47.660120: step 16513, loss 7.86873e-06, acc 1\n",
      "2018-10-26T18:04:47.841635: step 16514, loss 2.57044e-07, acc 1\n",
      "2018-10-26T18:04:48.007191: step 16515, loss 7.04938e-05, acc 1\n",
      "2018-10-26T18:04:48.178734: step 16516, loss 0.00222079, acc 1\n",
      "2018-10-26T18:04:48.347284: step 16517, loss 0.000979007, acc 1\n",
      "2018-10-26T18:04:48.522815: step 16518, loss 2.11024e-06, acc 1\n",
      "2018-10-26T18:04:48.686378: step 16519, loss 0.000140714, acc 1\n",
      "2018-10-26T18:04:48.866895: step 16520, loss 4.7984e-05, acc 1\n",
      "2018-10-26T18:04:49.044421: step 16521, loss 0.00129988, acc 1\n",
      "2018-10-26T18:04:49.219953: step 16522, loss 0.000277459, acc 1\n",
      "2018-10-26T18:04:49.396480: step 16523, loss 2.45868e-07, acc 1\n",
      "2018-10-26T18:04:49.568022: step 16524, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:04:49.746545: step 16525, loss 3.57624e-07, acc 1\n",
      "2018-10-26T18:04:49.911105: step 16526, loss 1.26472e-06, acc 1\n",
      "2018-10-26T18:04:50.089629: step 16527, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:04:50.259175: step 16528, loss 0.00116321, acc 1\n",
      "2018-10-26T18:04:50.446674: step 16529, loss 0.00121913, acc 1\n",
      "2018-10-26T18:04:50.617220: step 16530, loss 4.19093e-07, acc 1\n",
      "2018-10-26T18:04:50.789757: step 16531, loss 0.100026, acc 0.984375\n",
      "2018-10-26T18:04:50.955315: step 16532, loss 4.58271e-05, acc 1\n",
      "2018-10-26T18:04:51.133838: step 16533, loss 0.00393741, acc 1\n",
      "2018-10-26T18:04:51.299397: step 16534, loss 2.52024e-05, acc 1\n",
      "2018-10-26T18:04:51.478916: step 16535, loss 1.18647e-06, acc 1\n",
      "2018-10-26T18:04:51.643477: step 16536, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:04:51.819008: step 16537, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:04:51.984566: step 16538, loss 5.62062e-05, acc 1\n",
      "2018-10-26T18:04:52.159099: step 16539, loss 2.20898e-06, acc 1\n",
      "2018-10-26T18:04:52.324656: step 16540, loss 1.16971e-06, acc 1\n",
      "2018-10-26T18:04:52.501185: step 16541, loss 8.4935e-07, acc 1\n",
      "2018-10-26T18:04:52.662753: step 16542, loss 1.38148e-05, acc 1\n",
      "2018-10-26T18:04:52.836290: step 16543, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:04:53.010823: step 16544, loss 7.75814e-06, acc 1\n",
      "2018-10-26T18:04:53.186354: step 16545, loss 2.03953e-06, acc 1\n",
      "2018-10-26T18:04:53.366873: step 16546, loss 2.51456e-07, acc 1\n",
      "2018-10-26T18:04:53.530435: step 16547, loss 9.35023e-07, acc 1\n",
      "2018-10-26T18:04:53.705965: step 16548, loss 0.00221351, acc 1\n",
      "2018-10-26T18:04:53.866537: step 16549, loss 1.6098e-05, acc 1\n",
      "2018-10-26T18:04:54.041070: step 16550, loss 2.16238e-06, acc 1\n",
      "2018-10-26T18:04:54.204633: step 16551, loss 5.53668e-06, acc 1\n",
      "2018-10-26T18:04:54.380165: step 16552, loss 0.000171579, acc 1\n",
      "2018-10-26T18:04:54.547717: step 16553, loss 2.21326e-05, acc 1\n",
      "2018-10-26T18:04:54.724245: step 16554, loss 3.08833e-05, acc 1\n",
      "2018-10-26T18:04:54.889803: step 16555, loss 3.50402e-05, acc 1\n",
      "2018-10-26T18:04:55.069323: step 16556, loss 6.5071e-05, acc 1\n",
      "2018-10-26T18:04:55.235878: step 16557, loss 4.88005e-07, acc 1\n",
      "2018-10-26T18:04:55.415399: step 16558, loss 0.0305501, acc 0.984375\n",
      "2018-10-26T18:04:55.584946: step 16559, loss 3.83571e-05, acc 1\n",
      "2018-10-26T18:04:55.762471: step 16560, loss 0.000647056, acc 1\n",
      "2018-10-26T18:04:55.926034: step 16561, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:04:56.107550: step 16562, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:04:56.271112: step 16563, loss 0.00018872, acc 1\n",
      "2018-10-26T18:04:56.454621: step 16564, loss 4.44366e-06, acc 1\n",
      "2018-10-26T18:04:56.622175: step 16565, loss 2.46534e-05, acc 1\n",
      "2018-10-26T18:04:56.806681: step 16566, loss 2.88709e-07, acc 1\n",
      "2018-10-26T18:04:56.971241: step 16567, loss 6.44463e-07, acc 1\n",
      "2018-10-26T18:04:57.148767: step 16568, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:04:57.323301: step 16569, loss 1.13805e-06, acc 1\n",
      "2018-10-26T18:04:57.539722: step 16570, loss 0.000204392, acc 1\n",
      "2018-10-26T18:04:57.714256: step 16571, loss 1.77502e-06, acc 1\n",
      "2018-10-26T18:04:57.890785: step 16572, loss 1.4549e-05, acc 1\n",
      "2018-10-26T18:04:58.082273: step 16573, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:04:58.249826: step 16574, loss 0.0063093, acc 1\n",
      "2018-10-26T18:04:58.433335: step 16575, loss 2.45105e-06, acc 1\n",
      "2018-10-26T18:04:58.597895: step 16576, loss 0.00138489, acc 1\n",
      "2018-10-26T18:04:58.774423: step 16577, loss 3.42364e-05, acc 1\n",
      "2018-10-26T18:04:58.934995: step 16578, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:04:59.112520: step 16579, loss 4.18518e-06, acc 1\n",
      "2018-10-26T18:04:59.278078: step 16580, loss 2.81258e-07, acc 1\n",
      "2018-10-26T18:04:59.459592: step 16581, loss 3.96741e-07, acc 1\n",
      "2018-10-26T18:04:59.621161: step 16582, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:04:59.798688: step 16583, loss 2.81422e-06, acc 1\n",
      "2018-10-26T18:04:59.964245: step 16584, loss 1.20697e-06, acc 1\n",
      "2018-10-26T18:05:00.149749: step 16585, loss 9.63049e-05, acc 1\n",
      "2018-10-26T18:05:00.323286: step 16586, loss 2.55181e-07, acc 1\n",
      "2018-10-26T18:05:00.501808: step 16587, loss 2.17928e-07, acc 1\n",
      "2018-10-26T18:05:00.670358: step 16588, loss 5.23395e-07, acc 1\n",
      "2018-10-26T18:05:00.846887: step 16589, loss 4.99183e-07, acc 1\n",
      "2018-10-26T18:05:01.008455: step 16590, loss 0.0247428, acc 0.984375\n",
      "2018-10-26T18:05:01.191997: step 16591, loss 0.000708324, acc 1\n",
      "2018-10-26T18:05:01.366498: step 16592, loss 0.00218825, acc 1\n",
      "2018-10-26T18:05:01.539037: step 16593, loss 2.22943e-06, acc 1\n",
      "2018-10-26T18:05:01.702601: step 16594, loss 2.09915e-06, acc 1\n",
      "2018-10-26T18:05:01.888104: step 16595, loss 5.85153e-06, acc 1\n",
      "2018-10-26T18:05:02.054659: step 16596, loss 0.000579324, acc 1\n",
      "2018-10-26T18:05:02.230191: step 16597, loss 0.00075564, acc 1\n",
      "2018-10-26T18:05:02.393753: step 16598, loss 0.000205742, acc 1\n",
      "2018-10-26T18:05:02.572276: step 16599, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:05:02.733845: step 16600, loss 0.0025292, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:05:03.178657: step 16600, loss 5.84346, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16600\n",
      "\n",
      "2018-10-26T18:05:03.560094: step 16601, loss 4.36971e-05, acc 1\n",
      "2018-10-26T18:05:03.723656: step 16602, loss 1.25492e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:05:03.901181: step 16603, loss 0.000235025, acc 1\n",
      "2018-10-26T18:05:04.061753: step 16604, loss 1.73707e-05, acc 1\n",
      "2018-10-26T18:05:04.258227: step 16605, loss 0.00471324, acc 1\n",
      "2018-10-26T18:05:04.491614: step 16606, loss 0.000230712, acc 1\n",
      "2018-10-26T18:05:04.652176: step 16607, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:05:04.835686: step 16608, loss 7.29617e-05, acc 1\n",
      "2018-10-26T18:05:05.064074: step 16609, loss 4.19463e-05, acc 1\n",
      "2018-10-26T18:05:05.285484: step 16610, loss 0.00212991, acc 1\n",
      "2018-10-26T18:05:05.493926: step 16611, loss 1.6763e-06, acc 1\n",
      "2018-10-26T18:05:05.726306: step 16612, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:05:05.904828: step 16613, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:05:06.134216: step 16614, loss 3.94625e-05, acc 1\n",
      "2018-10-26T18:05:06.340666: step 16615, loss 0.010571, acc 1\n",
      "2018-10-26T18:05:06.546116: step 16616, loss 1.2231e-05, acc 1\n",
      "2018-10-26T18:05:06.735609: step 16617, loss 0.000152728, acc 1\n",
      "2018-10-26T18:05:06.931086: step 16618, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:05:07.143521: step 16619, loss 0.000552196, acc 1\n",
      "2018-10-26T18:05:07.321045: step 16620, loss 1.90301e-05, acc 1\n",
      "2018-10-26T18:05:07.548437: step 16621, loss 7.56219e-07, acc 1\n",
      "2018-10-26T18:05:07.769846: step 16622, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:05:07.964326: step 16623, loss 0.000362876, acc 1\n",
      "2018-10-26T18:05:08.205681: step 16624, loss 3.87427e-07, acc 1\n",
      "2018-10-26T18:05:08.414124: step 16625, loss 9.12695e-08, acc 1\n",
      "2018-10-26T18:05:08.641529: step 16626, loss 1.13619e-06, acc 1\n",
      "2018-10-26T18:05:08.850957: step 16627, loss 0.0174824, acc 0.984375\n",
      "2018-10-26T18:05:09.070372: step 16628, loss 0.00169216, acc 1\n",
      "2018-10-26T18:05:09.269838: step 16629, loss 3.08626e-06, acc 1\n",
      "2018-10-26T18:05:09.516179: step 16630, loss 0.169059, acc 0.96875\n",
      "2018-10-26T18:05:09.743572: step 16631, loss 2.79236e-05, acc 1\n",
      "2018-10-26T18:05:09.944037: step 16632, loss 5.66239e-07, acc 1\n",
      "2018-10-26T18:05:10.118570: step 16633, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:05:10.335024: step 16634, loss 8.72324e-06, acc 1\n",
      "2018-10-26T18:05:10.505536: step 16635, loss 5.07916e-05, acc 1\n",
      "2018-10-26T18:05:10.682066: step 16636, loss 5.71169e-06, acc 1\n",
      "2018-10-26T18:05:10.895495: step 16637, loss 0.000278517, acc 1\n",
      "2018-10-26T18:05:11.124883: step 16638, loss 3.68208e-06, acc 1\n",
      "2018-10-26T18:05:11.289441: step 16639, loss 0.000203229, acc 1\n",
      "2018-10-26T18:05:11.503869: step 16640, loss 0.00053242, acc 1\n",
      "2018-10-26T18:05:11.698349: step 16641, loss 0.000170865, acc 1\n",
      "2018-10-26T18:05:11.933719: step 16642, loss 3.37573e-05, acc 1\n",
      "2018-10-26T18:05:12.102270: step 16643, loss 6.77992e-07, acc 1\n",
      "2018-10-26T18:05:12.325674: step 16644, loss 6.85444e-05, acc 1\n",
      "2018-10-26T18:05:12.502201: step 16645, loss 0.000715812, acc 1\n",
      "2018-10-26T18:05:12.677733: step 16646, loss 1.48645e-05, acc 1\n",
      "2018-10-26T18:05:12.855260: step 16647, loss 0.000741497, acc 1\n",
      "2018-10-26T18:05:13.047744: step 16648, loss 0.00558166, acc 1\n",
      "2018-10-26T18:05:13.222278: step 16649, loss 0.0140966, acc 0.984375\n",
      "2018-10-26T18:05:13.391825: step 16650, loss 2.45754e-06, acc 1\n",
      "2018-10-26T18:05:13.599270: step 16651, loss 7.44605e-06, acc 1\n",
      "2018-10-26T18:05:13.775798: step 16652, loss 9.27035e-05, acc 1\n",
      "2018-10-26T18:05:13.954321: step 16653, loss 0.00857732, acc 1\n",
      "2018-10-26T18:05:14.152791: step 16654, loss 2.41203e-06, acc 1\n",
      "2018-10-26T18:05:14.330317: step 16655, loss 0.00628873, acc 1\n",
      "2018-10-26T18:05:14.508840: step 16656, loss 3.63366e-06, acc 1\n",
      "2018-10-26T18:05:14.679385: step 16657, loss 3.19294e-05, acc 1\n",
      "2018-10-26T18:05:14.878851: step 16658, loss 0.068475, acc 0.984375\n",
      "2018-10-26T18:05:15.063359: step 16659, loss 2.12144e-06, acc 1\n",
      "2018-10-26T18:05:15.240884: step 16660, loss 3.50175e-07, acc 1\n",
      "2018-10-26T18:05:15.413423: step 16661, loss 0.00230757, acc 1\n",
      "2018-10-26T18:05:15.577984: step 16662, loss 1.91285e-06, acc 1\n",
      "2018-10-26T18:05:15.777450: step 16663, loss 3.36568e-06, acc 1\n",
      "2018-10-26T18:05:15.949989: step 16664, loss 0.000459956, acc 1\n",
      "2018-10-26T18:05:16.123526: step 16665, loss 3.52373e-06, acc 1\n",
      "2018-10-26T18:05:16.284097: step 16666, loss 0.000487422, acc 1\n",
      "2018-10-26T18:05:16.464614: step 16667, loss 1.06603e-05, acc 1\n",
      "2018-10-26T18:05:16.630172: step 16668, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:05:16.799720: step 16669, loss 6.05517e-05, acc 1\n",
      "2018-10-26T18:05:16.964280: step 16670, loss 4.45168e-07, acc 1\n",
      "2018-10-26T18:05:17.140808: step 16671, loss 3.47253e-05, acc 1\n",
      "2018-10-26T18:05:17.302376: step 16672, loss 1.89249e-05, acc 1\n",
      "2018-10-26T18:05:17.479902: step 16673, loss 4.77539e-05, acc 1\n",
      "2018-10-26T18:05:17.651444: step 16674, loss 0.000111565, acc 1\n",
      "2018-10-26T18:05:17.826978: step 16675, loss 0.000108139, acc 1\n",
      "2018-10-26T18:05:18.004500: step 16676, loss 0.00448471, acc 1\n",
      "2018-10-26T18:05:18.181029: step 16677, loss 3.65806e-05, acc 1\n",
      "2018-10-26T18:05:18.357556: step 16678, loss 2.16066e-07, acc 1\n",
      "2018-10-26T18:05:18.538074: step 16679, loss 0.00381062, acc 1\n",
      "2018-10-26T18:05:18.722582: step 16680, loss 0.160645, acc 0.984375\n",
      "2018-10-26T18:05:18.884150: step 16681, loss 1.33796e-05, acc 1\n",
      "2018-10-26T18:05:19.070651: step 16682, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:05:19.236209: step 16683, loss 0.000116301, acc 1\n",
      "2018-10-26T18:05:19.410742: step 16684, loss 0.0035696, acc 1\n",
      "2018-10-26T18:05:19.587272: step 16685, loss 0.0765377, acc 0.984375\n",
      "2018-10-26T18:05:19.764797: step 16686, loss 3.51198e-05, acc 1\n",
      "2018-10-26T18:05:19.933347: step 16687, loss 0.000423423, acc 1\n",
      "2018-10-26T18:05:20.108877: step 16688, loss 5.99762e-07, acc 1\n",
      "2018-10-26T18:05:20.271444: step 16689, loss 0.000285728, acc 1\n",
      "2018-10-26T18:05:20.445977: step 16690, loss 0.00158053, acc 1\n",
      "2018-10-26T18:05:20.612532: step 16691, loss 1.01327e-06, acc 1\n",
      "2018-10-26T18:05:20.790058: step 16692, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:05:20.960601: step 16693, loss 0.00250095, acc 1\n",
      "2018-10-26T18:05:21.134138: step 16694, loss 1.04907e-05, acc 1\n",
      "2018-10-26T18:05:21.303686: step 16695, loss 0.0319856, acc 0.984375\n",
      "2018-10-26T18:05:21.478218: step 16696, loss 0.000102021, acc 1\n",
      "2018-10-26T18:05:21.650758: step 16697, loss 1.35645e-05, acc 1\n",
      "2018-10-26T18:05:21.829281: step 16698, loss 5.12469e-05, acc 1\n",
      "2018-10-26T18:05:22.006806: step 16699, loss 7.08943e-05, acc 1\n",
      "2018-10-26T18:05:22.173362: step 16700, loss 0.0205672, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:05:22.621166: step 16700, loss 5.98712, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16700\n",
      "\n",
      "2018-10-26T18:05:22.977213: step 16701, loss 0.000379812, acc 1\n",
      "2018-10-26T18:05:23.148756: step 16702, loss 1.48077e-06, acc 1\n",
      "2018-10-26T18:05:23.324286: step 16703, loss 0.000109203, acc 1\n",
      "2018-10-26T18:05:23.490842: step 16704, loss 9.92773e-07, acc 1\n",
      "2018-10-26T18:05:23.686319: step 16705, loss 8.27231e-05, acc 1\n",
      "2018-10-26T18:05:23.938646: step 16706, loss 1.25352e-06, acc 1\n",
      "2018-10-26T18:05:24.110186: step 16707, loss 5.03078e-06, acc 1\n",
      "2018-10-26T18:05:24.298683: step 16708, loss 0.00391532, acc 1\n",
      "2018-10-26T18:05:24.462245: step 16709, loss 0.000116709, acc 1\n",
      "2018-10-26T18:05:24.640769: step 16710, loss 1.61112e-06, acc 1\n",
      "2018-10-26T18:05:24.806327: step 16711, loss 0.0252174, acc 0.984375\n",
      "2018-10-26T18:05:24.985847: step 16712, loss 0.00117925, acc 1\n",
      "2018-10-26T18:05:25.156391: step 16713, loss 0.000152193, acc 1\n",
      "2018-10-26T18:05:25.333917: step 16714, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:05:25.504461: step 16715, loss 0.0288029, acc 0.984375\n",
      "2018-10-26T18:05:25.681987: step 16716, loss 0.0370271, acc 0.984375\n",
      "2018-10-26T18:05:25.855524: step 16717, loss 0.000133408, acc 1\n",
      "2018-10-26T18:05:26.037068: step 16718, loss 4.67101e-06, acc 1\n",
      "2018-10-26T18:05:26.208580: step 16719, loss 3.05461e-05, acc 1\n",
      "2018-10-26T18:05:26.386106: step 16720, loss 7.09937e-05, acc 1\n",
      "2018-10-26T18:05:26.549669: step 16721, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:05:26.724202: step 16722, loss 0.000574359, acc 1\n",
      "2018-10-26T18:05:26.892752: step 16723, loss 8.24665e-05, acc 1\n",
      "2018-10-26T18:05:27.072272: step 16724, loss 3.15321e-06, acc 1\n",
      "2018-10-26T18:05:27.236833: step 16725, loss 3.31549e-07, acc 1\n",
      "2018-10-26T18:05:27.417350: step 16726, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:05:27.585900: step 16727, loss 2.69693e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:05:27.760434: step 16728, loss 0.000141088, acc 1\n",
      "2018-10-26T18:05:27.928015: step 16729, loss 0.0859359, acc 0.984375\n",
      "2018-10-26T18:05:28.099528: step 16730, loss 3.25938e-05, acc 1\n",
      "2018-10-26T18:05:28.270072: step 16731, loss 4.4152e-05, acc 1\n",
      "2018-10-26T18:05:28.439619: step 16732, loss 4.66236e-05, acc 1\n",
      "2018-10-26T18:05:28.603182: step 16733, loss 0.0877858, acc 0.984375\n",
      "2018-10-26T18:05:28.779711: step 16734, loss 4.20954e-07, acc 1\n",
      "2018-10-26T18:05:28.944270: step 16735, loss 0.00118925, acc 1\n",
      "2018-10-26T18:05:29.113818: step 16736, loss 0.0466248, acc 0.984375\n",
      "2018-10-26T18:05:29.280372: step 16737, loss 1.59807e-06, acc 1\n",
      "2018-10-26T18:05:29.458896: step 16738, loss 0.00133661, acc 1\n",
      "2018-10-26T18:05:29.624453: step 16739, loss 0.000403262, acc 1\n",
      "2018-10-26T18:05:29.798987: step 16740, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:05:29.973521: step 16741, loss 3.66939e-07, acc 1\n",
      "2018-10-26T18:05:30.141073: step 16742, loss 9.74149e-07, acc 1\n",
      "2018-10-26T18:05:30.306631: step 16743, loss 2.53134e-05, acc 1\n",
      "2018-10-26T18:05:30.485153: step 16744, loss 2.00167e-05, acc 1\n",
      "2018-10-26T18:05:30.652707: step 16745, loss 0.000522843, acc 1\n",
      "2018-10-26T18:05:30.824248: step 16746, loss 4.61828e-05, acc 1\n",
      "2018-10-26T18:05:30.988808: step 16747, loss 8.41902e-07, acc 1\n",
      "2018-10-26T18:05:31.159352: step 16748, loss 3.14784e-07, acc 1\n",
      "2018-10-26T18:05:31.324910: step 16749, loss 1.41286e-05, acc 1\n",
      "2018-10-26T18:05:31.500441: step 16750, loss 0.00327222, acc 1\n",
      "2018-10-26T18:05:31.663007: step 16751, loss 6.59363e-07, acc 1\n",
      "2018-10-26T18:05:31.840533: step 16752, loss 7.31928e-05, acc 1\n",
      "2018-10-26T18:05:32.008085: step 16753, loss 0.000204124, acc 1\n",
      "2018-10-26T18:05:32.179626: step 16754, loss 6.44159e-06, acc 1\n",
      "2018-10-26T18:05:32.345184: step 16755, loss 5.10356e-07, acc 1\n",
      "2018-10-26T18:05:32.516725: step 16756, loss 1.5857e-05, acc 1\n",
      "2018-10-26T18:05:32.686273: step 16757, loss 0.000137495, acc 1\n",
      "2018-10-26T18:05:32.870781: step 16758, loss 6.66421e-05, acc 1\n",
      "2018-10-26T18:05:33.038331: step 16759, loss 5.08578e-05, acc 1\n",
      "2018-10-26T18:05:33.214860: step 16760, loss 5.41194e-06, acc 1\n",
      "2018-10-26T18:05:33.386403: step 16761, loss 9.25361e-06, acc 1\n",
      "2018-10-26T18:05:33.568914: step 16762, loss 2.60376e-06, acc 1\n",
      "2018-10-26T18:05:33.733474: step 16763, loss 4.73105e-07, acc 1\n",
      "2018-10-26T18:05:33.904019: step 16764, loss 0.000177151, acc 1\n",
      "2018-10-26T18:05:34.070574: step 16765, loss 2.84032e-06, acc 1\n",
      "2018-10-26T18:05:34.261066: step 16766, loss 3.59288e-06, acc 1\n",
      "2018-10-26T18:05:34.427620: step 16767, loss 6.63087e-05, acc 1\n",
      "2018-10-26T18:05:34.608138: step 16768, loss 7.22696e-07, acc 1\n",
      "2018-10-26T18:05:34.772698: step 16769, loss 1.41561e-07, acc 1\n",
      "2018-10-26T18:05:34.943242: step 16770, loss 0.00424154, acc 1\n",
      "2018-10-26T18:05:35.116779: step 16771, loss 4.55126e-05, acc 1\n",
      "2018-10-26T18:05:35.288320: step 16772, loss 0.000444302, acc 1\n",
      "2018-10-26T18:05:35.452881: step 16773, loss 1.57836e-05, acc 1\n",
      "2018-10-26T18:05:35.629409: step 16774, loss 0.000137261, acc 1\n",
      "2018-10-26T18:05:35.794966: step 16775, loss 0.0061765, acc 1\n",
      "2018-10-26T18:05:35.968503: step 16776, loss 1.74153e-06, acc 1\n",
      "2018-10-26T18:05:36.144044: step 16777, loss 2.0489e-07, acc 1\n",
      "2018-10-26T18:05:36.322556: step 16778, loss 0.000360791, acc 1\n",
      "2018-10-26T18:05:36.489112: step 16779, loss 1.28704e-05, acc 1\n",
      "2018-10-26T18:05:36.661650: step 16780, loss 0.000431368, acc 1\n",
      "2018-10-26T18:05:36.825214: step 16781, loss 1.2138e-05, acc 1\n",
      "2018-10-26T18:05:36.999748: step 16782, loss 1.69309e-06, acc 1\n",
      "2018-10-26T18:05:37.164308: step 16783, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:05:37.335850: step 16784, loss 1.93892e-06, acc 1\n",
      "2018-10-26T18:05:37.500411: step 16785, loss 1.80905e-05, acc 1\n",
      "2018-10-26T18:05:37.676939: step 16786, loss 0.000117582, acc 1\n",
      "2018-10-26T18:05:37.844491: step 16787, loss 1.28335e-05, acc 1\n",
      "2018-10-26T18:05:38.025021: step 16788, loss 0.139706, acc 0.984375\n",
      "2018-10-26T18:05:38.191564: step 16789, loss 5.15947e-07, acc 1\n",
      "2018-10-26T18:05:38.372081: step 16790, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:05:38.534647: step 16791, loss 1.96609e-05, acc 1\n",
      "2018-10-26T18:05:38.710178: step 16792, loss 9.19015e-05, acc 1\n",
      "2018-10-26T18:05:38.879725: step 16793, loss 4.98772e-06, acc 1\n",
      "2018-10-26T18:05:39.059245: step 16794, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:05:39.223806: step 16795, loss 6.77998e-07, acc 1\n",
      "2018-10-26T18:05:39.396344: step 16796, loss 7.50642e-07, acc 1\n",
      "2018-10-26T18:05:39.560905: step 16797, loss 1.13674e-05, acc 1\n",
      "2018-10-26T18:05:39.736436: step 16798, loss 7.09661e-07, acc 1\n",
      "2018-10-26T18:05:39.904987: step 16799, loss 2.46272e-05, acc 1\n",
      "2018-10-26T18:05:40.077524: step 16800, loss 9.119e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:05:40.530314: step 16800, loss 5.91328, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16800\n",
      "\n",
      "2018-10-26T18:05:40.887361: step 16801, loss 4.27434e-06, acc 1\n",
      "2018-10-26T18:05:41.083836: step 16802, loss 0.000250224, acc 1\n",
      "2018-10-26T18:05:41.274327: step 16803, loss 4.67755e-05, acc 1\n",
      "2018-10-26T18:05:41.450856: step 16804, loss 4.35857e-07, acc 1\n",
      "2018-10-26T18:05:41.686226: step 16805, loss 1.70055e-06, acc 1\n",
      "2018-10-26T18:05:41.872727: step 16806, loss 7.00348e-07, acc 1\n",
      "2018-10-26T18:05:42.048258: step 16807, loss 0.0274024, acc 0.984375\n",
      "2018-10-26T18:05:42.227779: step 16808, loss 6.89484e-06, acc 1\n",
      "2018-10-26T18:05:42.393337: step 16809, loss 5.17809e-07, acc 1\n",
      "2018-10-26T18:05:42.566873: step 16810, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:05:42.736420: step 16811, loss 0.000132606, acc 1\n",
      "2018-10-26T18:05:42.911951: step 16812, loss 7.8038e-05, acc 1\n",
      "2018-10-26T18:05:43.078506: step 16813, loss 0.0055652, acc 1\n",
      "2018-10-26T18:05:43.262016: step 16814, loss 2.43635e-05, acc 1\n",
      "2018-10-26T18:05:43.431563: step 16815, loss 3.67496e-05, acc 1\n",
      "2018-10-26T18:05:43.606097: step 16816, loss 8.86633e-06, acc 1\n",
      "2018-10-26T18:05:43.772651: step 16817, loss 9.872e-08, acc 1\n",
      "2018-10-26T18:05:43.946188: step 16818, loss 8.82662e-05, acc 1\n",
      "2018-10-26T18:05:44.110748: step 16819, loss 0.00120648, acc 1\n",
      "2018-10-26T18:05:44.286279: step 16820, loss 2.59365e-05, acc 1\n",
      "2018-10-26T18:05:44.453831: step 16821, loss 4.54849e-05, acc 1\n",
      "2018-10-26T18:05:44.629362: step 16822, loss 6.88945e-05, acc 1\n",
      "2018-10-26T18:05:44.793922: step 16823, loss 2.87017e-06, acc 1\n",
      "2018-10-26T18:05:44.972446: step 16824, loss 1.40251e-06, acc 1\n",
      "2018-10-26T18:05:45.137006: step 16825, loss 0.0174617, acc 0.984375\n",
      "2018-10-26T18:05:45.311540: step 16826, loss 0.00143201, acc 1\n",
      "2018-10-26T18:05:45.474105: step 16827, loss 9.18793e-06, acc 1\n",
      "2018-10-26T18:05:45.655622: step 16828, loss 4.20902e-06, acc 1\n",
      "2018-10-26T18:05:45.820181: step 16829, loss 0.00130675, acc 1\n",
      "2018-10-26T18:05:46.002693: step 16830, loss 2.13789e-05, acc 1\n",
      "2018-10-26T18:05:46.174235: step 16831, loss 1.96687e-06, acc 1\n",
      "2018-10-26T18:05:46.351760: step 16832, loss 5.3271e-07, acc 1\n",
      "2018-10-26T18:05:46.522305: step 16833, loss 8.75443e-08, acc 1\n",
      "2018-10-26T18:05:46.705815: step 16834, loss 6.3943e-05, acc 1\n",
      "2018-10-26T18:05:46.867382: step 16835, loss 2.14247e-05, acc 1\n",
      "2018-10-26T18:05:47.040919: step 16836, loss 0.000182042, acc 1\n",
      "2018-10-26T18:05:47.212461: step 16837, loss 0.000389796, acc 1\n",
      "2018-10-26T18:05:47.389986: step 16838, loss 1.80114e-06, acc 1\n",
      "2018-10-26T18:05:47.555544: step 16839, loss 6.08807e-06, acc 1\n",
      "2018-10-26T18:05:47.736082: step 16840, loss 1.78249e-06, acc 1\n",
      "2018-10-26T18:05:47.903614: step 16841, loss 0.000311459, acc 1\n",
      "2018-10-26T18:05:48.080143: step 16842, loss 9.72848e-05, acc 1\n",
      "2018-10-26T18:05:48.243705: step 16843, loss 0.00373149, acc 1\n",
      "2018-10-26T18:05:48.426217: step 16844, loss 2.28352e-06, acc 1\n",
      "2018-10-26T18:05:48.593770: step 16845, loss 0.000260203, acc 1\n",
      "2018-10-26T18:05:48.821162: step 16846, loss 6.78681e-06, acc 1\n",
      "2018-10-26T18:05:49.015643: step 16847, loss 7.7319e-06, acc 1\n",
      "2018-10-26T18:05:49.200150: step 16848, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:05:49.376678: step 16849, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:05:49.560188: step 16850, loss 0.000275935, acc 1\n",
      "2018-10-26T18:05:49.733725: step 16851, loss 3.81626e-05, acc 1\n",
      "2018-10-26T18:05:49.930200: step 16852, loss 0.000674024, acc 1\n",
      "2018-10-26T18:05:50.099746: step 16853, loss 2.34982e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:05:50.291234: step 16854, loss 0.0231052, acc 0.984375\n",
      "2018-10-26T18:05:50.463774: step 16855, loss 0.00013544, acc 1\n",
      "2018-10-26T18:05:50.636313: step 16856, loss 2.10987e-05, acc 1\n",
      "2018-10-26T18:05:50.800872: step 16857, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:05:50.970421: step 16858, loss 0.000118265, acc 1\n",
      "2018-10-26T18:05:51.153930: step 16859, loss 4.33963e-06, acc 1\n",
      "2018-10-26T18:05:51.326469: step 16860, loss 0.0002167, acc 1\n",
      "2018-10-26T18:05:51.504992: step 16861, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:05:51.672544: step 16862, loss 0.000101577, acc 1\n",
      "2018-10-26T18:05:51.845083: step 16863, loss 8.49227e-06, acc 1\n",
      "2018-10-26T18:05:52.014630: step 16864, loss 1.22334e-05, acc 1\n",
      "2018-10-26T18:05:52.187170: step 16865, loss 3.50313e-05, acc 1\n",
      "2018-10-26T18:05:52.352727: step 16866, loss 1.11569e-06, acc 1\n",
      "2018-10-26T18:05:52.526263: step 16867, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:05:52.697805: step 16868, loss 3.54385e-05, acc 1\n",
      "2018-10-26T18:05:52.879320: step 16869, loss 5.91204e-05, acc 1\n",
      "2018-10-26T18:05:53.039891: step 16870, loss 0.0387092, acc 0.984375\n",
      "2018-10-26T18:05:53.224398: step 16871, loss 0.000121381, acc 1\n",
      "2018-10-26T18:05:53.391950: step 16872, loss 4.06515e-05, acc 1\n",
      "2018-10-26T18:05:53.565487: step 16873, loss 2.59079e-06, acc 1\n",
      "2018-10-26T18:05:53.736031: step 16874, loss 0.000114785, acc 1\n",
      "2018-10-26T18:05:53.909567: step 16875, loss 0.00163385, acc 1\n",
      "2018-10-26T18:05:54.074128: step 16876, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:05:54.254645: step 16877, loss 0.000689686, acc 1\n",
      "2018-10-26T18:05:54.421199: step 16878, loss 0.000501899, acc 1\n",
      "2018-10-26T18:05:54.593739: step 16879, loss 1.06168e-06, acc 1\n",
      "2018-10-26T18:05:54.760294: step 16880, loss 1.81226e-06, acc 1\n",
      "2018-10-26T18:05:54.935824: step 16881, loss 0.000850168, acc 1\n",
      "2018-10-26T18:05:55.098390: step 16882, loss 6.34477e-06, acc 1\n",
      "2018-10-26T18:05:55.279906: step 16883, loss 1.08404e-06, acc 1\n",
      "2018-10-26T18:05:55.443469: step 16884, loss 0.0535505, acc 0.984375\n",
      "2018-10-26T18:05:55.615011: step 16885, loss 0.0315672, acc 0.984375\n",
      "2018-10-26T18:05:55.783560: step 16886, loss 1.91851e-07, acc 1\n",
      "2018-10-26T18:05:55.963080: step 16887, loss 0.000627281, acc 1\n",
      "2018-10-26T18:05:56.129646: step 16888, loss 2.66485e-05, acc 1\n",
      "2018-10-26T18:05:56.309155: step 16889, loss 1.61111e-06, acc 1\n",
      "2018-10-26T18:05:56.477706: step 16890, loss 0.000399966, acc 1\n",
      "2018-10-26T18:05:56.654234: step 16891, loss 6.65914e-05, acc 1\n",
      "2018-10-26T18:05:56.821786: step 16892, loss 9.03372e-07, acc 1\n",
      "2018-10-26T18:05:56.997317: step 16893, loss 7.91604e-07, acc 1\n",
      "2018-10-26T18:05:57.163872: step 16894, loss 5.87122e-05, acc 1\n",
      "2018-10-26T18:05:57.375307: step 16895, loss 0.00023311, acc 1\n",
      "2018-10-26T18:05:57.542859: step 16896, loss 0.000909822, acc 1\n",
      "2018-10-26T18:05:57.717393: step 16897, loss 4.80501e-06, acc 1\n",
      "2018-10-26T18:05:57.888934: step 16898, loss 4.41441e-07, acc 1\n",
      "2018-10-26T18:05:58.063469: step 16899, loss 3.74352e-05, acc 1\n",
      "2018-10-26T18:05:58.234012: step 16900, loss 3.85539e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:05:58.670845: step 16900, loss 5.85381, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-16900\n",
      "\n",
      "2018-10-26T18:05:59.024005: step 16901, loss 8.17692e-07, acc 1\n",
      "2018-10-26T18:05:59.187568: step 16902, loss 2.3375e-06, acc 1\n",
      "2018-10-26T18:05:59.357116: step 16903, loss 3.42554e-05, acc 1\n",
      "2018-10-26T18:05:59.522674: step 16904, loss 5.61468e-05, acc 1\n",
      "2018-10-26T18:05:59.715160: step 16905, loss 0.000684275, acc 1\n",
      "2018-10-26T18:05:59.948535: step 16906, loss 9.50316e-06, acc 1\n",
      "2018-10-26T18:06:00.129053: step 16907, loss 5.35984e-06, acc 1\n",
      "2018-10-26T18:06:00.317550: step 16908, loss 2.23517e-07, acc 1\n",
      "2018-10-26T18:06:00.482124: step 16909, loss 0.0761128, acc 0.984375\n",
      "2018-10-26T18:06:00.662628: step 16910, loss 3.84421e-06, acc 1\n",
      "2018-10-26T18:06:00.826191: step 16911, loss 0, acc 1\n",
      "2018-10-26T18:06:01.002719: step 16912, loss 3.61898e-06, acc 1\n",
      "2018-10-26T18:06:01.169274: step 16913, loss 0.00119109, acc 1\n",
      "2018-10-26T18:06:01.346800: step 16914, loss 4.11414e-06, acc 1\n",
      "2018-10-26T18:06:01.514352: step 16915, loss 3.44377e-06, acc 1\n",
      "2018-10-26T18:06:01.690880: step 16916, loss 1.87186e-06, acc 1\n",
      "2018-10-26T18:06:01.853446: step 16917, loss 6.56066e-05, acc 1\n",
      "2018-10-26T18:06:02.027979: step 16918, loss 2.75647e-06, acc 1\n",
      "2018-10-26T18:06:02.191543: step 16919, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:06:02.369068: step 16920, loss 0.0275861, acc 0.984375\n",
      "2018-10-26T18:06:02.539613: step 16921, loss 1.34853e-06, acc 1\n",
      "2018-10-26T18:06:02.711154: step 16922, loss 3.94646e-06, acc 1\n",
      "2018-10-26T18:06:02.887683: step 16923, loss 0.00010768, acc 1\n",
      "2018-10-26T18:06:03.068201: step 16924, loss 1.5478e-06, acc 1\n",
      "2018-10-26T18:06:03.248717: step 16925, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:06:03.412282: step 16926, loss 2.2964e-05, acc 1\n",
      "2018-10-26T18:06:03.593795: step 16927, loss 1.86628e-06, acc 1\n",
      "2018-10-26T18:06:03.756362: step 16928, loss 2.03028e-07, acc 1\n",
      "2018-10-26T18:06:03.930895: step 16929, loss 5.18995e-05, acc 1\n",
      "2018-10-26T18:06:04.095455: step 16930, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:06:04.276972: step 16931, loss 0.000506582, acc 1\n",
      "2018-10-26T18:06:04.449510: step 16932, loss 0.000871017, acc 1\n",
      "2018-10-26T18:06:04.628033: step 16933, loss 9.2205e-05, acc 1\n",
      "2018-10-26T18:06:04.792594: step 16934, loss 9.33172e-07, acc 1\n",
      "2018-10-26T18:06:04.967127: step 16935, loss 1.50497e-06, acc 1\n",
      "2018-10-26T18:06:05.131687: step 16936, loss 4.8776e-06, acc 1\n",
      "2018-10-26T18:06:05.310210: step 16937, loss 0.000754564, acc 1\n",
      "2018-10-26T18:06:05.474771: step 16938, loss 2.8801e-05, acc 1\n",
      "2018-10-26T18:06:05.651299: step 16939, loss 0.000392997, acc 1\n",
      "2018-10-26T18:06:05.818851: step 16940, loss 5.32526e-05, acc 1\n",
      "2018-10-26T18:06:05.997374: step 16941, loss 3.16648e-07, acc 1\n",
      "2018-10-26T18:06:06.162932: step 16942, loss 1.75538e-05, acc 1\n",
      "2018-10-26T18:06:06.337465: step 16943, loss 0.000149245, acc 1\n",
      "2018-10-26T18:06:06.506015: step 16944, loss 2.64494e-07, acc 1\n",
      "2018-10-26T18:06:06.685535: step 16945, loss 0.000115821, acc 1\n",
      "2018-10-26T18:06:06.851093: step 16946, loss 0.000147044, acc 1\n",
      "2018-10-26T18:06:07.030614: step 16947, loss 0.000203323, acc 1\n",
      "2018-10-26T18:06:07.200160: step 16948, loss 5.3859e-05, acc 1\n",
      "2018-10-26T18:06:07.382673: step 16949, loss 1.74079e-05, acc 1\n",
      "2018-10-26T18:06:07.547233: step 16950, loss 2.16562e-07, acc 1\n",
      "2018-10-26T18:06:07.729746: step 16951, loss 7.72979e-07, acc 1\n",
      "2018-10-26T18:06:07.899293: step 16952, loss 0.000113708, acc 1\n",
      "2018-10-26T18:06:08.076818: step 16953, loss 3.04886e-06, acc 1\n",
      "2018-10-26T18:06:08.240381: step 16954, loss 1.93521e-06, acc 1\n",
      "2018-10-26T18:06:08.416909: step 16955, loss 0.000406253, acc 1\n",
      "2018-10-26T18:06:08.583464: step 16956, loss 2.91291e-06, acc 1\n",
      "2018-10-26T18:06:08.761988: step 16957, loss 1.20697e-06, acc 1\n",
      "2018-10-26T18:06:08.931980: step 16958, loss 1.29647e-05, acc 1\n",
      "2018-10-26T18:06:09.110503: step 16959, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:06:09.273096: step 16960, loss 7.82309e-08, acc 1\n",
      "2018-10-26T18:06:09.451592: step 16961, loss 6.17731e-05, acc 1\n",
      "2018-10-26T18:06:09.620141: step 16962, loss 1.51612e-06, acc 1\n",
      "2018-10-26T18:06:09.804648: step 16963, loss 8.79335e-06, acc 1\n",
      "2018-10-26T18:06:09.972201: step 16964, loss 3.03609e-07, acc 1\n",
      "2018-10-26T18:06:10.152719: step 16965, loss 0.000823318, acc 1\n",
      "2018-10-26T18:06:10.317279: step 16966, loss 0.00139048, acc 1\n",
      "2018-10-26T18:06:10.494803: step 16967, loss 1.82192e-05, acc 1\n",
      "2018-10-26T18:06:10.733168: step 16968, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:06:10.962554: step 16969, loss 0.00200848, acc 1\n",
      "2018-10-26T18:06:11.179973: step 16970, loss 2.71945e-07, acc 1\n",
      "2018-10-26T18:06:11.370463: step 16971, loss 4.89461e-06, acc 1\n",
      "2018-10-26T18:06:11.596859: step 16972, loss 3.96995e-05, acc 1\n",
      "2018-10-26T18:06:11.792337: step 16973, loss 7.40788e-06, acc 1\n",
      "2018-10-26T18:06:12.030701: step 16974, loss 6.12803e-07, acc 1\n",
      "2018-10-26T18:06:12.209223: step 16975, loss 0.000638766, acc 1\n",
      "2018-10-26T18:06:12.437613: step 16976, loss 2.93528e-06, acc 1\n",
      "2018-10-26T18:06:12.661016: step 16977, loss 2.24068e-06, acc 1\n",
      "2018-10-26T18:06:12.870457: step 16978, loss 5.39384e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:06:13.090868: step 16979, loss 1.59994e-06, acc 1\n",
      "2018-10-26T18:06:13.333220: step 16980, loss 4.95178e-05, acc 1\n",
      "2018-10-26T18:06:13.528698: step 16981, loss 4.9041e-05, acc 1\n",
      "2018-10-26T18:06:13.761077: step 16982, loss 9.12695e-08, acc 1\n",
      "2018-10-26T18:06:13.959547: step 16983, loss 0.0112793, acc 0.984375\n",
      "2018-10-26T18:06:14.181952: step 16984, loss 1.81598e-06, acc 1\n",
      "2018-10-26T18:06:14.360476: step 16985, loss 1.54781e-06, acc 1\n",
      "2018-10-26T18:06:14.567921: step 16986, loss 0.049365, acc 0.984375\n",
      "2018-10-26T18:06:14.795314: step 16987, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:06:15.029688: step 16988, loss 0.000185773, acc 1\n",
      "2018-10-26T18:06:15.238131: step 16989, loss 0.0002052, acc 1\n",
      "2018-10-26T18:06:15.443581: step 16990, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:06:15.659008: step 16991, loss 1.45368e-05, acc 1\n",
      "2018-10-26T18:06:15.825561: step 16992, loss 6.49876e-05, acc 1\n",
      "2018-10-26T18:06:16.030014: step 16993, loss 0.000303229, acc 1\n",
      "2018-10-26T18:06:16.198565: step 16994, loss 5.4663e-06, acc 1\n",
      "2018-10-26T18:06:16.387469: step 16995, loss 6.46878e-05, acc 1\n",
      "2018-10-26T18:06:16.602894: step 16996, loss 0.035288, acc 0.984375\n",
      "2018-10-26T18:06:16.814330: step 16997, loss 1.51243e-06, acc 1\n",
      "2018-10-26T18:06:16.992851: step 16998, loss 4.21499e-06, acc 1\n",
      "2018-10-26T18:06:17.186334: step 16999, loss 1.02071e-06, acc 1\n",
      "2018-10-26T18:06:17.395776: step 17000, loss 0.000165451, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:06:17.870507: step 17000, loss 5.91448, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17000\n",
      "\n",
      "2018-10-26T18:06:18.242512: step 17001, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:06:18.433004: step 17002, loss 0.00187303, acc 1\n",
      "2018-10-26T18:06:18.597564: step 17003, loss 7.35879e-05, acc 1\n",
      "2018-10-26T18:06:18.772098: step 17004, loss 0.0639393, acc 0.96875\n",
      "2018-10-26T18:06:18.998494: step 17005, loss 1.26658e-06, acc 1\n",
      "2018-10-26T18:06:19.235859: step 17006, loss 5.0477e-07, acc 1\n",
      "2018-10-26T18:06:19.409395: step 17007, loss 0.000208, acc 1\n",
      "2018-10-26T18:06:19.606867: step 17008, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:06:19.770430: step 17009, loss 0.000140254, acc 1\n",
      "2018-10-26T18:06:19.942969: step 17010, loss 9.86967e-05, acc 1\n",
      "2018-10-26T18:06:20.145429: step 17011, loss 0.000107818, acc 1\n",
      "2018-10-26T18:06:20.325946: step 17012, loss 2.58887e-06, acc 1\n",
      "2018-10-26T18:06:20.498485: step 17013, loss 1.60369e-06, acc 1\n",
      "2018-10-26T18:06:20.679003: step 17014, loss 3.38999e-07, acc 1\n",
      "2018-10-26T18:06:20.857526: step 17015, loss 4.88874e-06, acc 1\n",
      "2018-10-26T18:06:21.053004: step 17016, loss 9.49947e-08, acc 1\n",
      "2018-10-26T18:06:21.227537: step 17017, loss 2.20359e-05, acc 1\n",
      "2018-10-26T18:06:21.402072: step 17018, loss 0.00606303, acc 1\n",
      "2018-10-26T18:06:21.578599: step 17019, loss 9.10472e-06, acc 1\n",
      "2018-10-26T18:06:21.742163: step 17020, loss 0.00292747, acc 1\n",
      "2018-10-26T18:06:21.914701: step 17021, loss 4.16817e-06, acc 1\n",
      "2018-10-26T18:06:22.083250: step 17022, loss 3.28449e-05, acc 1\n",
      "2018-10-26T18:06:22.289699: step 17023, loss 3.57625e-07, acc 1\n",
      "2018-10-26T18:06:22.506122: step 17024, loss 5.14337e-05, acc 1\n",
      "2018-10-26T18:06:22.702596: step 17025, loss 1.41558e-06, acc 1\n",
      "2018-10-26T18:06:22.911039: step 17026, loss 0.0222659, acc 0.984375\n",
      "2018-10-26T18:06:23.080586: step 17027, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:06:23.253125: step 17028, loss 0.000174707, acc 1\n",
      "2018-10-26T18:06:23.420677: step 17029, loss 3.07698e-06, acc 1\n",
      "2018-10-26T18:06:23.608176: step 17030, loss 1.2596e-05, acc 1\n",
      "2018-10-26T18:06:23.797672: step 17031, loss 7.4504e-07, acc 1\n",
      "2018-10-26T18:06:23.988161: step 17032, loss 4.66162e-06, acc 1\n",
      "2018-10-26T18:06:24.172668: step 17033, loss 2.3765e-05, acc 1\n",
      "2018-10-26T18:06:24.358173: step 17034, loss 3.57449e-05, acc 1\n",
      "2018-10-26T18:06:24.535699: step 17035, loss 7.8514e-05, acc 1\n",
      "2018-10-26T18:06:24.711230: step 17036, loss 0.074734, acc 0.984375\n",
      "2018-10-26T18:06:24.876787: step 17037, loss 0.000171607, acc 1\n",
      "2018-10-26T18:06:25.052317: step 17038, loss 1.48819e-06, acc 1\n",
      "2018-10-26T18:06:25.215881: step 17039, loss 2.10465e-06, acc 1\n",
      "2018-10-26T18:06:25.390414: step 17040, loss 1.41556e-06, acc 1\n",
      "2018-10-26T18:06:25.560959: step 17041, loss 7.87007e-05, acc 1\n",
      "2018-10-26T18:06:25.737487: step 17042, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:06:25.910027: step 17043, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:06:26.098523: step 17044, loss 1.62265e-05, acc 1\n",
      "2018-10-26T18:06:26.261089: step 17045, loss 2.95574e-06, acc 1\n",
      "2018-10-26T18:06:26.437617: step 17046, loss 1.59331e-05, acc 1\n",
      "2018-10-26T18:06:26.608161: step 17047, loss 0.00103682, acc 1\n",
      "2018-10-26T18:06:26.788678: step 17048, loss 2.34677e-06, acc 1\n",
      "2018-10-26T18:06:26.955234: step 17049, loss 2.803e-05, acc 1\n",
      "2018-10-26T18:06:27.143731: step 17050, loss 1.21684e-05, acc 1\n",
      "2018-10-26T18:06:27.311283: step 17051, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:06:27.482825: step 17052, loss 1.71545e-06, acc 1\n",
      "2018-10-26T18:06:27.648382: step 17053, loss 2.74916e-05, acc 1\n",
      "2018-10-26T18:06:27.824910: step 17054, loss 2.94829e-06, acc 1\n",
      "2018-10-26T18:06:27.999446: step 17055, loss 1.9799e-06, acc 1\n",
      "2018-10-26T18:06:28.181956: step 17056, loss 4.19862e-05, acc 1\n",
      "2018-10-26T18:06:28.355493: step 17057, loss 7.50637e-07, acc 1\n",
      "2018-10-26T18:06:28.536010: step 17058, loss 5.16112e-05, acc 1\n",
      "2018-10-26T18:06:28.706554: step 17059, loss 2.15417e-05, acc 1\n",
      "2018-10-26T18:06:28.902032: step 17060, loss 0.000104541, acc 1\n",
      "2018-10-26T18:06:29.088535: step 17061, loss 0.00470477, acc 1\n",
      "2018-10-26T18:06:29.285009: step 17062, loss 9.10815e-07, acc 1\n",
      "2018-10-26T18:06:29.474502: step 17063, loss 9.23855e-07, acc 1\n",
      "2018-10-26T18:06:29.651031: step 17064, loss 0.000107849, acc 1\n",
      "2018-10-26T18:06:29.828556: step 17065, loss 5.30599e-05, acc 1\n",
      "2018-10-26T18:06:30.008091: step 17066, loss 5.56832e-06, acc 1\n",
      "2018-10-26T18:06:30.174632: step 17067, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:06:30.349166: step 17068, loss 0.00112398, acc 1\n",
      "2018-10-26T18:06:30.513727: step 17069, loss 2.0489e-07, acc 1\n",
      "2018-10-26T18:06:30.691252: step 17070, loss 0.0928354, acc 0.984375\n",
      "2018-10-26T18:06:30.855812: step 17071, loss 9.01504e-07, acc 1\n",
      "2018-10-26T18:06:31.035333: step 17072, loss 8.75448e-06, acc 1\n",
      "2018-10-26T18:06:31.201888: step 17073, loss 8.32764e-06, acc 1\n",
      "2018-10-26T18:06:31.386395: step 17074, loss 6.77486e-05, acc 1\n",
      "2018-10-26T18:06:31.544970: step 17075, loss 4.47029e-07, acc 1\n",
      "2018-10-26T18:06:31.726486: step 17076, loss 0.000252932, acc 1\n",
      "2018-10-26T18:06:31.897030: step 17077, loss 1.82563e-05, acc 1\n",
      "2018-10-26T18:06:32.072561: step 17078, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:06:32.242108: step 17079, loss 0.00156492, acc 1\n",
      "2018-10-26T18:06:32.419634: step 17080, loss 2.90114e-05, acc 1\n",
      "2018-10-26T18:06:32.586188: step 17081, loss 0.000395873, acc 1\n",
      "2018-10-26T18:06:32.765710: step 17082, loss 3.50882e-05, acc 1\n",
      "2018-10-26T18:06:32.934258: step 17083, loss 6.55641e-07, acc 1\n",
      "2018-10-26T18:06:33.112782: step 17084, loss 8.44071e-06, acc 1\n",
      "2018-10-26T18:06:33.278339: step 17085, loss 3.034e-06, acc 1\n",
      "2018-10-26T18:06:33.460852: step 17086, loss 6.74348e-05, acc 1\n",
      "2018-10-26T18:06:33.630398: step 17087, loss 0.00281789, acc 1\n",
      "2018-10-26T18:06:33.806928: step 17088, loss 0.000155161, acc 1\n",
      "2018-10-26T18:06:33.979467: step 17089, loss 7.76704e-07, acc 1\n",
      "2018-10-26T18:06:34.164971: step 17090, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:06:34.328534: step 17091, loss 1.06171e-07, acc 1\n",
      "2018-10-26T18:06:34.509052: step 17092, loss 2.27242e-07, acc 1\n",
      "2018-10-26T18:06:34.670620: step 17093, loss 3.53433e-05, acc 1\n",
      "2018-10-26T18:06:34.855126: step 17094, loss 0.000402943, acc 1\n",
      "2018-10-26T18:06:35.025671: step 17095, loss 0.0147216, acc 0.984375\n",
      "2018-10-26T18:06:35.203197: step 17096, loss 0.000366747, acc 1\n",
      "2018-10-26T18:06:35.371745: step 17097, loss 0.000164682, acc 1\n",
      "2018-10-26T18:06:35.548275: step 17098, loss 5.64869e-06, acc 1\n",
      "2018-10-26T18:06:35.729790: step 17099, loss 0.0153325, acc 0.984375\n",
      "2018-10-26T18:06:35.908313: step 17100, loss 0.000172119, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:06:36.354122: step 17100, loss 5.98008, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17100\n",
      "\n",
      "2018-10-26T18:06:36.750601: step 17101, loss 0.00176785, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:06:36.928850: step 17102, loss 4.77697e-06, acc 1\n",
      "2018-10-26T18:06:37.088424: step 17103, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:06:37.261961: step 17104, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:06:37.453449: step 17105, loss 0, acc 1\n",
      "2018-10-26T18:06:37.674857: step 17106, loss 5.0477e-07, acc 1\n",
      "2018-10-26T18:06:37.840415: step 17107, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:06:38.013955: step 17108, loss 6.56447e-06, acc 1\n",
      "2018-10-26T18:06:38.180506: step 17109, loss 2.56138e-05, acc 1\n",
      "2018-10-26T18:06:38.355041: step 17110, loss 0.0045173, acc 1\n",
      "2018-10-26T18:06:38.518603: step 17111, loss 8.49358e-07, acc 1\n",
      "2018-10-26T18:06:38.692140: step 17112, loss 2.51442e-06, acc 1\n",
      "2018-10-26T18:06:38.859692: step 17113, loss 1.01883e-06, acc 1\n",
      "2018-10-26T18:06:39.037218: step 17114, loss 0.0247589, acc 0.984375\n",
      "2018-10-26T18:06:39.202777: step 17115, loss 3.49216e-06, acc 1\n",
      "2018-10-26T18:06:39.376312: step 17116, loss 0.00181394, acc 1\n",
      "2018-10-26T18:06:39.548850: step 17117, loss 6.59366e-07, acc 1\n",
      "2018-10-26T18:06:39.722387: step 17118, loss 4.67087e-06, acc 1\n",
      "2018-10-26T18:06:39.885950: step 17119, loss 1.96502e-06, acc 1\n",
      "2018-10-26T18:06:40.055497: step 17120, loss 0.000606695, acc 1\n",
      "2018-10-26T18:06:40.225044: step 17121, loss 3.38998e-07, acc 1\n",
      "2018-10-26T18:06:40.394591: step 17122, loss 3.56961e-05, acc 1\n",
      "2018-10-26T18:06:40.564137: step 17123, loss 1.15295e-06, acc 1\n",
      "2018-10-26T18:06:40.734682: step 17124, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:06:40.901237: step 17125, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:06:41.078763: step 17126, loss 2.68902e-05, acc 1\n",
      "2018-10-26T18:06:41.243323: step 17127, loss 2.6822e-07, acc 1\n",
      "2018-10-26T18:06:41.419852: step 17128, loss 2.68958e-05, acc 1\n",
      "2018-10-26T18:06:41.591394: step 17129, loss 1.87372e-06, acc 1\n",
      "2018-10-26T18:06:41.765927: step 17130, loss 8.31774e-06, acc 1\n",
      "2018-10-26T18:06:41.928492: step 17131, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:06:42.102029: step 17132, loss 1.69361e-05, acc 1\n",
      "2018-10-26T18:06:42.266589: step 17133, loss 4.66526e-06, acc 1\n",
      "2018-10-26T18:06:42.443120: step 17134, loss 0.00011777, acc 1\n",
      "2018-10-26T18:06:42.607677: step 17135, loss 9.01502e-07, acc 1\n",
      "2018-10-26T18:06:42.779220: step 17136, loss 3.92231e-06, acc 1\n",
      "2018-10-26T18:06:42.944777: step 17137, loss 0.00198588, acc 1\n",
      "2018-10-26T18:06:43.116320: step 17138, loss 6.65364e-05, acc 1\n",
      "2018-10-26T18:06:43.283872: step 17139, loss 0.185935, acc 0.984375\n",
      "2018-10-26T18:06:43.455412: step 17140, loss 9.72834e-05, acc 1\n",
      "2018-10-26T18:06:43.621969: step 17141, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:06:43.797498: step 17142, loss 7.15245e-07, acc 1\n",
      "2018-10-26T18:06:43.964054: step 17143, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:06:44.140582: step 17144, loss 0.00016573, acc 1\n",
      "2018-10-26T18:06:44.309132: step 17145, loss 5.06634e-07, acc 1\n",
      "2018-10-26T18:06:44.482668: step 17146, loss 1.11942e-06, acc 1\n",
      "2018-10-26T18:06:44.655207: step 17147, loss 7.28281e-07, acc 1\n",
      "2018-10-26T18:06:44.834728: step 17148, loss 0.00136772, acc 1\n",
      "2018-10-26T18:06:44.998291: step 17149, loss 0.00023965, acc 1\n",
      "2018-10-26T18:06:45.173822: step 17150, loss 2.38417e-07, acc 1\n",
      "2018-10-26T18:06:45.336387: step 17151, loss 6.18306e-05, acc 1\n",
      "2018-10-26T18:06:45.512916: step 17152, loss 5.36751e-06, acc 1\n",
      "2018-10-26T18:06:45.680468: step 17153, loss 0.00063841, acc 1\n",
      "2018-10-26T18:06:45.854004: step 17154, loss 8.37093e-06, acc 1\n",
      "2018-10-26T18:06:46.019562: step 17155, loss 2.57094e-05, acc 1\n",
      "2018-10-26T18:06:46.199082: step 17156, loss 4.46512e-05, acc 1\n",
      "2018-10-26T18:06:46.362645: step 17157, loss 1.16226e-06, acc 1\n",
      "2018-10-26T18:06:46.537179: step 17158, loss 1.80666e-06, acc 1\n",
      "2018-10-26T18:06:46.704731: step 17159, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:06:46.883254: step 17160, loss 2.37846e-06, acc 1\n",
      "2018-10-26T18:06:47.047814: step 17161, loss 3.18509e-07, acc 1\n",
      "2018-10-26T18:06:47.219357: step 17162, loss 5.99353e-05, acc 1\n",
      "2018-10-26T18:06:47.380924: step 17163, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:06:47.557453: step 17164, loss 0.0171303, acc 0.984375\n",
      "2018-10-26T18:06:47.722013: step 17165, loss 0.000728029, acc 1\n",
      "2018-10-26T18:06:47.898541: step 17166, loss 0.0091681, acc 1\n",
      "2018-10-26T18:06:48.064099: step 17167, loss 2.72288e-05, acc 1\n",
      "2018-10-26T18:06:48.237635: step 17168, loss 1.00662e-05, acc 1\n",
      "2018-10-26T18:06:48.401198: step 17169, loss 0.000139545, acc 1\n",
      "2018-10-26T18:06:48.580719: step 17170, loss 2.76697e-05, acc 1\n",
      "2018-10-26T18:06:48.752261: step 17171, loss 0.000468842, acc 1\n",
      "2018-10-26T18:06:48.928789: step 17172, loss 7.11193e-06, acc 1\n",
      "2018-10-26T18:06:49.093348: step 17173, loss 0.00068379, acc 1\n",
      "2018-10-26T18:06:49.277856: step 17174, loss 4.69394e-05, acc 1\n",
      "2018-10-26T18:06:49.441419: step 17175, loss 0.0001098, acc 1\n",
      "2018-10-26T18:06:49.615953: step 17176, loss 8.74559e-06, acc 1\n",
      "2018-10-26T18:06:49.783505: step 17177, loss 1.43424e-07, acc 1\n",
      "2018-10-26T18:06:49.956046: step 17178, loss 3.84878e-05, acc 1\n",
      "2018-10-26T18:06:50.131575: step 17179, loss 7.674e-07, acc 1\n",
      "2018-10-26T18:06:50.302120: step 17180, loss 7.32013e-07, acc 1\n",
      "2018-10-26T18:06:50.467677: step 17181, loss 3.87426e-07, acc 1\n",
      "2018-10-26T18:06:50.641213: step 17182, loss 0.019812, acc 0.984375\n",
      "2018-10-26T18:06:50.806771: step 17183, loss 0.000555721, acc 1\n",
      "2018-10-26T18:06:50.984296: step 17184, loss 2.25379e-07, acc 1\n",
      "2018-10-26T18:06:51.148857: step 17185, loss 4.15708e-06, acc 1\n",
      "2018-10-26T18:06:51.324389: step 17186, loss 0, acc 1\n",
      "2018-10-26T18:06:51.488949: step 17187, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:06:51.665476: step 17188, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:06:51.834026: step 17189, loss 0.000806043, acc 1\n",
      "2018-10-26T18:06:52.009558: step 17190, loss 0.00254963, acc 1\n",
      "2018-10-26T18:06:52.173121: step 17191, loss 0.000126385, acc 1\n",
      "2018-10-26T18:06:52.344662: step 17192, loss 0.0107871, acc 1\n",
      "2018-10-26T18:06:52.504235: step 17193, loss 0.0278359, acc 0.984375\n",
      "2018-10-26T18:06:52.682759: step 17194, loss 0.000191323, acc 1\n",
      "2018-10-26T18:06:52.854301: step 17195, loss 1.42463e-05, acc 1\n",
      "2018-10-26T18:06:53.029832: step 17196, loss 3.74347e-06, acc 1\n",
      "2018-10-26T18:06:53.193395: step 17197, loss 8.62158e-05, acc 1\n",
      "2018-10-26T18:06:53.370921: step 17198, loss 0.000171853, acc 1\n",
      "2018-10-26T18:06:53.539469: step 17199, loss 4.96624e-05, acc 1\n",
      "2018-10-26T18:06:53.711012: step 17200, loss 7.17102e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:06:54.151835: step 17200, loss 5.95804, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17200\n",
      "\n",
      "2018-10-26T18:06:54.504658: step 17201, loss 0.000730787, acc 1\n",
      "2018-10-26T18:06:54.675202: step 17202, loss 1.68189e-06, acc 1\n",
      "2018-10-26T18:06:54.856717: step 17203, loss 0.000132924, acc 1\n",
      "2018-10-26T18:06:55.020280: step 17204, loss 7.34647e-06, acc 1\n",
      "2018-10-26T18:06:55.216755: step 17205, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:06:55.435178: step 17206, loss 0, acc 1\n",
      "2018-10-26T18:06:55.616686: step 17207, loss 1.93285e-05, acc 1\n",
      "2018-10-26T18:06:55.795209: step 17208, loss 1.34108e-06, acc 1\n",
      "2018-10-26T18:06:55.963759: step 17209, loss 0.00383502, acc 1\n",
      "2018-10-26T18:06:56.141285: step 17210, loss 4.73108e-07, acc 1\n",
      "2018-10-26T18:06:56.305845: step 17211, loss 4.47035e-08, acc 1\n",
      "2018-10-26T18:06:56.487359: step 17212, loss 3.63834e-05, acc 1\n",
      "2018-10-26T18:06:56.653915: step 17213, loss 8.00936e-08, acc 1\n",
      "2018-10-26T18:06:56.864352: step 17214, loss 3.7033e-05, acc 1\n",
      "2018-10-26T18:06:57.026918: step 17215, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:06:57.209431: step 17216, loss 9.36883e-07, acc 1\n",
      "2018-10-26T18:06:57.377981: step 17217, loss 3.38172e-05, acc 1\n",
      "2018-10-26T18:06:57.553511: step 17218, loss 0.00343231, acc 1\n",
      "2018-10-26T18:06:57.722062: step 17219, loss 0.00341509, acc 1\n",
      "2018-10-26T18:06:57.902578: step 17220, loss 3.30143e-05, acc 1\n",
      "2018-10-26T18:06:58.073124: step 17221, loss 1.09927e-05, acc 1\n",
      "2018-10-26T18:06:58.248654: step 17222, loss 1.72195e-05, acc 1\n",
      "2018-10-26T18:06:58.413214: step 17223, loss 3.1007e-05, acc 1\n",
      "2018-10-26T18:06:58.597721: step 17224, loss 4.69813e-05, acc 1\n",
      "2018-10-26T18:06:58.767268: step 17225, loss 1.60179e-06, acc 1\n",
      "2018-10-26T18:06:58.946788: step 17226, loss 0.0319498, acc 0.984375\n",
      "2018-10-26T18:06:59.109354: step 17227, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:06:59.281893: step 17228, loss 5.45518e-06, acc 1\n",
      "2018-10-26T18:06:59.453435: step 17229, loss 0.0473616, acc 0.984375\n",
      "2018-10-26T18:06:59.620988: step 17230, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:06:59.788540: step 17231, loss 1.90916e-06, acc 1\n",
      "2018-10-26T18:06:59.964070: step 17232, loss 6.7209e-06, acc 1\n",
      "2018-10-26T18:07:00.141596: step 17233, loss 0.00101417, acc 1\n",
      "2018-10-26T18:07:00.323111: step 17234, loss 2.43368e-05, acc 1\n",
      "2018-10-26T18:07:00.489666: step 17235, loss 3.87407e-06, acc 1\n",
      "2018-10-26T18:07:00.662234: step 17236, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:07:00.822775: step 17237, loss 4.50157e-06, acc 1\n",
      "2018-10-26T18:07:00.994318: step 17238, loss 0.00216391, acc 1\n",
      "2018-10-26T18:07:01.165860: step 17239, loss 2.65221e-06, acc 1\n",
      "2018-10-26T18:07:01.341390: step 17240, loss 0.00366706, acc 1\n",
      "2018-10-26T18:07:01.504953: step 17241, loss 1.09932e-05, acc 1\n",
      "2018-10-26T18:07:01.674501: step 17242, loss 8.4709e-06, acc 1\n",
      "2018-10-26T18:07:01.836069: step 17243, loss 0.00282085, acc 1\n",
      "2018-10-26T18:07:02.010603: step 17244, loss 3.94878e-07, acc 1\n",
      "2018-10-26T18:07:02.176160: step 17245, loss 7.83304e-06, acc 1\n",
      "2018-10-26T18:07:02.353686: step 17246, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:07:02.522236: step 17247, loss 3.61349e-07, acc 1\n",
      "2018-10-26T18:07:02.692779: step 17248, loss 4.37637e-05, acc 1\n",
      "2018-10-26T18:07:02.861329: step 17249, loss 0.000756619, acc 1\n",
      "2018-10-26T18:07:03.033869: step 17250, loss 6.95387e-08, acc 1\n",
      "2018-10-26T18:07:03.206407: step 17251, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:07:03.391912: step 17252, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:07:03.558468: step 17253, loss 0.000846524, acc 1\n",
      "2018-10-26T18:07:03.731006: step 17254, loss 9.82057e-06, acc 1\n",
      "2018-10-26T18:07:03.903545: step 17255, loss 0.00015791, acc 1\n",
      "2018-10-26T18:07:04.077081: step 17256, loss 0.000132221, acc 1\n",
      "2018-10-26T18:07:04.246629: step 17257, loss 0.000460737, acc 1\n",
      "2018-10-26T18:07:04.423157: step 17258, loss 1.52737e-07, acc 1\n",
      "2018-10-26T18:07:04.590709: step 17259, loss 0.00019404, acc 1\n",
      "2018-10-26T18:07:04.766240: step 17260, loss 5.92084e-06, acc 1\n",
      "2018-10-26T18:07:04.933792: step 17261, loss 4.21116e-05, acc 1\n",
      "2018-10-26T18:07:05.115307: step 17262, loss 0.00125406, acc 1\n",
      "2018-10-26T18:07:05.283856: step 17263, loss 0.0039754, acc 1\n",
      "2018-10-26T18:07:05.453404: step 17264, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:07:05.622951: step 17265, loss 0.000226063, acc 1\n",
      "2018-10-26T18:07:05.797484: step 17266, loss 4.39578e-07, acc 1\n",
      "2018-10-26T18:07:05.961048: step 17267, loss 0.000200715, acc 1\n",
      "2018-10-26T18:07:06.131591: step 17268, loss 6.12804e-07, acc 1\n",
      "2018-10-26T18:07:06.293160: step 17269, loss 0.000575786, acc 1\n",
      "2018-10-26T18:07:06.471683: step 17270, loss 7.67782e-06, acc 1\n",
      "2018-10-26T18:07:06.638238: step 17271, loss 4.27982e-06, acc 1\n",
      "2018-10-26T18:07:06.810777: step 17272, loss 0.000117607, acc 1\n",
      "2018-10-26T18:07:06.975337: step 17273, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:07:07.151866: step 17274, loss 5.25258e-07, acc 1\n",
      "2018-10-26T18:07:07.318421: step 17275, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:07:07.489962: step 17276, loss 7.74843e-07, acc 1\n",
      "2018-10-26T18:07:07.659509: step 17277, loss 0.000108284, acc 1\n",
      "2018-10-26T18:07:07.834043: step 17278, loss 5.64372e-07, acc 1\n",
      "2018-10-26T18:07:08.001595: step 17279, loss 2.30796e-05, acc 1\n",
      "2018-10-26T18:07:08.177126: step 17280, loss 4.45041e-05, acc 1\n",
      "2018-10-26T18:07:08.343681: step 17281, loss 4.47032e-07, acc 1\n",
      "2018-10-26T18:07:08.514226: step 17282, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:07:08.683773: step 17283, loss 1.45082e-05, acc 1\n",
      "2018-10-26T18:07:08.859304: step 17284, loss 1.91851e-07, acc 1\n",
      "2018-10-26T18:07:09.022867: step 17285, loss 1.39721e-05, acc 1\n",
      "2018-10-26T18:07:09.194408: step 17286, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:07:09.360963: step 17287, loss 5.49664e-05, acc 1\n",
      "2018-10-26T18:07:09.533502: step 17288, loss 0.000329418, acc 1\n",
      "2018-10-26T18:07:09.700056: step 17289, loss 1.5534e-06, acc 1\n",
      "2018-10-26T18:07:09.875588: step 17290, loss 0.00450015, acc 1\n",
      "2018-10-26T18:07:10.049125: step 17291, loss 3.58545e-06, acc 1\n",
      "2018-10-26T18:07:10.226651: step 17292, loss 0.00254127, acc 1\n",
      "2018-10-26T18:07:10.394202: step 17293, loss 4.88035e-05, acc 1\n",
      "2018-10-26T18:07:10.570730: step 17294, loss 1.64236e-05, acc 1\n",
      "2018-10-26T18:07:10.736289: step 17295, loss 2.09099e-05, acc 1\n",
      "2018-10-26T18:07:10.906833: step 17296, loss 9.74286e-05, acc 1\n",
      "2018-10-26T18:07:11.071393: step 17297, loss 0.000295588, acc 1\n",
      "2018-10-26T18:07:11.247922: step 17298, loss 0.0437944, acc 0.984375\n",
      "2018-10-26T18:07:11.410487: step 17299, loss 0.000328262, acc 1\n",
      "2018-10-26T18:07:11.589010: step 17300, loss 2.45324e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:07:12.033821: step 17300, loss 6.09114, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17300\n",
      "\n",
      "2018-10-26T18:07:12.424098: step 17301, loss 3.4687e-05, acc 1\n",
      "2018-10-26T18:07:12.607608: step 17302, loss 4.95058e-06, acc 1\n",
      "2018-10-26T18:07:12.772168: step 17303, loss 1.44773e-05, acc 1\n",
      "2018-10-26T18:07:12.951689: step 17304, loss 6.2608e-05, acc 1\n",
      "2018-10-26T18:07:13.177087: step 17305, loss 2.1959e-06, acc 1\n",
      "2018-10-26T18:07:13.368575: step 17306, loss 0.000170863, acc 1\n",
      "2018-10-26T18:07:13.541114: step 17307, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:07:13.731605: step 17308, loss 0.000400032, acc 1\n",
      "2018-10-26T18:07:13.897163: step 17309, loss 8.06733e-05, acc 1\n",
      "2018-10-26T18:07:14.080671: step 17310, loss 3.12922e-07, acc 1\n",
      "2018-10-26T18:07:14.247249: step 17311, loss 5.62515e-07, acc 1\n",
      "2018-10-26T18:07:14.432731: step 17312, loss 0.000532628, acc 1\n",
      "2018-10-26T18:07:14.595297: step 17313, loss 3.86084e-06, acc 1\n",
      "2018-10-26T18:07:14.774817: step 17314, loss 0.000376506, acc 1\n",
      "2018-10-26T18:07:14.937383: step 17315, loss 0.00241564, acc 1\n",
      "2018-10-26T18:07:15.116904: step 17316, loss 3.57625e-07, acc 1\n",
      "2018-10-26T18:07:15.283458: step 17317, loss 0.0554396, acc 0.984375\n",
      "2018-10-26T18:07:15.458990: step 17318, loss 5.63547e-05, acc 1\n",
      "2018-10-26T18:07:15.627539: step 17319, loss 0.000660135, acc 1\n",
      "2018-10-26T18:07:15.804068: step 17320, loss 0.00048628, acc 1\n",
      "2018-10-26T18:07:15.971620: step 17321, loss 0.0330989, acc 0.984375\n",
      "2018-10-26T18:07:16.146153: step 17322, loss 2.4773e-07, acc 1\n",
      "2018-10-26T18:07:16.312709: step 17323, loss 0.00031532, acc 1\n",
      "2018-10-26T18:07:16.527135: step 17324, loss 2.98029e-05, acc 1\n",
      "2018-10-26T18:07:16.705658: step 17325, loss 2.65564e-05, acc 1\n",
      "2018-10-26T18:07:16.923078: step 17326, loss 1.1753e-06, acc 1\n",
      "2018-10-26T18:07:17.148475: step 17327, loss 0.000116856, acc 1\n",
      "2018-10-26T18:07:17.368886: step 17328, loss 2.87088e-05, acc 1\n",
      "2018-10-26T18:07:17.574337: step 17329, loss 4.33411e-06, acc 1\n",
      "2018-10-26T18:07:17.787767: step 17330, loss 0, acc 1\n",
      "2018-10-26T18:07:18.013165: step 17331, loss 3.90556e-06, acc 1\n",
      "2018-10-26T18:07:18.209640: step 17332, loss 2.54698e-05, acc 1\n",
      "2018-10-26T18:07:18.421076: step 17333, loss 3.83701e-07, acc 1\n",
      "2018-10-26T18:07:18.595609: step 17334, loss 0.000166451, acc 1\n",
      "2018-10-26T18:07:18.824995: step 17335, loss 6.15521e-06, acc 1\n",
      "2018-10-26T18:07:19.016485: step 17336, loss 5.70492e-05, acc 1\n",
      "2018-10-26T18:07:19.197002: step 17337, loss 0.000167797, acc 1\n",
      "2018-10-26T18:07:19.434368: step 17338, loss 0.00106561, acc 1\n",
      "2018-10-26T18:07:19.688688: step 17339, loss 4.59907e-05, acc 1\n",
      "2018-10-26T18:07:19.865216: step 17340, loss 9.88461e-06, acc 1\n",
      "2018-10-26T18:07:20.103580: step 17341, loss 1.95165e-05, acc 1\n",
      "2018-10-26T18:07:20.331969: step 17342, loss 0.000141301, acc 1\n",
      "2018-10-26T18:07:20.571332: step 17343, loss 2.76918e-05, acc 1\n",
      "2018-10-26T18:07:20.821662: step 17344, loss 5.91676e-05, acc 1\n",
      "2018-10-26T18:07:21.031101: step 17345, loss 1.58325e-07, acc 1\n",
      "2018-10-26T18:07:21.272456: step 17346, loss 8.48134e-06, acc 1\n",
      "2018-10-26T18:07:21.447988: step 17347, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:07:21.677375: step 17348, loss 0.000113902, acc 1\n",
      "2018-10-26T18:07:21.848917: step 17349, loss 0, acc 1\n",
      "2018-10-26T18:07:22.056362: step 17350, loss 1.87374e-06, acc 1\n",
      "2018-10-26T18:07:22.283754: step 17351, loss 7.01027e-06, acc 1\n",
      "2018-10-26T18:07:22.483221: step 17352, loss 0.000132209, acc 1\n",
      "2018-10-26T18:07:22.718594: step 17353, loss 0.000398075, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:07:22.928034: step 17354, loss 6.28625e-05, acc 1\n",
      "2018-10-26T18:07:23.101570: step 17355, loss 2.9391e-06, acc 1\n",
      "2018-10-26T18:07:23.328962: step 17356, loss 7.30144e-07, acc 1\n",
      "2018-10-26T18:07:23.507485: step 17357, loss 1.67819e-06, acc 1\n",
      "2018-10-26T18:07:23.684013: step 17358, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:07:23.897446: step 17359, loss 0.000314635, acc 1\n",
      "2018-10-26T18:07:24.092920: step 17360, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:07:24.268451: step 17361, loss 9.12695e-08, acc 1\n",
      "2018-10-26T18:07:24.485870: step 17362, loss 2.40452e-06, acc 1\n",
      "2018-10-26T18:07:24.653423: step 17363, loss 4.08238e-06, acc 1\n",
      "2018-10-26T18:07:24.829951: step 17364, loss 6.36078e-05, acc 1\n",
      "2018-10-26T18:07:25.022437: step 17365, loss 7.9249e-06, acc 1\n",
      "2018-10-26T18:07:25.202955: step 17366, loss 6.46318e-05, acc 1\n",
      "2018-10-26T18:07:25.382476: step 17367, loss 7.13377e-07, acc 1\n",
      "2018-10-26T18:07:25.556012: step 17368, loss 5.83833e-06, acc 1\n",
      "2018-10-26T18:07:25.754481: step 17369, loss 6.21223e-05, acc 1\n",
      "2018-10-26T18:07:25.927020: step 17370, loss 0.000281411, acc 1\n",
      "2018-10-26T18:07:26.103018: step 17371, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:07:26.267579: step 17372, loss 2.05996e-06, acc 1\n",
      "2018-10-26T18:07:26.470038: step 17373, loss 0.0021825, acc 1\n",
      "2018-10-26T18:07:26.639585: step 17374, loss 0.00226268, acc 1\n",
      "2018-10-26T18:07:26.817111: step 17375, loss 6.59363e-07, acc 1\n",
      "2018-10-26T18:07:26.984662: step 17376, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:07:27.164214: step 17377, loss 2.32583e-05, acc 1\n",
      "2018-10-26T18:07:27.331736: step 17378, loss 0.000399022, acc 1\n",
      "2018-10-26T18:07:27.503277: step 17379, loss 9.25719e-07, acc 1\n",
      "2018-10-26T18:07:27.673821: step 17380, loss 3.97639e-06, acc 1\n",
      "2018-10-26T18:07:27.843368: step 17381, loss 0.000204576, acc 1\n",
      "2018-10-26T18:07:28.008926: step 17382, loss 0.000122684, acc 1\n",
      "2018-10-26T18:07:28.182462: step 17383, loss 3.2566e-05, acc 1\n",
      "2018-10-26T18:07:28.346026: step 17384, loss 0.000271765, acc 1\n",
      "2018-10-26T18:07:28.522555: step 17385, loss 3.09196e-07, acc 1\n",
      "2018-10-26T18:07:28.687114: step 17386, loss 3.65076e-07, acc 1\n",
      "2018-10-26T18:07:28.863643: step 17387, loss 1.58608e-05, acc 1\n",
      "2018-10-26T18:07:29.027205: step 17388, loss 2.01165e-07, acc 1\n",
      "2018-10-26T18:07:29.217696: step 17389, loss 9.89891e-06, acc 1\n",
      "2018-10-26T18:07:29.387251: step 17390, loss 1.64278e-06, acc 1\n",
      "2018-10-26T18:07:29.562774: step 17391, loss 8.75442e-08, acc 1\n",
      "2018-10-26T18:07:29.727335: step 17392, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:07:29.904860: step 17393, loss 1.63567e-05, acc 1\n",
      "2018-10-26T18:07:30.073410: step 17394, loss 3.16649e-07, acc 1\n",
      "2018-10-26T18:07:30.257917: step 17395, loss 2.12516e-06, acc 1\n",
      "2018-10-26T18:07:30.423475: step 17396, loss 3.2596e-07, acc 1\n",
      "2018-10-26T18:07:30.597011: step 17397, loss 8.56816e-08, acc 1\n",
      "2018-10-26T18:07:30.764563: step 17398, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:07:30.940094: step 17399, loss 2.28882e-05, acc 1\n",
      "2018-10-26T18:07:31.098671: step 17400, loss 0.000102253, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:07:31.542486: step 17400, loss 6.11042, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17400\n",
      "\n",
      "2018-10-26T18:07:31.913711: step 17401, loss 5.90448e-07, acc 1\n",
      "2018-10-26T18:07:32.079270: step 17402, loss 1.95193e-06, acc 1\n",
      "2018-10-26T18:07:32.257792: step 17403, loss 6.56721e-05, acc 1\n",
      "2018-10-26T18:07:32.423350: step 17404, loss 2.03015e-06, acc 1\n",
      "2018-10-26T18:07:32.620822: step 17405, loss 0.0472283, acc 0.984375\n",
      "2018-10-26T18:07:32.852203: step 17406, loss 5.28722e-06, acc 1\n",
      "2018-10-26T18:07:33.020753: step 17407, loss 6.94759e-07, acc 1\n",
      "2018-10-26T18:07:33.197282: step 17408, loss 0.000177424, acc 1\n",
      "2018-10-26T18:07:33.371815: step 17409, loss 0.000142861, acc 1\n",
      "2018-10-26T18:07:33.551336: step 17410, loss 0.000180961, acc 1\n",
      "2018-10-26T18:07:33.721880: step 17411, loss 8.84731e-07, acc 1\n",
      "2018-10-26T18:07:33.894418: step 17412, loss 0.00102003, acc 1\n",
      "2018-10-26T18:07:34.057982: step 17413, loss 0.00255921, acc 1\n",
      "2018-10-26T18:07:34.239497: step 17414, loss 1.71172e-06, acc 1\n",
      "2018-10-26T18:07:34.401065: step 17415, loss 3.50329e-06, acc 1\n",
      "2018-10-26T18:07:34.574602: step 17416, loss 1.1399e-06, acc 1\n",
      "2018-10-26T18:07:34.745145: step 17417, loss 6.19808e-06, acc 1\n",
      "2018-10-26T18:07:34.922671: step 17418, loss 1.78703e-05, acc 1\n",
      "2018-10-26T18:07:35.092218: step 17419, loss 0.00379938, acc 1\n",
      "2018-10-26T18:07:35.269744: step 17420, loss 5.67451e-06, acc 1\n",
      "2018-10-26T18:07:35.434304: step 17421, loss 4.67519e-07, acc 1\n",
      "2018-10-26T18:07:35.608838: step 17422, loss 3.86451e-06, acc 1\n",
      "2018-10-26T18:07:35.774396: step 17423, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:07:35.951921: step 17424, loss 0, acc 1\n",
      "2018-10-26T18:07:36.120472: step 17425, loss 1.96426e-05, acc 1\n",
      "2018-10-26T18:07:36.293010: step 17426, loss 2.34726e-05, acc 1\n",
      "2018-10-26T18:07:36.462557: step 17427, loss 6.67259e-05, acc 1\n",
      "2018-10-26T18:07:36.636094: step 17428, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:07:36.803646: step 17429, loss 5.06028e-06, acc 1\n",
      "2018-10-26T18:07:36.978180: step 17430, loss 0.000590712, acc 1\n",
      "2018-10-26T18:07:37.150718: step 17431, loss 1.81913e-05, acc 1\n",
      "2018-10-26T18:07:37.329242: step 17432, loss 2.21653e-07, acc 1\n",
      "2018-10-26T18:07:37.500783: step 17433, loss 6.32436e-05, acc 1\n",
      "2018-10-26T18:07:37.668336: step 17434, loss 0.000357682, acc 1\n",
      "2018-10-26T18:07:37.853840: step 17435, loss 0.0372237, acc 0.984375\n",
      "2018-10-26T18:07:38.025381: step 17436, loss 5.60647e-05, acc 1\n",
      "2018-10-26T18:07:38.199916: step 17437, loss 6.17699e-05, acc 1\n",
      "2018-10-26T18:07:38.366470: step 17438, loss 5.19671e-07, acc 1\n",
      "2018-10-26T18:07:38.541004: step 17439, loss 0.00765978, acc 1\n",
      "2018-10-26T18:07:38.715538: step 17440, loss 4.80489e-06, acc 1\n",
      "2018-10-26T18:07:38.900044: step 17441, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:07:39.062611: step 17442, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:07:39.247119: step 17443, loss 0.000265493, acc 1\n",
      "2018-10-26T18:07:39.412675: step 17444, loss 5.4683e-06, acc 1\n",
      "2018-10-26T18:07:39.592195: step 17445, loss 1.70425e-06, acc 1\n",
      "2018-10-26T18:07:39.757753: step 17446, loss 5.20003e-05, acc 1\n",
      "2018-10-26T18:07:39.944254: step 17447, loss 0.000157148, acc 1\n",
      "2018-10-26T18:07:40.109812: step 17448, loss 4.68777e-05, acc 1\n",
      "2018-10-26T18:07:40.285344: step 17449, loss 1.6889e-05, acc 1\n",
      "2018-10-26T18:07:40.449907: step 17450, loss 0.000936025, acc 1\n",
      "2018-10-26T18:07:40.629424: step 17451, loss 8.62642e-05, acc 1\n",
      "2018-10-26T18:07:40.792987: step 17452, loss 0.061316, acc 0.984375\n",
      "2018-10-26T18:07:40.972507: step 17453, loss 0.000166099, acc 1\n",
      "2018-10-26T18:07:41.137067: step 17454, loss 1.73589e-06, acc 1\n",
      "2018-10-26T18:07:41.319581: step 17455, loss 2.6262e-06, acc 1\n",
      "2018-10-26T18:07:41.492118: step 17456, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:07:41.663660: step 17457, loss 1.81226e-06, acc 1\n",
      "2018-10-26T18:07:41.828221: step 17458, loss 8.76129e-06, acc 1\n",
      "2018-10-26T18:07:42.003753: step 17459, loss 5.04327e-06, acc 1\n",
      "2018-10-26T18:07:42.173299: step 17460, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:07:42.353817: step 17461, loss 8.60845e-05, acc 1\n",
      "2018-10-26T18:07:42.531343: step 17462, loss 0.00121054, acc 1\n",
      "2018-10-26T18:07:42.716847: step 17463, loss 4.11642e-07, acc 1\n",
      "2018-10-26T18:07:42.882405: step 17464, loss 0.000822774, acc 1\n",
      "2018-10-26T18:07:43.057935: step 17465, loss 0.00193873, acc 1\n",
      "2018-10-26T18:07:43.225487: step 17466, loss 0.000349806, acc 1\n",
      "2018-10-26T18:07:43.405007: step 17467, loss 7.68541e-06, acc 1\n",
      "2018-10-26T18:07:43.568571: step 17468, loss 0.000632591, acc 1\n",
      "2018-10-26T18:07:43.745100: step 17469, loss 1.24214e-05, acc 1\n",
      "2018-10-26T18:07:43.911655: step 17470, loss 5.21532e-07, acc 1\n",
      "2018-10-26T18:07:44.097159: step 17471, loss 1.65581e-06, acc 1\n",
      "2018-10-26T18:07:44.258726: step 17472, loss 0.000347402, acc 1\n",
      "2018-10-26T18:07:44.437250: step 17473, loss 0.000247067, acc 1\n",
      "2018-10-26T18:07:44.599815: step 17474, loss 0.00827742, acc 1\n",
      "2018-10-26T18:07:44.779336: step 17475, loss 3.12899e-06, acc 1\n",
      "2018-10-26T18:07:44.948883: step 17476, loss 2.27242e-07, acc 1\n",
      "2018-10-26T18:07:45.132394: step 17477, loss 0, acc 1\n",
      "2018-10-26T18:07:45.294959: step 17478, loss 0.000130397, acc 1\n",
      "2018-10-26T18:07:45.473481: step 17479, loss 0.04056, acc 0.984375\n",
      "2018-10-26T18:07:45.639040: step 17480, loss 1.06643e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:07:45.820554: step 17481, loss 0.00011544, acc 1\n",
      "2018-10-26T18:07:45.983120: step 17482, loss 3.80686e-06, acc 1\n",
      "2018-10-26T18:07:46.154661: step 17483, loss 4.91916e-05, acc 1\n",
      "2018-10-26T18:07:46.323211: step 17484, loss 4.94268e-06, acc 1\n",
      "2018-10-26T18:07:46.496747: step 17485, loss 8.80102e-05, acc 1\n",
      "2018-10-26T18:07:46.670284: step 17486, loss 2.53318e-07, acc 1\n",
      "2018-10-26T18:07:46.839830: step 17487, loss 0.0003312, acc 1\n",
      "2018-10-26T18:07:47.007383: step 17488, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:07:47.182914: step 17489, loss 0.103875, acc 0.984375\n",
      "2018-10-26T18:07:47.349470: step 17490, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:07:47.520013: step 17491, loss 4.23568e-05, acc 1\n",
      "2018-10-26T18:07:47.684574: step 17492, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:07:47.857112: step 17493, loss 0.00463655, acc 1\n",
      "2018-10-26T18:07:48.023667: step 17494, loss 4.35717e-05, acc 1\n",
      "2018-10-26T18:07:48.206180: step 17495, loss 7.93673e-05, acc 1\n",
      "2018-10-26T18:07:48.376724: step 17496, loss 1.27186e-05, acc 1\n",
      "2018-10-26T18:07:48.551258: step 17497, loss 0, acc 1\n",
      "2018-10-26T18:07:48.720805: step 17498, loss 2.5518e-07, acc 1\n",
      "2018-10-26T18:07:48.893344: step 17499, loss 1.73653e-05, acc 1\n",
      "2018-10-26T18:07:49.058902: step 17500, loss 2.88686e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:07:49.496732: step 17500, loss 6.12736, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17500\n",
      "\n",
      "2018-10-26T18:07:49.807270: step 17501, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:07:49.975820: step 17502, loss 0, acc 1\n",
      "2018-10-26T18:07:50.151350: step 17503, loss 3.07335e-07, acc 1\n",
      "2018-10-26T18:07:50.316907: step 17504, loss 1.93057e-05, acc 1\n",
      "2018-10-26T18:07:50.492438: step 17505, loss 0.0123023, acc 0.984375\n",
      "2018-10-26T18:07:50.742770: step 17506, loss 0.00667322, acc 1\n",
      "2018-10-26T18:07:50.913314: step 17507, loss 0.000154838, acc 1\n",
      "2018-10-26T18:07:51.085853: step 17508, loss 4.11642e-07, acc 1\n",
      "2018-10-26T18:07:51.254403: step 17509, loss 9.12695e-08, acc 1\n",
      "2018-10-26T18:07:51.427939: step 17510, loss 2.31148e-06, acc 1\n",
      "2018-10-26T18:07:51.595492: step 17511, loss 0.000404462, acc 1\n",
      "2018-10-26T18:07:51.775013: step 17512, loss 9.12666e-06, acc 1\n",
      "2018-10-26T18:07:51.943562: step 17513, loss 0.000369942, acc 1\n",
      "2018-10-26T18:07:52.118096: step 17514, loss 0.000503139, acc 1\n",
      "2018-10-26T18:07:52.287643: step 17515, loss 2.4548e-06, acc 1\n",
      "2018-10-26T18:07:52.459184: step 17516, loss 2.91477e-06, acc 1\n",
      "2018-10-26T18:07:52.630726: step 17517, loss 1.11943e-06, acc 1\n",
      "2018-10-26T18:07:52.808252: step 17518, loss 0.118279, acc 0.984375\n",
      "2018-10-26T18:07:52.977799: step 17519, loss 3.58792e-05, acc 1\n",
      "2018-10-26T18:07:53.151335: step 17520, loss 8.32589e-07, acc 1\n",
      "2018-10-26T18:07:53.315895: step 17521, loss 9.7239e-06, acc 1\n",
      "2018-10-26T18:07:53.494418: step 17522, loss 0.000199024, acc 1\n",
      "2018-10-26T18:07:53.663965: step 17523, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:07:53.842488: step 17524, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:07:54.008047: step 17525, loss 3.116e-05, acc 1\n",
      "2018-10-26T18:07:54.184574: step 17526, loss 1.52175e-06, acc 1\n",
      "2018-10-26T18:07:54.355119: step 17527, loss 5.17878e-05, acc 1\n",
      "2018-10-26T18:07:54.536635: step 17528, loss 2.14203e-07, acc 1\n",
      "2018-10-26T18:07:54.708176: step 17529, loss 0.000388717, acc 1\n",
      "2018-10-26T18:07:54.890688: step 17530, loss 5.53522e-06, acc 1\n",
      "2018-10-26T18:07:55.055264: step 17531, loss 0.00167475, acc 1\n",
      "2018-10-26T18:07:55.227786: step 17532, loss 1.0507e-05, acc 1\n",
      "2018-10-26T18:07:55.392346: step 17533, loss 4.69515e-05, acc 1\n",
      "2018-10-26T18:07:55.568876: step 17534, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:07:55.731441: step 17535, loss 1.45312e-05, acc 1\n",
      "2018-10-26T18:07:55.908968: step 17536, loss 0.00679466, acc 1\n",
      "2018-10-26T18:07:56.080509: step 17537, loss 4.48091e-06, acc 1\n",
      "2018-10-26T18:07:56.261027: step 17538, loss 0.00582525, acc 1\n",
      "2018-10-26T18:07:56.423591: step 17539, loss 6.18298e-05, acc 1\n",
      "2018-10-26T18:07:56.604110: step 17540, loss 5.90041e-05, acc 1\n",
      "2018-10-26T18:07:56.774653: step 17541, loss 7.02208e-07, acc 1\n",
      "2018-10-26T18:07:56.950185: step 17542, loss 5.2498e-05, acc 1\n",
      "2018-10-26T18:07:57.121726: step 17543, loss 3.53306e-05, acc 1\n",
      "2018-10-26T18:07:57.300249: step 17544, loss 0.000527241, acc 1\n",
      "2018-10-26T18:07:57.465807: step 17545, loss 6.89672e-06, acc 1\n",
      "2018-10-26T18:07:57.644330: step 17546, loss 0.00105125, acc 1\n",
      "2018-10-26T18:07:57.816870: step 17547, loss 1.34543e-05, acc 1\n",
      "2018-10-26T18:07:57.998384: step 17548, loss 7.60917e-06, acc 1\n",
      "2018-10-26T18:07:58.161949: step 17549, loss 0.0187584, acc 0.984375\n",
      "2018-10-26T18:07:58.329499: step 17550, loss 5.16573e-08, acc 1\n",
      "2018-10-26T18:07:58.502039: step 17551, loss 0.000566109, acc 1\n",
      "2018-10-26T18:07:58.681559: step 17552, loss 1.43046e-06, acc 1\n",
      "2018-10-26T18:07:58.854099: step 17553, loss 1.06842e-05, acc 1\n",
      "2018-10-26T18:07:59.025639: step 17554, loss 0.0010002, acc 1\n",
      "2018-10-26T18:07:59.234083: step 17555, loss 0.00327199, acc 1\n",
      "2018-10-26T18:07:59.410611: step 17556, loss 1.35784e-06, acc 1\n",
      "2018-10-26T18:07:59.584147: step 17557, loss 3.87428e-07, acc 1\n",
      "2018-10-26T18:07:59.749705: step 17558, loss 5.56925e-07, acc 1\n",
      "2018-10-26T18:07:59.928228: step 17559, loss 0.00053, acc 1\n",
      "2018-10-26T18:08:00.105753: step 17560, loss 5.42542e-06, acc 1\n",
      "2018-10-26T18:08:00.332150: step 17561, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:08:00.501696: step 17562, loss 2.97627e-06, acc 1\n",
      "2018-10-26T18:08:00.683211: step 17563, loss 5.73996e-05, acc 1\n",
      "2018-10-26T18:08:00.854752: step 17564, loss 2.13263e-06, acc 1\n",
      "2018-10-26T18:08:01.033275: step 17565, loss 1.08871e-05, acc 1\n",
      "2018-10-26T18:08:01.196838: step 17566, loss 0.000168793, acc 1\n",
      "2018-10-26T18:08:01.371372: step 17567, loss 1.95757e-06, acc 1\n",
      "2018-10-26T18:08:01.535932: step 17568, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:08:01.717447: step 17569, loss 0.00041133, acc 1\n",
      "2018-10-26T18:08:01.883006: step 17570, loss 3.15562e-05, acc 1\n",
      "2018-10-26T18:08:02.062525: step 17571, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:08:02.227086: step 17572, loss 0.0260859, acc 0.984375\n",
      "2018-10-26T18:08:02.406606: step 17573, loss 5.5134e-07, acc 1\n",
      "2018-10-26T18:08:02.575156: step 17574, loss 0.000487888, acc 1\n",
      "2018-10-26T18:08:02.757668: step 17575, loss 0.000196889, acc 1\n",
      "2018-10-26T18:08:02.928213: step 17576, loss 5.10884e-05, acc 1\n",
      "2018-10-26T18:08:03.107733: step 17577, loss 1.44085e-05, acc 1\n",
      "2018-10-26T18:08:03.275286: step 17578, loss 8.79146e-07, acc 1\n",
      "2018-10-26T18:08:03.455803: step 17579, loss 5.49472e-07, acc 1\n",
      "2018-10-26T18:08:03.632330: step 17580, loss 5.7741e-07, acc 1\n",
      "2018-10-26T18:08:03.808860: step 17581, loss 8.18033e-05, acc 1\n",
      "2018-10-26T18:08:03.989377: step 17582, loss 5.79275e-07, acc 1\n",
      "2018-10-26T18:08:04.166902: step 17583, loss 1.05017e-05, acc 1\n",
      "2018-10-26T18:08:04.331463: step 17584, loss 0.000225399, acc 1\n",
      "2018-10-26T18:08:04.513976: step 17585, loss 0.000449489, acc 1\n",
      "2018-10-26T18:08:04.687512: step 17586, loss 2.26485e-06, acc 1\n",
      "2018-10-26T18:08:04.860051: step 17587, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:08:05.032591: step 17588, loss 4.81996e-06, acc 1\n",
      "2018-10-26T18:08:05.213108: step 17589, loss 1.072e-05, acc 1\n",
      "2018-10-26T18:08:05.386645: step 17590, loss 2.6245e-05, acc 1\n",
      "2018-10-26T18:08:05.563172: step 17591, loss 1.11341e-05, acc 1\n",
      "2018-10-26T18:08:05.729727: step 17592, loss 0.00559466, acc 1\n",
      "2018-10-26T18:08:05.912239: step 17593, loss 3.33878e-05, acc 1\n",
      "2018-10-26T18:08:06.074806: step 17594, loss 2.19791e-07, acc 1\n",
      "2018-10-26T18:08:06.259312: step 17595, loss 2.30029e-05, acc 1\n",
      "2018-10-26T18:08:06.438832: step 17596, loss 3.42723e-07, acc 1\n",
      "2018-10-26T18:08:06.618353: step 17597, loss 0.000747223, acc 1\n",
      "2018-10-26T18:08:06.796876: step 17598, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:08:06.972407: step 17599, loss 1.25352e-06, acc 1\n",
      "2018-10-26T18:08:07.135970: step 17600, loss 1.45843e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:08:07.581778: step 17600, loss 6.38793, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17600\n",
      "\n",
      "2018-10-26T18:08:07.960122: step 17601, loss 6.64247e-05, acc 1\n",
      "2018-10-26T18:08:08.141637: step 17602, loss 3.19042e-06, acc 1\n",
      "2018-10-26T18:08:08.305200: step 17603, loss 2.75469e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:08:08.491702: step 17604, loss 1.61112e-06, acc 1\n",
      "2018-10-26T18:08:08.701143: step 17605, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:08:08.938508: step 17606, loss 0.000261759, acc 1\n",
      "2018-10-26T18:08:09.126006: step 17607, loss 5.58784e-07, acc 1\n",
      "2018-10-26T18:08:09.329464: step 17608, loss 4.33289e-05, acc 1\n",
      "2018-10-26T18:08:09.506990: step 17609, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:08:09.687506: step 17610, loss 4.82423e-07, acc 1\n",
      "2018-10-26T18:08:09.856056: step 17611, loss 2.04307e-05, acc 1\n",
      "2018-10-26T18:08:10.025603: step 17612, loss 1.61038e-05, acc 1\n",
      "2018-10-26T18:08:10.193156: step 17613, loss 0.000794439, acc 1\n",
      "2018-10-26T18:08:10.368686: step 17614, loss 5.66885e-05, acc 1\n",
      "2018-10-26T18:08:10.532250: step 17615, loss 1.42985e-05, acc 1\n",
      "2018-10-26T18:08:10.704788: step 17616, loss 6.40594e-05, acc 1\n",
      "2018-10-26T18:08:10.875333: step 17617, loss 3.00733e-05, acc 1\n",
      "2018-10-26T18:08:11.057846: step 17618, loss 5.32213e-05, acc 1\n",
      "2018-10-26T18:08:11.228390: step 17619, loss 3.09745e-06, acc 1\n",
      "2018-10-26T18:08:11.414891: step 17620, loss 2.18648e-05, acc 1\n",
      "2018-10-26T18:08:11.580449: step 17621, loss 3.28091e-05, acc 1\n",
      "2018-10-26T18:08:11.779916: step 17622, loss 1.49566e-06, acc 1\n",
      "2018-10-26T18:08:11.948465: step 17623, loss 0.00142802, acc 1\n",
      "2018-10-26T18:08:12.119010: step 17624, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:08:12.286562: step 17625, loss 0.00140952, acc 1\n",
      "2018-10-26T18:08:12.461096: step 17626, loss 0.000247219, acc 1\n",
      "2018-10-26T18:08:12.624659: step 17627, loss 3.50339e-06, acc 1\n",
      "2018-10-26T18:08:12.798196: step 17628, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:08:12.969737: step 17629, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:08:13.140281: step 17630, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:08:13.313818: step 17631, loss 3.89069e-06, acc 1\n",
      "2018-10-26T18:08:13.489349: step 17632, loss 5.06185e-06, acc 1\n",
      "2018-10-26T18:08:13.655904: step 17633, loss 0.000284427, acc 1\n",
      "2018-10-26T18:08:13.838416: step 17634, loss 1.16391e-05, acc 1\n",
      "2018-10-26T18:08:14.006965: step 17635, loss 0.00126835, acc 1\n",
      "2018-10-26T18:08:14.176512: step 17636, loss 1.29084e-05, acc 1\n",
      "2018-10-26T18:08:14.341073: step 17637, loss 0.0328656, acc 0.984375\n",
      "2018-10-26T18:08:14.518598: step 17638, loss 9.49947e-08, acc 1\n",
      "2018-10-26T18:08:14.685153: step 17639, loss 3.84614e-06, acc 1\n",
      "2018-10-26T18:08:14.866669: step 17640, loss 6.57615e-05, acc 1\n",
      "2018-10-26T18:08:15.035219: step 17641, loss 4.60598e-06, acc 1\n",
      "2018-10-26T18:08:15.213741: step 17642, loss 1.67638e-07, acc 1\n",
      "2018-10-26T18:08:15.379299: step 17643, loss 8.60869e-06, acc 1\n",
      "2018-10-26T18:08:15.556825: step 17644, loss 4.95527e-05, acc 1\n",
      "2018-10-26T18:08:15.724377: step 17645, loss 2.20512e-05, acc 1\n",
      "2018-10-26T18:08:15.899914: step 17646, loss 0.0937193, acc 0.984375\n",
      "2018-10-26T18:08:16.068458: step 17647, loss 2.26761e-05, acc 1\n",
      "2018-10-26T18:08:16.241994: step 17648, loss 3.63372e-06, acc 1\n",
      "2018-10-26T18:08:16.411541: step 17649, loss 1.8401e-05, acc 1\n",
      "2018-10-26T18:08:16.591061: step 17650, loss 7.11522e-07, acc 1\n",
      "2018-10-26T18:08:16.752630: step 17651, loss 0, acc 1\n",
      "2018-10-26T18:08:16.926167: step 17652, loss 6.96614e-07, acc 1\n",
      "2018-10-26T18:08:17.096710: step 17653, loss 3.12923e-07, acc 1\n",
      "2018-10-26T18:08:17.276231: step 17654, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:08:17.444781: step 17655, loss 1.26857e-05, acc 1\n",
      "2018-10-26T18:08:17.632298: step 17656, loss 3.96739e-07, acc 1\n",
      "2018-10-26T18:08:17.797837: step 17657, loss 2.70312e-05, acc 1\n",
      "2018-10-26T18:08:17.970376: step 17658, loss 0, acc 1\n",
      "2018-10-26T18:08:18.132941: step 17659, loss 3.3726e-05, acc 1\n",
      "2018-10-26T18:08:18.308472: step 17660, loss 0.000424247, acc 1\n",
      "2018-10-26T18:08:18.477023: step 17661, loss 6.44467e-07, acc 1\n",
      "2018-10-26T18:08:18.650558: step 17662, loss 0.00414704, acc 1\n",
      "2018-10-26T18:08:18.821102: step 17663, loss 1.20993e-05, acc 1\n",
      "2018-10-26T18:08:18.993641: step 17664, loss 8.68124e-06, acc 1\n",
      "2018-10-26T18:08:19.163189: step 17665, loss 0.000144601, acc 1\n",
      "2018-10-26T18:08:19.341712: step 17666, loss 0.000963344, acc 1\n",
      "2018-10-26T18:08:19.506272: step 17667, loss 6.68428e-06, acc 1\n",
      "2018-10-26T18:08:19.682800: step 17668, loss 1.27586e-06, acc 1\n",
      "2018-10-26T18:08:19.848358: step 17669, loss 0.0439905, acc 0.984375\n",
      "2018-10-26T18:08:20.030870: step 17670, loss 5.40122e-06, acc 1\n",
      "2018-10-26T18:08:20.196429: step 17671, loss 3.54033e-05, acc 1\n",
      "2018-10-26T18:08:20.373953: step 17672, loss 1.13534e-05, acc 1\n",
      "2018-10-26T18:08:20.543502: step 17673, loss 6.29471e-06, acc 1\n",
      "2018-10-26T18:08:20.719032: step 17674, loss 1.30194e-06, acc 1\n",
      "2018-10-26T18:08:20.885587: step 17675, loss 8.19556e-07, acc 1\n",
      "2018-10-26T18:08:21.062115: step 17676, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:08:21.231662: step 17677, loss 0.000114189, acc 1\n",
      "2018-10-26T18:08:21.404201: step 17678, loss 4.10602e-05, acc 1\n",
      "2018-10-26T18:08:21.573748: step 17679, loss 3.2596e-07, acc 1\n",
      "2018-10-26T18:08:21.756261: step 17680, loss 1.9744e-07, acc 1\n",
      "2018-10-26T18:08:21.923813: step 17681, loss 6.55642e-07, acc 1\n",
      "2018-10-26T18:08:22.137242: step 17682, loss 0.0498392, acc 0.984375\n",
      "2018-10-26T18:08:22.342694: step 17683, loss 0.0014472, acc 1\n",
      "2018-10-26T18:08:22.527201: step 17684, loss 5.1095e-05, acc 1\n",
      "2018-10-26T18:08:22.764566: step 17685, loss 0.000148493, acc 1\n",
      "2018-10-26T18:08:22.956054: step 17686, loss 7.85156e-05, acc 1\n",
      "2018-10-26T18:08:23.183447: step 17687, loss 5.14082e-07, acc 1\n",
      "2018-10-26T18:08:23.389896: step 17688, loss 1.63642e-05, acc 1\n",
      "2018-10-26T18:08:23.598339: step 17689, loss 9.89262e-05, acc 1\n",
      "2018-10-26T18:08:23.825732: step 17690, loss 0.000212789, acc 1\n",
      "2018-10-26T18:08:24.005251: step 17691, loss 1.03561e-06, acc 1\n",
      "2018-10-26T18:08:24.215689: step 17692, loss 1.97889e-05, acc 1\n",
      "2018-10-26T18:08:24.427125: step 17693, loss 7.82054e-06, acc 1\n",
      "2018-10-26T18:08:24.611632: step 17694, loss 6.29824e-05, acc 1\n",
      "2018-10-26T18:08:24.815088: step 17695, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:08:25.008571: step 17696, loss 0.117349, acc 0.984375\n",
      "2018-10-26T18:08:25.210033: step 17697, loss 0.00315644, acc 1\n",
      "2018-10-26T18:08:25.430443: step 17698, loss 0.000203783, acc 1\n",
      "2018-10-26T18:08:25.655844: step 17699, loss 7.99064e-07, acc 1\n",
      "2018-10-26T18:08:25.857303: step 17700, loss 9.79798e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:08:26.422792: step 17700, loss 6.30925, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17700\n",
      "\n",
      "2018-10-26T18:08:26.791806: step 17701, loss 0.00271107, acc 1\n",
      "2018-10-26T18:08:26.990276: step 17702, loss 0.000110409, acc 1\n",
      "2018-10-26T18:08:27.162814: step 17703, loss 0.000105081, acc 1\n",
      "2018-10-26T18:08:27.388212: step 17704, loss 8.36309e-07, acc 1\n",
      "2018-10-26T18:08:27.642533: step 17705, loss 1.51233e-05, acc 1\n",
      "2018-10-26T18:08:27.887878: step 17706, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:08:28.111281: step 17707, loss 2.89536e-05, acc 1\n",
      "2018-10-26T18:08:28.297783: step 17708, loss 4.87036e-05, acc 1\n",
      "2018-10-26T18:08:28.514205: step 17709, loss 4.74459e-05, acc 1\n",
      "2018-10-26T18:08:28.703699: step 17710, loss 0.000634553, acc 1\n",
      "2018-10-26T18:08:28.913138: step 17711, loss 3.59649e-06, acc 1\n",
      "2018-10-26T18:08:29.088669: step 17712, loss 4.61929e-07, acc 1\n",
      "2018-10-26T18:08:29.318057: step 17713, loss 1.1896e-05, acc 1\n",
      "2018-10-26T18:08:29.507550: step 17714, loss 7.87881e-07, acc 1\n",
      "2018-10-26T18:08:29.683081: step 17715, loss 9.65755e-05, acc 1\n",
      "2018-10-26T18:08:29.903493: step 17716, loss 3.75869e-05, acc 1\n",
      "2018-10-26T18:08:30.144861: step 17717, loss 1.68079e-05, acc 1\n",
      "2018-10-26T18:08:30.316389: step 17718, loss 0.000750575, acc 1\n",
      "2018-10-26T18:08:30.531814: step 17719, loss 2.12943e-05, acc 1\n",
      "2018-10-26T18:08:30.693382: step 17720, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:08:30.910801: step 17721, loss 1.4081e-06, acc 1\n",
      "2018-10-26T18:08:31.076359: step 17722, loss 2.49212e-06, acc 1\n",
      "2018-10-26T18:08:31.254881: step 17723, loss 1.37272e-06, acc 1\n",
      "2018-10-26T18:08:31.464322: step 17724, loss 9.71203e-06, acc 1\n",
      "2018-10-26T18:08:31.641848: step 17725, loss 0.00135135, acc 1\n",
      "2018-10-26T18:08:31.824373: step 17726, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:08:32.034798: step 17727, loss 5.12224e-07, acc 1\n",
      "2018-10-26T18:08:32.211326: step 17728, loss 0.0343857, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:08:32.400820: step 17729, loss 6.66815e-07, acc 1\n",
      "2018-10-26T18:08:32.568393: step 17730, loss 3.85706e-06, acc 1\n",
      "2018-10-26T18:08:32.771830: step 17731, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:08:32.947360: step 17732, loss 1.91015e-05, acc 1\n",
      "2018-10-26T18:08:33.118901: step 17733, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:08:33.295429: step 17734, loss 9.95496e-06, acc 1\n",
      "2018-10-26T18:08:33.459990: step 17735, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:08:33.630534: step 17736, loss 4.93596e-07, acc 1\n",
      "2018-10-26T18:08:33.800081: step 17737, loss 0.000180139, acc 1\n",
      "2018-10-26T18:08:33.974615: step 17738, loss 1.76951e-07, acc 1\n",
      "2018-10-26T18:08:34.140173: step 17739, loss 2.95202e-06, acc 1\n",
      "2018-10-26T18:08:34.313709: step 17740, loss 8.98152e-05, acc 1\n",
      "2018-10-26T18:08:34.493230: step 17741, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:08:34.668761: step 17742, loss 4.25372e-06, acc 1\n",
      "2018-10-26T18:08:34.834318: step 17743, loss 3.2094e-05, acc 1\n",
      "2018-10-26T18:08:35.003865: step 17744, loss 0.000286253, acc 1\n",
      "2018-10-26T18:08:35.167428: step 17745, loss 3.5796e-06, acc 1\n",
      "2018-10-26T18:08:35.339967: step 17746, loss 9.74143e-07, acc 1\n",
      "2018-10-26T18:08:35.504527: step 17747, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:08:35.674074: step 17748, loss 0.000446645, acc 1\n",
      "2018-10-26T18:08:35.838635: step 17749, loss 9.57381e-07, acc 1\n",
      "2018-10-26T18:08:36.017158: step 17750, loss 1.09706e-06, acc 1\n",
      "2018-10-26T18:08:36.183712: step 17751, loss 7.17963e-05, acc 1\n",
      "2018-10-26T18:08:36.361238: step 17752, loss 2.90191e-05, acc 1\n",
      "2018-10-26T18:08:36.529788: step 17753, loss 2.89819e-06, acc 1\n",
      "2018-10-26T18:08:36.704322: step 17754, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:08:36.868882: step 17755, loss 2.91365e-05, acc 1\n",
      "2018-10-26T18:08:37.045410: step 17756, loss 0.0118434, acc 0.984375\n",
      "2018-10-26T18:08:37.217949: step 17757, loss 2.26668e-06, acc 1\n",
      "2018-10-26T18:08:37.395475: step 17758, loss 5.41325e-05, acc 1\n",
      "2018-10-26T18:08:37.569011: step 17759, loss 0.000376591, acc 1\n",
      "2018-10-26T18:08:37.740553: step 17760, loss 2.25741e-06, acc 1\n",
      "2018-10-26T18:08:37.912095: step 17761, loss 4.64709e-05, acc 1\n",
      "2018-10-26T18:08:38.080644: step 17762, loss 2.51447e-06, acc 1\n",
      "2018-10-26T18:08:38.251189: step 17763, loss 2.51456e-07, acc 1\n",
      "2018-10-26T18:08:38.425722: step 17764, loss 0.00327699, acc 1\n",
      "2018-10-26T18:08:38.589285: step 17765, loss 1.01326e-06, acc 1\n",
      "2018-10-26T18:08:38.766811: step 17766, loss 0.00039222, acc 1\n",
      "2018-10-26T18:08:38.930374: step 17767, loss 0, acc 1\n",
      "2018-10-26T18:08:39.107899: step 17768, loss 6.89292e-06, acc 1\n",
      "2018-10-26T18:08:39.277447: step 17769, loss 4.51627e-06, acc 1\n",
      "2018-10-26T18:08:39.449985: step 17770, loss 1.33107e-05, acc 1\n",
      "2018-10-26T18:08:39.618534: step 17771, loss 9.49947e-08, acc 1\n",
      "2018-10-26T18:08:39.796061: step 17772, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:08:39.966606: step 17773, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:08:40.137150: step 17774, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:08:40.309689: step 17775, loss 5.85959e-05, acc 1\n",
      "2018-10-26T18:08:40.495194: step 17776, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:08:40.658756: step 17777, loss 6.37012e-07, acc 1\n",
      "2018-10-26T18:08:40.835285: step 17778, loss 0.000133328, acc 1\n",
      "2018-10-26T18:08:41.002837: step 17779, loss 2.62632e-07, acc 1\n",
      "2018-10-26T18:08:41.175375: step 17780, loss 4.20952e-07, acc 1\n",
      "2018-10-26T18:08:41.342927: step 17781, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:08:41.517462: step 17782, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:08:41.685014: step 17783, loss 3.16648e-07, acc 1\n",
      "2018-10-26T18:08:41.862540: step 17784, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:08:42.031090: step 17785, loss 0.000105549, acc 1\n",
      "2018-10-26T18:08:42.208615: step 17786, loss 2.88708e-07, acc 1\n",
      "2018-10-26T18:08:42.380157: step 17787, loss 7.28312e-06, acc 1\n",
      "2018-10-26T18:08:42.551698: step 17788, loss 0.00026298, acc 1\n",
      "2018-10-26T18:08:42.714263: step 17789, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:08:42.893785: step 17790, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:08:43.065326: step 17791, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:08:43.239860: step 17792, loss 3.59488e-07, acc 1\n",
      "2018-10-26T18:08:43.410436: step 17793, loss 2.36555e-07, acc 1\n",
      "2018-10-26T18:08:43.586932: step 17794, loss 0.000144217, acc 1\n",
      "2018-10-26T18:08:43.760469: step 17795, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:08:43.936998: step 17796, loss 0.000151731, acc 1\n",
      "2018-10-26T18:08:44.104549: step 17797, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:08:44.275094: step 17798, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:08:44.438657: step 17799, loss 1.15055e-05, acc 1\n",
      "2018-10-26T18:08:44.613190: step 17800, loss 4.48894e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:08:45.053016: step 17800, loss 6.34355, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17800\n",
      "\n",
      "2018-10-26T18:08:45.369811: step 17801, loss 0.000697645, acc 1\n",
      "2018-10-26T18:08:45.537364: step 17802, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:08:45.718879: step 17803, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:08:45.884436: step 17804, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:08:46.069941: step 17805, loss 5.69049e-05, acc 1\n",
      "2018-10-26T18:08:46.311296: step 17806, loss 3.12923e-07, acc 1\n",
      "2018-10-26T18:08:46.483835: step 17807, loss 3.76312e-05, acc 1\n",
      "2018-10-26T18:08:46.664353: step 17808, loss 7.6257e-06, acc 1\n",
      "2018-10-26T18:08:46.839884: step 17809, loss 4.62614e-06, acc 1\n",
      "2018-10-26T18:08:47.018406: step 17810, loss 0.000113248, acc 1\n",
      "2018-10-26T18:08:47.198924: step 17811, loss 9.84103e-06, acc 1\n",
      "2018-10-26T18:08:47.372461: step 17812, loss 2.09786e-05, acc 1\n",
      "2018-10-26T18:08:47.548989: step 17813, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:08:47.714547: step 17814, loss 1.39133e-06, acc 1\n",
      "2018-10-26T18:08:47.889081: step 17815, loss 0.000866829, acc 1\n",
      "2018-10-26T18:08:48.055636: step 17816, loss 1.46211e-06, acc 1\n",
      "2018-10-26T18:08:48.238147: step 17817, loss 0.000187403, acc 1\n",
      "2018-10-26T18:08:48.401712: step 17818, loss 2.18243e-05, acc 1\n",
      "2018-10-26T18:08:48.578239: step 17819, loss 0.000486174, acc 1\n",
      "2018-10-26T18:08:48.744794: step 17820, loss 2.44356e-05, acc 1\n",
      "2018-10-26T18:08:48.925312: step 17821, loss 3.79565e-06, acc 1\n",
      "2018-10-26T18:08:49.089872: step 17822, loss 4.67654e-05, acc 1\n",
      "2018-10-26T18:08:49.265403: step 17823, loss 4.27979e-06, acc 1\n",
      "2018-10-26T18:08:49.432955: step 17824, loss 1.43604e-06, acc 1\n",
      "2018-10-26T18:08:49.613473: step 17825, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:08:49.778033: step 17826, loss 0, acc 1\n",
      "2018-10-26T18:08:49.952567: step 17827, loss 1.23305e-06, acc 1\n",
      "2018-10-26T18:08:50.118125: step 17828, loss 5.22343e-05, acc 1\n",
      "2018-10-26T18:08:50.294653: step 17829, loss 0.000206686, acc 1\n",
      "2018-10-26T18:08:50.466195: step 17830, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:08:50.647709: step 17831, loss 4.71927e-06, acc 1\n",
      "2018-10-26T18:08:50.813268: step 17832, loss 0.000180321, acc 1\n",
      "2018-10-26T18:08:50.991791: step 17833, loss 0.000214442, acc 1\n",
      "2018-10-26T18:08:51.154360: step 17834, loss 1.56461e-07, acc 1\n",
      "2018-10-26T18:08:51.329888: step 17835, loss 0.0207841, acc 0.984375\n",
      "2018-10-26T18:08:51.499434: step 17836, loss 0.00226844, acc 1\n",
      "2018-10-26T18:08:51.677957: step 17837, loss 9.03707e-06, acc 1\n",
      "2018-10-26T18:08:51.846506: step 17838, loss 2.01165e-07, acc 1\n",
      "2018-10-26T18:08:52.023035: step 17839, loss 0.000101972, acc 1\n",
      "2018-10-26T18:08:52.189590: step 17840, loss 5.82732e-05, acc 1\n",
      "2018-10-26T18:08:52.364124: step 17841, loss 3.73975e-06, acc 1\n",
      "2018-10-26T18:08:52.528684: step 17842, loss 0.00105117, acc 1\n",
      "2018-10-26T18:08:52.708205: step 17843, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:08:52.871767: step 17844, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:08:53.048297: step 17845, loss 0.000549818, acc 1\n",
      "2018-10-26T18:08:53.209863: step 17846, loss 3.24098e-07, acc 1\n",
      "2018-10-26T18:08:53.389385: step 17847, loss 1.82538e-07, acc 1\n",
      "2018-10-26T18:08:53.553944: step 17848, loss 6.82889e-06, acc 1\n",
      "2018-10-26T18:08:53.736457: step 17849, loss 3.26864e-06, acc 1\n",
      "2018-10-26T18:08:53.894036: step 17850, loss 4.774e-06, acc 1\n",
      "2018-10-26T18:08:54.079540: step 17851, loss 2.29522e-05, acc 1\n",
      "2018-10-26T18:08:54.244100: step 17852, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:08:54.429605: step 17853, loss 7.20836e-07, acc 1\n",
      "2018-10-26T18:08:54.595163: step 17854, loss 2.19404e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:08:54.768698: step 17855, loss 2.65927e-05, acc 1\n",
      "2018-10-26T18:08:54.941239: step 17856, loss 3.87426e-07, acc 1\n",
      "2018-10-26T18:08:55.120758: step 17857, loss 0.00247345, acc 1\n",
      "2018-10-26T18:08:55.287313: step 17858, loss 1.65857e-05, acc 1\n",
      "2018-10-26T18:08:55.463841: step 17859, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:08:55.633388: step 17860, loss 1.12651e-05, acc 1\n",
      "2018-10-26T18:08:55.808919: step 17861, loss 1.58097e-05, acc 1\n",
      "2018-10-26T18:08:55.973479: step 17862, loss 9.15666e-05, acc 1\n",
      "2018-10-26T18:08:56.148014: step 17863, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:08:56.316563: step 17864, loss 4.21833e-06, acc 1\n",
      "2018-10-26T18:08:56.499075: step 17865, loss 6.53712e-05, acc 1\n",
      "2018-10-26T18:08:56.668622: step 17866, loss 2.60769e-07, acc 1\n",
      "2018-10-26T18:08:56.844156: step 17867, loss 0.000231399, acc 1\n",
      "2018-10-26T18:08:57.008713: step 17868, loss 0.000262791, acc 1\n",
      "2018-10-26T18:08:57.187237: step 17869, loss 2.33357e-05, acc 1\n",
      "2018-10-26T18:08:57.350800: step 17870, loss 7.47143e-05, acc 1\n",
      "2018-10-26T18:08:57.526332: step 17871, loss 0.000176436, acc 1\n",
      "2018-10-26T18:08:57.703857: step 17872, loss 0.000111515, acc 1\n",
      "2018-10-26T18:08:57.879387: step 17873, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:08:58.049932: step 17874, loss 0.00136866, acc 1\n",
      "2018-10-26T18:08:58.221473: step 17875, loss 3.78113e-07, acc 1\n",
      "2018-10-26T18:08:58.382044: step 17876, loss 3.34311e-06, acc 1\n",
      "2018-10-26T18:08:58.559571: step 17877, loss 1.2133e-05, acc 1\n",
      "2018-10-26T18:08:58.726126: step 17878, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:08:58.900658: step 17879, loss 7.55837e-05, acc 1\n",
      "2018-10-26T18:08:59.072200: step 17880, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:08:59.253715: step 17881, loss 1.98921e-06, acc 1\n",
      "2018-10-26T18:08:59.417279: step 17882, loss 2.38417e-07, acc 1\n",
      "2018-10-26T18:08:59.598793: step 17883, loss 0.000348602, acc 1\n",
      "2018-10-26T18:08:59.763354: step 17884, loss 4.23881e-06, acc 1\n",
      "2018-10-26T18:08:59.942875: step 17885, loss 1.25363e-05, acc 1\n",
      "2018-10-26T18:09:00.125386: step 17886, loss 0.000122478, acc 1\n",
      "2018-10-26T18:09:00.308897: step 17887, loss 1.62414e-06, acc 1\n",
      "2018-10-26T18:09:00.496395: step 17888, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:09:00.669932: step 17889, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:09:00.849452: step 17890, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:09:01.014013: step 17891, loss 8.97774e-07, acc 1\n",
      "2018-10-26T18:09:01.185554: step 17892, loss 1.00143e-05, acc 1\n",
      "2018-10-26T18:09:01.354104: step 17893, loss 1.5925e-06, acc 1\n",
      "2018-10-26T18:09:01.530632: step 17894, loss 0.00092002, acc 1\n",
      "2018-10-26T18:09:01.758024: step 17895, loss 0.000347503, acc 1\n",
      "2018-10-26T18:09:01.946520: step 17896, loss 3.6694e-07, acc 1\n",
      "2018-10-26T18:09:02.120057: step 17897, loss 0.000277651, acc 1\n",
      "2018-10-26T18:09:02.290601: step 17898, loss 5.99764e-07, acc 1\n",
      "2018-10-26T18:09:02.461145: step 17899, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:09:02.641663: step 17900, loss 1.55152e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:09:03.076502: step 17900, loss 6.29946, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-17900\n",
      "\n",
      "2018-10-26T18:09:03.436034: step 17901, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:09:03.601590: step 17902, loss 9.77858e-07, acc 1\n",
      "2018-10-26T18:09:03.786097: step 17903, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:09:03.958635: step 17904, loss 6.14661e-07, acc 1\n",
      "2018-10-26T18:09:04.164087: step 17905, loss 5.81404e-05, acc 1\n",
      "2018-10-26T18:09:04.399458: step 17906, loss 1.17343e-06, acc 1\n",
      "2018-10-26T18:09:04.575987: step 17907, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:09:04.755507: step 17908, loss 0.000186246, acc 1\n",
      "2018-10-26T18:09:04.929043: step 17909, loss 7.07792e-05, acc 1\n",
      "2018-10-26T18:09:05.103577: step 17910, loss 0.000164727, acc 1\n",
      "2018-10-26T18:09:05.285092: step 17911, loss 7.11182e-06, acc 1\n",
      "2018-10-26T18:09:05.469599: step 17912, loss 2.98485e-05, acc 1\n",
      "2018-10-26T18:09:05.637152: step 17913, loss 1.13431e-06, acc 1\n",
      "2018-10-26T18:09:05.817668: step 17914, loss 4.35853e-07, acc 1\n",
      "2018-10-26T18:09:05.985221: step 17915, loss 5.64373e-07, acc 1\n",
      "2018-10-26T18:09:06.163744: step 17916, loss 0, acc 1\n",
      "2018-10-26T18:09:06.333291: step 17917, loss 1.71362e-07, acc 1\n",
      "2018-10-26T18:09:06.510816: step 17918, loss 2.52836e-05, acc 1\n",
      "2018-10-26T18:09:06.674392: step 17919, loss 4.6038e-06, acc 1\n",
      "2018-10-26T18:09:06.852903: step 17920, loss 1.01712e-05, acc 1\n",
      "2018-10-26T18:09:07.023449: step 17921, loss 0.000306544, acc 1\n",
      "2018-10-26T18:09:07.200973: step 17922, loss 4.81243e-06, acc 1\n",
      "2018-10-26T18:09:07.364536: step 17923, loss 0.00031049, acc 1\n",
      "2018-10-26T18:09:07.542062: step 17924, loss 0.000314049, acc 1\n",
      "2018-10-26T18:09:07.710611: step 17925, loss 3.07334e-07, acc 1\n",
      "2018-10-26T18:09:07.897113: step 17926, loss 0.000592786, acc 1\n",
      "2018-10-26T18:09:08.067657: step 17927, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:09:08.240197: step 17928, loss 2.84982e-07, acc 1\n",
      "2018-10-26T18:09:08.402762: step 17929, loss 2.51251e-06, acc 1\n",
      "2018-10-26T18:09:08.576299: step 17930, loss 0.000107519, acc 1\n",
      "2018-10-26T18:09:08.748837: step 17931, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:09:08.924369: step 17932, loss 0.000148349, acc 1\n",
      "2018-10-26T18:09:09.086934: step 17933, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:09:09.261467: step 17934, loss 2.38759e-05, acc 1\n",
      "2018-10-26T18:09:09.432012: step 17935, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:09:09.605548: step 17936, loss 0.00296194, acc 1\n",
      "2018-10-26T18:09:09.777090: step 17937, loss 1.09698e-05, acc 1\n",
      "2018-10-26T18:09:09.953618: step 17938, loss 5.20922e-06, acc 1\n",
      "2018-10-26T18:09:10.124162: step 17939, loss 7.98351e-05, acc 1\n",
      "2018-10-26T18:09:10.298697: step 17940, loss 0.000698825, acc 1\n",
      "2018-10-26T18:09:10.465251: step 17941, loss 0.00107031, acc 1\n",
      "2018-10-26T18:09:10.648762: step 17942, loss 3.14785e-07, acc 1\n",
      "2018-10-26T18:09:10.825290: step 17943, loss 0.000474749, acc 1\n",
      "2018-10-26T18:09:11.001818: step 17944, loss 1.32057e-06, acc 1\n",
      "2018-10-26T18:09:11.174357: step 17945, loss 1.77765e-05, acc 1\n",
      "2018-10-26T18:09:11.340912: step 17946, loss 0.000242482, acc 1\n",
      "2018-10-26T18:09:11.524430: step 17947, loss 3.39e-07, acc 1\n",
      "2018-10-26T18:09:11.691973: step 17948, loss 0.000106929, acc 1\n",
      "2018-10-26T18:09:11.866508: step 17949, loss 5.26125e-06, acc 1\n",
      "2018-10-26T18:09:12.034060: step 17950, loss 0.00012031, acc 1\n",
      "2018-10-26T18:09:12.210588: step 17951, loss 5.58547e-05, acc 1\n",
      "2018-10-26T18:09:12.374151: step 17952, loss 2.52341e-05, acc 1\n",
      "2018-10-26T18:09:12.552675: step 17953, loss 4.62096e-06, acc 1\n",
      "2018-10-26T18:09:12.715240: step 17954, loss 1.11087e-05, acc 1\n",
      "2018-10-26T18:09:12.892765: step 17955, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:09:13.059320: step 17956, loss 0.00144891, acc 1\n",
      "2018-10-26T18:09:13.235849: step 17957, loss 4.34498e-06, acc 1\n",
      "2018-10-26T18:09:13.404398: step 17958, loss 8.71695e-07, acc 1\n",
      "2018-10-26T18:09:13.584917: step 17959, loss 0.000117575, acc 1\n",
      "2018-10-26T18:09:13.754463: step 17960, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:09:13.934981: step 17961, loss 2.22922e-05, acc 1\n",
      "2018-10-26T18:09:14.095551: step 17962, loss 0.00011738, acc 1\n",
      "2018-10-26T18:09:14.276070: step 17963, loss 6.61225e-07, acc 1\n",
      "2018-10-26T18:09:14.441626: step 17964, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:09:14.619152: step 17965, loss 1.3597e-06, acc 1\n",
      "2018-10-26T18:09:14.783713: step 17966, loss 0.00393499, acc 1\n",
      "2018-10-26T18:09:14.963233: step 17967, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:09:15.129788: step 17968, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:09:15.308312: step 17969, loss 4.41218e-06, acc 1\n",
      "2018-10-26T18:09:15.475863: step 17970, loss 2.75669e-07, acc 1\n",
      "2018-10-26T18:09:15.652392: step 17971, loss 5.53401e-05, acc 1\n",
      "2018-10-26T18:09:15.816953: step 17972, loss 5.98078e-05, acc 1\n",
      "2018-10-26T18:09:16.004451: step 17973, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:09:16.181977: step 17974, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:09:16.366484: step 17975, loss 0.00012726, acc 1\n",
      "2018-10-26T18:09:16.546004: step 17976, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:09:16.716548: step 17977, loss 0.0395644, acc 0.984375\n",
      "2018-10-26T18:09:16.902053: step 17978, loss 6.37454e-06, acc 1\n",
      "2018-10-26T18:09:17.070603: step 17979, loss 3.23695e-06, acc 1\n",
      "2018-10-26T18:09:17.256108: step 17980, loss 7.07805e-08, acc 1\n",
      "2018-10-26T18:09:17.420668: step 17981, loss 1.27214e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:09:17.602182: step 17982, loss 8.85458e-06, acc 1\n",
      "2018-10-26T18:09:17.775719: step 17983, loss 3.36937e-06, acc 1\n",
      "2018-10-26T18:09:17.952247: step 17984, loss 8.19563e-08, acc 1\n",
      "2018-10-26T18:09:18.123789: step 17985, loss 7.05112e-05, acc 1\n",
      "2018-10-26T18:09:18.295331: step 17986, loss 5.93028e-06, acc 1\n",
      "2018-10-26T18:09:18.469863: step 17987, loss 2.10478e-07, acc 1\n",
      "2018-10-26T18:09:18.634425: step 17988, loss 2.42314e-06, acc 1\n",
      "2018-10-26T18:09:18.805966: step 17989, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:09:18.983492: step 17990, loss 1.49769e-05, acc 1\n",
      "2018-10-26T18:09:19.159023: step 17991, loss 2.72547e-05, acc 1\n",
      "2018-10-26T18:09:19.334553: step 17992, loss 2.45867e-07, acc 1\n",
      "2018-10-26T18:09:19.498117: step 17993, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:09:19.679632: step 17994, loss 0.00251882, acc 1\n",
      "2018-10-26T18:09:19.849180: step 17995, loss 1.43821e-05, acc 1\n",
      "2018-10-26T18:09:20.022715: step 17996, loss 1.07237e-05, acc 1\n",
      "2018-10-26T18:09:20.191265: step 17997, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:09:20.366796: step 17998, loss 0.000471937, acc 1\n",
      "2018-10-26T18:09:20.538337: step 17999, loss 8.34446e-07, acc 1\n",
      "2018-10-26T18:09:20.707885: step 18000, loss 0.131774, acc 0.983333\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:09:21.150701: step 18000, loss 6.67882, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18000\n",
      "\n",
      "2018-10-26T18:09:21.504981: step 18001, loss 2.10086e-05, acc 1\n",
      "2018-10-26T18:09:21.677519: step 18002, loss 0.000137058, acc 1\n",
      "2018-10-26T18:09:21.859035: step 18003, loss 0.000695184, acc 1\n",
      "2018-10-26T18:09:22.031573: step 18004, loss 2.21654e-07, acc 1\n",
      "2018-10-26T18:09:22.228049: step 18005, loss 0.000433163, acc 1\n",
      "2018-10-26T18:09:22.458432: step 18006, loss 8.56815e-08, acc 1\n",
      "2018-10-26T18:09:22.625991: step 18007, loss 0.000289279, acc 1\n",
      "2018-10-26T18:09:22.809495: step 18008, loss 2.51455e-07, acc 1\n",
      "2018-10-26T18:09:22.988790: step 18009, loss 6.10937e-07, acc 1\n",
      "2018-10-26T18:09:23.162326: step 18010, loss 8.19545e-07, acc 1\n",
      "2018-10-26T18:09:23.336860: step 18011, loss 1.29635e-06, acc 1\n",
      "2018-10-26T18:09:23.513388: step 18012, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:09:23.686924: step 18013, loss 4.17229e-07, acc 1\n",
      "2018-10-26T18:09:23.860462: step 18014, loss 1.06167e-06, acc 1\n",
      "2018-10-26T18:09:24.035992: step 18015, loss 3.00043e-06, acc 1\n",
      "2018-10-26T18:09:24.196563: step 18016, loss 4.50905e-05, acc 1\n",
      "2018-10-26T18:09:24.377081: step 18017, loss 9.2012e-07, acc 1\n",
      "2018-10-26T18:09:24.539646: step 18018, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:09:24.727145: step 18019, loss 4.81236e-06, acc 1\n",
      "2018-10-26T18:09:24.897690: step 18020, loss 0, acc 1\n",
      "2018-10-26T18:09:25.072223: step 18021, loss 2.36352e-06, acc 1\n",
      "2018-10-26T18:09:25.238778: step 18022, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:09:25.411318: step 18023, loss 0.000835766, acc 1\n",
      "2018-10-26T18:09:25.574880: step 18024, loss 3.53962e-05, acc 1\n",
      "2018-10-26T18:09:25.756395: step 18025, loss 8.19563e-08, acc 1\n",
      "2018-10-26T18:09:25.922950: step 18026, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:09:26.097484: step 18027, loss 6.17519e-05, acc 1\n",
      "2018-10-26T18:09:26.268028: step 18028, loss 9.0522e-07, acc 1\n",
      "2018-10-26T18:09:26.446551: step 18029, loss 1.44195e-05, acc 1\n",
      "2018-10-26T18:09:26.614104: step 18030, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:09:26.793624: step 18031, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:09:26.965166: step 18032, loss 0, acc 1\n",
      "2018-10-26T18:09:27.144686: step 18033, loss 2.32829e-07, acc 1\n",
      "2018-10-26T18:09:27.314232: step 18034, loss 0.00123423, acc 1\n",
      "2018-10-26T18:09:27.492756: step 18035, loss 3.67103e-06, acc 1\n",
      "2018-10-26T18:09:27.660308: step 18036, loss 2.55181e-07, acc 1\n",
      "2018-10-26T18:09:27.863765: step 18037, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:09:28.045280: step 18038, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:09:28.244747: step 18039, loss 6.79851e-07, acc 1\n",
      "2018-10-26T18:09:28.481115: step 18040, loss 3.31926e-05, acc 1\n",
      "2018-10-26T18:09:28.699531: step 18041, loss 1.2561e-05, acc 1\n",
      "2018-10-26T18:09:28.895009: step 18042, loss 2.03409e-05, acc 1\n",
      "2018-10-26T18:09:29.140354: step 18043, loss 7.58081e-07, acc 1\n",
      "2018-10-26T18:09:29.363757: step 18044, loss 8.13966e-07, acc 1\n",
      "2018-10-26T18:09:29.542279: step 18045, loss 4.58769e-05, acc 1\n",
      "2018-10-26T18:09:29.792610: step 18046, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:09:30.006041: step 18047, loss 0.0894615, acc 0.984375\n",
      "2018-10-26T18:09:30.240416: step 18048, loss 6.79546e-06, acc 1\n",
      "2018-10-26T18:09:30.414948: step 18049, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:09:30.646330: step 18050, loss 1.95412e-05, acc 1\n",
      "2018-10-26T18:09:30.858762: step 18051, loss 3.03397e-06, acc 1\n",
      "2018-10-26T18:09:31.041274: step 18052, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:09:31.261686: step 18053, loss 4.74003e-06, acc 1\n",
      "2018-10-26T18:09:31.500050: step 18054, loss 2.72491e-06, acc 1\n",
      "2018-10-26T18:09:31.719463: step 18055, loss 6.72407e-07, acc 1\n",
      "2018-10-26T18:09:31.940871: step 18056, loss 7.04074e-07, acc 1\n",
      "2018-10-26T18:09:32.186215: step 18057, loss 4.62579e-05, acc 1\n",
      "2018-10-26T18:09:32.401640: step 18058, loss 1.34664e-06, acc 1\n",
      "2018-10-26T18:09:32.651971: step 18059, loss 5.88589e-07, acc 1\n",
      "2018-10-26T18:09:32.818526: step 18060, loss 0, acc 1\n",
      "2018-10-26T18:09:33.028963: step 18061, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:09:33.222446: step 18062, loss 0.00673299, acc 1\n",
      "2018-10-26T18:09:33.403962: step 18063, loss 6.70547e-07, acc 1\n",
      "2018-10-26T18:09:33.608416: step 18064, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:09:33.787936: step 18065, loss 1.76371e-05, acc 1\n",
      "2018-10-26T18:09:34.014332: step 18066, loss 0.0056175, acc 1\n",
      "2018-10-26T18:09:34.216790: step 18067, loss 0.000129096, acc 1\n",
      "2018-10-26T18:09:34.406283: step 18068, loss 2.75671e-07, acc 1\n",
      "2018-10-26T18:09:34.624702: step 18069, loss 0.00208426, acc 1\n",
      "2018-10-26T18:09:34.794248: step 18070, loss 1.04852e-05, acc 1\n",
      "2018-10-26T18:09:35.041587: step 18071, loss 1.66961e-05, acc 1\n",
      "2018-10-26T18:09:35.209139: step 18072, loss 1.75824e-06, acc 1\n",
      "2018-10-26T18:09:35.398632: step 18073, loss 7.06788e-06, acc 1\n",
      "2018-10-26T18:09:35.597103: step 18074, loss 3.27865e-05, acc 1\n",
      "2018-10-26T18:09:35.822504: step 18075, loss 0.000215826, acc 1\n",
      "2018-10-26T18:09:36.032938: step 18076, loss 0, acc 1\n",
      "2018-10-26T18:09:36.208468: step 18077, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:09:36.388986: step 18078, loss 4.80863e-06, acc 1\n",
      "2018-10-26T18:09:36.581473: step 18079, loss 0.000633222, acc 1\n",
      "2018-10-26T18:09:36.769969: step 18080, loss 3.69847e-05, acc 1\n",
      "2018-10-26T18:09:36.954476: step 18081, loss 0.00138484, acc 1\n",
      "2018-10-26T18:09:37.129009: step 18082, loss 0.00153669, acc 1\n",
      "2018-10-26T18:09:37.340445: step 18083, loss 0.00279017, acc 1\n",
      "2018-10-26T18:09:37.511986: step 18084, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:09:37.690509: step 18085, loss 3.89431e-06, acc 1\n",
      "2018-10-26T18:09:37.855069: step 18086, loss 0.011181, acc 0.984375\n",
      "2018-10-26T18:09:38.076502: step 18087, loss 0.00220173, acc 1\n",
      "2018-10-26T18:09:38.242036: step 18088, loss 5.77692e-06, acc 1\n",
      "2018-10-26T18:09:38.418565: step 18089, loss 0.00261497, acc 1\n",
      "2018-10-26T18:09:38.584121: step 18090, loss 1.37458e-06, acc 1\n",
      "2018-10-26T18:09:38.798549: step 18091, loss 2.05998e-05, acc 1\n",
      "2018-10-26T18:09:38.969093: step 18092, loss 5.62483e-06, acc 1\n",
      "2018-10-26T18:09:39.155594: step 18093, loss 5.96036e-07, acc 1\n",
      "2018-10-26T18:09:39.320155: step 18094, loss 0.000169207, acc 1\n",
      "2018-10-26T18:09:39.502667: step 18095, loss 6.92613e-06, acc 1\n",
      "2018-10-26T18:09:39.674209: step 18096, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:09:39.852732: step 18097, loss 0.000174367, acc 1\n",
      "2018-10-26T18:09:40.021282: step 18098, loss 0.000178104, acc 1\n",
      "2018-10-26T18:09:40.205789: step 18099, loss 2.98022e-07, acc 1\n",
      "2018-10-26T18:09:40.371345: step 18100, loss 5.55332e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:09:40.818152: step 18100, loss 6.37228, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18100\n",
      "\n",
      "2018-10-26T18:09:41.193495: step 18101, loss 1.30009e-06, acc 1\n",
      "2018-10-26T18:09:41.373017: step 18102, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:09:41.537576: step 18103, loss 9.18694e-05, acc 1\n",
      "2018-10-26T18:09:41.718094: step 18104, loss 3.48313e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:09:41.911577: step 18105, loss 1.91264e-05, acc 1\n",
      "2018-10-26T18:09:42.151936: step 18106, loss 7.63904e-05, acc 1\n",
      "2018-10-26T18:09:42.333451: step 18107, loss 1.7259e-05, acc 1\n",
      "2018-10-26T18:09:42.508981: step 18108, loss 1.27402e-06, acc 1\n",
      "2018-10-26T18:09:42.709445: step 18109, loss 0.00756293, acc 1\n",
      "2018-10-26T18:09:42.888966: step 18110, loss 0.000355221, acc 1\n",
      "2018-10-26T18:09:43.072476: step 18111, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:09:43.242023: step 18112, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:09:43.417554: step 18113, loss 3.00364e-05, acc 1\n",
      "2018-10-26T18:09:43.584121: step 18114, loss 0.0187816, acc 0.984375\n",
      "2018-10-26T18:09:43.758643: step 18115, loss 2.17872e-05, acc 1\n",
      "2018-10-26T18:09:43.923202: step 18116, loss 0.000463535, acc 1\n",
      "2018-10-26T18:09:44.098733: step 18117, loss 4.99673e-06, acc 1\n",
      "2018-10-26T18:09:44.265288: step 18118, loss 4.50695e-06, acc 1\n",
      "2018-10-26T18:09:44.433838: step 18119, loss 0.000193978, acc 1\n",
      "2018-10-26T18:09:44.602388: step 18120, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:09:44.783903: step 18121, loss 5.10707e-05, acc 1\n",
      "2018-10-26T18:09:44.952453: step 18122, loss 0.00713142, acc 1\n",
      "2018-10-26T18:09:45.127983: step 18123, loss 1.18833e-06, acc 1\n",
      "2018-10-26T18:09:45.295536: step 18124, loss 1.38392e-06, acc 1\n",
      "2018-10-26T18:09:45.467078: step 18125, loss 1.66465e-05, acc 1\n",
      "2018-10-26T18:09:45.631638: step 18126, loss 2.71456e-05, acc 1\n",
      "2018-10-26T18:09:45.805175: step 18127, loss 1.36715e-06, acc 1\n",
      "2018-10-26T18:09:45.973723: step 18128, loss 0.000185332, acc 1\n",
      "2018-10-26T18:09:46.150252: step 18129, loss 0.000256326, acc 1\n",
      "2018-10-26T18:09:46.323789: step 18130, loss 0, acc 1\n",
      "2018-10-26T18:09:46.494333: step 18131, loss 6.1402e-06, acc 1\n",
      "2018-10-26T18:09:46.671859: step 18132, loss 2.17929e-07, acc 1\n",
      "2018-10-26T18:09:46.843655: step 18133, loss 0.00365621, acc 1\n",
      "2018-10-26T18:09:47.017191: step 18134, loss 9.18201e-06, acc 1\n",
      "2018-10-26T18:09:47.187736: step 18135, loss 0.00022172, acc 1\n",
      "2018-10-26T18:09:47.353294: step 18136, loss 6.38946e-06, acc 1\n",
      "2018-10-26T18:09:47.526829: step 18137, loss 6.70542e-07, acc 1\n",
      "2018-10-26T18:09:47.695380: step 18138, loss 2.03028e-07, acc 1\n",
      "2018-10-26T18:09:47.866920: step 18139, loss 6.09388e-05, acc 1\n",
      "2018-10-26T18:09:48.035470: step 18140, loss 1.55341e-06, acc 1\n",
      "2018-10-26T18:09:48.212996: step 18141, loss 0, acc 1\n",
      "2018-10-26T18:09:48.387529: step 18142, loss 1.68935e-06, acc 1\n",
      "2018-10-26T18:09:48.573036: step 18143, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:09:48.744576: step 18144, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:09:48.919109: step 18145, loss 0.000516025, acc 1\n",
      "2018-10-26T18:09:49.085665: step 18146, loss 0.000975235, acc 1\n",
      "2018-10-26T18:09:49.270172: step 18147, loss 5.01044e-07, acc 1\n",
      "2018-10-26T18:09:49.435729: step 18148, loss 1.35036e-06, acc 1\n",
      "2018-10-26T18:09:49.611260: step 18149, loss 0, acc 1\n",
      "2018-10-26T18:09:49.779810: step 18150, loss 0.00118539, acc 1\n",
      "2018-10-26T18:09:49.959330: step 18151, loss 0.000280998, acc 1\n",
      "2018-10-26T18:09:50.129875: step 18152, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:09:50.309395: step 18153, loss 0.000103616, acc 1\n",
      "2018-10-26T18:09:50.481935: step 18154, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:09:50.657465: step 18155, loss 5.89719e-05, acc 1\n",
      "2018-10-26T18:09:50.822026: step 18156, loss 3.33412e-07, acc 1\n",
      "2018-10-26T18:09:51.001546: step 18157, loss 9.87175e-07, acc 1\n",
      "2018-10-26T18:09:51.172089: step 18158, loss 0.000440892, acc 1\n",
      "2018-10-26T18:09:51.345626: step 18159, loss 4.20953e-07, acc 1\n",
      "2018-10-26T18:09:51.512181: step 18160, loss 8.49346e-07, acc 1\n",
      "2018-10-26T18:09:51.684721: step 18161, loss 2.06753e-07, acc 1\n",
      "2018-10-26T18:09:51.855265: step 18162, loss 9.1267e-07, acc 1\n",
      "2018-10-26T18:09:52.027804: step 18163, loss 9.23949e-05, acc 1\n",
      "2018-10-26T18:09:52.194358: step 18164, loss 1.7191e-05, acc 1\n",
      "2018-10-26T18:09:52.367895: step 18165, loss 6.20223e-05, acc 1\n",
      "2018-10-26T18:09:52.530460: step 18166, loss 1.166e-06, acc 1\n",
      "2018-10-26T18:09:52.704995: step 18167, loss 0.000103358, acc 1\n",
      "2018-10-26T18:09:52.875539: step 18168, loss 0.000237178, acc 1\n",
      "2018-10-26T18:09:53.046083: step 18169, loss 2.89061e-06, acc 1\n",
      "2018-10-26T18:09:53.210643: step 18170, loss 2.36911e-06, acc 1\n",
      "2018-10-26T18:09:53.387172: step 18171, loss 5.68798e-06, acc 1\n",
      "2018-10-26T18:09:53.551732: step 18172, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:09:53.741226: step 18173, loss 7.24418e-06, acc 1\n",
      "2018-10-26T18:09:53.909775: step 18174, loss 1.37907e-05, acc 1\n",
      "2018-10-26T18:09:54.088299: step 18175, loss 0.000985686, acc 1\n",
      "2018-10-26T18:09:54.252858: step 18176, loss 1.12501e-06, acc 1\n",
      "2018-10-26T18:09:54.430384: step 18177, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:09:54.595942: step 18178, loss 2.90935e-05, acc 1\n",
      "2018-10-26T18:09:54.776460: step 18179, loss 0.000358765, acc 1\n",
      "2018-10-26T18:09:54.944012: step 18180, loss 0.000768867, acc 1\n",
      "2018-10-26T18:09:55.115554: step 18181, loss 4.97884e-05, acc 1\n",
      "2018-10-26T18:09:55.281111: step 18182, loss 2.01165e-07, acc 1\n",
      "2018-10-26T18:09:55.454648: step 18183, loss 1.48447e-06, acc 1\n",
      "2018-10-26T18:09:55.624195: step 18184, loss 3.72525e-07, acc 1\n",
      "2018-10-26T18:09:55.800722: step 18185, loss 1.51053e-06, acc 1\n",
      "2018-10-26T18:09:55.970270: step 18186, loss 8.13392e-06, acc 1\n",
      "2018-10-26T18:09:56.149790: step 18187, loss 1.59434e-06, acc 1\n",
      "2018-10-26T18:09:56.313353: step 18188, loss 8.64244e-07, acc 1\n",
      "2018-10-26T18:09:56.489881: step 18189, loss 1.06726e-06, acc 1\n",
      "2018-10-26T18:09:56.653445: step 18190, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:09:56.826981: step 18191, loss 0.000863619, acc 1\n",
      "2018-10-26T18:09:56.995543: step 18192, loss 1.0446e-05, acc 1\n",
      "2018-10-26T18:09:57.172059: step 18193, loss 1.94076e-06, acc 1\n",
      "2018-10-26T18:09:57.339612: step 18194, loss 0.000119255, acc 1\n",
      "2018-10-26T18:09:57.519132: step 18195, loss 0.000382745, acc 1\n",
      "2018-10-26T18:09:57.680700: step 18196, loss 6.12799e-07, acc 1\n",
      "2018-10-26T18:09:57.855234: step 18197, loss 4.11967e-05, acc 1\n",
      "2018-10-26T18:09:58.023783: step 18198, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:09:58.197320: step 18199, loss 1.19394e-06, acc 1\n",
      "2018-10-26T18:09:58.365869: step 18200, loss 2.90374e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:09:58.807689: step 18200, loss 6.42805, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18200\n",
      "\n",
      "2018-10-26T18:09:59.185755: step 18201, loss 8.71697e-07, acc 1\n",
      "2018-10-26T18:09:59.364278: step 18202, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:09:59.539809: step 18203, loss 2.19791e-07, acc 1\n",
      "2018-10-26T18:09:59.717335: step 18204, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:09:59.953705: step 18205, loss 5.04325e-06, acc 1\n",
      "2018-10-26T18:10:00.163143: step 18206, loss 1.09707e-06, acc 1\n",
      "2018-10-26T18:10:00.347650: step 18207, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:10:00.546120: step 18208, loss 2.51255e-06, acc 1\n",
      "2018-10-26T18:10:00.725640: step 18209, loss 0.00848505, acc 1\n",
      "2018-10-26T18:10:00.904164: step 18210, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:10:01.076702: step 18211, loss 0.0366054, acc 0.984375\n",
      "2018-10-26T18:10:01.254228: step 18212, loss 3.58911e-06, acc 1\n",
      "2018-10-26T18:10:01.422778: step 18213, loss 1.08067e-05, acc 1\n",
      "2018-10-26T18:10:01.605290: step 18214, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:10:01.772842: step 18215, loss 6.51924e-08, acc 1\n",
      "2018-10-26T18:10:01.948373: step 18216, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:10:02.114929: step 18217, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:10:02.295446: step 18218, loss 0.000111028, acc 1\n",
      "2018-10-26T18:10:02.460007: step 18219, loss 6.75441e-05, acc 1\n",
      "2018-10-26T18:10:02.637532: step 18220, loss 0.0396374, acc 0.984375\n",
      "2018-10-26T18:10:02.804088: step 18221, loss 2.59081e-06, acc 1\n",
      "2018-10-26T18:10:02.985602: step 18222, loss 6.79788e-05, acc 1\n",
      "2018-10-26T18:10:03.154151: step 18223, loss 1.18309e-05, acc 1\n",
      "2018-10-26T18:10:03.386532: step 18224, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:10:03.569043: step 18225, loss 1.57556e-05, acc 1\n",
      "2018-10-26T18:10:03.750557: step 18226, loss 0.000288443, acc 1\n",
      "2018-10-26T18:10:03.919108: step 18227, loss 9.29414e-06, acc 1\n",
      "2018-10-26T18:10:04.099625: step 18228, loss 7.3251e-05, acc 1\n",
      "2018-10-26T18:10:04.265183: step 18229, loss 8.38182e-07, acc 1\n",
      "2018-10-26T18:10:04.444703: step 18230, loss 3.31549e-07, acc 1\n",
      "2018-10-26T18:10:04.608267: step 18231, loss 9.68573e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:10:04.787786: step 18232, loss 4.77718e-05, acc 1\n",
      "2018-10-26T18:10:04.953345: step 18233, loss 4.05633e-06, acc 1\n",
      "2018-10-26T18:10:05.129873: step 18234, loss 1.70242e-06, acc 1\n",
      "2018-10-26T18:10:05.303411: step 18235, loss 2.41007e-05, acc 1\n",
      "2018-10-26T18:10:05.475948: step 18236, loss 3.39162e-06, acc 1\n",
      "2018-10-26T18:10:05.644498: step 18237, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:10:05.823021: step 18238, loss 0.000571886, acc 1\n",
      "2018-10-26T18:10:05.989576: step 18239, loss 1.471e-05, acc 1\n",
      "2018-10-26T18:10:06.174084: step 18240, loss 6.25989e-05, acc 1\n",
      "2018-10-26T18:10:06.341635: step 18241, loss 2.08615e-07, acc 1\n",
      "2018-10-26T18:10:06.515172: step 18242, loss 8.19563e-08, acc 1\n",
      "2018-10-26T18:10:06.680730: step 18243, loss 6.92771e-06, acc 1\n",
      "2018-10-26T18:10:06.862244: step 18244, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:10:07.030794: step 18245, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:10:07.202336: step 18246, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:10:07.368890: step 18247, loss 0.00748896, acc 1\n",
      "2018-10-26T18:10:07.541429: step 18248, loss 6.42601e-07, acc 1\n",
      "2018-10-26T18:10:07.704992: step 18249, loss 4.16532e-05, acc 1\n",
      "2018-10-26T18:10:07.881521: step 18250, loss 5.55063e-07, acc 1\n",
      "2018-10-26T18:10:08.053062: step 18251, loss 1.07658e-06, acc 1\n",
      "2018-10-26T18:10:08.227596: step 18252, loss 3.61352e-07, acc 1\n",
      "2018-10-26T18:10:08.391159: step 18253, loss 0.000189797, acc 1\n",
      "2018-10-26T18:10:08.562701: step 18254, loss 4.60203e-05, acc 1\n",
      "2018-10-26T18:10:08.727261: step 18255, loss 2.10122e-05, acc 1\n",
      "2018-10-26T18:10:08.904787: step 18256, loss 0.00022625, acc 1\n",
      "2018-10-26T18:10:09.076330: step 18257, loss 9.72288e-07, acc 1\n",
      "2018-10-26T18:10:09.263949: step 18258, loss 1.45839e-06, acc 1\n",
      "2018-10-26T18:10:09.432499: step 18259, loss 1.09707e-06, acc 1\n",
      "2018-10-26T18:10:09.606035: step 18260, loss 2.0586e-05, acc 1\n",
      "2018-10-26T18:10:09.781566: step 18261, loss 8.35784e-05, acc 1\n",
      "2018-10-26T18:10:09.962084: step 18262, loss 0.000138212, acc 1\n",
      "2018-10-26T18:10:10.135621: step 18263, loss 0.000123603, acc 1\n",
      "2018-10-26T18:10:10.312148: step 18264, loss 7.02207e-07, acc 1\n",
      "2018-10-26T18:10:10.478703: step 18265, loss 3.40862e-07, acc 1\n",
      "2018-10-26T18:10:10.648250: step 18266, loss 3.03582e-06, acc 1\n",
      "2018-10-26T18:10:10.828769: step 18267, loss 7.35275e-06, acc 1\n",
      "2018-10-26T18:10:11.002305: step 18268, loss 0.0275044, acc 0.984375\n",
      "2018-10-26T18:10:11.177835: step 18269, loss 2.51251e-06, acc 1\n",
      "2018-10-26T18:10:11.345388: step 18270, loss 1.5683e-06, acc 1\n",
      "2018-10-26T18:10:11.525905: step 18271, loss 2.49593e-07, acc 1\n",
      "2018-10-26T18:10:11.692461: step 18272, loss 4.32131e-07, acc 1\n",
      "2018-10-26T18:10:11.865997: step 18273, loss 9.57668e-06, acc 1\n",
      "2018-10-26T18:10:12.030557: step 18274, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:10:12.203096: step 18275, loss 0.000106927, acc 1\n",
      "2018-10-26T18:10:12.376633: step 18276, loss 5.8051e-06, acc 1\n",
      "2018-10-26T18:10:12.555155: step 18277, loss 1.16973e-06, acc 1\n",
      "2018-10-26T18:10:12.719716: step 18278, loss 4.27235e-06, acc 1\n",
      "2018-10-26T18:10:12.894250: step 18279, loss 1.71363e-07, acc 1\n",
      "2018-10-26T18:10:13.065791: step 18280, loss 6.14662e-07, acc 1\n",
      "2018-10-26T18:10:13.236335: step 18281, loss 0.000695309, acc 1\n",
      "2018-10-26T18:10:13.403889: step 18282, loss 0.000227885, acc 1\n",
      "2018-10-26T18:10:13.577426: step 18283, loss 2.68252e-05, acc 1\n",
      "2018-10-26T18:10:13.753953: step 18284, loss 4.50093e-05, acc 1\n",
      "2018-10-26T18:10:13.925494: step 18285, loss 5.9231e-07, acc 1\n",
      "2018-10-26T18:10:14.093046: step 18286, loss 0.000580397, acc 1\n",
      "2018-10-26T18:10:14.266599: step 18287, loss 0.0267966, acc 0.984375\n",
      "2018-10-26T18:10:14.441117: step 18288, loss 2.05064e-06, acc 1\n",
      "2018-10-26T18:10:14.620637: step 18289, loss 3.0325e-05, acc 1\n",
      "2018-10-26T18:10:14.784200: step 18290, loss 5.22993e-06, acc 1\n",
      "2018-10-26T18:10:14.957737: step 18291, loss 3.33381e-06, acc 1\n",
      "2018-10-26T18:10:15.125289: step 18292, loss 0.000440644, acc 1\n",
      "2018-10-26T18:10:15.297828: step 18293, loss 2.17438e-05, acc 1\n",
      "2018-10-26T18:10:15.461391: step 18294, loss 0.000200567, acc 1\n",
      "2018-10-26T18:10:15.636922: step 18295, loss 6.07216e-07, acc 1\n",
      "2018-10-26T18:10:15.806468: step 18296, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:10:15.982997: step 18297, loss 0.00627082, acc 1\n",
      "2018-10-26T18:10:16.148555: step 18298, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:10:16.321094: step 18299, loss 2.21654e-07, acc 1\n",
      "2018-10-26T18:10:16.495630: step 18300, loss 0.000161991, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:10:16.951409: step 18300, loss 6.39634, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18300\n",
      "\n",
      "2018-10-26T18:10:17.318544: step 18301, loss 1.31313e-06, acc 1\n",
      "2018-10-26T18:10:17.490084: step 18302, loss 7.56778e-05, acc 1\n",
      "2018-10-26T18:10:17.669605: step 18303, loss 5.49059e-06, acc 1\n",
      "2018-10-26T18:10:17.834167: step 18304, loss 1.36153e-06, acc 1\n",
      "2018-10-26T18:10:18.026652: step 18305, loss 8.17691e-07, acc 1\n",
      "2018-10-26T18:10:18.255042: step 18306, loss 1.47229e-05, acc 1\n",
      "2018-10-26T18:10:18.422593: step 18307, loss 0.000429852, acc 1\n",
      "2018-10-26T18:10:18.599121: step 18308, loss 9.29447e-07, acc 1\n",
      "2018-10-26T18:10:18.771660: step 18309, loss 1.17806e-05, acc 1\n",
      "2018-10-26T18:10:18.958162: step 18310, loss 2.60769e-07, acc 1\n",
      "2018-10-26T18:10:19.120727: step 18311, loss 3.14756e-06, acc 1\n",
      "2018-10-26T18:10:19.298253: step 18312, loss 7.24215e-06, acc 1\n",
      "2018-10-26T18:10:19.461816: step 18313, loss 6.24799e-06, acc 1\n",
      "2018-10-26T18:10:19.641336: step 18314, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:10:19.803903: step 18315, loss 0.000210864, acc 1\n",
      "2018-10-26T18:10:19.980430: step 18316, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:10:20.153967: step 18317, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:10:20.336479: step 18318, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:10:20.503034: step 18319, loss 0.000619563, acc 1\n",
      "2018-10-26T18:10:20.683552: step 18320, loss 3.06507e-05, acc 1\n",
      "2018-10-26T18:10:20.846117: step 18321, loss 1.18089e-06, acc 1\n",
      "2018-10-26T18:10:21.029628: step 18322, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:10:21.203176: step 18323, loss 0.0069119, acc 1\n",
      "2018-10-26T18:10:21.391660: step 18324, loss 2.5518e-07, acc 1\n",
      "2018-10-26T18:10:21.561207: step 18325, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:10:21.743719: step 18326, loss 5.66234e-07, acc 1\n",
      "2018-10-26T18:10:21.916258: step 18327, loss 0.00124767, acc 1\n",
      "2018-10-26T18:10:22.100766: step 18328, loss 0.00267784, acc 1\n",
      "2018-10-26T18:10:22.263332: step 18329, loss 0.000140461, acc 1\n",
      "2018-10-26T18:10:22.438862: step 18330, loss 6.61664e-05, acc 1\n",
      "2018-10-26T18:10:22.604419: step 18331, loss 0.000147544, acc 1\n",
      "2018-10-26T18:10:22.784938: step 18332, loss 2.57044e-07, acc 1\n",
      "2018-10-26T18:10:22.954485: step 18333, loss 5.4761e-07, acc 1\n",
      "2018-10-26T18:10:23.131013: step 18334, loss 7.26423e-07, acc 1\n",
      "2018-10-26T18:10:23.300560: step 18335, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:10:23.478097: step 18336, loss 0, acc 1\n",
      "2018-10-26T18:10:23.640652: step 18337, loss 4.87706e-05, acc 1\n",
      "2018-10-26T18:10:23.825158: step 18338, loss 1.65825e-05, acc 1\n",
      "2018-10-26T18:10:23.988721: step 18339, loss 7.11525e-07, acc 1\n",
      "2018-10-26T18:10:24.166247: step 18340, loss 7.18964e-07, acc 1\n",
      "2018-10-26T18:10:24.331805: step 18341, loss 2.25552e-06, acc 1\n",
      "2018-10-26T18:10:24.506339: step 18342, loss 8.2078e-05, acc 1\n",
      "2018-10-26T18:10:24.676883: step 18343, loss 5.69392e-05, acc 1\n",
      "2018-10-26T18:10:24.864382: step 18344, loss 2.23056e-05, acc 1\n",
      "2018-10-26T18:10:25.027944: step 18345, loss 3.59568e-05, acc 1\n",
      "2018-10-26T18:10:25.203475: step 18346, loss 2.57879e-05, acc 1\n",
      "2018-10-26T18:10:25.374020: step 18347, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:10:25.567503: step 18348, loss 0.000427091, acc 1\n",
      "2018-10-26T18:10:25.740042: step 18349, loss 7.35737e-07, acc 1\n",
      "2018-10-26T18:10:25.928553: step 18350, loss 0.0520326, acc 0.984375\n",
      "2018-10-26T18:10:26.099082: step 18351, loss 9.3132e-08, acc 1\n",
      "2018-10-26T18:10:26.282592: step 18352, loss 0.000327548, acc 1\n",
      "2018-10-26T18:10:26.448151: step 18353, loss 9.58953e-05, acc 1\n",
      "2018-10-26T18:10:26.625675: step 18354, loss 3.53304e-06, acc 1\n",
      "2018-10-26T18:10:26.791233: step 18355, loss 9.68357e-06, acc 1\n",
      "2018-10-26T18:10:26.972748: step 18356, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:10:27.137308: step 18357, loss 3.87428e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:10:27.314834: step 18358, loss 9.17551e-05, acc 1\n",
      "2018-10-26T18:10:27.485378: step 18359, loss 0.092679, acc 0.984375\n",
      "2018-10-26T18:10:27.657917: step 18360, loss 1.72832e-05, acc 1\n",
      "2018-10-26T18:10:27.824472: step 18361, loss 1.14846e-05, acc 1\n",
      "2018-10-26T18:10:27.998009: step 18362, loss 0.0071367, acc 1\n",
      "2018-10-26T18:10:28.164563: step 18363, loss 1.7303e-06, acc 1\n",
      "2018-10-26T18:10:28.343087: step 18364, loss 0.000148363, acc 1\n",
      "2018-10-26T18:10:28.507647: step 18365, loss 0.00804651, acc 1\n",
      "2018-10-26T18:10:28.680187: step 18366, loss 0.00320732, acc 1\n",
      "2018-10-26T18:10:28.904588: step 18367, loss 3.3155e-07, acc 1\n",
      "2018-10-26T18:10:29.092095: step 18368, loss 0.000139764, acc 1\n",
      "2018-10-26T18:10:29.283585: step 18369, loss 9.31311e-07, acc 1\n",
      "2018-10-26T18:10:29.456113: step 18370, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:10:29.629649: step 18371, loss 3.40861e-07, acc 1\n",
      "2018-10-26T18:10:29.794210: step 18372, loss 1.52424e-05, acc 1\n",
      "2018-10-26T18:10:29.982706: step 18373, loss 5.79868e-05, acc 1\n",
      "2018-10-26T18:10:30.152253: step 18374, loss 1.60926e-06, acc 1\n",
      "2018-10-26T18:10:30.323795: step 18375, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:10:30.489352: step 18376, loss 0, acc 1\n",
      "2018-10-26T18:10:30.659897: step 18377, loss 8.83777e-06, acc 1\n",
      "2018-10-26T18:10:30.826451: step 18378, loss 0.0577712, acc 0.984375\n",
      "2018-10-26T18:10:30.995002: step 18379, loss 5.3323e-06, acc 1\n",
      "2018-10-26T18:10:31.172529: step 18380, loss 5.47611e-07, acc 1\n",
      "2018-10-26T18:10:31.349055: step 18381, loss 0.00219299, acc 1\n",
      "2018-10-26T18:10:31.517628: step 18382, loss 1.89005e-05, acc 1\n",
      "2018-10-26T18:10:31.690144: step 18383, loss 0.00423602, acc 1\n",
      "2018-10-26T18:10:31.861686: step 18384, loss 4.1363e-05, acc 1\n",
      "2018-10-26T18:10:32.040209: step 18385, loss 0.000344845, acc 1\n",
      "2018-10-26T18:10:32.209756: step 18386, loss 0.000178427, acc 1\n",
      "2018-10-26T18:10:32.381297: step 18387, loss 2.14203e-07, acc 1\n",
      "2018-10-26T18:10:32.554833: step 18388, loss 7.65007e-06, acc 1\n",
      "2018-10-26T18:10:32.735351: step 18389, loss 0.000395615, acc 1\n",
      "2018-10-26T18:10:32.901908: step 18390, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:10:33.074446: step 18391, loss 5.87368e-06, acc 1\n",
      "2018-10-26T18:10:33.238009: step 18392, loss 3.36184e-06, acc 1\n",
      "2018-10-26T18:10:33.419523: step 18393, loss 2.61263e-05, acc 1\n",
      "2018-10-26T18:10:33.624974: step 18394, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:10:33.813472: step 18395, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:10:34.001968: step 18396, loss 0.000122763, acc 1\n",
      "2018-10-26T18:10:34.208417: step 18397, loss 3.53899e-07, acc 1\n",
      "2018-10-26T18:10:34.386939: step 18398, loss 2.96692e-06, acc 1\n",
      "2018-10-26T18:10:34.608347: step 18399, loss 3.67222e-05, acc 1\n",
      "2018-10-26T18:10:34.826764: step 18400, loss 4.28408e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:10:35.341389: step 18400, loss 6.49187, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18400\n",
      "\n",
      "2018-10-26T18:10:35.791311: step 18401, loss 2.89616e-06, acc 1\n",
      "2018-10-26T18:10:35.974821: step 18402, loss 1.21254e-06, acc 1\n",
      "2018-10-26T18:10:36.193238: step 18403, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:10:36.399686: step 18404, loss 0.00173902, acc 1\n",
      "2018-10-26T18:10:36.681931: step 18405, loss 0.000129433, acc 1\n",
      "2018-10-26T18:10:36.864443: step 18406, loss 7.31905e-05, acc 1\n",
      "2018-10-26T18:10:37.116770: step 18407, loss 8.00724e-05, acc 1\n",
      "2018-10-26T18:10:37.315239: step 18408, loss 1.2647e-06, acc 1\n",
      "2018-10-26T18:10:37.517698: step 18409, loss 0.000121993, acc 1\n",
      "2018-10-26T18:10:37.764040: step 18410, loss 5.14084e-07, acc 1\n",
      "2018-10-26T18:10:37.951539: step 18411, loss 0.000326753, acc 1\n",
      "2018-10-26T18:10:38.217828: step 18412, loss 1.15666e-06, acc 1\n",
      "2018-10-26T18:10:38.397348: step 18413, loss 3.59824e-06, acc 1\n",
      "2018-10-26T18:10:38.590831: step 18414, loss 2.25218e-05, acc 1\n",
      "2018-10-26T18:10:38.799274: step 18415, loss 5.34572e-07, acc 1\n",
      "2018-10-26T18:10:38.987771: step 18416, loss 1.81039e-06, acc 1\n",
      "2018-10-26T18:10:39.199205: step 18417, loss 0.0004259, acc 1\n",
      "2018-10-26T18:10:39.375734: step 18418, loss 0.000123278, acc 1\n",
      "2018-10-26T18:10:39.550267: step 18419, loss 1.19706e-05, acc 1\n",
      "2018-10-26T18:10:39.749734: step 18420, loss 3.40861e-07, acc 1\n",
      "2018-10-26T18:10:39.930252: step 18421, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:10:40.128722: step 18422, loss 0.000109877, acc 1\n",
      "2018-10-26T18:10:40.309240: step 18423, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:10:40.540621: step 18424, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:10:40.733107: step 18425, loss 0.00159828, acc 1\n",
      "2018-10-26T18:10:40.915619: step 18426, loss 0.00808413, acc 1\n",
      "2018-10-26T18:10:41.097134: step 18427, loss 9.94625e-07, acc 1\n",
      "2018-10-26T18:10:41.306575: step 18428, loss 1.96873e-06, acc 1\n",
      "2018-10-26T18:10:41.485101: step 18429, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:10:41.675589: step 18430, loss 0.000163093, acc 1\n",
      "2018-10-26T18:10:41.844139: step 18431, loss 5.32441e-06, acc 1\n",
      "2018-10-26T18:10:42.064550: step 18432, loss 2.01455e-05, acc 1\n",
      "2018-10-26T18:10:42.238086: step 18433, loss 2.69989e-05, acc 1\n",
      "2018-10-26T18:10:42.446530: step 18434, loss 9.01583e-06, acc 1\n",
      "2018-10-26T18:10:42.621062: step 18435, loss 2.19791e-07, acc 1\n",
      "2018-10-26T18:10:42.797592: step 18436, loss 0.000108169, acc 1\n",
      "2018-10-26T18:10:42.993069: step 18437, loss 1.97439e-07, acc 1\n",
      "2018-10-26T18:10:43.169597: step 18438, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:10:43.347123: step 18439, loss 9.81309e-06, acc 1\n",
      "2018-10-26T18:10:43.516670: step 18440, loss 0.000193826, acc 1\n",
      "2018-10-26T18:10:43.725113: step 18441, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:10:43.894660: step 18442, loss 0.00154937, acc 1\n",
      "2018-10-26T18:10:44.074181: step 18443, loss 2.35568e-05, acc 1\n",
      "2018-10-26T18:10:44.242730: step 18444, loss 2.25379e-07, acc 1\n",
      "2018-10-26T18:10:44.456159: step 18445, loss 4.08718e-05, acc 1\n",
      "2018-10-26T18:10:44.626704: step 18446, loss 2.10842e-05, acc 1\n",
      "2018-10-26T18:10:44.812208: step 18447, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:10:44.977766: step 18448, loss 9.21466e-05, acc 1\n",
      "2018-10-26T18:10:45.153296: step 18449, loss 0.000175081, acc 1\n",
      "2018-10-26T18:10:45.323842: step 18450, loss 4.21367e-06, acc 1\n",
      "2018-10-26T18:10:45.504359: step 18451, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:10:45.669917: step 18452, loss 3.89989e-06, acc 1\n",
      "2018-10-26T18:10:45.847443: step 18453, loss 6.89164e-07, acc 1\n",
      "2018-10-26T18:10:46.010007: step 18454, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:10:46.183544: step 18455, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:10:46.348105: step 18456, loss 1.14414e-05, acc 1\n",
      "2018-10-26T18:10:46.527625: step 18457, loss 1.11514e-05, acc 1\n",
      "2018-10-26T18:10:46.703157: step 18458, loss 5.97898e-07, acc 1\n",
      "2018-10-26T18:10:46.879684: step 18459, loss 2.06753e-07, acc 1\n",
      "2018-10-26T18:10:47.042249: step 18460, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:10:47.217782: step 18461, loss 5.18847e-06, acc 1\n",
      "2018-10-26T18:10:47.383339: step 18462, loss 0.000282551, acc 1\n",
      "2018-10-26T18:10:47.562859: step 18463, loss 3.68578e-05, acc 1\n",
      "2018-10-26T18:10:47.728416: step 18464, loss 6.10936e-07, acc 1\n",
      "2018-10-26T18:10:47.911926: step 18465, loss 6.29561e-07, acc 1\n",
      "2018-10-26T18:10:48.077484: step 18466, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:10:48.252017: step 18467, loss 1.6566e-05, acc 1\n",
      "2018-10-26T18:10:48.419570: step 18468, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:10:48.606072: step 18469, loss 0.00540978, acc 1\n",
      "2018-10-26T18:10:48.770658: step 18470, loss 0, acc 1\n",
      "2018-10-26T18:10:48.950152: step 18471, loss 1.39693e-06, acc 1\n",
      "2018-10-26T18:10:49.111720: step 18472, loss 0.000154745, acc 1\n",
      "2018-10-26T18:10:49.290243: step 18473, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:10:49.453806: step 18474, loss 1.03001e-06, acc 1\n",
      "2018-10-26T18:10:49.632330: step 18475, loss 2.18101e-06, acc 1\n",
      "2018-10-26T18:10:49.800879: step 18476, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:10:49.979404: step 18477, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:10:50.147952: step 18478, loss 7.26431e-08, acc 1\n",
      "2018-10-26T18:10:50.323483: step 18479, loss 7.26112e-06, acc 1\n",
      "2018-10-26T18:10:50.487046: step 18480, loss 8.19563e-08, acc 1\n",
      "2018-10-26T18:10:50.670556: step 18481, loss 1.60738e-05, acc 1\n",
      "2018-10-26T18:10:50.838108: step 18482, loss 0.00542806, acc 1\n",
      "2018-10-26T18:10:51.015645: step 18483, loss 1.82538e-07, acc 1\n",
      "2018-10-26T18:10:51.184183: step 18484, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:10:51.368690: step 18485, loss 0.00110753, acc 1\n",
      "2018-10-26T18:10:51.536242: step 18486, loss 2.83121e-07, acc 1\n",
      "2018-10-26T18:10:51.708781: step 18487, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:10:51.877332: step 18488, loss 9.42486e-07, acc 1\n",
      "2018-10-26T18:10:52.052862: step 18489, loss 0.000195088, acc 1\n",
      "2018-10-26T18:10:52.220415: step 18490, loss 0.000151072, acc 1\n",
      "2018-10-26T18:10:52.392954: step 18491, loss 8.10599e-06, acc 1\n",
      "2018-10-26T18:10:52.558512: step 18492, loss 3.59264e-06, acc 1\n",
      "2018-10-26T18:10:52.740027: step 18493, loss 3.78718e-05, acc 1\n",
      "2018-10-26T18:10:52.905584: step 18494, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:10:53.099067: step 18495, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:10:53.284572: step 18496, loss 0.000472507, acc 1\n",
      "2018-10-26T18:10:53.467083: step 18497, loss 6.44464e-07, acc 1\n",
      "2018-10-26T18:10:53.644609: step 18498, loss 1.11382e-06, acc 1\n",
      "2018-10-26T18:10:53.821138: step 18499, loss 7.18695e-06, acc 1\n",
      "2018-10-26T18:10:53.990701: step 18500, loss 2.67838e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:10:54.431507: step 18500, loss 6.51223, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18500\n",
      "\n",
      "2018-10-26T18:10:54.751634: step 18501, loss 0.000169301, acc 1\n",
      "2018-10-26T18:10:54.914199: step 18502, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:10:55.105688: step 18503, loss 0, acc 1\n",
      "2018-10-26T18:10:55.275274: step 18504, loss 2.49273e-05, acc 1\n",
      "2018-10-26T18:10:55.451802: step 18505, loss 4.51454e-06, acc 1\n",
      "2018-10-26T18:10:55.701135: step 18506, loss 2.11396e-06, acc 1\n",
      "2018-10-26T18:10:55.865696: step 18507, loss 2.74672e-05, acc 1\n",
      "2018-10-26T18:10:56.039232: step 18508, loss 1.0952e-06, acc 1\n",
      "2018-10-26T18:10:56.215761: step 18509, loss 0.000315604, acc 1\n",
      "2018-10-26T18:10:56.392289: step 18510, loss 0.000112392, acc 1\n",
      "2018-10-26T18:10:56.568817: step 18511, loss 3.669e-06, acc 1\n",
      "2018-10-26T18:10:56.737367: step 18512, loss 7.52893e-06, acc 1\n",
      "2018-10-26T18:10:56.914892: step 18513, loss 1.1846e-06, acc 1\n",
      "2018-10-26T18:10:57.091421: step 18514, loss 9.68573e-08, acc 1\n",
      "2018-10-26T18:10:57.267949: step 18515, loss 0.00112043, acc 1\n",
      "2018-10-26T18:10:57.439491: step 18516, loss 4.68799e-06, acc 1\n",
      "2018-10-26T18:10:57.610036: step 18517, loss 1.55151e-06, acc 1\n",
      "2018-10-26T18:10:57.795540: step 18518, loss 0.00144921, acc 1\n",
      "2018-10-26T18:10:57.964089: step 18519, loss 4.40454e-06, acc 1\n",
      "2018-10-26T18:10:58.139620: step 18520, loss 9.04126e-06, acc 1\n",
      "2018-10-26T18:10:58.311162: step 18521, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:10:58.486693: step 18522, loss 5.29463e-06, acc 1\n",
      "2018-10-26T18:10:58.662237: step 18523, loss 4.33564e-06, acc 1\n",
      "2018-10-26T18:10:58.850720: step 18524, loss 0.000166547, acc 1\n",
      "2018-10-26T18:10:59.026252: step 18525, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:10:59.203777: step 18526, loss 1.2442e-06, acc 1\n",
      "2018-10-26T18:10:59.385292: step 18527, loss 1.96685e-06, acc 1\n",
      "2018-10-26T18:10:59.547857: step 18528, loss 2.83655e-06, acc 1\n",
      "2018-10-26T18:10:59.724386: step 18529, loss 3.25878e-05, acc 1\n",
      "2018-10-26T18:10:59.897922: step 18530, loss 0.00128614, acc 1\n",
      "2018-10-26T18:11:00.081432: step 18531, loss 7.92512e-05, acc 1\n",
      "2018-10-26T18:11:00.246990: step 18532, loss 2.34575e-05, acc 1\n",
      "2018-10-26T18:11:00.417534: step 18533, loss 6.66813e-07, acc 1\n",
      "2018-10-26T18:11:00.581097: step 18534, loss 2.12705e-05, acc 1\n",
      "2018-10-26T18:11:00.760617: step 18535, loss 1.27217e-06, acc 1\n",
      "2018-10-26T18:11:00.932160: step 18536, loss 1.82807e-05, acc 1\n",
      "2018-10-26T18:11:01.104698: step 18537, loss 8.90331e-07, acc 1\n",
      "2018-10-26T18:11:01.276241: step 18538, loss 0.0821083, acc 0.984375\n",
      "2018-10-26T18:11:01.453790: step 18539, loss 9.29433e-07, acc 1\n",
      "2018-10-26T18:11:01.623312: step 18540, loss 9.12695e-08, acc 1\n",
      "2018-10-26T18:11:01.792859: step 18541, loss 0.0286826, acc 0.984375\n",
      "2018-10-26T18:11:01.963403: step 18542, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:11:02.135943: step 18543, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:11:02.299506: step 18544, loss 3.6038e-06, acc 1\n",
      "2018-10-26T18:11:02.482018: step 18545, loss 3.6194e-05, acc 1\n",
      "2018-10-26T18:11:02.649570: step 18546, loss 1.53478e-06, acc 1\n",
      "2018-10-26T18:11:02.828094: step 18547, loss 9.93008e-06, acc 1\n",
      "2018-10-26T18:11:02.999634: step 18548, loss 4.39581e-07, acc 1\n",
      "2018-10-26T18:11:03.170180: step 18549, loss 0.00016377, acc 1\n",
      "2018-10-26T18:11:03.339726: step 18550, loss 6.02104e-06, acc 1\n",
      "2018-10-26T18:11:03.514261: step 18551, loss 0.000180071, acc 1\n",
      "2018-10-26T18:11:03.677823: step 18552, loss 4.09581e-06, acc 1\n",
      "2018-10-26T18:11:03.863327: step 18553, loss 2.6209e-05, acc 1\n",
      "2018-10-26T18:11:04.029882: step 18554, loss 1.94229e-05, acc 1\n",
      "2018-10-26T18:11:04.203419: step 18555, loss 3.72017e-05, acc 1\n",
      "2018-10-26T18:11:04.366981: step 18556, loss 0.00390885, acc 1\n",
      "2018-10-26T18:11:04.540517: step 18557, loss 0.00236918, acc 1\n",
      "2018-10-26T18:11:04.712060: step 18558, loss 0.000748809, acc 1\n",
      "2018-10-26T18:11:04.919506: step 18559, loss 0.0409652, acc 0.984375\n",
      "2018-10-26T18:11:05.085063: step 18560, loss 0.000825429, acc 1\n",
      "2018-10-26T18:11:05.264584: step 18561, loss 0.0159378, acc 0.984375\n",
      "2018-10-26T18:11:05.428146: step 18562, loss 4.16982e-05, acc 1\n",
      "2018-10-26T18:11:05.622634: step 18563, loss 2.90417e-05, acc 1\n",
      "2018-10-26T18:11:05.788185: step 18564, loss 5.39343e-06, acc 1\n",
      "2018-10-26T18:11:05.962718: step 18565, loss 1.12872e-06, acc 1\n",
      "2018-10-26T18:11:06.132265: step 18566, loss 0.00023104, acc 1\n",
      "2018-10-26T18:11:06.312783: step 18567, loss 0.00013932, acc 1\n",
      "2018-10-26T18:11:06.475348: step 18568, loss 2.01164e-07, acc 1\n",
      "2018-10-26T18:11:06.648884: step 18569, loss 0.000109175, acc 1\n",
      "2018-10-26T18:11:06.823419: step 18570, loss 3.29974e-05, acc 1\n",
      "2018-10-26T18:11:07.002939: step 18571, loss 0, acc 1\n",
      "2018-10-26T18:11:07.183457: step 18572, loss 2.32829e-07, acc 1\n",
      "2018-10-26T18:11:07.355995: step 18573, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:11:07.532524: step 18574, loss 5.30068e-05, acc 1\n",
      "2018-10-26T18:11:07.709052: step 18575, loss 8.00936e-08, acc 1\n",
      "2018-10-26T18:11:07.896552: step 18576, loss 1.87004e-06, acc 1\n",
      "2018-10-26T18:11:08.062108: step 18577, loss 1.67638e-07, acc 1\n",
      "2018-10-26T18:11:08.248610: step 18578, loss 4.20952e-07, acc 1\n",
      "2018-10-26T18:11:08.418157: step 18579, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:11:08.596680: step 18580, loss 0.000651005, acc 1\n",
      "2018-10-26T18:11:08.762242: step 18581, loss 4.57772e-06, acc 1\n",
      "2018-10-26T18:11:08.939764: step 18582, loss 5.35672e-05, acc 1\n",
      "2018-10-26T18:11:09.107316: step 18583, loss 1.01512e-06, acc 1\n",
      "2018-10-26T18:11:09.287833: step 18584, loss 0.000153507, acc 1\n",
      "2018-10-26T18:11:09.458378: step 18585, loss 5.80186e-05, acc 1\n",
      "2018-10-26T18:11:09.633910: step 18586, loss 0.00357929, acc 1\n",
      "2018-10-26T18:11:09.796474: step 18587, loss 3.10114e-06, acc 1\n",
      "2018-10-26T18:11:09.974998: step 18588, loss 1.84529e-05, acc 1\n",
      "2018-10-26T18:11:10.142550: step 18589, loss 0.000139744, acc 1\n",
      "2018-10-26T18:11:10.322071: step 18590, loss 8.47274e-06, acc 1\n",
      "2018-10-26T18:11:10.487629: step 18591, loss 3.74929e-05, acc 1\n",
      "2018-10-26T18:11:10.665154: step 18592, loss 0.000422253, acc 1\n",
      "2018-10-26T18:11:10.837692: step 18593, loss 9.23845e-07, acc 1\n",
      "2018-10-26T18:11:11.014221: step 18594, loss 0.000811326, acc 1\n",
      "2018-10-26T18:11:11.190749: step 18595, loss 2.33936e-06, acc 1\n",
      "2018-10-26T18:11:11.361294: step 18596, loss 2.29283e-06, acc 1\n",
      "2018-10-26T18:11:11.538820: step 18597, loss 1.00253e-05, acc 1\n",
      "2018-10-26T18:11:11.702383: step 18598, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:11:11.885892: step 18599, loss 1.01325e-06, acc 1\n",
      "2018-10-26T18:11:12.051450: step 18600, loss 0.00189651, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:11:12.495264: step 18600, loss 6.51496, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18600\n",
      "\n",
      "2018-10-26T18:11:12.865623: step 18601, loss 1.84425e-05, acc 1\n",
      "2018-10-26T18:11:13.030182: step 18602, loss 2.73785e-06, acc 1\n",
      "2018-10-26T18:11:13.212705: step 18603, loss 2.7394e-05, acc 1\n",
      "2018-10-26T18:11:13.381244: step 18604, loss 0.000105189, acc 1\n",
      "2018-10-26T18:11:13.591681: step 18605, loss 0.00149386, acc 1\n",
      "2018-10-26T18:11:13.814087: step 18606, loss 2.17928e-07, acc 1\n",
      "2018-10-26T18:11:13.985629: step 18607, loss 8.38353e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:11:14.181108: step 18608, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:11:14.354674: step 18609, loss 9.59233e-07, acc 1\n",
      "2018-10-26T18:11:14.526184: step 18610, loss 5.13448e-06, acc 1\n",
      "2018-10-26T18:11:14.709695: step 18611, loss 1.0087e-05, acc 1\n",
      "2018-10-26T18:11:14.869268: step 18612, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:11:15.041807: step 18613, loss 6.03587e-06, acc 1\n",
      "2018-10-26T18:11:15.207365: step 18614, loss 7.26431e-08, acc 1\n",
      "2018-10-26T18:11:15.381898: step 18615, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:11:15.553440: step 18616, loss 4.29272e-05, acc 1\n",
      "2018-10-26T18:11:15.725979: step 18617, loss 1.67312e-05, acc 1\n",
      "2018-10-26T18:11:15.895526: step 18618, loss 4.58994e-05, acc 1\n",
      "2018-10-26T18:11:16.065073: step 18619, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:11:16.231654: step 18620, loss 5.04698e-06, acc 1\n",
      "2018-10-26T18:11:16.400178: step 18621, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:11:16.567735: step 18622, loss 1.05608e-06, acc 1\n",
      "2018-10-26T18:11:16.741267: step 18623, loss 6.77484e-06, acc 1\n",
      "2018-10-26T18:11:16.912807: step 18624, loss 1.50122e-06, acc 1\n",
      "2018-10-26T18:11:17.087342: step 18625, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:11:17.259881: step 18626, loss 2.92385e-05, acc 1\n",
      "2018-10-26T18:11:17.435412: step 18627, loss 4.84596e-06, acc 1\n",
      "2018-10-26T18:11:17.608948: step 18628, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:11:17.789467: step 18629, loss 2.73807e-07, acc 1\n",
      "2018-10-26T18:11:17.965994: step 18630, loss 6.94756e-07, acc 1\n",
      "2018-10-26T18:11:18.131553: step 18631, loss 4.64102e-06, acc 1\n",
      "2018-10-26T18:11:18.307082: step 18632, loss 3.76967e-05, acc 1\n",
      "2018-10-26T18:11:18.474653: step 18633, loss 7.7674e-06, acc 1\n",
      "2018-10-26T18:11:18.648172: step 18634, loss 1.15833e-05, acc 1\n",
      "2018-10-26T18:11:18.821723: step 18635, loss 0.11758, acc 0.984375\n",
      "2018-10-26T18:11:19.004220: step 18636, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:11:19.177756: step 18637, loss 0.0332287, acc 0.984375\n",
      "2018-10-26T18:11:19.352291: step 18638, loss 0.0374546, acc 0.984375\n",
      "2018-10-26T18:11:19.519842: step 18639, loss 3.2056e-05, acc 1\n",
      "2018-10-26T18:11:19.692381: step 18640, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:11:19.857938: step 18641, loss 0.000271017, acc 1\n",
      "2018-10-26T18:11:20.030478: step 18642, loss 5.17807e-07, acc 1\n",
      "2018-10-26T18:11:20.200024: step 18643, loss 4.52563e-05, acc 1\n",
      "2018-10-26T18:11:20.371567: step 18644, loss 0.00248646, acc 1\n",
      "2018-10-26T18:11:20.534131: step 18645, loss 2.35333e-05, acc 1\n",
      "2018-10-26T18:11:20.704676: step 18646, loss 0, acc 1\n",
      "2018-10-26T18:11:20.871231: step 18647, loss 1.06355e-06, acc 1\n",
      "2018-10-26T18:11:21.042773: step 18648, loss 0.000319335, acc 1\n",
      "2018-10-26T18:11:21.209328: step 18649, loss 2.17573e-05, acc 1\n",
      "2018-10-26T18:11:21.382864: step 18650, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:11:21.557399: step 18651, loss 1.4219e-05, acc 1\n",
      "2018-10-26T18:11:21.727943: step 18652, loss 2.84415e-05, acc 1\n",
      "2018-10-26T18:11:21.899484: step 18653, loss 1.4226e-05, acc 1\n",
      "2018-10-26T18:11:22.063047: step 18654, loss 2.55775e-05, acc 1\n",
      "2018-10-26T18:11:22.230600: step 18655, loss 6.1809e-06, acc 1\n",
      "2018-10-26T18:11:22.393165: step 18656, loss 6.63088e-07, acc 1\n",
      "2018-10-26T18:11:22.570721: step 18657, loss 1.17343e-06, acc 1\n",
      "2018-10-26T18:11:22.739240: step 18658, loss 1.63163e-06, acc 1\n",
      "2018-10-26T18:11:22.932731: step 18659, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:11:23.142165: step 18660, loss 4.25759e-06, acc 1\n",
      "2018-10-26T18:11:23.341631: step 18661, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:11:23.539103: step 18662, loss 5.94827e-06, acc 1\n",
      "2018-10-26T18:11:23.730592: step 18663, loss 5.06971e-05, acc 1\n",
      "2018-10-26T18:11:23.902134: step 18664, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:11:24.077670: step 18665, loss 1.61892e-05, acc 1\n",
      "2018-10-26T18:11:24.256188: step 18666, loss 6.57259e-06, acc 1\n",
      "2018-10-26T18:11:24.442690: step 18667, loss 9.28396e-05, acc 1\n",
      "2018-10-26T18:11:24.622210: step 18668, loss 4.97323e-07, acc 1\n",
      "2018-10-26T18:11:24.808711: step 18669, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:11:24.988231: step 18670, loss 0.000775723, acc 1\n",
      "2018-10-26T18:11:25.166755: step 18671, loss 0.0937563, acc 0.984375\n",
      "2018-10-26T18:11:25.329320: step 18672, loss 0.000224096, acc 1\n",
      "2018-10-26T18:11:25.507843: step 18673, loss 6.13642e-05, acc 1\n",
      "2018-10-26T18:11:25.672404: step 18674, loss 2.34117e-06, acc 1\n",
      "2018-10-26T18:11:25.845940: step 18675, loss 2.65269e-05, acc 1\n",
      "2018-10-26T18:11:26.017481: step 18676, loss 0.00101468, acc 1\n",
      "2018-10-26T18:11:26.196004: step 18677, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:11:26.359567: step 18678, loss 0.00024557, acc 1\n",
      "2018-10-26T18:11:26.541082: step 18679, loss 0.000112521, acc 1\n",
      "2018-10-26T18:11:26.708635: step 18680, loss 2.53318e-07, acc 1\n",
      "2018-10-26T18:11:26.894141: step 18681, loss 0.00320266, acc 1\n",
      "2018-10-26T18:11:27.068672: step 18682, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:11:27.241212: step 18683, loss 1.86264e-07, acc 1\n",
      "2018-10-26T18:11:27.412754: step 18684, loss 4.71218e-05, acc 1\n",
      "2018-10-26T18:11:27.590279: step 18685, loss 1.206e-05, acc 1\n",
      "2018-10-26T18:11:27.754840: step 18686, loss 0.000152644, acc 1\n",
      "2018-10-26T18:11:27.930371: step 18687, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:11:28.099917: step 18688, loss 8.44407e-05, acc 1\n",
      "2018-10-26T18:11:28.287418: step 18689, loss 6.48684e-06, acc 1\n",
      "2018-10-26T18:11:28.448986: step 18690, loss 1.56704e-05, acc 1\n",
      "2018-10-26T18:11:28.622521: step 18691, loss 5.3309e-05, acc 1\n",
      "2018-10-26T18:11:28.790073: step 18692, loss 0, acc 1\n",
      "2018-10-26T18:11:28.962612: step 18693, loss 6.65728e-05, acc 1\n",
      "2018-10-26T18:11:29.132160: step 18694, loss 0.000151847, acc 1\n",
      "2018-10-26T18:11:29.310682: step 18695, loss 9.01688e-05, acc 1\n",
      "2018-10-26T18:11:29.488210: step 18696, loss 4.41171e-05, acc 1\n",
      "2018-10-26T18:11:29.678721: step 18697, loss 4.24335e-05, acc 1\n",
      "2018-10-26T18:11:29.864203: step 18698, loss 0.000124261, acc 1\n",
      "2018-10-26T18:11:30.054725: step 18699, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:11:30.228231: step 18700, loss 1.18215e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:11:30.672044: step 18700, loss 6.50007, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18700\n",
      "\n",
      "2018-10-26T18:11:30.993187: step 18701, loss 3.65075e-07, acc 1\n",
      "2018-10-26T18:11:31.161737: step 18702, loss 0.0181895, acc 0.984375\n",
      "2018-10-26T18:11:31.337268: step 18703, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:11:31.508809: step 18704, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:11:31.693316: step 18705, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:11:31.943648: step 18706, loss 3.89288e-07, acc 1\n",
      "2018-10-26T18:11:32.116186: step 18707, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:11:32.298699: step 18708, loss 1.97439e-07, acc 1\n",
      "2018-10-26T18:11:32.471239: step 18709, loss 0.000333961, acc 1\n",
      "2018-10-26T18:11:32.651756: step 18710, loss 4.56344e-07, acc 1\n",
      "2018-10-26T18:11:32.820306: step 18711, loss 8.45089e-05, acc 1\n",
      "2018-10-26T18:11:33.001820: step 18712, loss 4.47035e-08, acc 1\n",
      "2018-10-26T18:11:33.166381: step 18713, loss 9.62622e-06, acc 1\n",
      "2018-10-26T18:11:33.343905: step 18714, loss 1.50122e-06, acc 1\n",
      "2018-10-26T18:11:33.517443: step 18715, loss 1.35781e-06, acc 1\n",
      "2018-10-26T18:11:33.701949: step 18716, loss 3.01913e-06, acc 1\n",
      "2018-10-26T18:11:33.865512: step 18717, loss 2.5219e-06, acc 1\n",
      "2018-10-26T18:11:34.054010: step 18718, loss 0.0233395, acc 0.984375\n",
      "2018-10-26T18:11:34.217573: step 18719, loss 0.000295062, acc 1\n",
      "2018-10-26T18:11:34.395097: step 18720, loss 0.00253362, acc 1\n",
      "2018-10-26T18:11:34.565642: step 18721, loss 3.97066e-06, acc 1\n",
      "2018-10-26T18:11:34.748154: step 18722, loss 2.96325e-06, acc 1\n",
      "2018-10-26T18:11:34.916703: step 18723, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:11:35.104202: step 18724, loss 0.0133989, acc 0.984375\n",
      "2018-10-26T18:11:35.269761: step 18725, loss 3.25961e-07, acc 1\n",
      "2018-10-26T18:11:35.453271: step 18726, loss 3.20372e-07, acc 1\n",
      "2018-10-26T18:11:35.615836: step 18727, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:11:35.792365: step 18728, loss 1.30754e-06, acc 1\n",
      "2018-10-26T18:11:35.969890: step 18729, loss 0.0123858, acc 0.984375\n",
      "2018-10-26T18:11:36.148413: step 18730, loss 6.24548e-05, acc 1\n",
      "2018-10-26T18:11:36.311976: step 18731, loss 0.000139439, acc 1\n",
      "2018-10-26T18:11:36.486509: step 18732, loss 0.000403413, acc 1\n",
      "2018-10-26T18:11:36.654061: step 18733, loss 3.37137e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:11:36.832585: step 18734, loss 5.27785e-06, acc 1\n",
      "2018-10-26T18:11:36.993156: step 18735, loss 1.01355e-05, acc 1\n",
      "2018-10-26T18:11:37.175696: step 18736, loss 0, acc 1\n",
      "2018-10-26T18:11:37.340229: step 18737, loss 0.000287656, acc 1\n",
      "2018-10-26T18:11:37.510773: step 18738, loss 0.00160393, acc 1\n",
      "2018-10-26T18:11:37.683312: step 18739, loss 6.10828e-06, acc 1\n",
      "2018-10-26T18:11:37.859840: step 18740, loss 3.6225e-06, acc 1\n",
      "2018-10-26T18:11:38.029387: step 18741, loss 0.0086197, acc 1\n",
      "2018-10-26T18:11:38.202924: step 18742, loss 6.22016e-06, acc 1\n",
      "2018-10-26T18:11:38.370476: step 18743, loss 1.84137e-05, acc 1\n",
      "2018-10-26T18:11:38.549997: step 18744, loss 8.97769e-07, acc 1\n",
      "2018-10-26T18:11:38.714557: step 18745, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:11:38.896072: step 18746, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:11:39.062627: step 18747, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:11:39.282040: step 18748, loss 1.21584e-05, acc 1\n",
      "2018-10-26T18:11:39.486495: step 18749, loss 4.40463e-06, acc 1\n",
      "2018-10-26T18:11:39.653049: step 18750, loss 2.2482e-05, acc 1\n",
      "2018-10-26T18:11:39.862489: step 18751, loss 8.03403e-06, acc 1\n",
      "2018-10-26T18:11:40.060959: step 18752, loss 0.000246406, acc 1\n",
      "2018-10-26T18:11:40.273391: step 18753, loss 2.20903e-06, acc 1\n",
      "2018-10-26T18:11:40.499787: step 18754, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:11:40.680305: step 18755, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:11:40.920662: step 18756, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:11:41.134093: step 18757, loss 1.67816e-06, acc 1\n",
      "2018-10-26T18:11:41.313613: step 18758, loss 9.83133e-06, acc 1\n",
      "2018-10-26T18:11:41.542999: step 18759, loss 0.000275816, acc 1\n",
      "2018-10-26T18:11:41.747453: step 18760, loss 3.21654e-06, acc 1\n",
      "2018-10-26T18:11:41.934952: step 18761, loss 3.1106e-07, acc 1\n",
      "2018-10-26T18:11:42.164340: step 18762, loss 7.14105e-05, acc 1\n",
      "2018-10-26T18:11:42.364804: step 18763, loss 1.63912e-07, acc 1\n",
      "2018-10-26T18:11:42.542329: step 18764, loss 0.00122486, acc 1\n",
      "2018-10-26T18:11:42.791663: step 18765, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:11:43.014072: step 18766, loss 3.49321e-05, acc 1\n",
      "2018-10-26T18:11:43.215531: step 18767, loss 7.6771e-06, acc 1\n",
      "2018-10-26T18:11:43.468853: step 18768, loss 1.36514e-05, acc 1\n",
      "2018-10-26T18:11:43.672311: step 18769, loss 0.000344441, acc 1\n",
      "2018-10-26T18:11:43.892722: step 18770, loss 1.79029e-05, acc 1\n",
      "2018-10-26T18:11:44.123105: step 18771, loss 2.36555e-07, acc 1\n",
      "2018-10-26T18:11:44.310604: step 18772, loss 4.18485e-06, acc 1\n",
      "2018-10-26T18:11:44.497107: step 18773, loss 1.18273e-06, acc 1\n",
      "2018-10-26T18:11:44.679619: step 18774, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:11:44.885069: step 18775, loss 0.000327015, acc 1\n",
      "2018-10-26T18:11:45.058606: step 18776, loss 3.02314e-05, acc 1\n",
      "2018-10-26T18:11:45.238127: step 18777, loss 3.42725e-07, acc 1\n",
      "2018-10-26T18:11:45.401712: step 18778, loss 0.00146725, acc 1\n",
      "2018-10-26T18:11:45.613124: step 18779, loss 8.59562e-06, acc 1\n",
      "2018-10-26T18:11:45.823563: step 18780, loss 0.000653632, acc 1\n",
      "2018-10-26T18:11:46.028017: step 18781, loss 6.81394e-06, acc 1\n",
      "2018-10-26T18:11:46.203547: step 18782, loss 2.12105e-05, acc 1\n",
      "2018-10-26T18:11:46.438918: step 18783, loss 0.0265384, acc 0.984375\n",
      "2018-10-26T18:11:46.601484: step 18784, loss 9.23858e-07, acc 1\n",
      "2018-10-26T18:11:46.804941: step 18785, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:11:46.980471: step 18786, loss 0.0030693, acc 1\n",
      "2018-10-26T18:11:47.206867: step 18787, loss 2.66356e-07, acc 1\n",
      "2018-10-26T18:11:47.384392: step 18788, loss 0.000194452, acc 1\n",
      "2018-10-26T18:11:47.554937: step 18789, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:11:47.764377: step 18790, loss 2.82496e-05, acc 1\n",
      "2018-10-26T18:11:47.930932: step 18791, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:11:48.103470: step 18792, loss 1.06913e-06, acc 1\n",
      "2018-10-26T18:11:48.298948: step 18793, loss 0.000551916, acc 1\n",
      "2018-10-26T18:11:48.483456: step 18794, loss 4.80558e-07, acc 1\n",
      "2018-10-26T18:11:48.661979: step 18795, loss 1.87419e-05, acc 1\n",
      "2018-10-26T18:11:48.847483: step 18796, loss 2.14751e-06, acc 1\n",
      "2018-10-26T18:11:49.022017: step 18797, loss 3.00836e-05, acc 1\n",
      "2018-10-26T18:11:49.197548: step 18798, loss 6.55643e-07, acc 1\n",
      "2018-10-26T18:11:49.365100: step 18799, loss 6.3951e-06, acc 1\n",
      "2018-10-26T18:11:49.570552: step 18800, loss 9.87201e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:11:50.016360: step 18800, loss 6.73022, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18800\n",
      "\n",
      "2018-10-26T18:11:50.413675: step 18801, loss 5.88553e-06, acc 1\n",
      "2018-10-26T18:11:50.619126: step 18802, loss 1.5103e-05, acc 1\n",
      "2018-10-26T18:11:50.794657: step 18803, loss 2.99318e-05, acc 1\n",
      "2018-10-26T18:11:50.973180: step 18804, loss 2.88708e-07, acc 1\n",
      "2018-10-26T18:11:51.230493: step 18805, loss 1.60738e-06, acc 1\n",
      "2018-10-26T18:11:51.409016: step 18806, loss 8.62382e-07, acc 1\n",
      "2018-10-26T18:11:51.587539: step 18807, loss 3.95596e-06, acc 1\n",
      "2018-10-26T18:11:51.762072: step 18808, loss 1.72108e-05, acc 1\n",
      "2018-10-26T18:11:51.943588: step 18809, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:11:52.118122: step 18810, loss 4.56501e-06, acc 1\n",
      "2018-10-26T18:11:52.282682: step 18811, loss 0.000499729, acc 1\n",
      "2018-10-26T18:11:52.460208: step 18812, loss 2.05697e-05, acc 1\n",
      "2018-10-26T18:11:52.627760: step 18813, loss 3.02417e-05, acc 1\n",
      "2018-10-26T18:11:52.805285: step 18814, loss 0.000517238, acc 1\n",
      "2018-10-26T18:11:52.968847: step 18815, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:11:53.150363: step 18816, loss 1.32616e-06, acc 1\n",
      "2018-10-26T18:11:53.314924: step 18817, loss 0.00165532, acc 1\n",
      "2018-10-26T18:11:53.500428: step 18818, loss 3.31547e-07, acc 1\n",
      "2018-10-26T18:11:53.666982: step 18819, loss 7.11514e-07, acc 1\n",
      "2018-10-26T18:11:53.843512: step 18820, loss 1.91852e-07, acc 1\n",
      "2018-10-26T18:11:54.014055: step 18821, loss 5.43885e-07, acc 1\n",
      "2018-10-26T18:11:54.188589: step 18822, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:11:54.350157: step 18823, loss 0.114154, acc 0.984375\n",
      "2018-10-26T18:11:54.529678: step 18824, loss 1.50681e-06, acc 1\n",
      "2018-10-26T18:11:54.696233: step 18825, loss 7.38815e-05, acc 1\n",
      "2018-10-26T18:11:54.882735: step 18826, loss 5.17438e-05, acc 1\n",
      "2018-10-26T18:11:55.045300: step 18827, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:11:55.222826: step 18828, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:11:55.395364: step 18829, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:11:55.572890: step 18830, loss 5.64051e-05, acc 1\n",
      "2018-10-26T18:11:55.738447: step 18831, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:11:55.918966: step 18832, loss 6.46997e-06, acc 1\n",
      "2018-10-26T18:11:56.087516: step 18833, loss 0, acc 1\n",
      "2018-10-26T18:11:56.270028: step 18834, loss 1.24793e-06, acc 1\n",
      "2018-10-26T18:11:56.436583: step 18835, loss 2.02831e-06, acc 1\n",
      "2018-10-26T18:11:56.617100: step 18836, loss 1.22693e-05, acc 1\n",
      "2018-10-26T18:11:56.780663: step 18837, loss 0.00649214, acc 1\n",
      "2018-10-26T18:11:56.962178: step 18838, loss 1.31684e-06, acc 1\n",
      "2018-10-26T18:11:57.126739: step 18839, loss 1.15852e-06, acc 1\n",
      "2018-10-26T18:11:57.304265: step 18840, loss 0, acc 1\n",
      "2018-10-26T18:11:57.482788: step 18841, loss 6.91449e-06, acc 1\n",
      "2018-10-26T18:11:57.661310: step 18842, loss 4.80557e-07, acc 1\n",
      "2018-10-26T18:11:57.827866: step 18843, loss 6.11015e-06, acc 1\n",
      "2018-10-26T18:11:58.004393: step 18844, loss 5.24005e-05, acc 1\n",
      "2018-10-26T18:11:58.166959: step 18845, loss 3.41946e-05, acc 1\n",
      "2018-10-26T18:11:58.341493: step 18846, loss 1.88126e-07, acc 1\n",
      "2018-10-26T18:11:58.508048: step 18847, loss 0.0499864, acc 0.984375\n",
      "2018-10-26T18:11:58.692555: step 18848, loss 0.000252041, acc 1\n",
      "2018-10-26T18:11:58.853125: step 18849, loss 0.000144234, acc 1\n",
      "2018-10-26T18:11:59.034641: step 18850, loss 0.0155616, acc 0.984375\n",
      "2018-10-26T18:11:59.201196: step 18851, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:11:59.378722: step 18852, loss 0, acc 1\n",
      "2018-10-26T18:11:59.546274: step 18853, loss 0.000573053, acc 1\n",
      "2018-10-26T18:11:59.727790: step 18854, loss 0.0373343, acc 0.984375\n",
      "2018-10-26T18:11:59.891353: step 18855, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:12:00.079849: step 18856, loss 2.08616e-07, acc 1\n",
      "2018-10-26T18:12:00.251390: step 18857, loss 2.44006e-07, acc 1\n",
      "2018-10-26T18:12:00.436895: step 18858, loss 5.10937e-05, acc 1\n",
      "2018-10-26T18:12:00.597465: step 18859, loss 2.60768e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:12:00.781973: step 18860, loss 9.23833e-06, acc 1\n",
      "2018-10-26T18:12:00.949526: step 18861, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:12:01.123061: step 18862, loss 4.63172e-06, acc 1\n",
      "2018-10-26T18:12:01.289616: step 18863, loss 5.70159e-05, acc 1\n",
      "2018-10-26T18:12:01.460160: step 18864, loss 4.04995e-05, acc 1\n",
      "2018-10-26T18:12:01.628710: step 18865, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:12:01.797260: step 18866, loss 0.000421506, acc 1\n",
      "2018-10-26T18:12:01.959825: step 18867, loss 0.0684375, acc 0.984375\n",
      "2018-10-26T18:12:02.137352: step 18868, loss 3.34847e-05, acc 1\n",
      "2018-10-26T18:12:02.308893: step 18869, loss 2.94655e-06, acc 1\n",
      "2018-10-26T18:12:02.477442: step 18870, loss 0.00472919, acc 1\n",
      "2018-10-26T18:12:02.642003: step 18871, loss 0.0107535, acc 1\n",
      "2018-10-26T18:12:02.811549: step 18872, loss 0.000200865, acc 1\n",
      "2018-10-26T18:12:02.984089: step 18873, loss 2.8312e-07, acc 1\n",
      "2018-10-26T18:12:03.161615: step 18874, loss 7.84163e-06, acc 1\n",
      "2018-10-26T18:12:03.325177: step 18875, loss 7.20832e-07, acc 1\n",
      "2018-10-26T18:12:03.499711: step 18876, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:12:03.664271: step 18877, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:12:03.842795: step 18878, loss 1.06171e-07, acc 1\n",
      "2018-10-26T18:12:04.005360: step 18879, loss 4.19965e-05, acc 1\n",
      "2018-10-26T18:12:04.183883: step 18880, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:12:04.347447: step 18881, loss 4.55911e-06, acc 1\n",
      "2018-10-26T18:12:04.530958: step 18882, loss 0.000616629, acc 1\n",
      "2018-10-26T18:12:04.703495: step 18883, loss 0.00249404, acc 1\n",
      "2018-10-26T18:12:04.877032: step 18884, loss 1.63912e-07, acc 1\n",
      "2018-10-26T18:12:05.042589: step 18885, loss 1.0207e-06, acc 1\n",
      "2018-10-26T18:12:05.216125: step 18886, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:12:05.387666: step 18887, loss 3.06191e-06, acc 1\n",
      "2018-10-26T18:12:05.568185: step 18888, loss 1.56386e-05, acc 1\n",
      "2018-10-26T18:12:05.731747: step 18889, loss 9.02566e-06, acc 1\n",
      "2018-10-26T18:12:05.910271: step 18890, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:12:06.132682: step 18891, loss 7.44328e-06, acc 1\n",
      "2018-10-26T18:12:06.319178: step 18892, loss 5.14565e-05, acc 1\n",
      "2018-10-26T18:12:06.492715: step 18893, loss 5.7741e-07, acc 1\n",
      "2018-10-26T18:12:06.664256: step 18894, loss 6.48978e-05, acc 1\n",
      "2018-10-26T18:12:06.839787: step 18895, loss 1.69819e-05, acc 1\n",
      "2018-10-26T18:12:07.014321: step 18896, loss 5.53197e-07, acc 1\n",
      "2018-10-26T18:12:07.182870: step 18897, loss 1.88531e-05, acc 1\n",
      "2018-10-26T18:12:07.362391: step 18898, loss 1.25165e-06, acc 1\n",
      "2018-10-26T18:12:07.540914: step 18899, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:12:07.703480: step 18900, loss 6.47691e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:12:08.148290: step 18900, loss 6.6228, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-18900\n",
      "\n",
      "2018-10-26T18:12:08.534336: step 18901, loss 0.00126224, acc 1\n",
      "2018-10-26T18:12:08.713857: step 18902, loss 1.40067e-06, acc 1\n",
      "2018-10-26T18:12:08.890385: step 18903, loss 4.49413e-06, acc 1\n",
      "2018-10-26T18:12:09.063922: step 18904, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:12:09.250423: step 18905, loss 1.0271e-05, acc 1\n",
      "2018-10-26T18:12:09.496766: step 18906, loss 4.65657e-07, acc 1\n",
      "2018-10-26T18:12:09.668307: step 18907, loss 0, acc 1\n",
      "2018-10-26T18:12:09.851817: step 18908, loss 1.30194e-06, acc 1\n",
      "2018-10-26T18:12:10.014382: step 18909, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:12:10.191908: step 18910, loss 9.61096e-07, acc 1\n",
      "2018-10-26T18:12:10.356468: step 18911, loss 2.12375e-05, acc 1\n",
      "2018-10-26T18:12:10.533994: step 18912, loss 7.63684e-08, acc 1\n",
      "2018-10-26T18:12:10.698554: step 18913, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:12:10.871093: step 18914, loss 1.1211e-05, acc 1\n",
      "2018-10-26T18:12:11.036651: step 18915, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:12:11.208192: step 18916, loss 0, acc 1\n",
      "2018-10-26T18:12:11.377740: step 18917, loss 5.44547e-06, acc 1\n",
      "2018-10-26T18:12:11.551275: step 18918, loss 3.13267e-06, acc 1\n",
      "2018-10-26T18:12:11.716833: step 18919, loss 1.24979e-06, acc 1\n",
      "2018-10-26T18:12:11.891367: step 18920, loss 6.1599e-05, acc 1\n",
      "2018-10-26T18:12:12.060915: step 18921, loss 6.849e-05, acc 1\n",
      "2018-10-26T18:12:12.235447: step 18922, loss 5.39143e-06, acc 1\n",
      "2018-10-26T18:12:12.404994: step 18923, loss 5.02908e-07, acc 1\n",
      "2018-10-26T18:12:12.580526: step 18924, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:12:12.743091: step 18925, loss 8.24003e-06, acc 1\n",
      "2018-10-26T18:12:12.922612: step 18926, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:12:13.089167: step 18927, loss 5.30828e-05, acc 1\n",
      "2018-10-26T18:12:13.260709: step 18928, loss 0, acc 1\n",
      "2018-10-26T18:12:13.431252: step 18929, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:12:13.608778: step 18930, loss 0.000146042, acc 1\n",
      "2018-10-26T18:12:13.773339: step 18931, loss 0.0674712, acc 0.984375\n",
      "2018-10-26T18:12:13.950865: step 18932, loss 0.000216334, acc 1\n",
      "2018-10-26T18:12:14.116423: step 18933, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:12:14.293947: step 18934, loss 4.42726e-06, acc 1\n",
      "2018-10-26T18:12:14.458509: step 18935, loss 1.15481e-06, acc 1\n",
      "2018-10-26T18:12:14.631047: step 18936, loss 0.00406979, acc 1\n",
      "2018-10-26T18:12:14.799596: step 18937, loss 1.31684e-06, acc 1\n",
      "2018-10-26T18:12:14.974131: step 18938, loss 1.41185e-06, acc 1\n",
      "2018-10-26T18:12:15.141683: step 18939, loss 0.000441334, acc 1\n",
      "2018-10-26T18:12:15.315220: step 18940, loss 2.62632e-07, acc 1\n",
      "2018-10-26T18:12:15.485787: step 18941, loss 1.2661e-05, acc 1\n",
      "2018-10-26T18:12:15.658303: step 18942, loss 1.81416e-06, acc 1\n",
      "2018-10-26T18:12:15.830841: step 18943, loss 8.88422e-06, acc 1\n",
      "2018-10-26T18:12:16.014353: step 18944, loss 3.44586e-07, acc 1\n",
      "2018-10-26T18:12:16.177915: step 18945, loss 5.30849e-07, acc 1\n",
      "2018-10-26T18:12:16.357435: step 18946, loss 0.000648677, acc 1\n",
      "2018-10-26T18:12:16.522992: step 18947, loss 0.0088711, acc 1\n",
      "2018-10-26T18:12:16.703509: step 18948, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:12:16.865078: step 18949, loss 0.000144639, acc 1\n",
      "2018-10-26T18:12:17.046594: step 18950, loss 2.32629e-06, acc 1\n",
      "2018-10-26T18:12:17.222124: step 18951, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:12:17.399650: step 18952, loss 1.72658e-06, acc 1\n",
      "2018-10-26T18:12:17.567202: step 18953, loss 0.000503966, acc 1\n",
      "2018-10-26T18:12:17.741735: step 18954, loss 2.6372e-05, acc 1\n",
      "2018-10-26T18:12:17.915273: step 18955, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:12:18.098782: step 18956, loss 9.72276e-06, acc 1\n",
      "2018-10-26T18:12:18.274313: step 18957, loss 1.29092e-05, acc 1\n",
      "2018-10-26T18:12:18.444858: step 18958, loss 7.339e-06, acc 1\n",
      "2018-10-26T18:12:18.620389: step 18959, loss 0.021962, acc 0.984375\n",
      "2018-10-26T18:12:18.789936: step 18960, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:12:18.976437: step 18961, loss 5.36433e-07, acc 1\n",
      "2018-10-26T18:12:19.144986: step 18962, loss 9.77872e-07, acc 1\n",
      "2018-10-26T18:12:19.319520: step 18963, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:12:19.486075: step 18964, loss 0.00044832, acc 1\n",
      "2018-10-26T18:12:19.661619: step 18965, loss 3.49397e-06, acc 1\n",
      "2018-10-26T18:12:19.831153: step 18966, loss 0.000147882, acc 1\n",
      "2018-10-26T18:12:20.010674: step 18967, loss 0.00148919, acc 1\n",
      "2018-10-26T18:12:20.180220: step 18968, loss 3.24632e-06, acc 1\n",
      "2018-10-26T18:12:20.362733: step 18969, loss 0.000174839, acc 1\n",
      "2018-10-26T18:12:20.534274: step 18970, loss 2.50506e-06, acc 1\n",
      "2018-10-26T18:12:20.711801: step 18971, loss 1.11759e-07, acc 1\n",
      "2018-10-26T18:12:20.877358: step 18972, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:12:21.057877: step 18973, loss 2.15686e-06, acc 1\n",
      "2018-10-26T18:12:21.229418: step 18974, loss 1.61671e-06, acc 1\n",
      "2018-10-26T18:12:21.407940: step 18975, loss 1.79634e-05, acc 1\n",
      "2018-10-26T18:12:21.576490: step 18976, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:12:21.759003: step 18977, loss 1.06586e-05, acc 1\n",
      "2018-10-26T18:12:21.925557: step 18978, loss 6.36366e-06, acc 1\n",
      "2018-10-26T18:12:22.100091: step 18979, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:12:22.270636: step 18980, loss 1.52048e-05, acc 1\n",
      "2018-10-26T18:12:22.444172: step 18981, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:12:22.613718: step 18982, loss 3.00483e-05, acc 1\n",
      "2018-10-26T18:12:22.797914: step 18983, loss 5.90259e-05, acc 1\n",
      "2018-10-26T18:12:22.962475: step 18984, loss 1.43604e-06, acc 1\n",
      "2018-10-26T18:12:23.142992: step 18985, loss 8.5307e-07, acc 1\n",
      "2018-10-26T18:12:23.310544: step 18986, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:12:23.486075: step 18987, loss 3.2686e-06, acc 1\n",
      "2018-10-26T18:12:23.656619: step 18988, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:12:23.836139: step 18989, loss 5.32708e-07, acc 1\n",
      "2018-10-26T18:12:24.004689: step 18990, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:12:24.175234: step 18991, loss 5.18647e-05, acc 1\n",
      "2018-10-26T18:12:24.340792: step 18992, loss 1.76522e-05, acc 1\n",
      "2018-10-26T18:12:24.513331: step 18993, loss 1.92367e-05, acc 1\n",
      "2018-10-26T18:12:24.687864: step 18994, loss 7.11518e-07, acc 1\n",
      "2018-10-26T18:12:24.861400: step 18995, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:12:25.032942: step 18996, loss 7.26431e-08, acc 1\n",
      "2018-10-26T18:12:25.205481: step 18997, loss 2.23323e-06, acc 1\n",
      "2018-10-26T18:12:25.375028: step 18998, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:12:25.550559: step 18999, loss 4.55685e-05, acc 1\n",
      "2018-10-26T18:12:25.714121: step 19000, loss 3.78444e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:12:26.159930: step 19000, loss 6.75679, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19000\n",
      "\n",
      "2018-10-26T18:12:26.532721: step 19001, loss 6.28764e-06, acc 1\n",
      "2018-10-26T18:12:26.705259: step 19002, loss 2.10477e-07, acc 1\n",
      "2018-10-26T18:12:26.879793: step 19003, loss 0.00207257, acc 1\n",
      "2018-10-26T18:12:27.049340: step 19004, loss 0.000265749, acc 1\n",
      "2018-10-26T18:12:27.233846: step 19005, loss 2.86846e-07, acc 1\n",
      "2018-10-26T18:12:27.476199: step 19006, loss 0.000108947, acc 1\n",
      "2018-10-26T18:12:27.642754: step 19007, loss 4.43304e-07, acc 1\n",
      "2018-10-26T18:12:27.822275: step 19008, loss 2.38647e-05, acc 1\n",
      "2018-10-26T18:12:27.992819: step 19009, loss 1.57386e-06, acc 1\n",
      "2018-10-26T18:12:28.164360: step 19010, loss 6.14665e-07, acc 1\n",
      "2018-10-26T18:12:28.334904: step 19011, loss 5.75551e-07, acc 1\n",
      "2018-10-26T18:12:28.514424: step 19012, loss 4.48895e-07, acc 1\n",
      "2018-10-26T18:12:28.693945: step 19013, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:12:28.868479: step 19014, loss 7.43178e-07, acc 1\n",
      "2018-10-26T18:12:29.038025: step 19015, loss 5.81044e-06, acc 1\n",
      "2018-10-26T18:12:29.213557: step 19016, loss 1.35173e-05, acc 1\n",
      "2018-10-26T18:12:29.377119: step 19017, loss 0.000236842, acc 1\n",
      "2018-10-26T18:12:29.548662: step 19018, loss 3.2053e-06, acc 1\n",
      "2018-10-26T18:12:29.720203: step 19019, loss 0.00383668, acc 1\n",
      "2018-10-26T18:12:29.901719: step 19020, loss 1.06119e-05, acc 1\n",
      "2018-10-26T18:12:30.068274: step 19021, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:12:30.244829: step 19022, loss 0.00770685, acc 1\n",
      "2018-10-26T18:12:30.413352: step 19023, loss 1.82253e-05, acc 1\n",
      "2018-10-26T18:12:30.587895: step 19024, loss 4.87718e-05, acc 1\n",
      "2018-10-26T18:12:30.756435: step 19025, loss 0.0312268, acc 0.984375\n",
      "2018-10-26T18:12:30.927976: step 19026, loss 4.87242e-06, acc 1\n",
      "2018-10-26T18:12:31.091539: step 19027, loss 0.000295234, acc 1\n",
      "2018-10-26T18:12:31.265076: step 19028, loss 0, acc 1\n",
      "2018-10-26T18:12:31.431631: step 19029, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:12:31.604170: step 19030, loss 0.000684198, acc 1\n",
      "2018-10-26T18:12:31.766735: step 19031, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:12:31.943263: step 19032, loss 1.32248e-07, acc 1\n",
      "2018-10-26T18:12:32.109819: step 19033, loss 2.42487e-05, acc 1\n",
      "2018-10-26T18:12:32.282358: step 19034, loss 1.0915e-06, acc 1\n",
      "2018-10-26T18:12:32.453899: step 19035, loss 3.25929e-06, acc 1\n",
      "2018-10-26T18:12:32.628433: step 19036, loss 0.00208005, acc 1\n",
      "2018-10-26T18:12:32.792993: step 19037, loss 0, acc 1\n",
      "2018-10-26T18:12:32.972514: step 19038, loss 0.000215631, acc 1\n",
      "2018-10-26T18:12:33.141063: step 19039, loss 0.00059403, acc 1\n",
      "2018-10-26T18:12:33.322578: step 19040, loss 1.38392e-06, acc 1\n",
      "2018-10-26T18:12:33.494120: step 19041, loss 0.000262867, acc 1\n",
      "2018-10-26T18:12:33.672643: step 19042, loss 0.157397, acc 0.984375\n",
      "2018-10-26T18:12:33.840196: step 19043, loss 0.0662489, acc 0.984375\n",
      "2018-10-26T18:12:34.016723: step 19044, loss 2.06753e-07, acc 1\n",
      "2018-10-26T18:12:34.180287: step 19045, loss 9.80714e-05, acc 1\n",
      "2018-10-26T18:12:34.357814: step 19046, loss 4.00419e-06, acc 1\n",
      "2018-10-26T18:12:34.526363: step 19047, loss 5.68097e-07, acc 1\n",
      "2018-10-26T18:12:34.700896: step 19048, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:12:34.865456: step 19049, loss 0.000159944, acc 1\n",
      "2018-10-26T18:12:35.036997: step 19050, loss 3.15903e-07, acc 1\n",
      "2018-10-26T18:12:35.202555: step 19051, loss 0, acc 1\n",
      "2018-10-26T18:12:35.377089: step 19052, loss 0.000131021, acc 1\n",
      "2018-10-26T18:12:35.546636: step 19053, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:12:35.725158: step 19054, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:12:35.896700: step 19055, loss 2.7454e-06, acc 1\n",
      "2018-10-26T18:12:36.070237: step 19056, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:12:36.235795: step 19057, loss 4.92452e-06, acc 1\n",
      "2018-10-26T18:12:36.411325: step 19058, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:12:36.585860: step 19059, loss 1.8381e-05, acc 1\n",
      "2018-10-26T18:12:36.769368: step 19060, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:12:36.944901: step 19061, loss 4.11516e-05, acc 1\n",
      "2018-10-26T18:12:37.126415: step 19062, loss 0.00215181, acc 1\n",
      "2018-10-26T18:12:37.294965: step 19063, loss 3.57401e-06, acc 1\n",
      "2018-10-26T18:12:37.470496: step 19064, loss 1.38836e-05, acc 1\n",
      "2018-10-26T18:12:37.639046: step 19065, loss 2.64106e-06, acc 1\n",
      "2018-10-26T18:12:37.820560: step 19066, loss 2.25379e-07, acc 1\n",
      "2018-10-26T18:12:37.993099: step 19067, loss 3.33193e-06, acc 1\n",
      "2018-10-26T18:12:38.174614: step 19068, loss 7.68617e-06, acc 1\n",
      "2018-10-26T18:12:38.347154: step 19069, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:12:38.514706: step 19070, loss 3.24098e-07, acc 1\n",
      "2018-10-26T18:12:38.691234: step 19071, loss 0.000223064, acc 1\n",
      "2018-10-26T18:12:38.855795: step 19072, loss 7.46908e-07, acc 1\n",
      "2018-10-26T18:12:39.029331: step 19073, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:12:39.200872: step 19074, loss 0, acc 1\n",
      "2018-10-26T18:12:39.374409: step 19075, loss 1.33892e-05, acc 1\n",
      "2018-10-26T18:12:39.538969: step 19076, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:12:39.722479: step 19077, loss 1.36342e-06, acc 1\n",
      "2018-10-26T18:12:39.895018: step 19078, loss 2.26482e-06, acc 1\n",
      "2018-10-26T18:12:40.068554: step 19079, loss 4.87056e-05, acc 1\n",
      "2018-10-26T18:12:40.233115: step 19080, loss 9.05237e-07, acc 1\n",
      "2018-10-26T18:12:40.411637: step 19081, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:12:40.579190: step 19082, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:12:40.763697: step 19083, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:12:40.928258: step 19084, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:12:41.102791: step 19085, loss 0.000294466, acc 1\n",
      "2018-10-26T18:12:41.273336: step 19086, loss 6.25538e-05, acc 1\n",
      "2018-10-26T18:12:41.451858: step 19087, loss 1.5309e-05, acc 1\n",
      "2018-10-26T18:12:41.614423: step 19088, loss 3.49636e-05, acc 1\n",
      "2018-10-26T18:12:41.798931: step 19089, loss 1.28941e-05, acc 1\n",
      "2018-10-26T18:12:41.963491: step 19090, loss 2.70082e-07, acc 1\n",
      "2018-10-26T18:12:42.143011: step 19091, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:12:42.317544: step 19092, loss 1.58693e-06, acc 1\n",
      "2018-10-26T18:12:42.490084: step 19093, loss 3.08369e-05, acc 1\n",
      "2018-10-26T18:12:42.666612: step 19094, loss 0.000293804, acc 1\n",
      "2018-10-26T18:12:42.850123: step 19095, loss 1.94339e-05, acc 1\n",
      "2018-10-26T18:12:43.025654: step 19096, loss 0.076003, acc 0.984375\n",
      "2018-10-26T18:12:43.200186: step 19097, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:12:43.380704: step 19098, loss 3.0361e-07, acc 1\n",
      "2018-10-26T18:12:43.553244: step 19099, loss 2.33746e-06, acc 1\n",
      "2018-10-26T18:12:43.732764: step 19100, loss 4.56341e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:12:44.189543: step 19100, loss 6.70811, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19100\n",
      "\n",
      "2018-10-26T18:12:44.527774: step 19101, loss 2.18317e-05, acc 1\n",
      "2018-10-26T18:12:44.693332: step 19102, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:12:44.866868: step 19103, loss 4.61467e-05, acc 1\n",
      "2018-10-26T18:12:45.044393: step 19104, loss 0, acc 1\n",
      "2018-10-26T18:12:45.251840: step 19105, loss 3.32827e-06, acc 1\n",
      "2018-10-26T18:12:45.521119: step 19106, loss 0.000145131, acc 1\n",
      "2018-10-26T18:12:45.726572: step 19107, loss 2.14203e-07, acc 1\n",
      "2018-10-26T18:12:45.925040: step 19108, loss 3.19987e-06, acc 1\n",
      "2018-10-26T18:12:46.140464: step 19109, loss 0.00229113, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:12:46.371847: step 19110, loss 3.31907e-06, acc 1\n",
      "2018-10-26T18:12:46.597246: step 19111, loss 9.94498e-05, acc 1\n",
      "2018-10-26T18:12:46.805689: step 19112, loss 2.9057e-07, acc 1\n",
      "2018-10-26T18:12:47.021114: step 19113, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:12:47.258478: step 19114, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:12:47.466921: step 19115, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:12:47.663396: step 19116, loss 2.5518e-07, acc 1\n",
      "2018-10-26T18:12:47.882810: step 19117, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:12:48.054352: step 19118, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:12:48.277754: step 19119, loss 3.24097e-07, acc 1\n",
      "2018-10-26T18:12:48.491185: step 19120, loss 1.00455e-05, acc 1\n",
      "2018-10-26T18:12:48.706608: step 19121, loss 5.56054e-05, acc 1\n",
      "2018-10-26T18:12:48.972897: step 19122, loss 0.00020722, acc 1\n",
      "2018-10-26T18:12:49.207272: step 19123, loss 3.51466e-06, acc 1\n",
      "2018-10-26T18:12:49.412724: step 19124, loss 0.000176116, acc 1\n",
      "2018-10-26T18:12:49.609197: step 19125, loss 0.000526737, acc 1\n",
      "2018-10-26T18:12:49.844568: step 19126, loss 0.00363211, acc 1\n",
      "2018-10-26T18:12:50.091907: step 19127, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:12:50.294367: step 19128, loss 1.79242e-05, acc 1\n",
      "2018-10-26T18:12:50.480874: step 19129, loss 5.30851e-07, acc 1\n",
      "2018-10-26T18:12:50.709259: step 19130, loss 3.2295e-06, acc 1\n",
      "2018-10-26T18:12:50.880799: step 19131, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:12:51.108192: step 19132, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:12:51.298684: step 19133, loss 1.91282e-06, acc 1\n",
      "2018-10-26T18:12:51.519094: step 19134, loss 0.000657467, acc 1\n",
      "2018-10-26T18:12:51.734519: step 19135, loss 1.01218e-05, acc 1\n",
      "2018-10-26T18:12:51.929996: step 19136, loss 2.37101e-06, acc 1\n",
      "2018-10-26T18:12:52.153400: step 19137, loss 1.38575e-06, acc 1\n",
      "2018-10-26T18:12:52.332920: step 19138, loss 0.0030409, acc 1\n",
      "2018-10-26T18:12:52.536376: step 19139, loss 6.88406e-05, acc 1\n",
      "2018-10-26T18:12:52.756788: step 19140, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:12:52.946281: step 19141, loss 0.000170153, acc 1\n",
      "2018-10-26T18:12:53.132782: step 19142, loss 3.23708e-06, acc 1\n",
      "2018-10-26T18:12:53.342223: step 19143, loss 6.48196e-07, acc 1\n",
      "2018-10-26T18:12:53.548671: step 19144, loss 1.24108e-05, acc 1\n",
      "2018-10-26T18:12:53.726197: step 19145, loss 6.79852e-07, acc 1\n",
      "2018-10-26T18:12:53.904721: step 19146, loss 5.24805e-06, acc 1\n",
      "2018-10-26T18:12:54.115158: step 19147, loss 6.2176e-05, acc 1\n",
      "2018-10-26T18:12:54.283707: step 19148, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:12:54.460236: step 19149, loss 7.13167e-05, acc 1\n",
      "2018-10-26T18:12:54.631778: step 19150, loss 0.000224247, acc 1\n",
      "2018-10-26T18:12:54.842215: step 19151, loss 1.29262e-06, acc 1\n",
      "2018-10-26T18:12:55.009767: step 19152, loss 1.06171e-07, acc 1\n",
      "2018-10-26T18:12:55.181309: step 19153, loss 0.00162186, acc 1\n",
      "2018-10-26T18:12:55.349859: step 19154, loss 0.0334095, acc 0.984375\n",
      "2018-10-26T18:12:55.554312: step 19155, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:12:55.726852: step 19156, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:12:55.899391: step 19157, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:12:56.066943: step 19158, loss 6.39959e-05, acc 1\n",
      "2018-10-26T18:12:56.246463: step 19159, loss 1.17158e-06, acc 1\n",
      "2018-10-26T18:12:56.440944: step 19160, loss 9.60025e-05, acc 1\n",
      "2018-10-26T18:12:56.617472: step 19161, loss 0, acc 1\n",
      "2018-10-26T18:12:56.790011: step 19162, loss 1.50531e-05, acc 1\n",
      "2018-10-26T18:12:56.957563: step 19163, loss 1.63904e-06, acc 1\n",
      "2018-10-26T18:12:57.132097: step 19164, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:12:57.293665: step 19165, loss 9.70423e-07, acc 1\n",
      "2018-10-26T18:12:57.473186: step 19166, loss 0.00535512, acc 1\n",
      "2018-10-26T18:12:57.651709: step 19167, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:12:57.827240: step 19168, loss 5.40158e-07, acc 1\n",
      "2018-10-26T18:12:58.002771: step 19169, loss 2.25378e-07, acc 1\n",
      "2018-10-26T18:12:58.176307: step 19170, loss 0.000591922, acc 1\n",
      "2018-10-26T18:12:58.351838: step 19171, loss 0.000802838, acc 1\n",
      "2018-10-26T18:12:58.514404: step 19172, loss 4.73474e-05, acc 1\n",
      "2018-10-26T18:12:58.700905: step 19173, loss 3.72525e-07, acc 1\n",
      "2018-10-26T18:12:58.867461: step 19174, loss 3.55761e-07, acc 1\n",
      "2018-10-26T18:12:59.040000: step 19175, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:12:59.209546: step 19176, loss 2.89625e-05, acc 1\n",
      "2018-10-26T18:12:59.390065: step 19177, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:12:59.552630: step 19178, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:12:59.727164: step 19179, loss 7.04072e-07, acc 1\n",
      "2018-10-26T18:12:59.893718: step 19180, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:13:00.082215: step 19181, loss 2.32829e-07, acc 1\n",
      "2018-10-26T18:13:00.250765: step 19182, loss 1.23118e-06, acc 1\n",
      "2018-10-26T18:13:00.427293: step 19183, loss 2.66357e-07, acc 1\n",
      "2018-10-26T18:13:00.591853: step 19184, loss 1.76951e-07, acc 1\n",
      "2018-10-26T18:13:00.762397: step 19185, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:13:00.928952: step 19186, loss 0.000226646, acc 1\n",
      "2018-10-26T18:13:01.102489: step 19187, loss 8.56816e-08, acc 1\n",
      "2018-10-26T18:13:01.272036: step 19188, loss 1.29249e-05, acc 1\n",
      "2018-10-26T18:13:01.442580: step 19189, loss 5.83e-07, acc 1\n",
      "2018-10-26T18:13:01.611130: step 19190, loss 0.000382727, acc 1\n",
      "2018-10-26T18:13:01.788657: step 19191, loss 1.30335e-05, acc 1\n",
      "2018-10-26T18:13:01.954213: step 19192, loss 9.49948e-08, acc 1\n",
      "2018-10-26T18:13:02.128746: step 19193, loss 1.09895e-06, acc 1\n",
      "2018-10-26T18:13:02.299291: step 19194, loss 1.65208e-06, acc 1\n",
      "2018-10-26T18:13:02.469836: step 19195, loss 1.57576e-06, acc 1\n",
      "2018-10-26T18:13:02.638386: step 19196, loss 0, acc 1\n",
      "2018-10-26T18:13:02.816908: step 19197, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:13:02.977480: step 19198, loss 0.00143437, acc 1\n",
      "2018-10-26T18:13:03.152013: step 19199, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:13:03.323555: step 19200, loss 7.96697e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:13:03.768366: step 19200, loss 6.90673, acc 0.712946\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19200\n",
      "\n",
      "2018-10-26T18:13:04.098821: step 19201, loss 4.60795e-06, acc 1\n",
      "2018-10-26T18:13:04.262384: step 19202, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:13:04.437914: step 19203, loss 0.000312675, acc 1\n",
      "2018-10-26T18:13:04.605467: step 19204, loss 2.02248e-05, acc 1\n",
      "2018-10-26T18:13:04.796956: step 19205, loss 0.00033502, acc 1\n",
      "2018-10-26T18:13:05.047287: step 19206, loss 0.00462122, acc 1\n",
      "2018-10-26T18:13:05.215836: step 19207, loss 2.9057e-07, acc 1\n",
      "2018-10-26T18:13:05.394360: step 19208, loss 0.000168004, acc 1\n",
      "2018-10-26T18:13:05.565901: step 19209, loss 5.81719e-05, acc 1\n",
      "2018-10-26T18:13:05.742429: step 19210, loss 5.2704e-05, acc 1\n",
      "2018-10-26T18:13:05.913971: step 19211, loss 5.66234e-07, acc 1\n",
      "2018-10-26T18:13:06.100474: step 19212, loss 1.00378e-05, acc 1\n",
      "2018-10-26T18:13:06.265033: step 19213, loss 0.000972996, acc 1\n",
      "2018-10-26T18:13:06.440564: step 19214, loss 2.11403e-06, acc 1\n",
      "2018-10-26T18:13:06.617093: step 19215, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:13:06.795616: step 19216, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:13:06.963168: step 19217, loss 4.15365e-07, acc 1\n",
      "2018-10-26T18:13:07.138698: step 19218, loss 6.72407e-07, acc 1\n",
      "2018-10-26T18:13:07.305254: step 19219, loss 2.88878e-06, acc 1\n",
      "2018-10-26T18:13:07.484775: step 19220, loss 6.83576e-07, acc 1\n",
      "2018-10-26T18:13:07.695212: step 19221, loss 0.000203928, acc 1\n",
      "2018-10-26T18:13:07.873736: step 19222, loss 1.56462e-07, acc 1\n",
      "2018-10-26T18:13:08.047271: step 19223, loss 1.72662e-06, acc 1\n",
      "2018-10-26T18:13:08.226792: step 19224, loss 0.000345956, acc 1\n",
      "2018-10-26T18:13:08.391352: step 19225, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:13:08.567881: step 19226, loss 1.0356e-06, acc 1\n",
      "2018-10-26T18:13:08.739422: step 19227, loss 4.37474e-06, acc 1\n",
      "2018-10-26T18:13:08.918946: step 19228, loss 3.26689e-06, acc 1\n",
      "2018-10-26T18:13:09.089486: step 19229, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:13:09.271002: step 19230, loss 0.00012342, acc 1\n",
      "2018-10-26T18:13:09.433567: step 19231, loss 9.4434e-07, acc 1\n",
      "2018-10-26T18:13:09.610096: step 19232, loss 8.31265e-06, acc 1\n",
      "2018-10-26T18:13:09.785627: step 19233, loss 0.000151287, acc 1\n",
      "2018-10-26T18:13:09.964150: step 19234, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:13:10.136689: step 19235, loss 0.00153742, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:13:10.316209: step 19236, loss 8.00936e-08, acc 1\n",
      "2018-10-26T18:13:10.485756: step 19237, loss 5.29285e-06, acc 1\n",
      "2018-10-26T18:13:10.675824: step 19238, loss 2.73612e-06, acc 1\n",
      "2018-10-26T18:13:10.841381: step 19239, loss 4.7865e-06, acc 1\n",
      "2018-10-26T18:13:11.015915: step 19240, loss 0.000954335, acc 1\n",
      "2018-10-26T18:13:11.183471: step 19241, loss 5.28983e-07, acc 1\n",
      "2018-10-26T18:13:11.356006: step 19242, loss 2.02463e-06, acc 1\n",
      "2018-10-26T18:13:11.529543: step 19243, loss 4.18481e-05, acc 1\n",
      "2018-10-26T18:13:11.701084: step 19244, loss 1.03747e-06, acc 1\n",
      "2018-10-26T18:13:11.868636: step 19245, loss 9.57384e-07, acc 1\n",
      "2018-10-26T18:13:12.044167: step 19246, loss 3.84634e-05, acc 1\n",
      "2018-10-26T18:13:12.208728: step 19247, loss 2.32637e-06, acc 1\n",
      "2018-10-26T18:13:12.383261: step 19248, loss 2.0823e-06, acc 1\n",
      "2018-10-26T18:13:12.547822: step 19249, loss 0, acc 1\n",
      "2018-10-26T18:13:12.727342: step 19250, loss 5.83044e-05, acc 1\n",
      "2018-10-26T18:13:12.897885: step 19251, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:13:13.074415: step 19252, loss 6.6495e-07, acc 1\n",
      "2018-10-26T18:13:13.240970: step 19253, loss 0.000268049, acc 1\n",
      "2018-10-26T18:13:13.414506: step 19254, loss 9.70746e-06, acc 1\n",
      "2018-10-26T18:13:13.587045: step 19255, loss 0, acc 1\n",
      "2018-10-26T18:13:13.759583: step 19256, loss 7.72914e-06, acc 1\n",
      "2018-10-26T18:13:13.935115: step 19257, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:13:14.099675: step 19258, loss 2.02087e-06, acc 1\n",
      "2018-10-26T18:13:14.271217: step 19259, loss 1.06726e-06, acc 1\n",
      "2018-10-26T18:13:14.438769: step 19260, loss 5.23395e-07, acc 1\n",
      "2018-10-26T18:13:14.612305: step 19261, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:13:14.777863: step 19262, loss 5.99767e-07, acc 1\n",
      "2018-10-26T18:13:14.950402: step 19263, loss 2.99883e-07, acc 1\n",
      "2018-10-26T18:13:15.114979: step 19264, loss 0.000106307, acc 1\n",
      "2018-10-26T18:13:15.293486: step 19265, loss 0.000215548, acc 1\n",
      "2018-10-26T18:13:15.461037: step 19266, loss 7.99624e-05, acc 1\n",
      "2018-10-26T18:13:15.635572: step 19267, loss 8.1582e-07, acc 1\n",
      "2018-10-26T18:13:15.806115: step 19268, loss 8.15496e-06, acc 1\n",
      "2018-10-26T18:13:15.988629: step 19269, loss 4.97454e-06, acc 1\n",
      "2018-10-26T18:13:16.151193: step 19270, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:13:16.322736: step 19271, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:13:16.490287: step 19272, loss 2.37289e-06, acc 1\n",
      "2018-10-26T18:13:16.666817: step 19273, loss 9.49925e-07, acc 1\n",
      "2018-10-26T18:13:16.833371: step 19274, loss 4.28405e-07, acc 1\n",
      "2018-10-26T18:13:17.007904: step 19275, loss 9.64277e-06, acc 1\n",
      "2018-10-26T18:13:17.177452: step 19276, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:13:17.352983: step 19277, loss 1.71363e-07, acc 1\n",
      "2018-10-26T18:13:17.529511: step 19278, loss 3.62272e-06, acc 1\n",
      "2018-10-26T18:13:17.699058: step 19279, loss 1.00766e-06, acc 1\n",
      "2018-10-26T18:13:17.865613: step 19280, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:13:18.043138: step 19281, loss 1.10265e-06, acc 1\n",
      "2018-10-26T18:13:18.213683: step 19282, loss 6.73422e-05, acc 1\n",
      "2018-10-26T18:13:18.395198: step 19283, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:13:18.567736: step 19284, loss 2.40278e-05, acc 1\n",
      "2018-10-26T18:13:18.742272: step 19285, loss 1.30977e-05, acc 1\n",
      "2018-10-26T18:13:18.910821: step 19286, loss 2.73807e-07, acc 1\n",
      "2018-10-26T18:13:19.090341: step 19287, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:13:19.252906: step 19288, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:13:19.427440: step 19289, loss 1.72472e-06, acc 1\n",
      "2018-10-26T18:13:19.599979: step 19290, loss 1.06281e-05, acc 1\n",
      "2018-10-26T18:13:19.772518: step 19291, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:13:19.936080: step 19292, loss 2.65044e-06, acc 1\n",
      "2018-10-26T18:13:20.111612: step 19293, loss 2.57958e-06, acc 1\n",
      "2018-10-26T18:13:20.284152: step 19294, loss 3.8184e-07, acc 1\n",
      "2018-10-26T18:13:20.457688: step 19295, loss 2.64089e-05, acc 1\n",
      "2018-10-26T18:13:20.623245: step 19296, loss 3.19783e-06, acc 1\n",
      "2018-10-26T18:13:20.796781: step 19297, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:13:20.962339: step 19298, loss 1.65976e-05, acc 1\n",
      "2018-10-26T18:13:21.141860: step 19299, loss 2.48228e-05, acc 1\n",
      "2018-10-26T18:13:21.310410: step 19300, loss 4.84287e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:13:21.749236: step 19300, loss 6.85407, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19300\n",
      "\n",
      "2018-10-26T18:13:22.074254: step 19301, loss 5.86997e-06, acc 1\n",
      "2018-10-26T18:13:22.242804: step 19302, loss 6.77988e-07, acc 1\n",
      "2018-10-26T18:13:22.422323: step 19303, loss 5.83e-07, acc 1\n",
      "2018-10-26T18:13:22.588882: step 19304, loss 2.42787e-05, acc 1\n",
      "2018-10-26T18:13:22.781365: step 19305, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:13:23.039675: step 19306, loss 1.47148e-07, acc 1\n",
      "2018-10-26T18:13:23.211217: step 19307, loss 5.19407e-06, acc 1\n",
      "2018-10-26T18:13:23.391735: step 19308, loss 6.68676e-07, acc 1\n",
      "2018-10-26T18:13:23.557292: step 19309, loss 2.37469e-06, acc 1\n",
      "2018-10-26T18:13:23.735815: step 19310, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:13:23.901372: step 19311, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:13:24.076903: step 19312, loss 1.20881e-06, acc 1\n",
      "2018-10-26T18:13:24.239470: step 19313, loss 0.000619422, acc 1\n",
      "2018-10-26T18:13:24.415000: step 19314, loss 9.31244e-06, acc 1\n",
      "2018-10-26T18:13:24.577565: step 19315, loss 2.60769e-07, acc 1\n",
      "2018-10-26T18:13:24.749107: step 19316, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:13:24.919652: step 19317, loss 0.000106618, acc 1\n",
      "2018-10-26T18:13:25.096180: step 19318, loss 2.79878e-05, acc 1\n",
      "2018-10-26T18:13:25.265727: step 19319, loss 8.15441e-06, acc 1\n",
      "2018-10-26T18:13:25.443253: step 19320, loss 3.67143e-05, acc 1\n",
      "2018-10-26T18:13:25.605818: step 19321, loss 3.12983e-05, acc 1\n",
      "2018-10-26T18:13:25.781349: step 19322, loss 1.03568e-05, acc 1\n",
      "2018-10-26T18:13:25.952891: step 19323, loss 5.19671e-07, acc 1\n",
      "2018-10-26T18:13:26.125430: step 19324, loss 2.17929e-07, acc 1\n",
      "2018-10-26T18:13:26.288993: step 19325, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:13:26.469510: step 19326, loss 0.00198774, acc 1\n",
      "2018-10-26T18:13:26.636066: step 19327, loss 0.0207132, acc 0.984375\n",
      "2018-10-26T18:13:26.814589: step 19328, loss 2.1976e-05, acc 1\n",
      "2018-10-26T18:13:26.993111: step 19329, loss 7.25601e-05, acc 1\n",
      "2018-10-26T18:13:27.175624: step 19330, loss 5.17364e-06, acc 1\n",
      "2018-10-26T18:13:27.340185: step 19331, loss 3.10848e-06, acc 1\n",
      "2018-10-26T18:13:27.516712: step 19332, loss 3.81865e-05, acc 1\n",
      "2018-10-26T18:13:27.688254: step 19333, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:13:27.874756: step 19334, loss 3.84217e-06, acc 1\n",
      "2018-10-26T18:13:28.047296: step 19335, loss 7.93444e-05, acc 1\n",
      "2018-10-26T18:13:28.224821: step 19336, loss 0.0112529, acc 0.984375\n",
      "2018-10-26T18:13:28.388383: step 19337, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:13:28.562944: step 19338, loss 4.37716e-07, acc 1\n",
      "2018-10-26T18:13:28.726481: step 19339, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:13:28.913979: step 19340, loss 8.38826e-05, acc 1\n",
      "2018-10-26T18:13:29.082529: step 19341, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:13:29.264044: step 19342, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:13:29.431597: step 19343, loss 1.25362e-05, acc 1\n",
      "2018-10-26T18:13:29.611117: step 19344, loss 5.27808e-05, acc 1\n",
      "2018-10-26T18:13:29.777672: step 19345, loss 0.000477395, acc 1\n",
      "2018-10-26T18:13:29.961182: step 19346, loss 0.00227602, acc 1\n",
      "2018-10-26T18:13:30.123748: step 19347, loss 1.6856e-06, acc 1\n",
      "2018-10-26T18:13:30.300275: step 19348, loss 0.000102772, acc 1\n",
      "2018-10-26T18:13:30.470820: step 19349, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:13:30.647348: step 19350, loss 0.000331037, acc 1\n",
      "2018-10-26T18:13:30.812906: step 19351, loss 8.43756e-07, acc 1\n",
      "2018-10-26T18:13:30.991428: step 19352, loss 4.31553e-06, acc 1\n",
      "2018-10-26T18:13:31.160975: step 19353, loss 0, acc 1\n",
      "2018-10-26T18:13:31.338502: step 19354, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:13:31.506053: step 19355, loss 4.50488e-05, acc 1\n",
      "2018-10-26T18:13:31.695548: step 19356, loss 0.00013026, acc 1\n",
      "2018-10-26T18:13:31.863100: step 19357, loss 1.49753e-06, acc 1\n",
      "2018-10-26T18:13:32.039628: step 19358, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:13:32.209174: step 19359, loss 8.40037e-07, acc 1\n",
      "2018-10-26T18:13:32.394681: step 19360, loss 0.000180162, acc 1\n",
      "2018-10-26T18:13:32.561235: step 19361, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:13:32.741752: step 19362, loss 9.51345e-06, acc 1\n",
      "2018-10-26T18:13:32.912296: step 19363, loss 7.44883e-06, acc 1\n",
      "2018-10-26T18:13:33.087828: step 19364, loss 1.60552e-06, acc 1\n",
      "2018-10-26T18:13:33.262362: step 19365, loss 4.02994e-05, acc 1\n",
      "2018-10-26T18:13:33.432906: step 19366, loss 6.31424e-07, acc 1\n",
      "2018-10-26T18:13:33.598464: step 19367, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:13:33.779995: step 19368, loss 8.22181e-06, acc 1\n",
      "2018-10-26T18:13:33.950523: step 19369, loss 5.9929e-06, acc 1\n",
      "2018-10-26T18:13:34.124059: step 19370, loss 0.00085796, acc 1\n",
      "2018-10-26T18:13:34.291611: step 19371, loss 0.000203085, acc 1\n",
      "2018-10-26T18:13:34.470133: step 19372, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:13:34.646663: step 19373, loss 8.13019e-06, acc 1\n",
      "2018-10-26T18:13:34.842140: step 19374, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:13:35.031634: step 19375, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:13:35.251049: step 19376, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:13:35.423587: step 19377, loss 0.00195492, acc 1\n",
      "2018-10-26T18:13:35.595128: step 19378, loss 0.000335664, acc 1\n",
      "2018-10-26T18:13:35.760686: step 19379, loss 0.000249322, acc 1\n",
      "2018-10-26T18:13:35.944196: step 19380, loss 0, acc 1\n",
      "2018-10-26T18:13:36.107758: step 19381, loss 1.2512e-05, acc 1\n",
      "2018-10-26T18:13:36.286281: step 19382, loss 7.16441e-06, acc 1\n",
      "2018-10-26T18:13:36.454832: step 19383, loss 4.71737e-06, acc 1\n",
      "2018-10-26T18:13:36.630362: step 19384, loss 2.95029e-05, acc 1\n",
      "2018-10-26T18:13:36.796918: step 19385, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:13:36.972449: step 19386, loss 6.19434e-05, acc 1\n",
      "2018-10-26T18:13:37.143990: step 19387, loss 2.83121e-07, acc 1\n",
      "2018-10-26T18:13:37.310545: step 19388, loss 7.22219e-05, acc 1\n",
      "2018-10-26T18:13:37.478098: step 19389, loss 5.42782e-05, acc 1\n",
      "2018-10-26T18:13:37.660609: step 19390, loss 7.46341e-05, acc 1\n",
      "2018-10-26T18:13:37.836142: step 19391, loss 1.80666e-06, acc 1\n",
      "2018-10-26T18:13:38.008680: step 19392, loss 0, acc 1\n",
      "2018-10-26T18:13:38.169251: step 19393, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:13:38.348771: step 19394, loss 8.8063e-06, acc 1\n",
      "2018-10-26T18:13:38.513332: step 19395, loss 0.000445422, acc 1\n",
      "2018-10-26T18:13:38.690857: step 19396, loss 1.8812e-06, acc 1\n",
      "2018-10-26T18:13:38.855418: step 19397, loss 9.09481e-06, acc 1\n",
      "2018-10-26T18:13:39.035935: step 19398, loss 1.4156e-07, acc 1\n",
      "2018-10-26T18:13:39.207476: step 19399, loss 7.6553e-07, acc 1\n",
      "2018-10-26T18:13:39.385011: step 19400, loss 4.13503e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:13:39.829813: step 19400, loss 6.84136, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19400\n",
      "\n",
      "2018-10-26T18:13:40.222688: step 19401, loss 1.18089e-06, acc 1\n",
      "2018-10-26T18:13:40.408192: step 19402, loss 1.09706e-06, acc 1\n",
      "2018-10-26T18:13:40.587712: step 19403, loss 4.05842e-06, acc 1\n",
      "2018-10-26T18:13:40.762245: step 19404, loss 0.000955541, acc 1\n",
      "2018-10-26T18:13:41.008587: step 19405, loss 7.52491e-07, acc 1\n",
      "2018-10-26T18:13:41.187109: step 19406, loss 0, acc 1\n",
      "2018-10-26T18:13:41.360646: step 19407, loss 1.32989e-06, acc 1\n",
      "2018-10-26T18:13:41.539169: step 19408, loss 6.4074e-07, acc 1\n",
      "2018-10-26T18:13:41.714700: step 19409, loss 5.04769e-07, acc 1\n",
      "2018-10-26T18:13:41.885244: step 19410, loss 2.28042e-05, acc 1\n",
      "2018-10-26T18:13:42.064764: step 19411, loss 2.64183e-05, acc 1\n",
      "2018-10-26T18:13:42.229325: step 19412, loss 0.000131763, acc 1\n",
      "2018-10-26T18:13:42.405854: step 19413, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:13:42.576398: step 19414, loss 9.38751e-07, acc 1\n",
      "2018-10-26T18:13:42.748937: step 19415, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:13:42.918483: step 19416, loss 1.21809e-05, acc 1\n",
      "2018-10-26T18:13:43.093017: step 19417, loss 9.39195e-06, acc 1\n",
      "2018-10-26T18:13:43.259572: step 19418, loss 1.63958e-05, acc 1\n",
      "2018-10-26T18:13:43.437098: step 19419, loss 1.40418e-05, acc 1\n",
      "2018-10-26T18:13:43.602656: step 19420, loss 1.26516e-05, acc 1\n",
      "2018-10-26T18:13:43.787162: step 19421, loss 4.61317e-06, acc 1\n",
      "2018-10-26T18:13:43.957707: step 19422, loss 1.41182e-06, acc 1\n",
      "2018-10-26T18:13:44.135232: step 19423, loss 0.000838812, acc 1\n",
      "2018-10-26T18:13:44.299793: step 19424, loss 2.03027e-07, acc 1\n",
      "2018-10-26T18:13:44.475324: step 19425, loss 8.64216e-06, acc 1\n",
      "2018-10-26T18:13:44.640882: step 19426, loss 9.90901e-07, acc 1\n",
      "2018-10-26T18:13:44.819404: step 19427, loss 0.000105986, acc 1\n",
      "2018-10-26T18:13:44.988969: step 19428, loss 3.35274e-07, acc 1\n",
      "2018-10-26T18:13:45.170467: step 19429, loss 3.8999e-06, acc 1\n",
      "2018-10-26T18:13:45.339016: step 19430, loss 2.99742e-05, acc 1\n",
      "2018-10-26T18:13:45.511555: step 19431, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:13:45.679107: step 19432, loss 1.59567e-05, acc 1\n",
      "2018-10-26T18:13:45.854638: step 19433, loss 5.47155e-06, acc 1\n",
      "2018-10-26T18:13:46.023187: step 19434, loss 6.48943e-05, acc 1\n",
      "2018-10-26T18:13:46.198720: step 19435, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:13:46.365274: step 19436, loss 3.77262e-05, acc 1\n",
      "2018-10-26T18:13:46.537813: step 19437, loss 5.80154e-06, acc 1\n",
      "2018-10-26T18:13:46.703371: step 19438, loss 0.00020985, acc 1\n",
      "2018-10-26T18:13:46.890870: step 19439, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:13:47.055430: step 19440, loss 0.000123339, acc 1\n",
      "2018-10-26T18:13:47.230967: step 19441, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:13:47.394524: step 19442, loss 4.20955e-07, acc 1\n",
      "2018-10-26T18:13:47.573047: step 19443, loss 0, acc 1\n",
      "2018-10-26T18:13:47.739602: step 19444, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:13:47.923112: step 19445, loss 4.70433e-06, acc 1\n",
      "2018-10-26T18:13:48.086675: step 19446, loss 2.1879e-05, acc 1\n",
      "2018-10-26T18:13:48.264201: step 19447, loss 1.67443e-06, acc 1\n",
      "2018-10-26T18:13:48.438734: step 19448, loss 7.62994e-05, acc 1\n",
      "2018-10-26T18:13:48.612271: step 19449, loss 0, acc 1\n",
      "2018-10-26T18:13:48.781817: step 19450, loss 2.61121e-06, acc 1\n",
      "2018-10-26T18:13:48.961338: step 19451, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:13:49.126895: step 19452, loss 2.05064e-06, acc 1\n",
      "2018-10-26T18:13:49.304421: step 19453, loss 0.000101153, acc 1\n",
      "2018-10-26T18:13:49.470976: step 19454, loss 1.41561e-07, acc 1\n",
      "2018-10-26T18:13:49.648502: step 19455, loss 3.59487e-07, acc 1\n",
      "2018-10-26T18:13:49.823035: step 19456, loss 0.000828631, acc 1\n",
      "2018-10-26T18:13:50.002556: step 19457, loss 3.71132e-05, acc 1\n",
      "2018-10-26T18:13:50.182076: step 19458, loss 3.70665e-07, acc 1\n",
      "2018-10-26T18:13:50.348631: step 19459, loss 1.04534e-05, acc 1\n",
      "2018-10-26T18:13:50.526157: step 19460, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:13:50.718642: step 19461, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:13:50.910131: step 19462, loss 0, acc 1\n",
      "2018-10-26T18:13:51.104611: step 19463, loss 1.27027e-06, acc 1\n",
      "2018-10-26T18:13:51.309065: step 19464, loss 0.000611463, acc 1\n",
      "2018-10-26T18:13:51.495566: step 19465, loss 6.50128e-06, acc 1\n",
      "2018-10-26T18:13:51.696033: step 19466, loss 1.16525e-05, acc 1\n",
      "2018-10-26T18:13:51.918437: step 19467, loss 3.72495e-05, acc 1\n",
      "2018-10-26T18:13:52.141840: step 19468, loss 6.89165e-07, acc 1\n",
      "2018-10-26T18:13:52.357265: step 19469, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:13:52.561718: step 19470, loss 9.49948e-08, acc 1\n",
      "2018-10-26T18:13:52.805088: step 19471, loss 0.000103144, acc 1\n",
      "2018-10-26T18:13:53.009523: step 19472, loss 1.50495e-06, acc 1\n",
      "2018-10-26T18:13:53.212001: step 19473, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:13:53.433389: step 19474, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:13:53.642829: step 19475, loss 0.000197908, acc 1\n",
      "2018-10-26T18:13:53.858254: step 19476, loss 3.46981e-06, acc 1\n",
      "2018-10-26T18:13:54.030794: step 19477, loss 6.12803e-07, acc 1\n",
      "2018-10-26T18:13:54.241231: step 19478, loss 1.7005e-06, acc 1\n",
      "2018-10-26T18:13:54.442693: step 19479, loss 9.8905e-07, acc 1\n",
      "2018-10-26T18:13:54.632186: step 19480, loss 3.41007e-05, acc 1\n",
      "2018-10-26T18:13:54.902463: step 19481, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:13:55.097946: step 19482, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:13:55.285441: step 19483, loss 4.4885e-06, acc 1\n",
      "2018-10-26T18:13:55.546742: step 19484, loss 0, acc 1\n",
      "2018-10-26T18:13:55.763164: step 19485, loss 2.37292e-06, acc 1\n",
      "2018-10-26T18:13:55.982578: step 19486, loss 2.96494e-05, acc 1\n",
      "2018-10-26T18:13:56.169080: step 19487, loss 1.54964e-06, acc 1\n",
      "2018-10-26T18:13:56.379517: step 19488, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:13:56.581000: step 19489, loss 2.47342e-06, acc 1\n",
      "2018-10-26T18:13:56.790420: step 19490, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:13:56.993876: step 19491, loss 4.80518e-06, acc 1\n",
      "2018-10-26T18:13:57.181375: step 19492, loss 1.36344e-06, acc 1\n",
      "2018-10-26T18:13:57.362889: step 19493, loss 0.000617128, acc 1\n",
      "2018-10-26T18:13:57.534432: step 19494, loss 9.11723e-06, acc 1\n",
      "2018-10-26T18:13:57.741877: step 19495, loss 3.00751e-05, acc 1\n",
      "2018-10-26T18:13:57.957306: step 19496, loss 1.34853e-06, acc 1\n",
      "2018-10-26T18:13:58.157766: step 19497, loss 1.16746e-05, acc 1\n",
      "2018-10-26T18:13:58.333297: step 19498, loss 0.000149057, acc 1\n",
      "2018-10-26T18:13:58.564679: step 19499, loss 1.96125e-06, acc 1\n",
      "2018-10-26T18:13:58.727245: step 19500, loss 2.52324e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:13:59.206964: step 19500, loss 6.82944, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19500\n",
      "\n",
      "2018-10-26T18:13:59.580964: step 19501, loss 4.18698e-05, acc 1\n",
      "2018-10-26T18:13:59.756494: step 19502, loss 2.90921e-05, acc 1\n",
      "2018-10-26T18:13:59.959951: step 19503, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:14:00.144458: step 19504, loss 1.92772e-06, acc 1\n",
      "2018-10-26T18:14:00.381825: step 19505, loss 8.38176e-07, acc 1\n",
      "2018-10-26T18:14:00.619189: step 19506, loss 6.20514e-06, acc 1\n",
      "2018-10-26T18:14:00.788737: step 19507, loss 6.22879e-05, acc 1\n",
      "2018-10-26T18:14:00.977233: step 19508, loss 2.47764e-05, acc 1\n",
      "2018-10-26T18:14:01.149772: step 19509, loss 0.000568533, acc 1\n",
      "2018-10-26T18:14:01.345250: step 19510, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:14:01.510807: step 19511, loss 3.3375e-06, acc 1\n",
      "2018-10-26T18:14:01.686338: step 19512, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:14:01.853891: step 19513, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:14:02.064330: step 19514, loss 1.41545e-05, acc 1\n",
      "2018-10-26T18:14:02.231881: step 19515, loss 2.51251e-06, acc 1\n",
      "2018-10-26T18:14:02.403422: step 19516, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:14:02.567983: step 19517, loss 3.91151e-07, acc 1\n",
      "2018-10-26T18:14:02.776425: step 19518, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:14:02.948966: step 19519, loss 9.73316e-06, acc 1\n",
      "2018-10-26T18:14:03.122502: step 19520, loss 2.56093e-06, acc 1\n",
      "2018-10-26T18:14:03.289057: step 19521, loss 1.64463e-06, acc 1\n",
      "2018-10-26T18:14:03.463590: step 19522, loss 5.82997e-07, acc 1\n",
      "2018-10-26T18:14:03.632139: step 19523, loss 0.000394313, acc 1\n",
      "2018-10-26T18:14:03.802684: step 19524, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:14:03.975223: step 19525, loss 2.93339e-06, acc 1\n",
      "2018-10-26T18:14:04.151751: step 19526, loss 1.67637e-07, acc 1\n",
      "2018-10-26T18:14:04.322296: step 19527, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:14:04.500819: step 19528, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:14:04.671363: step 19529, loss 0.000182378, acc 1\n",
      "2018-10-26T18:14:04.844899: step 19530, loss 2.51442e-06, acc 1\n",
      "2018-10-26T18:14:05.008462: step 19531, loss 2.6572e-05, acc 1\n",
      "2018-10-26T18:14:05.189978: step 19532, loss 3.77699e-06, acc 1\n",
      "2018-10-26T18:14:05.358527: step 19533, loss 1.17901e-06, acc 1\n",
      "2018-10-26T18:14:05.532063: step 19534, loss 7.80669e-06, acc 1\n",
      "2018-10-26T18:14:05.702608: step 19535, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:14:05.880133: step 19536, loss 6.38068e-06, acc 1\n",
      "2018-10-26T18:14:06.046689: step 19537, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:14:06.221222: step 19538, loss 0.000202349, acc 1\n",
      "2018-10-26T18:14:06.394759: step 19539, loss 0.000205674, acc 1\n",
      "2018-10-26T18:14:06.570289: step 19540, loss 6.10937e-07, acc 1\n",
      "2018-10-26T18:14:06.738839: step 19541, loss 0.000571523, acc 1\n",
      "2018-10-26T18:14:06.919357: step 19542, loss 1.40232e-05, acc 1\n",
      "2018-10-26T18:14:07.091896: step 19543, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:14:07.261442: step 19544, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:14:07.430990: step 19545, loss 3.06314e-05, acc 1\n",
      "2018-10-26T18:14:07.615496: step 19546, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:14:07.786042: step 19547, loss 2.15873e-06, acc 1\n",
      "2018-10-26T18:14:07.958580: step 19548, loss 9.67723e-06, acc 1\n",
      "2018-10-26T18:14:08.127129: step 19549, loss 0.000821426, acc 1\n",
      "2018-10-26T18:14:08.306649: step 19550, loss 0, acc 1\n",
      "2018-10-26T18:14:08.468218: step 19551, loss 5.38297e-07, acc 1\n",
      "2018-10-26T18:14:08.643750: step 19552, loss 3.37136e-07, acc 1\n",
      "2018-10-26T18:14:08.809307: step 19553, loss 7.56708e-06, acc 1\n",
      "2018-10-26T18:14:08.981845: step 19554, loss 1.79921e-06, acc 1\n",
      "2018-10-26T18:14:09.152390: step 19555, loss 8.69831e-07, acc 1\n",
      "2018-10-26T18:14:09.330913: step 19556, loss 3.63213e-07, acc 1\n",
      "2018-10-26T18:14:09.503420: step 19557, loss 5.9092e-06, acc 1\n",
      "2018-10-26T18:14:09.676956: step 19558, loss 2.46785e-06, acc 1\n",
      "2018-10-26T18:14:09.845506: step 19559, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:14:10.020038: step 19560, loss 0.000486129, acc 1\n",
      "2018-10-26T18:14:10.228482: step 19561, loss 9.96493e-07, acc 1\n",
      "2018-10-26T18:14:10.428946: step 19562, loss 4.49182e-05, acc 1\n",
      "2018-10-26T18:14:10.606472: step 19563, loss 8.88996e-06, acc 1\n",
      "2018-10-26T18:14:10.781034: step 19564, loss 2.53858e-06, acc 1\n",
      "2018-10-26T18:14:10.969503: step 19565, loss 3.47531e-06, acc 1\n",
      "2018-10-26T18:14:11.134063: step 19566, loss 3.48311e-07, acc 1\n",
      "2018-10-26T18:14:11.310590: step 19567, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:14:11.492106: step 19568, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:14:11.678608: step 19569, loss 0.0958198, acc 0.984375\n",
      "2018-10-26T18:14:11.850149: step 19570, loss 3.27822e-07, acc 1\n",
      "2018-10-26T18:14:12.024683: step 19571, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:14:12.188247: step 19572, loss 3.25961e-07, acc 1\n",
      "2018-10-26T18:14:12.367766: step 19573, loss 1.3839e-06, acc 1\n",
      "2018-10-26T18:14:12.541303: step 19574, loss 0, acc 1\n",
      "2018-10-26T18:14:12.718828: step 19575, loss 4.79558e-06, acc 1\n",
      "2018-10-26T18:14:12.897351: step 19576, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:14:13.062909: step 19577, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:14:13.238439: step 19578, loss 2.11926e-05, acc 1\n",
      "2018-10-26T18:14:13.405992: step 19579, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:14:13.594489: step 19580, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:14:13.759049: step 19581, loss 0.00773827, acc 1\n",
      "2018-10-26T18:14:13.934580: step 19582, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:14:14.100138: step 19583, loss 0.00187257, acc 1\n",
      "2018-10-26T18:14:14.278661: step 19584, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:14:14.448207: step 19585, loss 3.84071e-05, acc 1\n",
      "2018-10-26T18:14:14.628725: step 19586, loss 4.33992e-07, acc 1\n",
      "2018-10-26T18:14:14.793286: step 19587, loss 7.01184e-06, acc 1\n",
      "2018-10-26T18:14:14.971809: step 19588, loss 5.62513e-07, acc 1\n",
      "2018-10-26T18:14:15.140359: step 19589, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:14:15.317883: step 19590, loss 2.32813e-06, acc 1\n",
      "2018-10-26T18:14:15.489426: step 19591, loss 0.000783215, acc 1\n",
      "2018-10-26T18:14:15.673933: step 19592, loss 0.000130209, acc 1\n",
      "2018-10-26T18:14:15.838493: step 19593, loss 1.97439e-07, acc 1\n",
      "2018-10-26T18:14:16.020008: step 19594, loss 0.000816869, acc 1\n",
      "2018-10-26T18:14:16.190552: step 19595, loss 3.71149e-05, acc 1\n",
      "2018-10-26T18:14:16.374062: step 19596, loss 1.33734e-06, acc 1\n",
      "2018-10-26T18:14:16.544606: step 19597, loss 3.30969e-06, acc 1\n",
      "2018-10-26T18:14:16.722131: step 19598, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:14:16.887690: step 19599, loss 0.0210823, acc 0.984375\n",
      "2018-10-26T18:14:17.063221: step 19600, loss 0.00165366, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:14:17.514016: step 19600, loss 6.91011, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19600\n",
      "\n",
      "2018-10-26T18:14:17.844739: step 19601, loss 3.42329e-05, acc 1\n",
      "2018-10-26T18:14:18.013289: step 19602, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:14:18.192810: step 19603, loss 0.000131042, acc 1\n",
      "2018-10-26T18:14:18.360363: step 19604, loss 0.000211656, acc 1\n",
      "2018-10-26T18:14:18.558831: step 19605, loss 1.04352e-05, acc 1\n",
      "2018-10-26T18:14:18.801184: step 19606, loss 1.43421e-06, acc 1\n",
      "2018-10-26T18:14:18.972726: step 19607, loss 4.52117e-05, acc 1\n",
      "2018-10-26T18:14:19.162221: step 19608, loss 8.00936e-08, acc 1\n",
      "2018-10-26T18:14:19.328775: step 19609, loss 0.000199093, acc 1\n",
      "2018-10-26T18:14:19.514278: step 19610, loss 4.97319e-05, acc 1\n",
      "2018-10-26T18:14:19.687816: step 19611, loss 5.42021e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:14:19.872322: step 19612, loss 8.67977e-07, acc 1\n",
      "2018-10-26T18:14:20.039875: step 19613, loss 6.54397e-06, acc 1\n",
      "2018-10-26T18:14:20.213411: step 19614, loss 5.77961e-05, acc 1\n",
      "2018-10-26T18:14:20.381961: step 19615, loss 2.38417e-07, acc 1\n",
      "2018-10-26T18:14:20.556495: step 19616, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:14:20.727038: step 19617, loss 8.51214e-07, acc 1\n",
      "2018-10-26T18:14:20.905562: step 19618, loss 3.09181e-06, acc 1\n",
      "2018-10-26T18:14:21.089071: step 19619, loss 6.92889e-07, acc 1\n",
      "2018-10-26T18:14:21.266597: step 19620, loss 7.1034e-05, acc 1\n",
      "2018-10-26T18:14:21.445121: step 19621, loss 5.2712e-07, acc 1\n",
      "2018-10-26T18:14:21.607686: step 19622, loss 0.00995022, acc 1\n",
      "2018-10-26T18:14:21.781222: step 19623, loss 0, acc 1\n",
      "2018-10-26T18:14:21.958747: step 19624, loss 2.13265e-06, acc 1\n",
      "2018-10-26T18:14:22.138268: step 19625, loss 0, acc 1\n",
      "2018-10-26T18:14:22.306818: step 19626, loss 1.32619e-06, acc 1\n",
      "2018-10-26T18:14:22.481352: step 19627, loss 6.35154e-07, acc 1\n",
      "2018-10-26T18:14:22.648903: step 19628, loss 5.40129e-06, acc 1\n",
      "2018-10-26T18:14:22.827428: step 19629, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:14:22.993982: step 19630, loss 3.66526e-06, acc 1\n",
      "2018-10-26T18:14:23.164526: step 19631, loss 7.83138e-05, acc 1\n",
      "2018-10-26T18:14:23.330084: step 19632, loss 0.000128166, acc 1\n",
      "2018-10-26T18:14:23.503620: step 19633, loss 0.00221403, acc 1\n",
      "2018-10-26T18:14:23.668181: step 19634, loss 1.04055e-05, acc 1\n",
      "2018-10-26T18:14:23.838726: step 19635, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:14:24.012262: step 19636, loss 1.17156e-06, acc 1\n",
      "2018-10-26T18:14:24.186794: step 19637, loss 0, acc 1\n",
      "2018-10-26T18:14:24.354347: step 19638, loss 9.43659e-05, acc 1\n",
      "2018-10-26T18:14:24.525889: step 19639, loss 8.75434e-07, acc 1\n",
      "2018-10-26T18:14:24.696433: step 19640, loss 0.000678011, acc 1\n",
      "2018-10-26T18:14:24.869970: step 19641, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:14:25.034530: step 19642, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:14:25.205074: step 19643, loss 2.20307e-05, acc 1\n",
      "2018-10-26T18:14:25.374621: step 19644, loss 3.63212e-07, acc 1\n",
      "2018-10-26T18:14:25.559129: step 19645, loss 2.43507e-05, acc 1\n",
      "2018-10-26T18:14:25.721694: step 19646, loss 9.94621e-07, acc 1\n",
      "2018-10-26T18:14:25.905204: step 19647, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:14:26.076746: step 19648, loss 1.18276e-06, acc 1\n",
      "2018-10-26T18:14:26.244297: step 19649, loss 1.30958e-05, acc 1\n",
      "2018-10-26T18:14:26.410852: step 19650, loss 0.03107, acc 0.983333\n",
      "2018-10-26T18:14:26.583392: step 19651, loss 0.000124037, acc 1\n",
      "2018-10-26T18:14:26.751941: step 19652, loss 0.000167544, acc 1\n",
      "2018-10-26T18:14:26.929467: step 19653, loss 0, acc 1\n",
      "2018-10-26T18:14:27.098017: step 19654, loss 2.70082e-07, acc 1\n",
      "2018-10-26T18:14:27.271553: step 19655, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:14:27.438108: step 19656, loss 2.73806e-07, acc 1\n",
      "2018-10-26T18:14:27.620621: step 19657, loss 4.47069e-05, acc 1\n",
      "2018-10-26T18:14:27.783185: step 19658, loss 9.12695e-08, acc 1\n",
      "2018-10-26T18:14:27.958717: step 19659, loss 0.000107101, acc 1\n",
      "2018-10-26T18:14:28.129262: step 19660, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:14:28.299805: step 19661, loss 4.52616e-07, acc 1\n",
      "2018-10-26T18:14:28.464366: step 19662, loss 3.35274e-07, acc 1\n",
      "2018-10-26T18:14:28.636905: step 19663, loss 0.00173987, acc 1\n",
      "2018-10-26T18:14:28.806451: step 19664, loss 0.000112369, acc 1\n",
      "2018-10-26T18:14:28.986969: step 19665, loss 8.53339e-06, acc 1\n",
      "2018-10-26T18:14:29.154522: step 19666, loss 1.73519e-05, acc 1\n",
      "2018-10-26T18:14:29.329055: step 19667, loss 2.10478e-07, acc 1\n",
      "2018-10-26T18:14:29.498602: step 19668, loss 3.61526e-05, acc 1\n",
      "2018-10-26T18:14:29.677125: step 19669, loss 3.46787e-06, acc 1\n",
      "2018-10-26T18:14:29.845675: step 19670, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:14:30.028187: step 19671, loss 3.38523e-05, acc 1\n",
      "2018-10-26T18:14:30.193745: step 19672, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:14:30.375260: step 19673, loss 1.59995e-06, acc 1\n",
      "2018-10-26T18:14:30.538823: step 19674, loss 2.27242e-07, acc 1\n",
      "2018-10-26T18:14:30.715351: step 19675, loss 1.95193e-06, acc 1\n",
      "2018-10-26T18:14:30.889885: step 19676, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:14:31.075389: step 19677, loss 0.167622, acc 0.984375\n",
      "2018-10-26T18:14:31.241944: step 19678, loss 6.07212e-07, acc 1\n",
      "2018-10-26T18:14:31.424456: step 19679, loss 0.00263099, acc 1\n",
      "2018-10-26T18:14:31.593006: step 19680, loss 2.7502e-05, acc 1\n",
      "2018-10-26T18:14:31.766543: step 19681, loss 3.62457e-05, acc 1\n",
      "2018-10-26T18:14:31.935092: step 19682, loss 2.35624e-05, acc 1\n",
      "2018-10-26T18:14:32.107631: step 19683, loss 1.40773e-05, acc 1\n",
      "2018-10-26T18:14:32.281167: step 19684, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:14:32.451712: step 19685, loss 0.000199644, acc 1\n",
      "2018-10-26T18:14:32.629238: step 19686, loss 0, acc 1\n",
      "2018-10-26T18:14:32.800780: step 19687, loss 0.000260979, acc 1\n",
      "2018-10-26T18:14:32.969330: step 19688, loss 9.59233e-07, acc 1\n",
      "2018-10-26T18:14:33.143863: step 19689, loss 5.27229e-05, acc 1\n",
      "2018-10-26T18:14:33.310418: step 19690, loss 2.14936e-06, acc 1\n",
      "2018-10-26T18:14:33.481960: step 19691, loss 1.52942e-05, acc 1\n",
      "2018-10-26T18:14:33.653502: step 19692, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:14:33.830029: step 19693, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:14:33.998579: step 19694, loss 3.33751e-06, acc 1\n",
      "2018-10-26T18:14:34.173113: step 19695, loss 4.60068e-07, acc 1\n",
      "2018-10-26T18:14:34.338670: step 19696, loss 9.58474e-06, acc 1\n",
      "2018-10-26T18:14:34.520186: step 19697, loss 0.00137879, acc 1\n",
      "2018-10-26T18:14:34.685743: step 19698, loss 4.15921e-05, acc 1\n",
      "2018-10-26T18:14:34.864266: step 19699, loss 8.45229e-06, acc 1\n",
      "2018-10-26T18:14:35.029824: step 19700, loss 3.55307e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:14:35.475632: step 19700, loss 6.87769, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19700\n",
      "\n",
      "2018-10-26T18:14:35.822049: step 19701, loss 2.74878e-05, acc 1\n",
      "2018-10-26T18:14:35.993589: step 19702, loss 7.01901e-05, acc 1\n",
      "2018-10-26T18:14:36.174107: step 19703, loss 1.87559e-06, acc 1\n",
      "2018-10-26T18:14:36.336672: step 19704, loss 3.02839e-06, acc 1\n",
      "2018-10-26T18:14:36.534145: step 19705, loss 0.0765587, acc 0.984375\n",
      "2018-10-26T18:14:36.779490: step 19706, loss 3.20373e-07, acc 1\n",
      "2018-10-26T18:14:36.958012: step 19707, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:14:37.140526: step 19708, loss 5.71039e-05, acc 1\n",
      "2018-10-26T18:14:37.308077: step 19709, loss 3.46449e-07, acc 1\n",
      "2018-10-26T18:14:37.485602: step 19710, loss 2.75647e-06, acc 1\n",
      "2018-10-26T18:14:37.656146: step 19711, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:14:37.831678: step 19712, loss 0.000127372, acc 1\n",
      "2018-10-26T18:14:38.002222: step 19713, loss 0.00092484, acc 1\n",
      "2018-10-26T18:14:38.184735: step 19714, loss 1.05423e-06, acc 1\n",
      "2018-10-26T18:14:38.351290: step 19715, loss 1.2888e-05, acc 1\n",
      "2018-10-26T18:14:38.532806: step 19716, loss 1.05611e-06, acc 1\n",
      "2018-10-26T18:14:38.700357: step 19717, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:14:38.884865: step 19718, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:14:39.053414: step 19719, loss 0.000300064, acc 1\n",
      "2018-10-26T18:14:39.231937: step 19720, loss 5.26738e-05, acc 1\n",
      "2018-10-26T18:14:39.397495: step 19721, loss 4.67517e-07, acc 1\n",
      "2018-10-26T18:14:39.575021: step 19722, loss 1.54596e-06, acc 1\n",
      "2018-10-26T18:14:39.748557: step 19723, loss 0.000882432, acc 1\n",
      "2018-10-26T18:14:39.931088: step 19724, loss 5.42164e-05, acc 1\n",
      "2018-10-26T18:14:40.102610: step 19725, loss 6.75218e-05, acc 1\n",
      "2018-10-26T18:14:40.283129: step 19726, loss 0.000576444, acc 1\n",
      "2018-10-26T18:14:40.444696: step 19727, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:14:40.623229: step 19728, loss 1.1842e-05, acc 1\n",
      "2018-10-26T18:14:40.791770: step 19729, loss 2.44658e-05, acc 1\n",
      "2018-10-26T18:14:40.973284: step 19730, loss 0.00204047, acc 1\n",
      "2018-10-26T18:14:41.145822: step 19731, loss 0.0698311, acc 0.984375\n",
      "2018-10-26T18:14:41.328336: step 19732, loss 0.0803244, acc 0.984375\n",
      "2018-10-26T18:14:41.498881: step 19733, loss 3.31549e-07, acc 1\n",
      "2018-10-26T18:14:41.675409: step 19734, loss 4.19091e-07, acc 1\n",
      "2018-10-26T18:14:41.840965: step 19735, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:14:42.017494: step 19736, loss 2.17356e-06, acc 1\n",
      "2018-10-26T18:14:42.182054: step 19737, loss 1.30385e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:14:42.362572: step 19738, loss 8.13955e-07, acc 1\n",
      "2018-10-26T18:14:42.532119: step 19739, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:14:42.705655: step 19740, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:14:42.872210: step 19741, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:14:43.054722: step 19742, loss 8.62842e-06, acc 1\n",
      "2018-10-26T18:14:43.216291: step 19743, loss 2.38962e-06, acc 1\n",
      "2018-10-26T18:14:43.388831: step 19744, loss 0.000197226, acc 1\n",
      "2018-10-26T18:14:43.555385: step 19745, loss 0.000134254, acc 1\n",
      "2018-10-26T18:14:43.726927: step 19746, loss 0.00131386, acc 1\n",
      "2018-10-26T18:14:43.896475: step 19747, loss 9.22282e-05, acc 1\n",
      "2018-10-26T18:14:44.075994: step 19748, loss 3.00986e-06, acc 1\n",
      "2018-10-26T18:14:44.245541: step 19749, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:14:44.432044: step 19750, loss 0.000379786, acc 1\n",
      "2018-10-26T18:14:44.596604: step 19751, loss 1.37298e-05, acc 1\n",
      "2018-10-26T18:14:44.777120: step 19752, loss 3.46601e-06, acc 1\n",
      "2018-10-26T18:14:44.943676: step 19753, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:14:45.121201: step 19754, loss 0.00120609, acc 1\n",
      "2018-10-26T18:14:45.290748: step 19755, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:14:45.468274: step 19756, loss 1.56462e-07, acc 1\n",
      "2018-10-26T18:14:45.631837: step 19757, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:14:45.808365: step 19758, loss 3.21275e-06, acc 1\n",
      "2018-10-26T18:14:45.975917: step 19759, loss 0.000121352, acc 1\n",
      "2018-10-26T18:14:46.150452: step 19760, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:14:46.321993: step 19761, loss 2.20678e-05, acc 1\n",
      "2018-10-26T18:14:46.496526: step 19762, loss 0.000544884, acc 1\n",
      "2018-10-26T18:14:46.666074: step 19763, loss 4.07917e-07, acc 1\n",
      "2018-10-26T18:14:46.843599: step 19764, loss 1.12127e-06, acc 1\n",
      "2018-10-26T18:14:47.011151: step 19765, loss 3.4774e-06, acc 1\n",
      "2018-10-26T18:14:47.188677: step 19766, loss 0.00299057, acc 1\n",
      "2018-10-26T18:14:47.358225: step 19767, loss 0.000514302, acc 1\n",
      "2018-10-26T18:14:47.530763: step 19768, loss 0.00327005, acc 1\n",
      "2018-10-26T18:14:47.701308: step 19769, loss 0.0700956, acc 0.984375\n",
      "2018-10-26T18:14:47.878834: step 19770, loss 6.73227e-06, acc 1\n",
      "2018-10-26T18:14:48.058356: step 19771, loss 0.0244405, acc 0.984375\n",
      "2018-10-26T18:14:48.228898: step 19772, loss 2.4897e-05, acc 1\n",
      "2018-10-26T18:14:48.405427: step 19773, loss 1.18274e-06, acc 1\n",
      "2018-10-26T18:14:48.568990: step 19774, loss 0.00101631, acc 1\n",
      "2018-10-26T18:14:48.741529: step 19775, loss 4.89871e-07, acc 1\n",
      "2018-10-26T18:14:48.911075: step 19776, loss 9.74148e-07, acc 1\n",
      "2018-10-26T18:14:49.081620: step 19777, loss 1.82899e-05, acc 1\n",
      "2018-10-26T18:14:49.247177: step 19778, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:14:49.429691: step 19779, loss 4.11595e-05, acc 1\n",
      "2018-10-26T18:14:49.595247: step 19780, loss 3.16648e-07, acc 1\n",
      "2018-10-26T18:14:49.766791: step 19781, loss 1.35282e-05, acc 1\n",
      "2018-10-26T18:14:49.936336: step 19782, loss 3.44587e-07, acc 1\n",
      "2018-10-26T18:14:50.112864: step 19783, loss 3.25961e-07, acc 1\n",
      "2018-10-26T18:14:50.284406: step 19784, loss 3.50157e-06, acc 1\n",
      "2018-10-26T18:14:50.463926: step 19785, loss 0.00237563, acc 1\n",
      "2018-10-26T18:14:50.632476: step 19786, loss 2.12896e-06, acc 1\n",
      "2018-10-26T18:14:50.807009: step 19787, loss 3.24099e-07, acc 1\n",
      "2018-10-26T18:14:50.970574: step 19788, loss 2.28251e-05, acc 1\n",
      "2018-10-26T18:14:51.151090: step 19789, loss 8.1955e-07, acc 1\n",
      "2018-10-26T18:14:51.324626: step 19790, loss 1.12127e-06, acc 1\n",
      "2018-10-26T18:14:51.495171: step 19791, loss 6.60038e-06, acc 1\n",
      "2018-10-26T18:14:51.666712: step 19792, loss 0.000168152, acc 1\n",
      "2018-10-26T18:14:51.845237: step 19793, loss 2.34684e-06, acc 1\n",
      "2018-10-26T18:14:52.013785: step 19794, loss 0.000760742, acc 1\n",
      "2018-10-26T18:14:52.189316: step 19795, loss 5.75282e-06, acc 1\n",
      "2018-10-26T18:14:52.360859: step 19796, loss 6.70538e-07, acc 1\n",
      "2018-10-26T18:14:52.537387: step 19797, loss 0.000208127, acc 1\n",
      "2018-10-26T18:14:52.704938: step 19798, loss 2.80373e-05, acc 1\n",
      "2018-10-26T18:14:52.879472: step 19799, loss 0.156079, acc 0.984375\n",
      "2018-10-26T18:14:53.047025: step 19800, loss 0.000312327, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:14:53.490840: step 19800, loss 7.27067, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19800\n",
      "\n",
      "2018-10-26T18:14:53.850754: step 19801, loss 3.46968e-05, acc 1\n",
      "2018-10-26T18:14:54.018305: step 19802, loss 7.51936e-05, acc 1\n",
      "2018-10-26T18:14:54.193837: step 19803, loss 9.25122e-06, acc 1\n",
      "2018-10-26T18:14:54.366375: step 19804, loss 0.000196205, acc 1\n",
      "2018-10-26T18:14:54.558862: step 19805, loss 1.91884e-05, acc 1\n",
      "2018-10-26T18:14:54.794233: step 19806, loss 9.84192e-05, acc 1\n",
      "2018-10-26T18:14:54.963780: step 19807, loss 0.0621059, acc 0.984375\n",
      "2018-10-26T18:14:55.143301: step 19808, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:14:55.308858: step 19809, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:14:55.479402: step 19810, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:14:55.643962: step 19811, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:14:55.823483: step 19812, loss 4.31919e-06, acc 1\n",
      "2018-10-26T18:14:55.990038: step 19813, loss 3.85668e-05, acc 1\n",
      "2018-10-26T18:14:56.167564: step 19814, loss 7.45044e-07, acc 1\n",
      "2018-10-26T18:14:56.331127: step 19815, loss 8.91406e-06, acc 1\n",
      "2018-10-26T18:14:56.504663: step 19816, loss 2.21654e-07, acc 1\n",
      "2018-10-26T18:14:56.694156: step 19817, loss 2.95389e-06, acc 1\n",
      "2018-10-26T18:14:56.877667: step 19818, loss 0, acc 1\n",
      "2018-10-26T18:14:57.106057: step 19819, loss 8.59841e-05, acc 1\n",
      "2018-10-26T18:14:57.293554: step 19820, loss 1.39746e-05, acc 1\n",
      "2018-10-26T18:14:57.482052: step 19821, loss 6.38726e-05, acc 1\n",
      "2018-10-26T18:14:57.660575: step 19822, loss 1.06989e-05, acc 1\n",
      "2018-10-26T18:14:57.895946: step 19823, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:14:58.073471: step 19824, loss 3.74388e-07, acc 1\n",
      "2018-10-26T18:14:58.318816: step 19825, loss 2.47346e-06, acc 1\n",
      "2018-10-26T18:14:58.514294: step 19826, loss 2.60768e-07, acc 1\n",
      "2018-10-26T18:14:58.705782: step 19827, loss 8.95129e-06, acc 1\n",
      "2018-10-26T18:14:58.945143: step 19828, loss 1.88675e-06, acc 1\n",
      "2018-10-26T18:14:59.113691: step 19829, loss 0.000150799, acc 1\n",
      "2018-10-26T18:14:59.299197: step 19830, loss 9.12693e-08, acc 1\n",
      "2018-10-26T18:14:59.519607: step 19831, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:14:59.754980: step 19832, loss 6.10303e-06, acc 1\n",
      "2018-10-26T18:14:59.931507: step 19833, loss 9.33615e-05, acc 1\n",
      "2018-10-26T18:15:00.186824: step 19834, loss 0.0153614, acc 0.984375\n",
      "2018-10-26T18:15:00.361358: step 19835, loss 0.000198801, acc 1\n",
      "2018-10-26T18:15:00.575785: step 19836, loss 1.00021e-06, acc 1\n",
      "2018-10-26T18:15:00.764282: step 19837, loss 6.32858e-06, acc 1\n",
      "2018-10-26T18:15:01.011621: step 19838, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:15:01.229040: step 19839, loss 1.44508e-05, acc 1\n",
      "2018-10-26T18:15:01.451447: step 19840, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:15:01.666870: step 19841, loss 4.39578e-07, acc 1\n",
      "2018-10-26T18:15:01.883291: step 19842, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:15:02.092733: step 19843, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:15:02.292199: step 19844, loss 4.09504e-05, acc 1\n",
      "2018-10-26T18:15:02.510616: step 19845, loss 4.47035e-08, acc 1\n",
      "2018-10-26T18:15:02.683155: step 19846, loss 0.0500612, acc 0.984375\n",
      "2018-10-26T18:15:02.894590: step 19847, loss 4.04189e-07, acc 1\n",
      "2018-10-26T18:15:03.066132: step 19848, loss 7.54357e-07, acc 1\n",
      "2018-10-26T18:15:03.245652: step 19849, loss 0.000707111, acc 1\n",
      "2018-10-26T18:15:03.419188: step 19850, loss 1.64041e-05, acc 1\n",
      "2018-10-26T18:15:03.642592: step 19851, loss 5.72836e-05, acc 1\n",
      "2018-10-26T18:15:03.806154: step 19852, loss 0, acc 1\n",
      "2018-10-26T18:15:03.992657: step 19853, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:15:04.192123: step 19854, loss 3.32267e-06, acc 1\n",
      "2018-10-26T18:15:04.412534: step 19855, loss 1.93714e-07, acc 1\n",
      "2018-10-26T18:15:04.598038: step 19856, loss 1.54413e-05, acc 1\n",
      "2018-10-26T18:15:04.810471: step 19857, loss 4.11639e-07, acc 1\n",
      "2018-10-26T18:15:04.987000: step 19858, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:15:05.223368: step 19859, loss 0.00237442, acc 1\n",
      "2018-10-26T18:15:05.395907: step 19860, loss 0.00492505, acc 1\n",
      "2018-10-26T18:15:05.589389: step 19861, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:15:05.765918: step 19862, loss 4.25668e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:15:05.951423: step 19863, loss 4.9732e-07, acc 1\n",
      "2018-10-26T18:15:06.151886: step 19864, loss 0.00315269, acc 1\n",
      "2018-10-26T18:15:06.327417: step 19865, loss 0.0485948, acc 0.984375\n",
      "2018-10-26T18:15:06.499957: step 19866, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:15:06.688454: step 19867, loss 0.00942842, acc 1\n",
      "2018-10-26T18:15:06.876950: step 19868, loss 1.91162e-05, acc 1\n",
      "2018-10-26T18:15:07.057467: step 19869, loss 1.87885e-05, acc 1\n",
      "2018-10-26T18:15:07.260924: step 19870, loss 6.08967e-06, acc 1\n",
      "2018-10-26T18:15:07.443436: step 19871, loss 6.33955e-06, acc 1\n",
      "2018-10-26T18:15:07.626946: step 19872, loss 1.19764e-06, acc 1\n",
      "2018-10-26T18:15:07.793501: step 19873, loss 0.00174599, acc 1\n",
      "2018-10-26T18:15:08.002941: step 19874, loss 0.000194578, acc 1\n",
      "2018-10-26T18:15:08.172488: step 19875, loss 6.6822e-06, acc 1\n",
      "2018-10-26T18:15:08.355002: step 19876, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:15:08.519562: step 19877, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:15:08.768895: step 19878, loss 0, acc 1\n",
      "2018-10-26T18:15:08.969359: step 19879, loss 6.92207e-06, acc 1\n",
      "2018-10-26T18:15:09.156858: step 19880, loss 2.90571e-07, acc 1\n",
      "2018-10-26T18:15:09.339371: step 19881, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:15:09.514901: step 19882, loss 3.16646e-07, acc 1\n",
      "2018-10-26T18:15:09.692427: step 19883, loss 0.0308172, acc 0.984375\n",
      "2018-10-26T18:15:09.889900: step 19884, loss 3.78114e-07, acc 1\n",
      "2018-10-26T18:15:10.066427: step 19885, loss 1.50263e-05, acc 1\n",
      "2018-10-26T18:15:10.229991: step 19886, loss 2.73806e-07, acc 1\n",
      "2018-10-26T18:15:10.404524: step 19887, loss 3.0321e-06, acc 1\n",
      "2018-10-26T18:15:10.572077: step 19888, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:15:10.747608: step 19889, loss 7.37992e-06, acc 1\n",
      "2018-10-26T18:15:10.925133: step 19890, loss 0.000462858, acc 1\n",
      "2018-10-26T18:15:11.103657: step 19891, loss 0.000500497, acc 1\n",
      "2018-10-26T18:15:11.276195: step 19892, loss 6.11016e-06, acc 1\n",
      "2018-10-26T18:15:11.445742: step 19893, loss 0.000337782, acc 1\n",
      "2018-10-26T18:15:11.648202: step 19894, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:15:11.855648: step 19895, loss 8.49527e-05, acc 1\n",
      "2018-10-26T18:15:12.024197: step 19896, loss 0.00114061, acc 1\n",
      "2018-10-26T18:15:12.199729: step 19897, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:15:12.372267: step 19898, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:15:12.547798: step 19899, loss 2.58906e-07, acc 1\n",
      "2018-10-26T18:15:12.713355: step 19900, loss 2.84983e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:15:13.156173: step 19900, loss 7.12109, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-19900\n",
      "\n",
      "2018-10-26T18:15:13.487214: step 19901, loss 1.00535e-05, acc 1\n",
      "2018-10-26T18:15:13.653769: step 19902, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:15:13.831295: step 19903, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:15:13.997850: step 19904, loss 0.0689631, acc 0.984375\n",
      "2018-10-26T18:15:14.175375: step 19905, loss 7.9608e-06, acc 1\n",
      "2018-10-26T18:15:14.415733: step 19906, loss 0.0110586, acc 0.984375\n",
      "2018-10-26T18:15:14.582288: step 19907, loss 1.13432e-06, acc 1\n",
      "2018-10-26T18:15:14.754827: step 19908, loss 4.59654e-05, acc 1\n",
      "2018-10-26T18:15:14.929361: step 19909, loss 2.60013e-06, acc 1\n",
      "2018-10-26T18:15:15.105890: step 19910, loss 7.89145e-05, acc 1\n",
      "2018-10-26T18:15:15.272444: step 19911, loss 2.29086e-05, acc 1\n",
      "2018-10-26T18:15:15.444984: step 19912, loss 0.000168168, acc 1\n",
      "2018-10-26T18:15:15.607549: step 19913, loss 5.79278e-07, acc 1\n",
      "2018-10-26T18:15:15.787070: step 19914, loss 3.7919e-06, acc 1\n",
      "2018-10-26T18:15:15.955618: step 19915, loss 0.000630132, acc 1\n",
      "2018-10-26T18:15:16.131149: step 19916, loss 9.91747e-06, acc 1\n",
      "2018-10-26T18:15:16.299700: step 19917, loss 3.88562e-05, acc 1\n",
      "2018-10-26T18:15:16.482211: step 19918, loss 1.14553e-05, acc 1\n",
      "2018-10-26T18:15:16.654751: step 19919, loss 0.000184348, acc 1\n",
      "2018-10-26T18:15:16.837263: step 19920, loss 2.78041e-05, acc 1\n",
      "2018-10-26T18:15:17.006810: step 19921, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:15:17.192314: step 19922, loss 0.00071449, acc 1\n",
      "2018-10-26T18:15:17.360864: step 19923, loss 5.56279e-05, acc 1\n",
      "2018-10-26T18:15:17.537392: step 19924, loss 2.65409e-06, acc 1\n",
      "2018-10-26T18:15:17.714919: step 19925, loss 5.59095e-05, acc 1\n",
      "2018-10-26T18:15:17.910395: step 19926, loss 0, acc 1\n",
      "2018-10-26T18:15:18.107868: step 19927, loss 0.000130916, acc 1\n",
      "2018-10-26T18:15:18.298360: step 19928, loss 2.81257e-07, acc 1\n",
      "2018-10-26T18:15:18.497826: step 19929, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:15:18.687320: step 19930, loss 8.56816e-08, acc 1\n",
      "2018-10-26T18:15:18.877811: step 19931, loss 0.000387673, acc 1\n",
      "2018-10-26T18:15:19.076281: step 19932, loss 0.000364761, acc 1\n",
      "2018-10-26T18:15:19.260788: step 19933, loss 3.20409e-05, acc 1\n",
      "2018-10-26T18:15:19.439311: step 19934, loss 1.42246e-05, acc 1\n",
      "2018-10-26T18:15:19.620826: step 19935, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:15:19.785386: step 19936, loss 0.000310401, acc 1\n",
      "2018-10-26T18:15:19.964907: step 19937, loss 2.03028e-07, acc 1\n",
      "2018-10-26T18:15:20.139441: step 19938, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:15:20.326941: step 19939, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:15:20.494491: step 19940, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:15:20.671020: step 19941, loss 1.94448e-06, acc 1\n",
      "2018-10-26T18:15:20.842562: step 19942, loss 3.48311e-07, acc 1\n",
      "2018-10-26T18:15:21.017095: step 19943, loss 2.40521e-05, acc 1\n",
      "2018-10-26T18:15:21.185645: step 19944, loss 3.63213e-07, acc 1\n",
      "2018-10-26T18:15:21.366162: step 19945, loss 0.00224281, acc 1\n",
      "2018-10-26T18:15:21.536707: step 19946, loss 0, acc 1\n",
      "2018-10-26T18:15:21.712238: step 19947, loss 3.1851e-07, acc 1\n",
      "2018-10-26T18:15:21.878792: step 19948, loss 3.4363e-06, acc 1\n",
      "2018-10-26T18:15:22.057316: step 19949, loss 1.97075e-05, acc 1\n",
      "2018-10-26T18:15:22.219881: step 19950, loss 1.92909e-06, acc 1\n",
      "2018-10-26T18:15:22.396410: step 19951, loss 6.28091e-05, acc 1\n",
      "2018-10-26T18:15:22.560970: step 19952, loss 5.66238e-07, acc 1\n",
      "2018-10-26T18:15:22.750464: step 19953, loss 5.3457e-07, acc 1\n",
      "2018-10-26T18:15:22.915025: step 19954, loss 2.0489e-07, acc 1\n",
      "2018-10-26T18:15:23.092550: step 19955, loss 9.23547e-05, acc 1\n",
      "2018-10-26T18:15:23.262097: step 19956, loss 0.00022501, acc 1\n",
      "2018-10-26T18:15:23.441617: step 19957, loss 1.5143e-06, acc 1\n",
      "2018-10-26T18:15:23.616151: step 19958, loss 2.66356e-07, acc 1\n",
      "2018-10-26T18:15:23.805645: step 19959, loss 5.56275e-06, acc 1\n",
      "2018-10-26T18:15:23.980182: step 19960, loss 0.000394681, acc 1\n",
      "2018-10-26T18:15:24.167678: step 19961, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:15:24.364152: step 19962, loss 0.00236205, acc 1\n",
      "2018-10-26T18:15:24.537689: step 19963, loss 4.93584e-05, acc 1\n",
      "2018-10-26T18:15:24.716212: step 19964, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:15:24.885760: step 19965, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:15:25.061290: step 19966, loss 7.54354e-07, acc 1\n",
      "2018-10-26T18:15:25.226848: step 19967, loss 4.11592e-06, acc 1\n",
      "2018-10-26T18:15:25.417339: step 19968, loss 1.57594e-05, acc 1\n",
      "2018-10-26T18:15:25.581899: step 19969, loss 5.52922e-06, acc 1\n",
      "2018-10-26T18:15:25.767403: step 19970, loss 1.32988e-06, acc 1\n",
      "2018-10-26T18:15:25.942987: step 19971, loss 1.81927e-05, acc 1\n",
      "2018-10-26T18:15:26.119512: step 19972, loss 1.19294e-05, acc 1\n",
      "2018-10-26T18:15:26.297038: step 19973, loss 5.68935e-06, acc 1\n",
      "2018-10-26T18:15:26.474563: step 19974, loss 3.59486e-07, acc 1\n",
      "2018-10-26T18:15:26.642116: step 19975, loss 0.00937247, acc 1\n",
      "2018-10-26T18:15:26.825626: step 19976, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:15:26.992181: step 19977, loss 2.37865e-05, acc 1\n",
      "2018-10-26T18:15:27.175690: step 19978, loss 2.5926e-06, acc 1\n",
      "2018-10-26T18:15:27.345237: step 19979, loss 8.43382e-06, acc 1\n",
      "2018-10-26T18:15:27.531739: step 19980, loss 4.9732e-07, acc 1\n",
      "2018-10-26T18:15:27.695302: step 19981, loss 2.03027e-07, acc 1\n",
      "2018-10-26T18:15:27.877814: step 19982, loss 2.69688e-06, acc 1\n",
      "2018-10-26T18:15:28.044368: step 19983, loss 6.87017e-05, acc 1\n",
      "2018-10-26T18:15:28.225884: step 19984, loss 1.95758e-06, acc 1\n",
      "2018-10-26T18:15:28.397426: step 19985, loss 8.49343e-07, acc 1\n",
      "2018-10-26T18:15:28.577944: step 19986, loss 3.63985e-05, acc 1\n",
      "2018-10-26T18:15:28.742504: step 19987, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:15:28.923022: step 19988, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:15:29.091572: step 19989, loss 1.7553e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:15:29.277076: step 19990, loss 8.23269e-07, acc 1\n",
      "2018-10-26T18:15:29.446623: step 19991, loss 8.73305e-06, acc 1\n",
      "2018-10-26T18:15:29.618164: step 19992, loss 1.57947e-06, acc 1\n",
      "2018-10-26T18:15:29.783722: step 19993, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:15:29.957258: step 19994, loss 0.0009484, acc 1\n",
      "2018-10-26T18:15:30.125808: step 19995, loss 2.51456e-07, acc 1\n",
      "2018-10-26T18:15:30.299343: step 19996, loss 0, acc 1\n",
      "2018-10-26T18:15:30.466896: step 19997, loss 0.000153427, acc 1\n",
      "2018-10-26T18:15:30.648411: step 19998, loss 2.14203e-07, acc 1\n",
      "2018-10-26T18:15:30.816962: step 19999, loss 8.86594e-07, acc 1\n",
      "2018-10-26T18:15:30.997479: step 20000, loss 2.21647e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:15:31.438301: step 20000, loss 7.09353, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20000\n",
      "\n",
      "2018-10-26T18:15:31.789009: step 20001, loss 0, acc 1\n",
      "2018-10-26T18:15:31.956561: step 20002, loss 1.25503e-05, acc 1\n",
      "2018-10-26T18:15:32.130097: step 20003, loss 9.92764e-07, acc 1\n",
      "2018-10-26T18:15:32.294657: step 20004, loss 0.00213066, acc 1\n",
      "2018-10-26T18:15:32.496145: step 20005, loss 4.86144e-07, acc 1\n",
      "2018-10-26T18:15:32.730493: step 20006, loss 1.47072e-05, acc 1\n",
      "2018-10-26T18:15:32.896050: step 20007, loss 0.105966, acc 0.984375\n",
      "2018-10-26T18:15:33.075571: step 20008, loss 2.71376e-06, acc 1\n",
      "2018-10-26T18:15:33.242126: step 20009, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:15:33.416659: step 20010, loss 0.000113899, acc 1\n",
      "2018-10-26T18:15:33.596181: step 20011, loss 6.70913e-05, acc 1\n",
      "2018-10-26T18:15:33.766724: step 20012, loss 3.10658e-06, acc 1\n",
      "2018-10-26T18:15:33.933280: step 20013, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:15:34.112800: step 20014, loss 1.44594e-05, acc 1\n",
      "2018-10-26T18:15:34.279354: step 20015, loss 7.43183e-07, acc 1\n",
      "2018-10-26T18:15:34.453889: step 20016, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:15:34.619446: step 20017, loss 9.76025e-06, acc 1\n",
      "2018-10-26T18:15:34.794976: step 20018, loss 7.60035e-05, acc 1\n",
      "2018-10-26T18:15:34.963526: step 20019, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:15:35.144044: step 20020, loss 0.00197873, acc 1\n",
      "2018-10-26T18:15:35.313591: step 20021, loss 1.46783e-05, acc 1\n",
      "2018-10-26T18:15:35.487128: step 20022, loss 2.6219e-05, acc 1\n",
      "2018-10-26T18:15:35.654680: step 20023, loss 1.93889e-06, acc 1\n",
      "2018-10-26T18:15:35.827218: step 20024, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:15:35.993774: step 20025, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:15:36.167310: step 20026, loss 2.14616e-05, acc 1\n",
      "2018-10-26T18:15:36.337855: step 20027, loss 2.03027e-07, acc 1\n",
      "2018-10-26T18:15:36.516378: step 20028, loss 3.29467e-06, acc 1\n",
      "2018-10-26T18:15:36.685925: step 20029, loss 2.48401e-05, acc 1\n",
      "2018-10-26T18:15:36.861456: step 20030, loss 0.0444199, acc 0.984375\n",
      "2018-10-26T18:15:37.032000: step 20031, loss 0.109349, acc 0.984375\n",
      "2018-10-26T18:15:37.213515: step 20032, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:15:37.379073: step 20033, loss 1.99292e-06, acc 1\n",
      "2018-10-26T18:15:37.552609: step 20034, loss 0.000482653, acc 1\n",
      "2018-10-26T18:15:37.726145: step 20035, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:15:37.899682: step 20036, loss 6.7585e-06, acc 1\n",
      "2018-10-26T18:15:38.076209: step 20037, loss 0.00187365, acc 1\n",
      "2018-10-26T18:15:38.258722: step 20038, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:15:38.434253: step 20039, loss 6.09888e-05, acc 1\n",
      "2018-10-26T18:15:38.603801: step 20040, loss 8.9791e-06, acc 1\n",
      "2018-10-26T18:15:38.792297: step 20041, loss 0.000173575, acc 1\n",
      "2018-10-26T18:15:38.955860: step 20042, loss 0, acc 1\n",
      "2018-10-26T18:15:39.137375: step 20043, loss 2.0489e-07, acc 1\n",
      "2018-10-26T18:15:39.306922: step 20044, loss 0.0149363, acc 0.984375\n",
      "2018-10-26T18:15:39.485445: step 20045, loss 1.86998e-06, acc 1\n",
      "2018-10-26T18:15:39.662970: step 20046, loss 8.40812e-05, acc 1\n",
      "2018-10-26T18:15:39.835509: step 20047, loss 9.68574e-08, acc 1\n",
      "2018-10-26T18:15:40.013035: step 20048, loss 3.96888e-06, acc 1\n",
      "2018-10-26T18:15:40.192556: step 20049, loss 9.15042e-05, acc 1\n",
      "2018-10-26T18:15:40.372075: step 20050, loss 0.00300799, acc 1\n",
      "2018-10-26T18:15:40.540625: step 20051, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:15:40.715159: step 20052, loss 8.23271e-07, acc 1\n",
      "2018-10-26T18:15:40.882712: step 20053, loss 0.000955952, acc 1\n",
      "2018-10-26T18:15:41.065223: step 20054, loss 5.46963e-06, acc 1\n",
      "2018-10-26T18:15:41.230782: step 20055, loss 0.0268375, acc 0.984375\n",
      "2018-10-26T18:15:41.408307: step 20056, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:15:41.583839: step 20057, loss 5.47533e-05, acc 1\n",
      "2018-10-26T18:15:41.765353: step 20058, loss 0.0908413, acc 0.984375\n",
      "2018-10-26T18:15:41.937892: step 20059, loss 4.09778e-07, acc 1\n",
      "2018-10-26T18:15:42.112426: step 20060, loss 8.71707e-07, acc 1\n",
      "2018-10-26T18:15:42.283968: step 20061, loss 0, acc 1\n",
      "2018-10-26T18:15:42.462491: step 20062, loss 2.3541e-05, acc 1\n",
      "2018-10-26T18:15:42.629046: step 20063, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:15:42.809563: step 20064, loss 1.63604e-05, acc 1\n",
      "2018-10-26T18:15:42.974123: step 20065, loss 7.2269e-07, acc 1\n",
      "2018-10-26T18:15:43.155638: step 20066, loss 1.63642e-05, acc 1\n",
      "2018-10-26T18:15:43.323191: step 20067, loss 2.36563e-05, acc 1\n",
      "2018-10-26T18:15:43.497724: step 20068, loss 7.67399e-07, acc 1\n",
      "2018-10-26T18:15:43.677244: step 20069, loss 6.57099e-05, acc 1\n",
      "2018-10-26T18:15:43.854773: step 20070, loss 1.56082e-06, acc 1\n",
      "2018-10-26T18:15:44.023320: step 20071, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:15:44.207828: step 20072, loss 1.90351e-06, acc 1\n",
      "2018-10-26T18:15:44.379369: step 20073, loss 8.75419e-07, acc 1\n",
      "2018-10-26T18:15:44.557893: step 20074, loss 4.88005e-07, acc 1\n",
      "2018-10-26T18:15:44.727438: step 20075, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:15:44.910948: step 20076, loss 3.10103e-05, acc 1\n",
      "2018-10-26T18:15:45.085481: step 20077, loss 0.000198215, acc 1\n",
      "2018-10-26T18:15:45.260015: step 20078, loss 4.32128e-07, acc 1\n",
      "2018-10-26T18:15:45.428565: step 20079, loss 2.1121e-06, acc 1\n",
      "2018-10-26T18:15:45.605094: step 20080, loss 2.63589e-05, acc 1\n",
      "2018-10-26T18:15:45.770652: step 20081, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:15:45.953164: step 20082, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:15:46.120716: step 20083, loss 0.000276001, acc 1\n",
      "2018-10-26T18:15:46.299240: step 20084, loss 0, acc 1\n",
      "2018-10-26T18:15:46.466791: step 20085, loss 2.32632e-06, acc 1\n",
      "2018-10-26T18:15:46.645314: step 20086, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:15:46.812867: step 20087, loss 3.52752e-06, acc 1\n",
      "2018-10-26T18:15:47.007347: step 20088, loss 7.91612e-07, acc 1\n",
      "2018-10-26T18:15:47.171908: step 20089, loss 4.69381e-07, acc 1\n",
      "2018-10-26T18:15:47.346441: step 20090, loss 0.000446622, acc 1\n",
      "2018-10-26T18:15:47.516986: step 20091, loss 0.000251819, acc 1\n",
      "2018-10-26T18:15:47.701493: step 20092, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:15:47.872036: step 20093, loss 0.00592333, acc 1\n",
      "2018-10-26T18:15:48.055547: step 20094, loss 3.19062e-06, acc 1\n",
      "2018-10-26T18:15:48.234071: step 20095, loss 2.49646e-05, acc 1\n",
      "2018-10-26T18:15:48.420572: step 20096, loss 0.000141372, acc 1\n",
      "2018-10-26T18:15:48.601089: step 20097, loss 1.30368e-05, acc 1\n",
      "2018-10-26T18:15:48.777618: step 20098, loss 2.55537e-06, acc 1\n",
      "2018-10-26T18:15:48.945170: step 20099, loss 7.0597e-06, acc 1\n",
      "2018-10-26T18:15:49.118707: step 20100, loss 1.08876e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:15:49.568504: step 20100, loss 7.19429, acc 0.707317\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20100\n",
      "\n",
      "2018-10-26T18:15:49.943247: step 20101, loss 5.76416e-05, acc 1\n",
      "2018-10-26T18:15:50.123765: step 20102, loss 0.00073249, acc 1\n",
      "2018-10-26T18:15:50.296304: step 20103, loss 8.8141e-05, acc 1\n",
      "2018-10-26T18:15:50.477819: step 20104, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:15:50.674294: step 20105, loss 0.000371752, acc 1\n",
      "2018-10-26T18:15:50.904679: step 20106, loss 8.38188e-08, acc 1\n",
      "2018-10-26T18:15:51.069239: step 20107, loss 2.86282e-05, acc 1\n",
      "2018-10-26T18:15:51.254744: step 20108, loss 0.000691873, acc 1\n",
      "2018-10-26T18:15:51.419304: step 20109, loss 0.000118967, acc 1\n",
      "2018-10-26T18:15:51.598825: step 20110, loss 3.5967e-05, acc 1\n",
      "2018-10-26T18:15:51.768372: step 20111, loss 1.86264e-07, acc 1\n",
      "2018-10-26T18:15:51.947892: step 20112, loss 8.33157e-06, acc 1\n",
      "2018-10-26T18:15:52.119433: step 20113, loss 1.30385e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:15:52.295961: step 20114, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:15:52.461519: step 20115, loss 6.27586e-06, acc 1\n",
      "2018-10-26T18:15:52.634058: step 20116, loss 0.000156303, acc 1\n",
      "2018-10-26T18:15:52.807594: step 20117, loss 9.46294e-06, acc 1\n",
      "2018-10-26T18:15:52.981131: step 20118, loss 2.25923e-06, acc 1\n",
      "2018-10-26T18:15:53.160652: step 20119, loss 5.75293e-06, acc 1\n",
      "2018-10-26T18:15:53.330209: step 20120, loss 0, acc 1\n",
      "2018-10-26T18:15:53.498748: step 20121, loss 1.59062e-06, acc 1\n",
      "2018-10-26T18:15:53.664305: step 20122, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:15:53.849810: step 20123, loss 8.43758e-07, acc 1\n",
      "2018-10-26T18:15:54.020369: step 20124, loss 1.06726e-06, acc 1\n",
      "2018-10-26T18:15:54.188904: step 20125, loss 0.00028112, acc 1\n",
      "2018-10-26T18:15:54.353464: step 20126, loss 1.76846e-05, acc 1\n",
      "2018-10-26T18:15:54.526003: step 20127, loss 6.64448e-05, acc 1\n",
      "2018-10-26T18:15:54.687572: step 20128, loss 1.22373e-06, acc 1\n",
      "2018-10-26T18:15:54.865097: step 20129, loss 2.57955e-06, acc 1\n",
      "2018-10-26T18:15:55.033647: step 20130, loss 2.8312e-07, acc 1\n",
      "2018-10-26T18:15:55.203198: step 20131, loss 3.04143e-06, acc 1\n",
      "2018-10-26T18:15:55.369749: step 20132, loss 5.80477e-05, acc 1\n",
      "2018-10-26T18:15:55.543284: step 20133, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:15:55.708843: step 20134, loss 2.11565e-05, acc 1\n",
      "2018-10-26T18:15:55.883377: step 20135, loss 2.38672e-05, acc 1\n",
      "2018-10-26T18:15:56.047937: step 20136, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:15:56.220476: step 20137, loss 0.0915279, acc 0.984375\n",
      "2018-10-26T18:15:56.390023: step 20138, loss 9.03371e-07, acc 1\n",
      "2018-10-26T18:15:56.564557: step 20139, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:15:56.728120: step 20140, loss 5.60648e-07, acc 1\n",
      "2018-10-26T18:15:56.904648: step 20141, loss 2.51445e-06, acc 1\n",
      "2018-10-26T18:15:57.075193: step 20142, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:15:57.251726: step 20143, loss 4.84282e-07, acc 1\n",
      "2018-10-26T18:15:57.420270: step 20144, loss 9.38248e-06, acc 1\n",
      "2018-10-26T18:15:57.589817: step 20145, loss 4.20952e-07, acc 1\n",
      "2018-10-26T18:15:57.763353: step 20146, loss 6.42602e-07, acc 1\n",
      "2018-10-26T18:15:57.940879: step 20147, loss 0.108723, acc 0.984375\n",
      "2018-10-26T18:15:58.105440: step 20148, loss 9.49948e-08, acc 1\n",
      "2018-10-26T18:15:58.283963: step 20149, loss 1.54594e-06, acc 1\n",
      "2018-10-26T18:15:58.447526: step 20150, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:15:58.629040: step 20151, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:15:58.795595: step 20152, loss 9.3132e-08, acc 1\n",
      "2018-10-26T18:15:58.974118: step 20153, loss 1.76387e-06, acc 1\n",
      "2018-10-26T18:15:59.137682: step 20154, loss 2.27241e-07, acc 1\n",
      "2018-10-26T18:15:59.307254: step 20155, loss 2.60562e-06, acc 1\n",
      "2018-10-26T18:15:59.472786: step 20156, loss 4.01726e-06, acc 1\n",
      "2018-10-26T18:15:59.648317: step 20157, loss 0.000138824, acc 1\n",
      "2018-10-26T18:15:59.816867: step 20158, loss 0.000167636, acc 1\n",
      "2018-10-26T18:15:59.993396: step 20159, loss 1.55155e-06, acc 1\n",
      "2018-10-26T18:16:00.175907: step 20160, loss 0.000231546, acc 1\n",
      "2018-10-26T18:16:00.346452: step 20161, loss 2.12329e-06, acc 1\n",
      "2018-10-26T18:16:00.510015: step 20162, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:16:00.685546: step 20163, loss 2.70636e-06, acc 1\n",
      "2018-10-26T18:16:00.856090: step 20164, loss 5.02908e-07, acc 1\n",
      "2018-10-26T18:16:01.031622: step 20165, loss 1.82539e-07, acc 1\n",
      "2018-10-26T18:16:01.198177: step 20166, loss 5.4016e-07, acc 1\n",
      "2018-10-26T18:16:01.367723: step 20167, loss 1.29829e-05, acc 1\n",
      "2018-10-26T18:16:01.531286: step 20168, loss 6.88168e-06, acc 1\n",
      "2018-10-26T18:16:01.702828: step 20169, loss 0.00426448, acc 1\n",
      "2018-10-26T18:16:01.875366: step 20170, loss 5.31514e-06, acc 1\n",
      "2018-10-26T18:16:02.054888: step 20171, loss 1.2273e-05, acc 1\n",
      "2018-10-26T18:16:02.221442: step 20172, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:16:02.432877: step 20173, loss 0, acc 1\n",
      "2018-10-26T18:16:02.633342: step 20174, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:16:02.807875: step 20175, loss 5.02908e-07, acc 1\n",
      "2018-10-26T18:16:03.034271: step 20176, loss 0.0486318, acc 0.984375\n",
      "2018-10-26T18:16:03.241716: step 20177, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:16:03.415252: step 20178, loss 6.89439e-06, acc 1\n",
      "2018-10-26T18:16:03.626688: step 20179, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:16:03.823163: step 20180, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:16:04.064518: step 20181, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:16:04.245036: step 20182, loss 2.81794e-06, acc 1\n",
      "2018-10-26T18:16:04.479409: step 20183, loss 7.1239e-05, acc 1\n",
      "2018-10-26T18:16:04.666908: step 20184, loss 1.42581e-05, acc 1\n",
      "2018-10-26T18:16:04.893303: step 20185, loss 4.11639e-07, acc 1\n",
      "2018-10-26T18:16:05.100749: step 20186, loss 5.38008e-05, acc 1\n",
      "2018-10-26T18:16:05.310189: step 20187, loss 1.45841e-06, acc 1\n",
      "2018-10-26T18:16:05.534592: step 20188, loss 5.0291e-07, acc 1\n",
      "2018-10-26T18:16:05.737048: step 20189, loss 8.06506e-07, acc 1\n",
      "2018-10-26T18:16:05.963444: step 20190, loss 9.25123e-06, acc 1\n",
      "2018-10-26T18:16:06.165903: step 20191, loss 9.20262e-05, acc 1\n",
      "2018-10-26T18:16:06.390303: step 20192, loss 1.80432e-05, acc 1\n",
      "2018-10-26T18:16:06.625675: step 20193, loss 2.01134e-05, acc 1\n",
      "2018-10-26T18:16:06.844091: step 20194, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:16:07.068492: step 20195, loss 5.97264e-06, acc 1\n",
      "2018-10-26T18:16:07.301868: step 20196, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:16:07.485377: step 20197, loss 3.36683e-05, acc 1\n",
      "2018-10-26T18:16:07.715761: step 20198, loss 3.4457e-06, acc 1\n",
      "2018-10-26T18:16:07.892290: step 20199, loss 0.00158316, acc 1\n",
      "2018-10-26T18:16:08.109709: step 20200, loss 0.0246722, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:16:08.577460: step 20200, loss 7.05645, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20200\n",
      "\n",
      "2018-10-26T18:16:08.943482: step 20201, loss 0.000135563, acc 1\n",
      "2018-10-26T18:16:09.112032: step 20202, loss 1.4845e-06, acc 1\n",
      "2018-10-26T18:16:09.288560: step 20203, loss 0.00106691, acc 1\n",
      "2018-10-26T18:16:09.490022: step 20204, loss 9.12695e-08, acc 1\n",
      "2018-10-26T18:16:09.707441: step 20205, loss 3.44811e-05, acc 1\n",
      "2018-10-26T18:16:09.942812: step 20206, loss 0.000246547, acc 1\n",
      "2018-10-26T18:16:10.171203: step 20207, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:16:10.395602: step 20208, loss 8.15828e-07, acc 1\n",
      "2018-10-26T18:16:10.609031: step 20209, loss 1.91846e-06, acc 1\n",
      "2018-10-26T18:16:10.827448: step 20210, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:16:11.043870: step 20211, loss 9.44347e-07, acc 1\n",
      "2018-10-26T18:16:11.260291: step 20212, loss 4.22798e-05, acc 1\n",
      "2018-10-26T18:16:11.448788: step 20213, loss 2.4773e-07, acc 1\n",
      "2018-10-26T18:16:11.675183: step 20214, loss 1.98359e-06, acc 1\n",
      "2018-10-26T18:16:11.846725: step 20215, loss 7.15991e-05, acc 1\n",
      "2018-10-26T18:16:12.052176: step 20216, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:16:12.222720: step 20217, loss 1.76196e-06, acc 1\n",
      "2018-10-26T18:16:12.413211: step 20218, loss 2.07956e-05, acc 1\n",
      "2018-10-26T18:16:12.594726: step 20219, loss 9.67122e-06, acc 1\n",
      "2018-10-26T18:16:12.777250: step 20220, loss 3.27823e-07, acc 1\n",
      "2018-10-26T18:16:12.989671: step 20221, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:16:13.239006: step 20222, loss 9.05236e-07, acc 1\n",
      "2018-10-26T18:16:13.419523: step 20223, loss 0.00362921, acc 1\n",
      "2018-10-26T18:16:13.649922: step 20224, loss 0.000498444, acc 1\n",
      "2018-10-26T18:16:13.816461: step 20225, loss 0.000427642, acc 1\n",
      "2018-10-26T18:16:13.999971: step 20226, loss 0.00129618, acc 1\n",
      "2018-10-26T18:16:14.206420: step 20227, loss 3.09181e-06, acc 1\n",
      "2018-10-26T18:16:14.382948: step 20228, loss 0.159092, acc 0.984375\n",
      "2018-10-26T18:16:14.561471: step 20229, loss 0.000143683, acc 1\n",
      "2018-10-26T18:16:14.755952: step 20230, loss 9.33207e-05, acc 1\n",
      "2018-10-26T18:16:14.944447: step 20231, loss 6.68174e-06, acc 1\n",
      "2018-10-26T18:16:15.121974: step 20232, loss 1.37085e-06, acc 1\n",
      "2018-10-26T18:16:15.303489: step 20233, loss 4.40836e-06, acc 1\n",
      "2018-10-26T18:16:15.488993: step 20234, loss 5.27694e-05, acc 1\n",
      "2018-10-26T18:16:15.661532: step 20235, loss 0, acc 1\n",
      "2018-10-26T18:16:15.842050: step 20236, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:16:16.007608: step 20237, loss 1.15107e-06, acc 1\n",
      "2018-10-26T18:16:16.187128: step 20238, loss 2.37653e-05, acc 1\n",
      "2018-10-26T18:16:16.352686: step 20239, loss 0.00535517, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:16:16.538189: step 20240, loss 2.56845e-06, acc 1\n",
      "2018-10-26T18:16:16.705742: step 20241, loss 3.40861e-07, acc 1\n",
      "2018-10-26T18:16:16.884265: step 20242, loss 2.15972e-05, acc 1\n",
      "2018-10-26T18:16:17.054809: step 20243, loss 9.68572e-08, acc 1\n",
      "2018-10-26T18:16:17.228345: step 20244, loss 6.77988e-07, acc 1\n",
      "2018-10-26T18:16:17.397893: step 20245, loss 4.28405e-07, acc 1\n",
      "2018-10-26T18:16:17.580405: step 20246, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:16:17.749952: step 20247, loss 0.00469795, acc 1\n",
      "2018-10-26T18:16:17.923489: step 20248, loss 3.22235e-07, acc 1\n",
      "2018-10-26T18:16:18.087051: step 20249, loss 2.71739e-06, acc 1\n",
      "2018-10-26T18:16:18.253607: step 20250, loss 0, acc 1\n",
      "2018-10-26T18:16:18.421159: step 20251, loss 8.19542e-07, acc 1\n",
      "2018-10-26T18:16:18.594696: step 20252, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:16:18.759255: step 20253, loss 6.07211e-07, acc 1\n",
      "2018-10-26T18:16:18.933790: step 20254, loss 0.000178372, acc 1\n",
      "2018-10-26T18:16:19.104333: step 20255, loss 7.72855e-05, acc 1\n",
      "2018-10-26T18:16:19.277870: step 20256, loss 7.13377e-07, acc 1\n",
      "2018-10-26T18:16:19.446419: step 20257, loss 1.23596e-05, acc 1\n",
      "2018-10-26T18:16:19.618959: step 20258, loss 0, acc 1\n",
      "2018-10-26T18:16:19.784517: step 20259, loss 1.10452e-06, acc 1\n",
      "2018-10-26T18:16:19.961045: step 20260, loss 1.54516e-05, acc 1\n",
      "2018-10-26T18:16:20.137573: step 20261, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:16:20.317093: step 20262, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:16:20.495616: step 20263, loss 0.000507945, acc 1\n",
      "2018-10-26T18:16:20.666160: step 20264, loss 0.000626527, acc 1\n",
      "2018-10-26T18:16:20.839697: step 20265, loss 0.00038568, acc 1\n",
      "2018-10-26T18:16:21.010241: step 20266, loss 6.53898e-06, acc 1\n",
      "2018-10-26T18:16:21.183778: step 20267, loss 2.09906e-06, acc 1\n",
      "2018-10-26T18:16:21.350333: step 20268, loss 7.11183e-06, acc 1\n",
      "2018-10-26T18:16:21.527858: step 20269, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:16:21.697405: step 20270, loss 3.95895e-05, acc 1\n",
      "2018-10-26T18:16:21.870942: step 20271, loss 0.000819029, acc 1\n",
      "2018-10-26T18:16:22.040489: step 20272, loss 0, acc 1\n",
      "2018-10-26T18:16:22.215022: step 20273, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:16:22.382575: step 20274, loss 7.99616e-06, acc 1\n",
      "2018-10-26T18:16:22.556111: step 20275, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:16:22.723664: step 20276, loss 1.4156e-07, acc 1\n",
      "2018-10-26T18:16:22.906176: step 20277, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:16:23.073728: step 20278, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:16:23.250257: step 20279, loss 4.83471e-06, acc 1\n",
      "2018-10-26T18:16:23.473684: step 20280, loss 8.54943e-07, acc 1\n",
      "2018-10-26T18:16:23.650188: step 20281, loss 8.17682e-07, acc 1\n",
      "2018-10-26T18:16:23.854641: step 20282, loss 3.53899e-07, acc 1\n",
      "2018-10-26T18:16:24.079042: step 20283, loss 8.75419e-07, acc 1\n",
      "2018-10-26T18:16:24.271527: step 20284, loss 0.000137826, acc 1\n",
      "2018-10-26T18:16:24.478974: step 20285, loss 9.76002e-07, acc 1\n",
      "2018-10-26T18:16:24.647522: step 20286, loss 0, acc 1\n",
      "2018-10-26T18:16:24.828041: step 20287, loss 3.57401e-06, acc 1\n",
      "2018-10-26T18:16:25.011551: step 20288, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:16:25.202041: step 20289, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:16:25.388544: step 20290, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:16:25.573050: step 20291, loss 0.000194008, acc 1\n",
      "2018-10-26T18:16:25.742597: step 20292, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:16:25.919125: step 20293, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:16:26.088672: step 20294, loss 2.22372e-05, acc 1\n",
      "2018-10-26T18:16:26.266197: step 20295, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:16:26.431756: step 20296, loss 3.40549e-05, acc 1\n",
      "2018-10-26T18:16:26.606290: step 20297, loss 2.84982e-07, acc 1\n",
      "2018-10-26T18:16:26.772844: step 20298, loss 1.09334e-06, acc 1\n",
      "2018-10-26T18:16:26.943389: step 20299, loss 6.29562e-07, acc 1\n",
      "2018-10-26T18:16:27.111939: step 20300, loss 1.11759e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:16:27.557748: step 20300, loss 7.11764, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20300\n",
      "\n",
      "2018-10-26T18:16:27.917713: step 20301, loss 1.28334e-06, acc 1\n",
      "2018-10-26T18:16:28.086263: step 20302, loss 1.57388e-06, acc 1\n",
      "2018-10-26T18:16:28.261793: step 20303, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:16:28.426352: step 20304, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:16:28.618839: step 20305, loss 8.81023e-07, acc 1\n",
      "2018-10-26T18:16:28.853213: step 20306, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:16:29.019768: step 20307, loss 0.000179233, acc 1\n",
      "2018-10-26T18:16:29.204274: step 20308, loss 2.80188e-05, acc 1\n",
      "2018-10-26T18:16:29.370829: step 20309, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:16:29.550349: step 20310, loss 4.10139e-05, acc 1\n",
      "2018-10-26T18:16:29.722889: step 20311, loss 3.09369e-06, acc 1\n",
      "2018-10-26T18:16:29.901413: step 20312, loss 9.11897e-06, acc 1\n",
      "2018-10-26T18:16:30.085920: step 20313, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:16:30.296358: step 20314, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:16:30.477872: step 20315, loss 2.21653e-07, acc 1\n",
      "2018-10-26T18:16:30.671356: step 20316, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:16:30.833920: step 20317, loss 7.61177e-05, acc 1\n",
      "2018-10-26T18:16:31.007456: step 20318, loss 3.82727e-06, acc 1\n",
      "2018-10-26T18:16:31.176007: step 20319, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:16:31.355527: step 20320, loss 2.76588e-06, acc 1\n",
      "2018-10-26T18:16:31.524077: step 20321, loss 1.03513e-05, acc 1\n",
      "2018-10-26T18:16:31.706589: step 20322, loss 0.141361, acc 0.984375\n",
      "2018-10-26T18:16:31.887107: step 20323, loss 2.9409e-06, acc 1\n",
      "2018-10-26T18:16:32.063635: step 20324, loss 7.3759e-07, acc 1\n",
      "2018-10-26T18:16:32.240163: step 20325, loss 3.78358e-05, acc 1\n",
      "2018-10-26T18:16:32.408713: step 20326, loss 0.000109167, acc 1\n",
      "2018-10-26T18:16:32.588233: step 20327, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:16:32.760772: step 20328, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:16:32.942687: step 20329, loss 0.00013529, acc 1\n",
      "2018-10-26T18:16:33.107248: step 20330, loss 6.76126e-07, acc 1\n",
      "2018-10-26T18:16:33.287765: step 20331, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:16:33.455317: step 20332, loss 6.85439e-07, acc 1\n",
      "2018-10-26T18:16:33.635841: step 20333, loss 2.43063e-06, acc 1\n",
      "2018-10-26T18:16:33.814358: step 20334, loss 4.33529e-05, acc 1\n",
      "2018-10-26T18:16:34.000860: step 20335, loss 0.00986055, acc 1\n",
      "2018-10-26T18:16:34.164423: step 20336, loss 0.00113164, acc 1\n",
      "2018-10-26T18:16:34.339954: step 20337, loss 0.000224464, acc 1\n",
      "2018-10-26T18:16:34.506509: step 20338, loss 2.78106e-05, acc 1\n",
      "2018-10-26T18:16:34.693012: step 20339, loss 9.20124e-07, acc 1\n",
      "2018-10-26T18:16:34.856574: step 20340, loss 9.7041e-07, acc 1\n",
      "2018-10-26T18:16:35.036094: step 20341, loss 1.0654e-06, acc 1\n",
      "2018-10-26T18:16:35.198659: step 20342, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:16:35.382170: step 20343, loss 7.47488e-06, acc 1\n",
      "2018-10-26T18:16:35.552714: step 20344, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:16:35.736223: step 20345, loss 0.000166444, acc 1\n",
      "2018-10-26T18:16:35.900784: step 20346, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:16:36.086288: step 20347, loss 2.56448e-05, acc 1\n",
      "2018-10-26T18:16:36.255835: step 20348, loss 2.0321e-06, acc 1\n",
      "2018-10-26T18:16:36.431366: step 20349, loss 7.75877e-06, acc 1\n",
      "2018-10-26T18:16:36.605899: step 20350, loss 1.11758e-07, acc 1\n",
      "2018-10-26T18:16:36.783425: step 20351, loss 0.0691273, acc 0.984375\n",
      "2018-10-26T18:16:36.957959: step 20352, loss 5.26301e-06, acc 1\n",
      "2018-10-26T18:16:37.143463: step 20353, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:16:37.308023: step 20354, loss 1.05164e-05, acc 1\n",
      "2018-10-26T18:16:37.486547: step 20355, loss 1.38202e-06, acc 1\n",
      "2018-10-26T18:16:37.659085: step 20356, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:16:37.836611: step 20357, loss 1.11759e-07, acc 1\n",
      "2018-10-26T18:16:38.003166: step 20358, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:16:38.182687: step 20359, loss 8.02035e-06, acc 1\n",
      "2018-10-26T18:16:38.356223: step 20360, loss 4.42861e-05, acc 1\n",
      "2018-10-26T18:16:38.527766: step 20361, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:16:38.692325: step 20362, loss 9.66683e-07, acc 1\n",
      "2018-10-26T18:16:38.875835: step 20363, loss 6.26184e-05, acc 1\n",
      "2018-10-26T18:16:39.042390: step 20364, loss 4.67487e-06, acc 1\n",
      "2018-10-26T18:16:39.222907: step 20365, loss 2.02747e-05, acc 1\n",
      "2018-10-26T18:16:39.388465: step 20366, loss 1.2051e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:16:39.567985: step 20367, loss 5.49475e-07, acc 1\n",
      "2018-10-26T18:16:39.736535: step 20368, loss 0.0010242, acc 1\n",
      "2018-10-26T18:16:39.914061: step 20369, loss 8.41893e-07, acc 1\n",
      "2018-10-26T18:16:40.079620: step 20370, loss 0.000199841, acc 1\n",
      "2018-10-26T18:16:40.250163: step 20371, loss 6.16256e-05, acc 1\n",
      "2018-10-26T18:16:40.416719: step 20372, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:16:40.587262: step 20373, loss 2.97582e-05, acc 1\n",
      "2018-10-26T18:16:40.756809: step 20374, loss 4.34869e-06, acc 1\n",
      "2018-10-26T18:16:40.940319: step 20375, loss 1.60273e-05, acc 1\n",
      "2018-10-26T18:16:41.119840: step 20376, loss 5.69864e-05, acc 1\n",
      "2018-10-26T18:16:41.298362: step 20377, loss 0.0555573, acc 0.984375\n",
      "2018-10-26T18:16:41.481872: step 20378, loss 4.73105e-07, acc 1\n",
      "2018-10-26T18:16:41.648427: step 20379, loss 2.07181e-05, acc 1\n",
      "2018-10-26T18:16:41.825952: step 20380, loss 0, acc 1\n",
      "2018-10-26T18:16:41.992508: step 20381, loss 7.60975e-05, acc 1\n",
      "2018-10-26T18:16:42.162055: step 20382, loss 3.53306e-06, acc 1\n",
      "2018-10-26T18:16:42.333596: step 20383, loss 5.97452e-06, acc 1\n",
      "2018-10-26T18:16:42.508129: step 20384, loss 1.14921e-05, acc 1\n",
      "2018-10-26T18:16:42.672691: step 20385, loss 2.66336e-06, acc 1\n",
      "2018-10-26T18:16:42.855203: step 20386, loss 8.21419e-07, acc 1\n",
      "2018-10-26T18:16:43.027741: step 20387, loss 1.91851e-07, acc 1\n",
      "2018-10-26T18:16:43.202275: step 20388, loss 1.45243e-05, acc 1\n",
      "2018-10-26T18:16:43.378803: step 20389, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:16:43.550345: step 20390, loss 1.3778e-05, acc 1\n",
      "2018-10-26T18:16:43.722884: step 20391, loss 0, acc 1\n",
      "2018-10-26T18:16:43.897419: step 20392, loss 0.000161387, acc 1\n",
      "2018-10-26T18:16:44.063972: step 20393, loss 0.0141248, acc 0.984375\n",
      "2018-10-26T18:16:44.237509: step 20394, loss 3.57517e-05, acc 1\n",
      "2018-10-26T18:16:44.414037: step 20395, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:16:44.585580: step 20396, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:16:44.760113: step 20397, loss 0.00155951, acc 1\n",
      "2018-10-26T18:16:44.935644: step 20398, loss 0.000272636, acc 1\n",
      "2018-10-26T18:16:45.113169: step 20399, loss 9.3235e-06, acc 1\n",
      "2018-10-26T18:16:45.270749: step 20400, loss 9.55778e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:16:45.715560: step 20400, loss 7.24011, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20400\n",
      "\n",
      "2018-10-26T18:16:46.081886: step 20401, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:16:46.258414: step 20402, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:16:46.423971: step 20403, loss 5.3457e-07, acc 1\n",
      "2018-10-26T18:16:46.592521: step 20404, loss 5.00427e-05, acc 1\n",
      "2018-10-26T18:16:46.778025: step 20405, loss 0.000559459, acc 1\n",
      "2018-10-26T18:16:47.025365: step 20406, loss 1.56831e-06, acc 1\n",
      "2018-10-26T18:16:47.190922: step 20407, loss 4.72514e-05, acc 1\n",
      "2018-10-26T18:16:47.370442: step 20408, loss 1.13618e-06, acc 1\n",
      "2018-10-26T18:16:47.539990: step 20409, loss 2.0489e-07, acc 1\n",
      "2018-10-26T18:16:47.717515: step 20410, loss 1.86263e-07, acc 1\n",
      "2018-10-26T18:16:47.881078: step 20411, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:16:48.059601: step 20412, loss 1.3783e-06, acc 1\n",
      "2018-10-26T18:16:48.235131: step 20413, loss 0.000296581, acc 1\n",
      "2018-10-26T18:16:48.406674: step 20414, loss 0, acc 1\n",
      "2018-10-26T18:16:48.575223: step 20415, loss 7.00348e-07, acc 1\n",
      "2018-10-26T18:16:48.755741: step 20416, loss 4.92609e-06, acc 1\n",
      "2018-10-26T18:16:48.926285: step 20417, loss 5.92522e-05, acc 1\n",
      "2018-10-26T18:16:49.098825: step 20418, loss 7.9786e-05, acc 1\n",
      "2018-10-26T18:16:49.268371: step 20419, loss 8.94115e-05, acc 1\n",
      "2018-10-26T18:16:49.441908: step 20420, loss 2.12142e-06, acc 1\n",
      "2018-10-26T18:16:49.611455: step 20421, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:16:49.784992: step 20422, loss 0.000642305, acc 1\n",
      "2018-10-26T18:16:49.956533: step 20423, loss 3.78449e-05, acc 1\n",
      "2018-10-26T18:16:50.140043: step 20424, loss 0.00190617, acc 1\n",
      "2018-10-26T18:16:50.305600: step 20425, loss 0, acc 1\n",
      "2018-10-26T18:16:50.483126: step 20426, loss 1.67074e-06, acc 1\n",
      "2018-10-26T18:16:50.656662: step 20427, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:16:50.842167: step 20428, loss 9.08959e-07, acc 1\n",
      "2018-10-26T18:16:51.017699: step 20429, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:16:51.195224: step 20430, loss 1.4156e-07, acc 1\n",
      "2018-10-26T18:16:51.364770: step 20431, loss 6.40644e-05, acc 1\n",
      "2018-10-26T18:16:51.550275: step 20432, loss 2.0396e-05, acc 1\n",
      "2018-10-26T18:16:51.725806: step 20433, loss 0, acc 1\n",
      "2018-10-26T18:16:51.904334: step 20434, loss 6.06045e-06, acc 1\n",
      "2018-10-26T18:16:52.069886: step 20435, loss 8.01172e-06, acc 1\n",
      "2018-10-26T18:16:52.248410: step 20436, loss 3.07866e-05, acc 1\n",
      "2018-10-26T18:16:52.412970: step 20437, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:16:52.591518: step 20438, loss 0.00211351, acc 1\n",
      "2018-10-26T18:16:52.757050: step 20439, loss 1.11758e-07, acc 1\n",
      "2018-10-26T18:16:52.932581: step 20440, loss 3.26301e-06, acc 1\n",
      "2018-10-26T18:16:53.099136: step 20441, loss 2.5256e-06, acc 1\n",
      "2018-10-26T18:16:53.276662: step 20442, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:16:53.446208: step 20443, loss 1.25335e-05, acc 1\n",
      "2018-10-26T18:16:53.627724: step 20444, loss 2.42142e-07, acc 1\n",
      "2018-10-26T18:16:53.793282: step 20445, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:16:53.967815: step 20446, loss 2.34894e-05, acc 1\n",
      "2018-10-26T18:16:54.132376: step 20447, loss 3.40862e-07, acc 1\n",
      "2018-10-26T18:16:54.314888: step 20448, loss 1.86813e-06, acc 1\n",
      "2018-10-26T18:16:54.480446: step 20449, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:16:54.654979: step 20450, loss 3.5647e-06, acc 1\n",
      "2018-10-26T18:16:54.821534: step 20451, loss 0, acc 1\n",
      "2018-10-26T18:16:55.004046: step 20452, loss 0.00020598, acc 1\n",
      "2018-10-26T18:16:55.168607: step 20453, loss 4.47035e-08, acc 1\n",
      "2018-10-26T18:16:55.344139: step 20454, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:16:55.517674: step 20455, loss 1.15852e-06, acc 1\n",
      "2018-10-26T18:16:55.695200: step 20456, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:16:55.866741: step 20457, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:16:56.045265: step 20458, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:16:56.217803: step 20459, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:16:56.395330: step 20460, loss 7.25609e-05, acc 1\n",
      "2018-10-26T18:16:56.568866: step 20461, loss 0.000251633, acc 1\n",
      "2018-10-26T18:16:56.747389: step 20462, loss 8.1765e-05, acc 1\n",
      "2018-10-26T18:16:56.913943: step 20463, loss 2.34692e-07, acc 1\n",
      "2018-10-26T18:16:57.090472: step 20464, loss 2.57043e-07, acc 1\n",
      "2018-10-26T18:16:57.267998: step 20465, loss 1.01723e-05, acc 1\n",
      "2018-10-26T18:16:57.454499: step 20466, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:16:57.622052: step 20467, loss 0, acc 1\n",
      "2018-10-26T18:16:57.796585: step 20468, loss 8.67559e-05, acc 1\n",
      "2018-10-26T18:16:57.964139: step 20469, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:16:58.141664: step 20470, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:16:58.309217: step 20471, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:16:58.494720: step 20472, loss 2.32424e-05, acc 1\n",
      "2018-10-26T18:16:58.661275: step 20473, loss 9.46149e-06, acc 1\n",
      "2018-10-26T18:16:58.837803: step 20474, loss 0.0013927, acc 1\n",
      "2018-10-26T18:16:59.004358: step 20475, loss 7.81486e-05, acc 1\n",
      "2018-10-26T18:16:59.184876: step 20476, loss 2.25364e-06, acc 1\n",
      "2018-10-26T18:16:59.355420: step 20477, loss 0, acc 1\n",
      "2018-10-26T18:16:59.534941: step 20478, loss 8.3306e-06, acc 1\n",
      "2018-10-26T18:16:59.704489: step 20479, loss 9.68572e-08, acc 1\n",
      "2018-10-26T18:16:59.882014: step 20480, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:17:00.067517: step 20481, loss 1.03492e-05, acc 1\n",
      "2018-10-26T18:17:00.246041: step 20482, loss 3.57114e-05, acc 1\n",
      "2018-10-26T18:17:00.426558: step 20483, loss 2.04691e-06, acc 1\n",
      "2018-10-26T18:17:00.612063: step 20484, loss 0, acc 1\n",
      "2018-10-26T18:17:00.778618: step 20485, loss 9.05237e-07, acc 1\n",
      "2018-10-26T18:17:00.966117: step 20486, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:17:01.129680: step 20487, loss 2.4459e-05, acc 1\n",
      "2018-10-26T18:17:01.311195: step 20488, loss 3.24099e-07, acc 1\n",
      "2018-10-26T18:17:01.479745: step 20489, loss 2.92434e-07, acc 1\n",
      "2018-10-26T18:17:01.662256: step 20490, loss 5.21532e-07, acc 1\n",
      "2018-10-26T18:17:01.846763: step 20491, loss 2.60769e-07, acc 1\n",
      "2018-10-26T18:17:02.049222: step 20492, loss 3.12707e-06, acc 1\n",
      "2018-10-26T18:17:02.263650: step 20493, loss 1.18274e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:17:02.464115: step 20494, loss 4.724e-05, acc 1\n",
      "2018-10-26T18:17:02.669565: step 20495, loss 5.28823e-05, acc 1\n",
      "2018-10-26T18:17:02.850083: step 20496, loss 0, acc 1\n",
      "2018-10-26T18:17:03.025614: step 20497, loss 4.43304e-07, acc 1\n",
      "2018-10-26T18:17:03.194164: step 20498, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:17:03.363710: step 20499, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:17:03.533258: step 20500, loss 0.000279559, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:17:03.977072: step 20500, loss 7.20334, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20500\n",
      "\n",
      "2018-10-26T18:17:04.364299: step 20501, loss 0.000115856, acc 1\n",
      "2018-10-26T18:17:04.540827: step 20502, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:17:04.704389: step 20503, loss 6.01629e-07, acc 1\n",
      "2018-10-26T18:17:04.877926: step 20504, loss 0.0219621, acc 0.984375\n",
      "2018-10-26T18:17:05.059440: step 20505, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:17:05.311768: step 20506, loss 2.32829e-07, acc 1\n",
      "2018-10-26T18:17:05.479319: step 20507, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:17:05.662828: step 20508, loss 5.43002e-05, acc 1\n",
      "2018-10-26T18:17:05.839357: step 20509, loss 0.00725358, acc 1\n",
      "2018-10-26T18:17:06.014887: step 20510, loss 1.84205e-06, acc 1\n",
      "2018-10-26T18:17:06.198398: step 20511, loss 1.41368e-06, acc 1\n",
      "2018-10-26T18:17:06.363956: step 20512, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:17:06.537492: step 20513, loss 2.73591e-05, acc 1\n",
      "2018-10-26T18:17:06.721001: step 20514, loss 3.17847e-05, acc 1\n",
      "2018-10-26T18:17:06.902516: step 20515, loss 0.000199617, acc 1\n",
      "2018-10-26T18:17:07.083034: step 20516, loss 6.60421e-06, acc 1\n",
      "2018-10-26T18:17:07.260560: step 20517, loss 3.90014e-06, acc 1\n",
      "2018-10-26T18:17:07.429110: step 20518, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:17:07.620598: step 20519, loss 0.00102243, acc 1\n",
      "2018-10-26T18:17:07.789148: step 20520, loss 9.399e-06, acc 1\n",
      "2018-10-26T18:17:08.003575: step 20521, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:17:08.196060: step 20522, loss 0.00122175, acc 1\n",
      "2018-10-26T18:17:08.384556: step 20523, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:17:08.610952: step 20524, loss 9.38753e-07, acc 1\n",
      "2018-10-26T18:17:08.775511: step 20525, loss 1.40403e-05, acc 1\n",
      "2018-10-26T18:17:08.986946: step 20526, loss 3.65664e-05, acc 1\n",
      "2018-10-26T18:17:09.153502: step 20527, loss 5.0477e-07, acc 1\n",
      "2018-10-26T18:17:09.356958: step 20528, loss 4.41755e-05, acc 1\n",
      "2018-10-26T18:17:09.574378: step 20529, loss 7.21641e-06, acc 1\n",
      "2018-10-26T18:17:09.755892: step 20530, loss 5.30702e-05, acc 1\n",
      "2018-10-26T18:17:09.975306: step 20531, loss 8.79144e-07, acc 1\n",
      "2018-10-26T18:17:10.170784: step 20532, loss 4.89988e-06, acc 1\n",
      "2018-10-26T18:17:10.363270: step 20533, loss 1.32173e-05, acc 1\n",
      "2018-10-26T18:17:10.542790: step 20534, loss 3.78674e-05, acc 1\n",
      "2018-10-26T18:17:10.776167: step 20535, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:17:11.012534: step 20536, loss 3.94717e-05, acc 1\n",
      "2018-10-26T18:17:11.237933: step 20537, loss 0.00427582, acc 1\n",
      "2018-10-26T18:17:11.440392: step 20538, loss 0, acc 1\n",
      "2018-10-26T18:17:11.644845: step 20539, loss 7.67392e-07, acc 1\n",
      "2018-10-26T18:17:11.865257: step 20540, loss 4.28404e-07, acc 1\n",
      "2018-10-26T18:17:12.118580: step 20541, loss 9.3132e-08, acc 1\n",
      "2018-10-26T18:17:12.355945: step 20542, loss 9.37588e-06, acc 1\n",
      "2018-10-26T18:17:12.597301: step 20543, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:17:12.785797: step 20544, loss 2.92226e-06, acc 1\n",
      "2018-10-26T18:17:13.028151: step 20545, loss 0.00256484, acc 1\n",
      "2018-10-26T18:17:13.195702: step 20546, loss 7.84157e-07, acc 1\n",
      "2018-10-26T18:17:13.433067: step 20547, loss 1.41797e-05, acc 1\n",
      "2018-10-26T18:17:13.602615: step 20548, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:17:13.816044: step 20549, loss 0.00122502, acc 1\n",
      "2018-10-26T18:17:14.005538: step 20550, loss 3.54428e-06, acc 1\n",
      "2018-10-26T18:17:14.182067: step 20551, loss 2.56815e-05, acc 1\n",
      "2018-10-26T18:17:14.386521: step 20552, loss 8.73558e-07, acc 1\n",
      "2018-10-26T18:17:14.586985: step 20553, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:17:14.764510: step 20554, loss 1.05609e-06, acc 1\n",
      "2018-10-26T18:17:14.930068: step 20555, loss 2.55914e-06, acc 1\n",
      "2018-10-26T18:17:15.165439: step 20556, loss 8.33531e-05, acc 1\n",
      "2018-10-26T18:17:15.365902: step 20557, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:17:15.555399: step 20558, loss 0.0240062, acc 0.984375\n",
      "2018-10-26T18:17:15.737909: step 20559, loss 4.84288e-08, acc 1\n",
      "2018-10-26T18:17:15.961312: step 20560, loss 0.000108223, acc 1\n",
      "2018-10-26T18:17:16.177734: step 20561, loss 3.53334e-05, acc 1\n",
      "2018-10-26T18:17:16.402134: step 20562, loss 6.45277e-06, acc 1\n",
      "2018-10-26T18:17:16.572679: step 20563, loss 9.49947e-08, acc 1\n",
      "2018-10-26T18:17:16.815032: step 20564, loss 0.000278543, acc 1\n",
      "2018-10-26T18:17:16.988568: step 20565, loss 3.31707e-06, acc 1\n",
      "2018-10-26T18:17:17.200003: step 20566, loss 7.26431e-08, acc 1\n",
      "2018-10-26T18:17:17.373539: step 20567, loss 4.79198e-06, acc 1\n",
      "2018-10-26T18:17:17.585972: step 20568, loss 5.49774e-05, acc 1\n",
      "2018-10-26T18:17:17.755518: step 20569, loss 6.61665e-05, acc 1\n",
      "2018-10-26T18:17:17.935038: step 20570, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:17:18.147472: step 20571, loss 0, acc 1\n",
      "2018-10-26T18:17:18.320011: step 20572, loss 1.99104e-06, acc 1\n",
      "2018-10-26T18:17:18.508507: step 20573, loss 1.39506e-05, acc 1\n",
      "2018-10-26T18:17:18.717947: step 20574, loss 1.55523e-06, acc 1\n",
      "2018-10-26T18:17:18.887494: step 20575, loss 9.85326e-07, acc 1\n",
      "2018-10-26T18:17:19.068012: step 20576, loss 3.12897e-06, acc 1\n",
      "2018-10-26T18:17:19.260498: step 20577, loss 1.60739e-05, acc 1\n",
      "2018-10-26T18:17:19.446001: step 20578, loss 5.84618e-06, acc 1\n",
      "2018-10-26T18:17:19.622531: step 20579, loss 4.36573e-06, acc 1\n",
      "2018-10-26T18:17:19.793074: step 20580, loss 2.45664e-06, acc 1\n",
      "2018-10-26T18:17:20.013486: step 20581, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:17:20.177049: step 20582, loss 1.71363e-07, acc 1\n",
      "2018-10-26T18:17:20.363551: step 20583, loss 4.71242e-07, acc 1\n",
      "2018-10-26T18:17:20.539081: step 20584, loss 3.55917e-06, acc 1\n",
      "2018-10-26T18:17:20.751514: step 20585, loss 7.23343e-05, acc 1\n",
      "2018-10-26T18:17:20.926048: step 20586, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:17:21.102575: step 20587, loss 2.317e-06, acc 1\n",
      "2018-10-26T18:17:21.267137: step 20588, loss 3.04525e-06, acc 1\n",
      "2018-10-26T18:17:21.456630: step 20589, loss 1.37836e-07, acc 1\n",
      "2018-10-26T18:17:21.624182: step 20590, loss 9.49948e-08, acc 1\n",
      "2018-10-26T18:17:21.804700: step 20591, loss 2.92222e-06, acc 1\n",
      "2018-10-26T18:17:21.984221: step 20592, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:17:22.163741: step 20593, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:17:22.343261: step 20594, loss 9.12693e-08, acc 1\n",
      "2018-10-26T18:17:22.511811: step 20595, loss 3.28537e-06, acc 1\n",
      "2018-10-26T18:17:22.686345: step 20596, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:17:22.855892: step 20597, loss 1.77128e-06, acc 1\n",
      "2018-10-26T18:17:23.044389: step 20598, loss 2.23502e-06, acc 1\n",
      "2018-10-26T18:17:23.209946: step 20599, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:17:23.394452: step 20600, loss 1.09896e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:17:23.843253: step 20600, loss 7.17019, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20600\n",
      "\n",
      "2018-10-26T18:17:24.173900: step 20601, loss 0.000175909, acc 1\n",
      "2018-10-26T18:17:24.336467: step 20602, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:17:24.512995: step 20603, loss 0, acc 1\n",
      "2018-10-26T18:17:24.682541: step 20604, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:17:24.872035: step 20605, loss 1.3718e-05, acc 1\n",
      "2018-10-26T18:17:25.120372: step 20606, loss 1.06912e-06, acc 1\n",
      "2018-10-26T18:17:25.285938: step 20607, loss 4.33994e-07, acc 1\n",
      "2018-10-26T18:17:25.474426: step 20608, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:17:25.645969: step 20609, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:17:25.823494: step 20610, loss 0.000238032, acc 1\n",
      "2018-10-26T18:17:25.990048: step 20611, loss 3.27055e-05, acc 1\n",
      "2018-10-26T18:17:26.169569: step 20612, loss 5.02906e-07, acc 1\n",
      "2018-10-26T18:17:26.334130: step 20613, loss 2.90372e-06, acc 1\n",
      "2018-10-26T18:17:26.521628: step 20614, loss 4.61175e-05, acc 1\n",
      "2018-10-26T18:17:26.691174: step 20615, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:17:26.873688: step 20616, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:17:27.041240: step 20617, loss 4.20953e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:17:27.216770: step 20618, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:17:27.387316: step 20619, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:17:27.561849: step 20620, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:17:27.735411: step 20621, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:17:27.909919: step 20622, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:17:28.078473: step 20623, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:17:28.253002: step 20624, loss 6.0349e-07, acc 1\n",
      "2018-10-26T18:17:28.427537: step 20625, loss 6.40743e-07, acc 1\n",
      "2018-10-26T18:17:28.605062: step 20626, loss 7.96878e-05, acc 1\n",
      "2018-10-26T18:17:28.800539: step 20627, loss 0.06936, acc 0.984375\n",
      "2018-10-26T18:17:28.995019: step 20628, loss 5.10357e-07, acc 1\n",
      "2018-10-26T18:17:29.195484: step 20629, loss 2.22012e-06, acc 1\n",
      "2018-10-26T18:17:29.369021: step 20630, loss 2.44919e-06, acc 1\n",
      "2018-10-26T18:17:29.543553: step 20631, loss 7.58087e-07, acc 1\n",
      "2018-10-26T18:17:29.712104: step 20632, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:17:29.907582: step 20633, loss 3.78258e-06, acc 1\n",
      "2018-10-26T18:17:30.074136: step 20634, loss 1.17157e-06, acc 1\n",
      "2018-10-26T18:17:30.252660: step 20635, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:17:30.419215: step 20636, loss 2.45867e-07, acc 1\n",
      "2018-10-26T18:17:30.590756: step 20637, loss 0.000535647, acc 1\n",
      "2018-10-26T18:17:30.755316: step 20638, loss 1.82117e-05, acc 1\n",
      "2018-10-26T18:17:30.929850: step 20639, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:17:31.099399: step 20640, loss 3.74856e-05, acc 1\n",
      "2018-10-26T18:17:31.289888: step 20641, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:17:31.455446: step 20642, loss 9.81106e-06, acc 1\n",
      "2018-10-26T18:17:31.627985: step 20643, loss 2.5833e-06, acc 1\n",
      "2018-10-26T18:17:31.799526: step 20644, loss 0.0042575, acc 1\n",
      "2018-10-26T18:17:31.977053: step 20645, loss 0.00027232, acc 1\n",
      "2018-10-26T18:17:32.144605: step 20646, loss 2.96133e-06, acc 1\n",
      "2018-10-26T18:17:32.320135: step 20647, loss 5.78619e-06, acc 1\n",
      "2018-10-26T18:17:32.486690: step 20648, loss 6.14673e-08, acc 1\n",
      "2018-10-26T18:17:32.658232: step 20649, loss 5.64374e-07, acc 1\n",
      "2018-10-26T18:17:32.823789: step 20650, loss 8.43379e-06, acc 1\n",
      "2018-10-26T18:17:33.005305: step 20651, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:17:33.169865: step 20652, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:17:33.342404: step 20653, loss 2.10478e-07, acc 1\n",
      "2018-10-26T18:17:33.514942: step 20654, loss 1.93896e-06, acc 1\n",
      "2018-10-26T18:17:33.699451: step 20655, loss 6.18388e-07, acc 1\n",
      "2018-10-26T18:17:33.870993: step 20656, loss 3.00105e-05, acc 1\n",
      "2018-10-26T18:17:34.050512: step 20657, loss 1.64183e-05, acc 1\n",
      "2018-10-26T18:17:34.216070: step 20658, loss 9.61097e-07, acc 1\n",
      "2018-10-26T18:17:34.388609: step 20659, loss 5.23399e-07, acc 1\n",
      "2018-10-26T18:17:34.554167: step 20660, loss 1.26655e-06, acc 1\n",
      "2018-10-26T18:17:34.729697: step 20661, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:17:34.901238: step 20662, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:17:35.076771: step 20663, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:17:35.241330: step 20664, loss 2.40644e-06, acc 1\n",
      "2018-10-26T18:17:35.416861: step 20665, loss 2.98908e-05, acc 1\n",
      "2018-10-26T18:17:35.583417: step 20666, loss 4.93593e-07, acc 1\n",
      "2018-10-26T18:17:35.754958: step 20667, loss 2.15499e-06, acc 1\n",
      "2018-10-26T18:17:35.923508: step 20668, loss 0.000273074, acc 1\n",
      "2018-10-26T18:17:36.101033: step 20669, loss 3.39258e-05, acc 1\n",
      "2018-10-26T18:17:36.264596: step 20670, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:17:36.445114: step 20671, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:17:36.608677: step 20672, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:17:36.781216: step 20673, loss 1.93714e-07, acc 1\n",
      "2018-10-26T18:17:36.954752: step 20674, loss 1.29822e-06, acc 1\n",
      "2018-10-26T18:17:37.138263: step 20675, loss 2.11086e-05, acc 1\n",
      "2018-10-26T18:17:37.309804: step 20676, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:17:37.482344: step 20677, loss 0, acc 1\n",
      "2018-10-26T18:17:37.648898: step 20678, loss 1.97483e-05, acc 1\n",
      "2018-10-26T18:17:37.828417: step 20679, loss 6.24798e-06, acc 1\n",
      "2018-10-26T18:17:37.995971: step 20680, loss 4.84281e-07, acc 1\n",
      "2018-10-26T18:17:38.172499: step 20681, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:17:38.338057: step 20682, loss 9.79731e-07, acc 1\n",
      "2018-10-26T18:17:38.509598: step 20683, loss 0.000209719, acc 1\n",
      "2018-10-26T18:17:38.681140: step 20684, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:17:38.854676: step 20685, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:17:39.026218: step 20686, loss 0, acc 1\n",
      "2018-10-26T18:17:39.205738: step 20687, loss 4.30269e-07, acc 1\n",
      "2018-10-26T18:17:39.377280: step 20688, loss 0.000155126, acc 1\n",
      "2018-10-26T18:17:39.560790: step 20689, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:17:39.724354: step 20690, loss 0.000242799, acc 1\n",
      "2018-10-26T18:17:39.896891: step 20691, loss 4.1909e-07, acc 1\n",
      "2018-10-26T18:17:40.069431: step 20692, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:17:40.249948: step 20693, loss 1.43045e-06, acc 1\n",
      "2018-10-26T18:17:40.418497: step 20694, loss 9.75239e-05, acc 1\n",
      "2018-10-26T18:17:40.594029: step 20695, loss 1.22185e-06, acc 1\n",
      "2018-10-26T18:17:40.765572: step 20696, loss 3.24271e-05, acc 1\n",
      "2018-10-26T18:17:40.941101: step 20697, loss 2.06752e-07, acc 1\n",
      "2018-10-26T18:17:41.110648: step 20698, loss 0.0276084, acc 0.984375\n",
      "2018-10-26T18:17:41.285182: step 20699, loss 6.17158e-05, acc 1\n",
      "2018-10-26T18:17:41.444756: step 20700, loss 4.30292e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:17:41.896549: step 20700, loss 7.3339, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20700\n",
      "\n",
      "2018-10-26T18:17:42.266691: step 20701, loss 6.23975e-07, acc 1\n",
      "2018-10-26T18:17:42.449204: step 20702, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:17:42.617753: step 20703, loss 0, acc 1\n",
      "2018-10-26T18:17:42.817220: step 20704, loss 5.54939e-05, acc 1\n",
      "2018-10-26T18:17:43.061569: step 20705, loss 1.99663e-06, acc 1\n",
      "2018-10-26T18:17:43.241087: step 20706, loss 5.80792e-05, acc 1\n",
      "2018-10-26T18:17:43.418614: step 20707, loss 2.55181e-07, acc 1\n",
      "2018-10-26T18:17:43.589158: step 20708, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:17:43.766683: step 20709, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:17:43.931244: step 20710, loss 0.000182077, acc 1\n",
      "2018-10-26T18:17:44.114753: step 20711, loss 5.57412e-06, acc 1\n",
      "2018-10-26T18:17:44.279313: step 20712, loss 2.96333e-06, acc 1\n",
      "2018-10-26T18:17:44.456838: step 20713, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:17:44.623395: step 20714, loss 2.17722e-05, acc 1\n",
      "2018-10-26T18:17:44.798925: step 20715, loss 1.71362e-07, acc 1\n",
      "2018-10-26T18:17:44.972463: step 20716, loss 1.31672e-05, acc 1\n",
      "2018-10-26T18:17:45.156968: step 20717, loss 0.000293721, acc 1\n",
      "2018-10-26T18:17:45.320531: step 20718, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:17:45.504042: step 20719, loss 2.26514e-05, acc 1\n",
      "2018-10-26T18:17:45.672591: step 20720, loss 1.40065e-06, acc 1\n",
      "2018-10-26T18:17:45.848122: step 20721, loss 1.2934e-05, acc 1\n",
      "2018-10-26T18:17:46.022656: step 20722, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:17:46.197189: step 20723, loss 4.19092e-07, acc 1\n",
      "2018-10-26T18:17:46.368731: step 20724, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:17:46.545260: step 20725, loss 2.08615e-07, acc 1\n",
      "2018-10-26T18:17:46.713809: step 20726, loss 2.06375e-06, acc 1\n",
      "2018-10-26T18:17:46.897319: step 20727, loss 1.72136e-05, acc 1\n",
      "2018-10-26T18:17:47.067862: step 20728, loss 0, acc 1\n",
      "2018-10-26T18:17:47.246386: step 20729, loss 5.06631e-07, acc 1\n",
      "2018-10-26T18:17:47.409949: step 20730, loss 3.75464e-06, acc 1\n",
      "2018-10-26T18:17:47.587475: step 20731, loss 1.66516e-06, acc 1\n",
      "2018-10-26T18:17:47.754030: step 20732, loss 3.55311e-05, acc 1\n",
      "2018-10-26T18:17:47.939534: step 20733, loss 2.58151e-06, acc 1\n",
      "2018-10-26T18:17:48.109081: step 20734, loss 4.06201e-06, acc 1\n",
      "2018-10-26T18:17:48.288602: step 20735, loss 4.03013e-05, acc 1\n",
      "2018-10-26T18:17:48.460143: step 20736, loss 3.55366e-05, acc 1\n",
      "2018-10-26T18:17:48.635674: step 20737, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:17:48.814198: step 20738, loss 6.72088e-06, acc 1\n",
      "2018-10-26T18:17:48.996710: step 20739, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:17:49.161269: step 20740, loss 3.36189e-06, acc 1\n",
      "2018-10-26T18:17:49.351763: step 20741, loss 5.02581e-05, acc 1\n",
      "2018-10-26T18:17:49.522305: step 20742, loss 3.7625e-07, acc 1\n",
      "2018-10-26T18:17:49.704817: step 20743, loss 3.56769e-05, acc 1\n",
      "2018-10-26T18:17:49.875362: step 20744, loss 4.7497e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:17:50.065853: step 20745, loss 0.00052501, acc 1\n",
      "2018-10-26T18:17:50.237394: step 20746, loss 0.00031384, acc 1\n",
      "2018-10-26T18:17:50.421901: step 20747, loss 5.38296e-07, acc 1\n",
      "2018-10-26T18:17:50.586462: step 20748, loss 0.00379051, acc 1\n",
      "2018-10-26T18:17:50.766979: step 20749, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:17:50.938521: step 20750, loss 3.16647e-07, acc 1\n",
      "2018-10-26T18:17:51.112058: step 20751, loss 2.23866e-05, acc 1\n",
      "2018-10-26T18:17:51.289583: step 20752, loss 0.000295746, acc 1\n",
      "2018-10-26T18:17:51.470101: step 20753, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:17:51.641643: step 20754, loss 2.37284e-06, acc 1\n",
      "2018-10-26T18:17:51.816176: step 20755, loss 0.188117, acc 0.984375\n",
      "2018-10-26T18:17:51.993702: step 20756, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:17:52.163248: step 20757, loss 2.61651e-05, acc 1\n",
      "2018-10-26T18:17:52.331798: step 20758, loss 3.14784e-07, acc 1\n",
      "2018-10-26T18:17:52.506332: step 20759, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:17:52.673884: step 20760, loss 3.01747e-07, acc 1\n",
      "2018-10-26T18:17:52.844429: step 20761, loss 1.65581e-06, acc 1\n",
      "2018-10-26T18:17:53.017965: step 20762, loss 2.17871e-05, acc 1\n",
      "2018-10-26T18:17:53.190505: step 20763, loss 2.89999e-06, acc 1\n",
      "2018-10-26T18:17:53.354067: step 20764, loss 9.85311e-07, acc 1\n",
      "2018-10-26T18:17:53.532590: step 20765, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:17:53.698148: step 20766, loss 0.0444141, acc 0.984375\n",
      "2018-10-26T18:17:53.876671: step 20767, loss 4.84284e-07, acc 1\n",
      "2018-10-26T18:17:54.045221: step 20768, loss 0.000169355, acc 1\n",
      "2018-10-26T18:17:54.217760: step 20769, loss 2.62631e-07, acc 1\n",
      "2018-10-26T18:17:54.385311: step 20770, loss 8.60057e-06, acc 1\n",
      "2018-10-26T18:17:54.562837: step 20771, loss 0.000405689, acc 1\n",
      "2018-10-26T18:17:54.728395: step 20772, loss 1.19315e-05, acc 1\n",
      "2018-10-26T18:17:54.901932: step 20773, loss 0, acc 1\n",
      "2018-10-26T18:17:55.073473: step 20774, loss 7.05062e-06, acc 1\n",
      "2018-10-26T18:17:55.251996: step 20775, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:17:55.424535: step 20776, loss 4.73108e-07, acc 1\n",
      "2018-10-26T18:17:55.597074: step 20777, loss 2.51623e-06, acc 1\n",
      "2018-10-26T18:17:55.768615: step 20778, loss 2.68943e-06, acc 1\n",
      "2018-10-26T18:17:55.945144: step 20779, loss 3.27823e-07, acc 1\n",
      "2018-10-26T18:17:56.114691: step 20780, loss 0.000417158, acc 1\n",
      "2018-10-26T18:17:56.288227: step 20781, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:17:56.456777: step 20782, loss 1.47329e-06, acc 1\n",
      "2018-10-26T18:17:56.630315: step 20783, loss 0.0035532, acc 1\n",
      "2018-10-26T18:17:56.797866: step 20784, loss 1.63682e-05, acc 1\n",
      "2018-10-26T18:17:56.966415: step 20785, loss 1.85886e-06, acc 1\n",
      "2018-10-26T18:17:57.134965: step 20786, loss 1.8067e-06, acc 1\n",
      "2018-10-26T18:17:57.312506: step 20787, loss 0.00446728, acc 1\n",
      "2018-10-26T18:17:57.478049: step 20788, loss 4.71242e-07, acc 1\n",
      "2018-10-26T18:17:57.651585: step 20789, loss 2.59446e-06, acc 1\n",
      "2018-10-26T18:17:57.820134: step 20790, loss 4.3958e-07, acc 1\n",
      "2018-10-26T18:17:57.999655: step 20791, loss 0.000549781, acc 1\n",
      "2018-10-26T18:17:58.167207: step 20792, loss 4.60829e-05, acc 1\n",
      "2018-10-26T18:17:58.340744: step 20793, loss 7.45864e-05, acc 1\n",
      "2018-10-26T18:17:58.507298: step 20794, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:17:58.683827: step 20795, loss 3.61351e-07, acc 1\n",
      "2018-10-26T18:17:58.847390: step 20796, loss 2.44212e-05, acc 1\n",
      "2018-10-26T18:17:59.039877: step 20797, loss 1.28924e-05, acc 1\n",
      "2018-10-26T18:17:59.207428: step 20798, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:17:59.382959: step 20799, loss 6.37502e-06, acc 1\n",
      "2018-10-26T18:17:59.548516: step 20800, loss 1.86265e-09, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:17:59.992330: step 20800, loss 7.43131, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20800\n",
      "\n",
      "2018-10-26T18:18:00.320130: step 20801, loss 0.00114127, acc 1\n",
      "2018-10-26T18:18:00.487683: step 20802, loss 0.00289372, acc 1\n",
      "2018-10-26T18:18:00.664210: step 20803, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:18:00.828771: step 20804, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:18:01.009289: step 20805, loss 3.23328e-06, acc 1\n",
      "2018-10-26T18:18:01.264606: step 20806, loss 8.02524e-05, acc 1\n",
      "2018-10-26T18:18:01.433156: step 20807, loss 6.79851e-07, acc 1\n",
      "2018-10-26T18:18:01.618662: step 20808, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:18:01.785216: step 20809, loss 0.0141792, acc 0.984375\n",
      "2018-10-26T18:18:01.971717: step 20810, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:18:02.138272: step 20811, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:18:02.309813: step 20812, loss 0.00760863, acc 1\n",
      "2018-10-26T18:18:02.478363: step 20813, loss 0.000950376, acc 1\n",
      "2018-10-26T18:18:02.654892: step 20814, loss 0.000105784, acc 1\n",
      "2018-10-26T18:18:02.819452: step 20815, loss 3.5779e-05, acc 1\n",
      "2018-10-26T18:18:02.998973: step 20816, loss 0.000345036, acc 1\n",
      "2018-10-26T18:18:03.177495: step 20817, loss 2.45867e-07, acc 1\n",
      "2018-10-26T18:18:03.360008: step 20818, loss 9.46196e-07, acc 1\n",
      "2018-10-26T18:18:03.526563: step 20819, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:18:03.710072: step 20820, loss 7.82309e-08, acc 1\n",
      "2018-10-26T18:18:03.885604: step 20821, loss 0, acc 1\n",
      "2018-10-26T18:18:04.073103: step 20822, loss 6.96474e-06, acc 1\n",
      "2018-10-26T18:18:04.246639: step 20823, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:18:04.425162: step 20824, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:18:04.592714: step 20825, loss 1.00395e-06, acc 1\n",
      "2018-10-26T18:18:04.765254: step 20826, loss 2.01165e-07, acc 1\n",
      "2018-10-26T18:18:04.934800: step 20827, loss 0.000997759, acc 1\n",
      "2018-10-26T18:18:05.109334: step 20828, loss 4.93595e-07, acc 1\n",
      "2018-10-26T18:18:05.284865: step 20829, loss 4.19419e-06, acc 1\n",
      "2018-10-26T18:18:05.468376: step 20830, loss 0, acc 1\n",
      "2018-10-26T18:18:05.633932: step 20831, loss 1.10844e-05, acc 1\n",
      "2018-10-26T18:18:05.816452: step 20832, loss 1.18274e-06, acc 1\n",
      "2018-10-26T18:18:05.982002: step 20833, loss 3.86083e-06, acc 1\n",
      "2018-10-26T18:18:06.164515: step 20834, loss 3.87425e-07, acc 1\n",
      "2018-10-26T18:18:06.340046: step 20835, loss 5.81041e-06, acc 1\n",
      "2018-10-26T18:18:06.521561: step 20836, loss 2.00035e-06, acc 1\n",
      "2018-10-26T18:18:06.718036: step 20837, loss 6.07211e-07, acc 1\n",
      "2018-10-26T18:18:06.892570: step 20838, loss 2.18101e-06, acc 1\n",
      "2018-10-26T18:18:07.067103: step 20839, loss 0.218389, acc 0.984375\n",
      "2018-10-26T18:18:07.232661: step 20840, loss 1.27857e-05, acc 1\n",
      "2018-10-26T18:18:07.412180: step 20841, loss 0.0242225, acc 0.984375\n",
      "2018-10-26T18:18:07.586715: step 20842, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:18:07.767232: step 20843, loss 0.000477647, acc 1\n",
      "2018-10-26T18:18:07.950742: step 20844, loss 2.74685e-05, acc 1\n",
      "2018-10-26T18:18:08.116300: step 20845, loss 0.0466314, acc 0.984375\n",
      "2018-10-26T18:18:08.298812: step 20846, loss 5.48826e-06, acc 1\n",
      "2018-10-26T18:18:08.469356: step 20847, loss 1.13059e-06, acc 1\n",
      "2018-10-26T18:18:08.654861: step 20848, loss 5.3244e-06, acc 1\n",
      "2018-10-26T18:18:08.821416: step 20849, loss 0.00171133, acc 1\n",
      "2018-10-26T18:18:08.993955: step 20850, loss 5.16573e-08, acc 1\n",
      "2018-10-26T18:18:09.165496: step 20851, loss 2.00154e-05, acc 1\n",
      "2018-10-26T18:18:09.344019: step 20852, loss 7.63684e-08, acc 1\n",
      "2018-10-26T18:18:09.511572: step 20853, loss 1.82532e-06, acc 1\n",
      "2018-10-26T18:18:09.708112: step 20854, loss 5.41607e-06, acc 1\n",
      "2018-10-26T18:18:09.873670: step 20855, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:18:10.064166: step 20856, loss 4.69384e-07, acc 1\n",
      "2018-10-26T18:18:10.234704: step 20857, loss 6.79161e-05, acc 1\n",
      "2018-10-26T18:18:10.413228: step 20858, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:18:10.586764: step 20859, loss 0.000168046, acc 1\n",
      "2018-10-26T18:18:10.773267: step 20860, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:18:10.948797: step 20861, loss 1.84857e-05, acc 1\n",
      "2018-10-26T18:18:11.120339: step 20862, loss 0.00136624, acc 1\n",
      "2018-10-26T18:18:11.305843: step 20863, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:18:11.476388: step 20864, loss 4.88007e-07, acc 1\n",
      "2018-10-26T18:18:11.656906: step 20865, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:18:11.825454: step 20866, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:18:12.002981: step 20867, loss 4.0768e-06, acc 1\n",
      "2018-10-26T18:18:12.171530: step 20868, loss 8.00992e-05, acc 1\n",
      "2018-10-26T18:18:12.349056: step 20869, loss 2.34692e-07, acc 1\n",
      "2018-10-26T18:18:12.519601: step 20870, loss 2.3989e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:18:12.699121: step 20871, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:18:12.868668: step 20872, loss 0, acc 1\n",
      "2018-10-26T18:18:13.045196: step 20873, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:18:13.217734: step 20874, loss 0.0951083, acc 0.984375\n",
      "2018-10-26T18:18:13.398252: step 20875, loss 3.94877e-07, acc 1\n",
      "2018-10-26T18:18:13.608691: step 20876, loss 0.000371281, acc 1\n",
      "2018-10-26T18:18:13.793197: step 20877, loss 1.83462e-06, acc 1\n",
      "2018-10-26T18:18:13.987678: step 20878, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:18:14.186147: step 20879, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:18:14.373647: step 20880, loss 9.36894e-07, acc 1\n",
      "2018-10-26T18:18:14.590068: step 20881, loss 2.5889e-06, acc 1\n",
      "2018-10-26T18:18:14.767593: step 20882, loss 9.68575e-08, acc 1\n",
      "2018-10-26T18:18:14.967060: step 20883, loss 9.26251e-06, acc 1\n",
      "2018-10-26T18:18:15.187473: step 20884, loss 2.43801e-06, acc 1\n",
      "2018-10-26T18:18:15.420849: step 20885, loss 3.25573e-06, acc 1\n",
      "2018-10-26T18:18:15.627296: step 20886, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:18:15.845715: step 20887, loss 7.32011e-07, acc 1\n",
      "2018-10-26T18:18:16.049170: step 20888, loss 7.63126e-06, acc 1\n",
      "2018-10-26T18:18:16.274568: step 20889, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:18:16.447106: step 20890, loss 5.26892e-06, acc 1\n",
      "2018-10-26T18:18:16.678488: step 20891, loss 1.1888e-05, acc 1\n",
      "2018-10-26T18:18:16.888926: step 20892, loss 0.000120217, acc 1\n",
      "2018-10-26T18:18:17.064457: step 20893, loss 0.0254783, acc 0.984375\n",
      "2018-10-26T18:18:17.324762: step 20894, loss 1.68893e-05, acc 1\n",
      "2018-10-26T18:18:17.501290: step 20895, loss 0.000203755, acc 1\n",
      "2018-10-26T18:18:17.707739: step 20896, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:18:17.885263: step 20897, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:18:18.121632: step 20898, loss 0.00110043, acc 1\n",
      "2018-10-26T18:18:18.329078: step 20899, loss 0.000215759, acc 1\n",
      "2018-10-26T18:18:18.520567: step 20900, loss 2.82725e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:18:19.083063: step 20900, loss 7.4638, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-20900\n",
      "\n",
      "2018-10-26T18:18:19.458061: step 20901, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:18:19.635587: step 20902, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:18:19.811118: step 20903, loss 3.94876e-07, acc 1\n",
      "2018-10-26T18:18:20.028537: step 20904, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:18:20.270892: step 20905, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:18:20.465370: step 20906, loss 5.06636e-07, acc 1\n",
      "2018-10-26T18:18:20.687790: step 20907, loss 0, acc 1\n",
      "2018-10-26T18:18:20.864304: step 20908, loss 1.80671e-06, acc 1\n",
      "2018-10-26T18:18:21.039836: step 20909, loss 0, acc 1\n",
      "2018-10-26T18:18:21.205393: step 20910, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:18:21.417825: step 20911, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:18:21.584380: step 20912, loss 2.94297e-07, acc 1\n",
      "2018-10-26T18:18:21.757916: step 20913, loss 1.25729e-05, acc 1\n",
      "2018-10-26T18:18:21.928461: step 20914, loss 0, acc 1\n",
      "2018-10-26T18:18:22.148872: step 20915, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:18:22.361304: step 20916, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:18:22.543817: step 20917, loss 3.91387e-05, acc 1\n",
      "2018-10-26T18:18:22.767220: step 20918, loss 0.000387864, acc 1\n",
      "2018-10-26T18:18:22.933774: step 20919, loss 1.47149e-07, acc 1\n",
      "2018-10-26T18:18:23.170143: step 20920, loss 2.60935e-06, acc 1\n",
      "2018-10-26T18:18:23.343679: step 20921, loss 7.48766e-07, acc 1\n",
      "2018-10-26T18:18:23.555114: step 20922, loss 7.63511e-06, acc 1\n",
      "2018-10-26T18:18:23.728651: step 20923, loss 7.24041e-06, acc 1\n",
      "2018-10-26T18:18:23.910166: step 20924, loss 5.23272e-05, acc 1\n",
      "2018-10-26T18:18:24.129579: step 20925, loss 1.56455e-06, acc 1\n",
      "2018-10-26T18:18:24.296135: step 20926, loss 1.80297e-06, acc 1\n",
      "2018-10-26T18:18:24.470669: step 20927, loss 0.000108499, acc 1\n",
      "2018-10-26T18:18:24.660163: step 20928, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:18:24.850653: step 20929, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:18:25.030174: step 20930, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:18:25.202713: step 20931, loss 1.00395e-06, acc 1\n",
      "2018-10-26T18:18:25.413151: step 20932, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:18:25.582698: step 20933, loss 6.98476e-07, acc 1\n",
      "2018-10-26T18:18:25.758229: step 20934, loss 0.000300075, acc 1\n",
      "2018-10-26T18:18:25.945728: step 20935, loss 2.11588e-06, acc 1\n",
      "2018-10-26T18:18:26.129236: step 20936, loss 0.000107554, acc 1\n",
      "2018-10-26T18:18:26.306763: step 20937, loss 7.63684e-08, acc 1\n",
      "2018-10-26T18:18:26.475312: step 20938, loss 1.24979e-06, acc 1\n",
      "2018-10-26T18:18:26.660816: step 20939, loss 3.96114e-05, acc 1\n",
      "2018-10-26T18:18:26.854299: step 20940, loss 1.52533e-05, acc 1\n",
      "2018-10-26T18:18:27.035815: step 20941, loss 1.33371e-05, acc 1\n",
      "2018-10-26T18:18:27.214338: step 20942, loss 0.00018259, acc 1\n",
      "2018-10-26T18:18:27.378899: step 20943, loss 8.10241e-07, acc 1\n",
      "2018-10-26T18:18:27.558419: step 20944, loss 0, acc 1\n",
      "2018-10-26T18:18:27.729960: step 20945, loss 0, acc 1\n",
      "2018-10-26T18:18:27.911474: step 20946, loss 5.02912e-07, acc 1\n",
      "2018-10-26T18:18:28.082020: step 20947, loss 3.11661e-05, acc 1\n",
      "2018-10-26T18:18:28.260543: step 20948, loss 2.5193e-05, acc 1\n",
      "2018-10-26T18:18:28.430090: step 20949, loss 9.68572e-08, acc 1\n",
      "2018-10-26T18:18:28.606618: step 20950, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:18:28.772176: step 20951, loss 0.00120101, acc 1\n",
      "2018-10-26T18:18:28.951695: step 20952, loss 0, acc 1\n",
      "2018-10-26T18:18:29.116257: step 20953, loss 0, acc 1\n",
      "2018-10-26T18:18:29.291787: step 20954, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:18:29.467319: step 20955, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:18:29.651825: step 20956, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:18:29.814754: step 20957, loss 0, acc 1\n",
      "2018-10-26T18:18:29.998263: step 20958, loss 0, acc 1\n",
      "2018-10-26T18:18:30.164818: step 20959, loss 5.20524e-06, acc 1\n",
      "2018-10-26T18:18:30.343340: step 20960, loss 0.000151322, acc 1\n",
      "2018-10-26T18:18:30.505907: step 20961, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:18:30.688419: step 20962, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:18:30.856968: step 20963, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:18:31.030504: step 20964, loss 2.46599e-06, acc 1\n",
      "2018-10-26T18:18:31.196062: step 20965, loss 0.000136991, acc 1\n",
      "2018-10-26T18:18:31.384576: step 20966, loss 0.000152247, acc 1\n",
      "2018-10-26T18:18:31.548122: step 20967, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:18:31.739610: step 20968, loss 7.7678e-06, acc 1\n",
      "2018-10-26T18:18:31.912150: step 20969, loss 3.54811e-06, acc 1\n",
      "2018-10-26T18:18:32.090671: step 20970, loss 1.14735e-06, acc 1\n",
      "2018-10-26T18:18:32.257226: step 20971, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:18:32.432758: step 20972, loss 0, acc 1\n",
      "2018-10-26T18:18:32.596321: step 20973, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:18:32.774844: step 20974, loss 0, acc 1\n",
      "2018-10-26T18:18:32.941399: step 20975, loss 6.40739e-07, acc 1\n",
      "2018-10-26T18:18:33.123911: step 20976, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:18:33.292461: step 20977, loss 0, acc 1\n",
      "2018-10-26T18:18:33.474974: step 20978, loss 1.30008e-06, acc 1\n",
      "2018-10-26T18:18:33.642526: step 20979, loss 8.0837e-07, acc 1\n",
      "2018-10-26T18:18:33.822045: step 20980, loss 6.00259e-05, acc 1\n",
      "2018-10-26T18:18:33.992596: step 20981, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:18:34.169118: step 20982, loss 9.20935e-06, acc 1\n",
      "2018-10-26T18:18:34.340660: step 20983, loss 2.84982e-07, acc 1\n",
      "2018-10-26T18:18:34.523173: step 20984, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:18:34.698703: step 20985, loss 0.000332146, acc 1\n",
      "2018-10-26T18:18:34.882214: step 20986, loss 1.89429e-05, acc 1\n",
      "2018-10-26T18:18:35.059739: step 20987, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:18:35.225297: step 20988, loss 1.00439e-05, acc 1\n",
      "2018-10-26T18:18:35.406812: step 20989, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:18:35.576359: step 20990, loss 0, acc 1\n",
      "2018-10-26T18:18:35.760866: step 20991, loss 0.00960263, acc 1\n",
      "2018-10-26T18:18:35.931410: step 20992, loss 1.1697e-05, acc 1\n",
      "2018-10-26T18:18:36.110931: step 20993, loss 2.52836e-05, acc 1\n",
      "2018-10-26T18:18:36.279481: step 20994, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:18:36.466980: step 20995, loss 4.11639e-07, acc 1\n",
      "2018-10-26T18:18:36.633534: step 20996, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:18:36.816047: step 20997, loss 0.000928109, acc 1\n",
      "2018-10-26T18:18:36.984597: step 20998, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:18:37.170101: step 20999, loss 2.14751e-06, acc 1\n",
      "2018-10-26T18:18:37.333664: step 21000, loss 1.40391e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:18:37.778475: step 21000, loss 7.4163, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21000\n",
      "\n",
      "2018-10-26T18:18:38.133800: step 21001, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:18:38.311327: step 21002, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:18:38.480874: step 21003, loss 3.22234e-07, acc 1\n",
      "2018-10-26T18:18:38.655407: step 21004, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:18:38.836923: step 21005, loss 2.38869e-05, acc 1\n",
      "2018-10-26T18:18:39.086256: step 21006, loss 0, acc 1\n",
      "2018-10-26T18:18:39.260790: step 21007, loss 8.34457e-07, acc 1\n",
      "2018-10-26T18:18:39.441308: step 21008, loss 2.77532e-07, acc 1\n",
      "2018-10-26T18:18:39.617836: step 21009, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:18:39.783393: step 21010, loss 5.95038e-05, acc 1\n",
      "2018-10-26T18:18:39.957927: step 21011, loss 2.64494e-07, acc 1\n",
      "2018-10-26T18:18:40.129468: step 21012, loss 4.7792e-06, acc 1\n",
      "2018-10-26T18:18:40.302007: step 21013, loss 1.86263e-07, acc 1\n",
      "2018-10-26T18:18:40.469560: step 21014, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:18:40.641102: step 21015, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:18:40.809651: step 21016, loss 3.94876e-07, acc 1\n",
      "2018-10-26T18:18:40.988174: step 21017, loss 0, acc 1\n",
      "2018-10-26T18:18:41.163705: step 21018, loss 3.40861e-07, acc 1\n",
      "2018-10-26T18:18:41.334249: step 21019, loss 0, acc 1\n",
      "2018-10-26T18:18:41.505792: step 21020, loss 0, acc 1\n",
      "2018-10-26T18:18:41.678331: step 21021, loss 0, acc 1\n",
      "2018-10-26T18:18:41.852864: step 21022, loss 0.000132849, acc 1\n",
      "2018-10-26T18:18:42.025403: step 21023, loss 0.000591617, acc 1\n",
      "2018-10-26T18:18:42.192955: step 21024, loss 0, acc 1\n",
      "2018-10-26T18:18:42.371478: step 21025, loss 1.35972e-07, acc 1\n",
      "2018-10-26T18:18:42.538034: step 21026, loss 2.5181e-06, acc 1\n",
      "2018-10-26T18:18:42.707581: step 21027, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:18:42.877128: step 21028, loss 8.38188e-08, acc 1\n",
      "2018-10-26T18:18:43.056648: step 21029, loss 3.3097e-06, acc 1\n",
      "2018-10-26T18:18:43.225197: step 21030, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:18:43.394745: step 21031, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:18:43.559305: step 21032, loss 3.91293e-06, acc 1\n",
      "2018-10-26T18:18:43.742815: step 21033, loss 0.000834951, acc 1\n",
      "2018-10-26T18:18:43.923332: step 21034, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:18:44.095871: step 21035, loss 1.67679e-05, acc 1\n",
      "2018-10-26T18:18:44.271402: step 21036, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:18:44.436959: step 21037, loss 4.52377e-06, acc 1\n",
      "2018-10-26T18:18:44.609524: step 21038, loss 2.15435e-05, acc 1\n",
      "2018-10-26T18:18:44.774059: step 21039, loss 3.11059e-07, acc 1\n",
      "2018-10-26T18:18:44.947596: step 21040, loss 2.89614e-06, acc 1\n",
      "2018-10-26T18:18:45.119137: step 21041, loss 2.4436e-06, acc 1\n",
      "2018-10-26T18:18:45.296664: step 21042, loss 3.26322e-06, acc 1\n",
      "2018-10-26T18:18:45.461223: step 21043, loss 0.000179132, acc 1\n",
      "2018-10-26T18:18:45.635757: step 21044, loss 0.000117881, acc 1\n",
      "2018-10-26T18:18:45.810291: step 21045, loss 1.49835e-05, acc 1\n",
      "2018-10-26T18:18:45.987816: step 21046, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:18:46.159358: step 21047, loss 2.34303e-06, acc 1\n",
      "2018-10-26T18:18:46.339875: step 21048, loss 4.09914e-06, acc 1\n",
      "2018-10-26T18:18:46.511418: step 21049, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:18:46.689941: step 21050, loss 3.96742e-07, acc 1\n",
      "2018-10-26T18:18:46.860484: step 21051, loss 9.97985e-06, acc 1\n",
      "2018-10-26T18:18:47.042000: step 21052, loss 7.61079e-06, acc 1\n",
      "2018-10-26T18:18:47.215541: step 21053, loss 4.49549e-05, acc 1\n",
      "2018-10-26T18:18:47.393061: step 21054, loss 0, acc 1\n",
      "2018-10-26T18:18:47.565601: step 21055, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:18:47.739136: step 21056, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:18:47.908683: step 21057, loss 1.14362e-06, acc 1\n",
      "2018-10-26T18:18:48.084215: step 21058, loss 1.31871e-06, acc 1\n",
      "2018-10-26T18:18:48.254758: step 21059, loss 9.69579e-06, acc 1\n",
      "2018-10-26T18:18:48.437272: step 21060, loss 3.43077e-06, acc 1\n",
      "2018-10-26T18:18:48.602829: step 21061, loss 2.39613e-05, acc 1\n",
      "2018-10-26T18:18:48.782350: step 21062, loss 6.35584e-05, acc 1\n",
      "2018-10-26T18:18:48.953890: step 21063, loss 4.54479e-07, acc 1\n",
      "2018-10-26T18:18:49.136404: step 21064, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:18:49.304954: step 21065, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:18:49.482479: step 21066, loss 0, acc 1\n",
      "2018-10-26T18:18:49.655018: step 21067, loss 8.3819e-08, acc 1\n",
      "2018-10-26T18:18:49.832544: step 21068, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:18:50.000095: step 21069, loss 1.17715e-06, acc 1\n",
      "2018-10-26T18:18:50.177622: step 21070, loss 9.36132e-05, acc 1\n",
      "2018-10-26T18:18:50.351158: step 21071, loss 4.07915e-07, acc 1\n",
      "2018-10-26T18:18:50.524694: step 21072, loss 1.32615e-06, acc 1\n",
      "2018-10-26T18:18:50.691250: step 21073, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:18:50.867778: step 21074, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:18:51.036327: step 21075, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:18:51.217842: step 21076, loss 1.55476e-05, acc 1\n",
      "2018-10-26T18:18:51.383399: step 21077, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:18:51.555939: step 21078, loss 9.47191e-05, acc 1\n",
      "2018-10-26T18:18:51.718505: step 21079, loss 7.82309e-08, acc 1\n",
      "2018-10-26T18:18:51.902015: step 21080, loss 4.82418e-07, acc 1\n",
      "2018-10-26T18:18:52.068570: step 21081, loss 0.0336035, acc 0.984375\n",
      "2018-10-26T18:18:52.250084: step 21082, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:18:52.417637: step 21083, loss 1.38671e-05, acc 1\n",
      "2018-10-26T18:18:52.603141: step 21084, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:18:52.767702: step 21085, loss 0.0034928, acc 1\n",
      "2018-10-26T18:18:52.951211: step 21086, loss 6.63091e-07, acc 1\n",
      "2018-10-26T18:18:53.116768: step 21087, loss 9.94097e-05, acc 1\n",
      "2018-10-26T18:18:53.298283: step 21088, loss 1.79887e-05, acc 1\n",
      "2018-10-26T18:18:53.469825: step 21089, loss 1.12314e-06, acc 1\n",
      "2018-10-26T18:18:53.647350: step 21090, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:18:53.827868: step 21091, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:18:54.006392: step 21092, loss 1.14921e-06, acc 1\n",
      "2018-10-26T18:18:54.185912: step 21093, loss 0.0271189, acc 0.984375\n",
      "2018-10-26T18:18:54.364435: step 21094, loss 2.39537e-05, acc 1\n",
      "2018-10-26T18:18:54.542958: step 21095, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:18:54.708515: step 21096, loss 0, acc 1\n",
      "2018-10-26T18:18:54.888036: step 21097, loss 8.82925e-06, acc 1\n",
      "2018-10-26T18:18:55.049630: step 21098, loss 1.50494e-06, acc 1\n",
      "2018-10-26T18:18:55.226133: step 21099, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:18:55.391690: step 21100, loss 6.92897e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:18:55.849467: step 21100, loss 7.47488, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21100\n",
      "\n",
      "2018-10-26T18:18:56.212831: step 21101, loss 3.83309e-06, acc 1\n",
      "2018-10-26T18:18:56.388362: step 21102, loss 0.0683711, acc 0.984375\n",
      "2018-10-26T18:18:56.568878: step 21103, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:18:56.733438: step 21104, loss 3.25929e-06, acc 1\n",
      "2018-10-26T18:18:56.934901: step 21105, loss 0.000193709, acc 1\n",
      "2018-10-26T18:18:57.172266: step 21106, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:18:57.343808: step 21107, loss 2.79933e-06, acc 1\n",
      "2018-10-26T18:18:57.535297: step 21108, loss 4.32129e-07, acc 1\n",
      "2018-10-26T18:18:57.697861: step 21109, loss 3.19625e-05, acc 1\n",
      "2018-10-26T18:18:57.881372: step 21110, loss 0.000439161, acc 1\n",
      "2018-10-26T18:18:58.052913: step 21111, loss 2.5423e-06, acc 1\n",
      "2018-10-26T18:18:58.233431: step 21112, loss 2.17928e-07, acc 1\n",
      "2018-10-26T18:18:58.396994: step 21113, loss 0.000944596, acc 1\n",
      "2018-10-26T18:18:58.587485: step 21114, loss 0, acc 1\n",
      "2018-10-26T18:18:58.752045: step 21115, loss 3.101e-06, acc 1\n",
      "2018-10-26T18:18:58.944531: step 21116, loss 1.99303e-07, acc 1\n",
      "2018-10-26T18:18:59.113081: step 21117, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:18:59.295593: step 21118, loss 2.05623e-06, acc 1\n",
      "2018-10-26T18:18:59.460154: step 21119, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:18:59.651643: step 21120, loss 3.98604e-07, acc 1\n",
      "2018-10-26T18:18:59.826177: step 21121, loss 3.24565e-05, acc 1\n",
      "2018-10-26T18:19:00.011680: step 21122, loss 1.11758e-07, acc 1\n",
      "2018-10-26T18:19:00.188209: step 21123, loss 0.000425246, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:19:00.370720: step 21124, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:19:00.535281: step 21125, loss 9.94635e-07, acc 1\n",
      "2018-10-26T18:19:00.709814: step 21126, loss 1.2525e-05, acc 1\n",
      "2018-10-26T18:19:00.873378: step 21127, loss 7.81599e-05, acc 1\n",
      "2018-10-26T18:19:01.048909: step 21128, loss 6.24606e-06, acc 1\n",
      "2018-10-26T18:19:01.222445: step 21129, loss 2.35979e-06, acc 1\n",
      "2018-10-26T18:19:01.395981: step 21130, loss 0, acc 1\n",
      "2018-10-26T18:19:01.559544: step 21131, loss 5.78998e-05, acc 1\n",
      "2018-10-26T18:19:01.731086: step 21132, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:19:01.899635: step 21133, loss 2.8669e-05, acc 1\n",
      "2018-10-26T18:19:02.076164: step 21134, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:19:02.243716: step 21135, loss 0.00169243, acc 1\n",
      "2018-10-26T18:19:02.414260: step 21136, loss 8.3819e-08, acc 1\n",
      "2018-10-26T18:19:02.581813: step 21137, loss 3.06218e-05, acc 1\n",
      "2018-10-26T18:19:02.754351: step 21138, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:19:02.921904: step 21139, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:19:03.093446: step 21140, loss 0, acc 1\n",
      "2018-10-26T18:19:03.258006: step 21141, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:19:03.428551: step 21142, loss 3.83983e-05, acc 1\n",
      "2018-10-26T18:19:03.593111: step 21143, loss 0.000433337, acc 1\n",
      "2018-10-26T18:19:03.769639: step 21144, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:19:03.935196: step 21145, loss 0, acc 1\n",
      "2018-10-26T18:19:04.105741: step 21146, loss 0.0010576, acc 1\n",
      "2018-10-26T18:19:04.272296: step 21147, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:19:04.439849: step 21148, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:19:04.607401: step 21149, loss 0.0425529, acc 0.984375\n",
      "2018-10-26T18:19:04.782932: step 21150, loss 2.78155e-08, acc 1\n",
      "2018-10-26T18:19:04.953476: step 21151, loss 0, acc 1\n",
      "2018-10-26T18:19:05.123023: step 21152, loss 1.37644e-06, acc 1\n",
      "2018-10-26T18:19:05.290575: step 21153, loss 7.84164e-07, acc 1\n",
      "2018-10-26T18:19:05.465109: step 21154, loss 9.23314e-05, acc 1\n",
      "2018-10-26T18:19:05.634656: step 21155, loss 3.15315e-06, acc 1\n",
      "2018-10-26T18:19:05.812182: step 21156, loss 1.86209e-05, acc 1\n",
      "2018-10-26T18:19:05.983724: step 21157, loss 5.27971e-06, acc 1\n",
      "2018-10-26T18:19:06.155265: step 21158, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:19:06.324812: step 21159, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:06.498349: step 21160, loss 7.823e-07, acc 1\n",
      "2018-10-26T18:19:06.671885: step 21161, loss 2.1655e-05, acc 1\n",
      "2018-10-26T18:19:06.850408: step 21162, loss 0.00332343, acc 1\n",
      "2018-10-26T18:19:07.016963: step 21163, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:07.192494: step 21164, loss 2.73808e-07, acc 1\n",
      "2018-10-26T18:19:07.364036: step 21165, loss 4.4511e-06, acc 1\n",
      "2018-10-26T18:19:07.541561: step 21166, loss 8.12091e-06, acc 1\n",
      "2018-10-26T18:19:07.713102: step 21167, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:19:07.886640: step 21168, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:19:08.052196: step 21169, loss 1.19499e-05, acc 1\n",
      "2018-10-26T18:19:08.222741: step 21170, loss 0.000112588, acc 1\n",
      "2018-10-26T18:19:08.391291: step 21171, loss 0.000575167, acc 1\n",
      "2018-10-26T18:19:08.569814: step 21172, loss 5.38298e-07, acc 1\n",
      "2018-10-26T18:19:08.736368: step 21173, loss 0.00205065, acc 1\n",
      "2018-10-26T18:19:08.915889: step 21174, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:19:09.084438: step 21175, loss 9.87201e-08, acc 1\n",
      "2018-10-26T18:19:09.267948: step 21176, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:19:09.435501: step 21177, loss 3.35427e-06, acc 1\n",
      "2018-10-26T18:19:09.611032: step 21178, loss 1.73403e-06, acc 1\n",
      "2018-10-26T18:19:09.777589: step 21179, loss 4.44393e-06, acc 1\n",
      "2018-10-26T18:19:09.957108: step 21180, loss 6.44764e-05, acc 1\n",
      "2018-10-26T18:19:10.124660: step 21181, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:19:10.300190: step 21182, loss 4.26115e-06, acc 1\n",
      "2018-10-26T18:19:10.468740: step 21183, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:19:10.646265: step 21184, loss 0, acc 1\n",
      "2018-10-26T18:19:10.810826: step 21185, loss 5.64713e-06, acc 1\n",
      "2018-10-26T18:19:10.989350: step 21186, loss 1.85434e-05, acc 1\n",
      "2018-10-26T18:19:11.155904: step 21187, loss 3.31189e-05, acc 1\n",
      "2018-10-26T18:19:11.342407: step 21188, loss 2.83667e-05, acc 1\n",
      "2018-10-26T18:19:11.514945: step 21189, loss 5.71823e-07, acc 1\n",
      "2018-10-26T18:19:11.686486: step 21190, loss 8.2865e-05, acc 1\n",
      "2018-10-26T18:19:11.863015: step 21191, loss 5.26671e-06, acc 1\n",
      "2018-10-26T18:19:12.057497: step 21192, loss 5.15945e-07, acc 1\n",
      "2018-10-26T18:19:12.219063: step 21193, loss 0.000792766, acc 1\n",
      "2018-10-26T18:19:12.397587: step 21194, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:19:12.567133: step 21195, loss 0.083498, acc 0.984375\n",
      "2018-10-26T18:19:12.744659: step 21196, loss 1.20626e-05, acc 1\n",
      "2018-10-26T18:19:12.910217: step 21197, loss 7.10066e-06, acc 1\n",
      "2018-10-26T18:19:13.091732: step 21198, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:19:13.261278: step 21199, loss 1.72104e-06, acc 1\n",
      "2018-10-26T18:19:13.433818: step 21200, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:19:13.892592: step 21200, loss 7.47357, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21200\n",
      "\n",
      "2018-10-26T18:19:14.298573: step 21201, loss 1.52786e-05, acc 1\n",
      "2018-10-26T18:19:14.475101: step 21202, loss 2.2444e-06, acc 1\n",
      "2018-10-26T18:19:14.646646: step 21203, loss 2.95574e-06, acc 1\n",
      "2018-10-26T18:19:14.824169: step 21204, loss 4.32633e-06, acc 1\n",
      "2018-10-26T18:19:15.060538: step 21205, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:19:15.248038: step 21206, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:15.427579: step 21207, loss 2.12421e-05, acc 1\n",
      "2018-10-26T18:19:15.620042: step 21208, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:19:15.785599: step 21209, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:19:15.963126: step 21210, loss 0, acc 1\n",
      "2018-10-26T18:19:16.129680: step 21211, loss 6.53782e-07, acc 1\n",
      "2018-10-26T18:19:16.313191: step 21212, loss 5.88586e-07, acc 1\n",
      "2018-10-26T18:19:16.489718: step 21213, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:19:16.670236: step 21214, loss 2.47201e-05, acc 1\n",
      "2018-10-26T18:19:16.838785: step 21215, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:19:17.035261: step 21216, loss 1.63346e-06, acc 1\n",
      "2018-10-26T18:19:17.204808: step 21217, loss 2.74258e-05, acc 1\n",
      "2018-10-26T18:19:17.387320: step 21218, loss 4.48892e-07, acc 1\n",
      "2018-10-26T18:19:17.551880: step 21219, loss 2.62632e-07, acc 1\n",
      "2018-10-26T18:19:17.732398: step 21220, loss 1.48606e-05, acc 1\n",
      "2018-10-26T18:19:17.907930: step 21221, loss 2.30967e-07, acc 1\n",
      "2018-10-26T18:19:18.083460: step 21222, loss 1.47146e-06, acc 1\n",
      "2018-10-26T18:19:18.250016: step 21223, loss 0.000513528, acc 1\n",
      "2018-10-26T18:19:18.431530: step 21224, loss 8.14675e-05, acc 1\n",
      "2018-10-26T18:19:18.607062: step 21225, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:19:18.781595: step 21226, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:19:18.953136: step 21227, loss 3.55259e-05, acc 1\n",
      "2018-10-26T18:19:19.134652: step 21228, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:19:19.347084: step 21229, loss 0, acc 1\n",
      "2018-10-26T18:19:19.546552: step 21230, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:19:19.741031: step 21231, loss 5.47152e-06, acc 1\n",
      "2018-10-26T18:19:19.967427: step 21232, loss 0.000119983, acc 1\n",
      "2018-10-26T18:19:20.165897: step 21233, loss 3.7625e-07, acc 1\n",
      "2018-10-26T18:19:20.354393: step 21234, loss 0.000384139, acc 1\n",
      "2018-10-26T18:19:20.571812: step 21235, loss 2.03027e-07, acc 1\n",
      "2018-10-26T18:19:20.791226: step 21236, loss 1.93152e-06, acc 1\n",
      "2018-10-26T18:19:21.010639: step 21237, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:19:21.202127: step 21238, loss 9.82244e-06, acc 1\n",
      "2018-10-26T18:19:21.465427: step 21239, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:19:21.695808: step 21240, loss 0, acc 1\n",
      "2018-10-26T18:19:21.930183: step 21241, loss 8.16202e-05, acc 1\n",
      "2018-10-26T18:19:22.166552: step 21242, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:19:22.353052: step 21243, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:19:22.600392: step 21244, loss 5.79272e-07, acc 1\n",
      "2018-10-26T18:19:22.770937: step 21245, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:19:23.022265: step 21246, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:19:23.234697: step 21247, loss 0.0260612, acc 0.984375\n",
      "2018-10-26T18:19:23.411225: step 21248, loss 5.42021e-07, acc 1\n",
      "2018-10-26T18:19:23.655573: step 21249, loss 7.82309e-08, acc 1\n",
      "2018-10-26T18:19:23.828111: step 21250, loss 5.58793e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:19:24.050517: step 21251, loss 0, acc 1\n",
      "2018-10-26T18:19:24.256966: step 21252, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:19:24.431499: step 21253, loss 3.33779e-06, acc 1\n",
      "2018-10-26T18:19:24.648918: step 21254, loss 3.1106e-07, acc 1\n",
      "2018-10-26T18:19:24.828439: step 21255, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:19:25.035886: step 21256, loss 7.07805e-08, acc 1\n",
      "2018-10-26T18:19:25.248317: step 21257, loss 4.1164e-07, acc 1\n",
      "2018-10-26T18:19:25.422850: step 21258, loss 0, acc 1\n",
      "2018-10-26T18:19:25.641267: step 21259, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:19:25.822782: step 21260, loss 0.000304453, acc 1\n",
      "2018-10-26T18:19:26.002302: step 21261, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:19:26.215732: step 21262, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:26.393257: step 21263, loss 2.38417e-07, acc 1\n",
      "2018-10-26T18:19:26.566794: step 21264, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:26.773243: step 21265, loss 4.30151e-05, acc 1\n",
      "2018-10-26T18:19:26.980689: step 21266, loss 0.000477472, acc 1\n",
      "2018-10-26T18:19:27.161207: step 21267, loss 9.49925e-07, acc 1\n",
      "2018-10-26T18:19:27.381617: step 21268, loss 6.87911e-06, acc 1\n",
      "2018-10-26T18:19:27.552161: step 21269, loss 1.88814e-05, acc 1\n",
      "2018-10-26T18:19:27.764594: step 21270, loss 4.28381e-06, acc 1\n",
      "2018-10-26T18:19:27.934140: step 21271, loss 4.05669e-05, acc 1\n",
      "2018-10-26T18:19:28.143582: step 21272, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:28.370973: step 21273, loss 3.91474e-05, acc 1\n",
      "2018-10-26T18:19:28.550493: step 21274, loss 4.39578e-07, acc 1\n",
      "2018-10-26T18:19:28.750959: step 21275, loss 6.122e-05, acc 1\n",
      "2018-10-26T18:19:28.954415: step 21276, loss 7.15239e-07, acc 1\n",
      "2018-10-26T18:19:29.151886: step 21277, loss 3.56288e-06, acc 1\n",
      "2018-10-26T18:19:29.334399: step 21278, loss 1.26516e-05, acc 1\n",
      "2018-10-26T18:19:29.561792: step 21279, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:19:29.729344: step 21280, loss 1.9557e-06, acc 1\n",
      "2018-10-26T18:19:29.926817: step 21281, loss 0.00226239, acc 1\n",
      "2018-10-26T18:19:30.104342: step 21282, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:19:30.277878: step 21283, loss 2.72299e-06, acc 1\n",
      "2018-10-26T18:19:30.474354: step 21284, loss 1.81832e-05, acc 1\n",
      "2018-10-26T18:19:30.645896: step 21285, loss 1.02133e-05, acc 1\n",
      "2018-10-26T18:19:30.824418: step 21286, loss 8.52185e-05, acc 1\n",
      "2018-10-26T18:19:31.016904: step 21287, loss 6.724e-07, acc 1\n",
      "2018-10-26T18:19:31.198419: step 21288, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:19:31.370958: step 21289, loss 2.51456e-07, acc 1\n",
      "2018-10-26T18:19:31.549481: step 21290, loss 0.000143661, acc 1\n",
      "2018-10-26T18:19:31.747951: step 21291, loss 7.03797e-05, acc 1\n",
      "2018-10-26T18:19:31.929465: step 21292, loss 1.00216e-05, acc 1\n",
      "2018-10-26T18:19:32.108987: step 21293, loss 2.14203e-07, acc 1\n",
      "2018-10-26T18:19:32.276539: step 21294, loss 0.000413852, acc 1\n",
      "2018-10-26T18:19:32.489968: step 21295, loss 2.96691e-06, acc 1\n",
      "2018-10-26T18:19:32.658517: step 21296, loss 0.00428555, acc 1\n",
      "2018-10-26T18:19:32.843025: step 21297, loss 0.000696956, acc 1\n",
      "2018-10-26T18:19:33.014566: step 21298, loss 7.27939e-06, acc 1\n",
      "2018-10-26T18:19:33.191094: step 21299, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:19:33.354658: step 21300, loss 4.20437e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:19:33.803459: step 21300, loss 7.54182, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21300\n",
      "\n",
      "2018-10-26T18:19:34.185211: step 21301, loss 0, acc 1\n",
      "2018-10-26T18:19:34.362737: step 21302, loss 0.00033788, acc 1\n",
      "2018-10-26T18:19:34.540261: step 21303, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:19:34.705820: step 21304, loss 4.35855e-07, acc 1\n",
      "2018-10-26T18:19:34.894316: step 21305, loss 8.88611e-05, acc 1\n",
      "2018-10-26T18:19:35.131682: step 21306, loss 0.000452035, acc 1\n",
      "2018-10-26T18:19:35.306216: step 21307, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:19:35.493727: step 21308, loss 6.72405e-07, acc 1\n",
      "2018-10-26T18:19:35.658276: step 21309, loss 8.93069e-06, acc 1\n",
      "2018-10-26T18:19:35.834803: step 21310, loss 1.11759e-07, acc 1\n",
      "2018-10-26T18:19:36.002356: step 21311, loss 1.64465e-06, acc 1\n",
      "2018-10-26T18:19:36.182873: step 21312, loss 0, acc 1\n",
      "2018-10-26T18:19:36.352421: step 21313, loss 5.88589e-07, acc 1\n",
      "2018-10-26T18:19:36.528948: step 21314, loss 3.08796e-06, acc 1\n",
      "2018-10-26T18:19:36.701488: step 21315, loss 8.96239e-06, acc 1\n",
      "2018-10-26T18:19:36.885995: step 21316, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:19:37.049559: step 21317, loss 2.05623e-06, acc 1\n",
      "2018-10-26T18:19:37.229079: step 21318, loss 1.73226e-07, acc 1\n",
      "2018-10-26T18:19:37.406604: step 21319, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:19:37.594103: step 21320, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:19:37.759661: step 21321, loss 6.42916e-06, acc 1\n",
      "2018-10-26T18:19:37.942173: step 21322, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:19:38.106734: step 21323, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:19:38.281266: step 21324, loss 3.51358e-05, acc 1\n",
      "2018-10-26T18:19:38.452809: step 21325, loss 5.17809e-07, acc 1\n",
      "2018-10-26T18:19:38.633326: step 21326, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:38.805865: step 21327, loss 0.000105214, acc 1\n",
      "2018-10-26T18:19:38.987381: step 21328, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:19:39.162911: step 21329, loss 3.60948e-06, acc 1\n",
      "2018-10-26T18:19:39.328469: step 21330, loss 3.88129e-06, acc 1\n",
      "2018-10-26T18:19:39.512976: step 21331, loss 1.35523e-05, acc 1\n",
      "2018-10-26T18:19:39.688508: step 21332, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:19:39.866033: step 21333, loss 1.29847e-05, acc 1\n",
      "2018-10-26T18:19:40.034582: step 21334, loss 1.76011e-06, acc 1\n",
      "2018-10-26T18:19:40.221085: step 21335, loss 0.000723092, acc 1\n",
      "2018-10-26T18:19:40.386642: step 21336, loss 1.35972e-07, acc 1\n",
      "2018-10-26T18:19:40.574140: step 21337, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:19:40.741693: step 21338, loss 4.97658e-05, acc 1\n",
      "2018-10-26T18:19:40.922210: step 21339, loss 9.91024e-06, acc 1\n",
      "2018-10-26T18:19:41.084777: step 21340, loss 0, acc 1\n",
      "2018-10-26T18:19:41.267289: step 21341, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:41.432846: step 21342, loss 0.00149758, acc 1\n",
      "2018-10-26T18:19:41.613364: step 21343, loss 0.000222421, acc 1\n",
      "2018-10-26T18:19:41.785902: step 21344, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:19:41.973402: step 21345, loss 1.1697e-06, acc 1\n",
      "2018-10-26T18:19:42.141951: step 21346, loss 0.000292565, acc 1\n",
      "2018-10-26T18:19:42.321472: step 21347, loss 1.95379e-06, acc 1\n",
      "2018-10-26T18:19:42.488027: step 21348, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:19:42.670539: step 21349, loss 2.8312e-07, acc 1\n",
      "2018-10-26T18:19:42.836097: step 21350, loss 5.28325e-05, acc 1\n",
      "2018-10-26T18:19:43.020634: step 21351, loss 8.14695e-06, acc 1\n",
      "2018-10-26T18:19:43.184168: step 21352, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:19:43.358701: step 21353, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:19:43.520269: step 21354, loss 0, acc 1\n",
      "2018-10-26T18:19:43.701784: step 21355, loss 9.53664e-05, acc 1\n",
      "2018-10-26T18:19:43.876319: step 21356, loss 6.66113e-05, acc 1\n",
      "2018-10-26T18:19:44.061822: step 21357, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:19:44.228377: step 21358, loss 4.04007e-05, acc 1\n",
      "2018-10-26T18:19:44.413881: step 21359, loss 0, acc 1\n",
      "2018-10-26T18:19:44.577444: step 21360, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:19:44.758960: step 21361, loss 5.06349e-05, acc 1\n",
      "2018-10-26T18:19:44.923521: step 21362, loss 3.65074e-07, acc 1\n",
      "2018-10-26T18:19:45.104038: step 21363, loss 2.40263e-06, acc 1\n",
      "2018-10-26T18:19:45.272587: step 21364, loss 2.70083e-07, acc 1\n",
      "2018-10-26T18:19:45.451110: step 21365, loss 0.00187756, acc 1\n",
      "2018-10-26T18:19:45.613676: step 21366, loss 1.47149e-07, acc 1\n",
      "2018-10-26T18:19:45.791203: step 21367, loss 3.51941e-05, acc 1\n",
      "2018-10-26T18:19:45.964738: step 21368, loss 0.00142253, acc 1\n",
      "2018-10-26T18:19:46.140269: step 21369, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:19:46.308818: step 21370, loss 0.00186327, acc 1\n",
      "2018-10-26T18:19:46.492328: step 21371, loss 7.0591e-05, acc 1\n",
      "2018-10-26T18:19:46.658883: step 21372, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:19:46.834414: step 21373, loss 1.35595e-06, acc 1\n",
      "2018-10-26T18:19:46.998974: step 21374, loss 3.78285e-05, acc 1\n",
      "2018-10-26T18:19:47.181487: step 21375, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:19:47.354026: step 21376, loss 1.12127e-06, acc 1\n",
      "2018-10-26T18:19:47.533546: step 21377, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:19:47.702097: step 21378, loss 9.31322e-09, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:19:47.883611: step 21379, loss 0.00663113, acc 1\n",
      "2018-10-26T18:19:48.052160: step 21380, loss 5.69679e-06, acc 1\n",
      "2018-10-26T18:19:48.232678: step 21381, loss 4.69964e-05, acc 1\n",
      "2018-10-26T18:19:48.396251: step 21382, loss 8.56815e-08, acc 1\n",
      "2018-10-26T18:19:48.572769: step 21383, loss 0.000280879, acc 1\n",
      "2018-10-26T18:19:48.793185: step 21384, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:19:48.991650: step 21385, loss 4.26117e-06, acc 1\n",
      "2018-10-26T18:19:49.200094: step 21386, loss 2.03768e-06, acc 1\n",
      "2018-10-26T18:19:49.378645: step 21387, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:19:49.555145: step 21388, loss 0, acc 1\n",
      "2018-10-26T18:19:49.727683: step 21389, loss 1.6409e-06, acc 1\n",
      "2018-10-26T18:19:49.920170: step 21390, loss 1.721e-06, acc 1\n",
      "2018-10-26T18:19:50.092708: step 21391, loss 1.30194e-06, acc 1\n",
      "2018-10-26T18:19:50.273225: step 21392, loss 0.000105316, acc 1\n",
      "2018-10-26T18:19:50.452746: step 21393, loss 0.00108215, acc 1\n",
      "2018-10-26T18:19:50.623291: step 21394, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:19:50.797825: step 21395, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:19:50.966375: step 21396, loss 6.41701e-05, acc 1\n",
      "2018-10-26T18:19:51.145894: step 21397, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:19:51.322423: step 21398, loss 3.12921e-07, acc 1\n",
      "2018-10-26T18:19:51.501943: step 21399, loss 1.66506e-05, acc 1\n",
      "2018-10-26T18:19:51.679469: step 21400, loss 1.39696e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:19:52.128269: step 21400, loss 7.57662, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21400\n",
      "\n",
      "2018-10-26T18:19:52.493603: step 21401, loss 5.88468e-05, acc 1\n",
      "2018-10-26T18:19:52.661156: step 21402, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:19:52.842675: step 21403, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:53.013215: step 21404, loss 0.00022824, acc 1\n",
      "2018-10-26T18:19:53.212682: step 21405, loss 9.10458e-06, acc 1\n",
      "2018-10-26T18:19:53.447056: step 21406, loss 2.94296e-07, acc 1\n",
      "2018-10-26T18:19:53.618598: step 21407, loss 9.57371e-07, acc 1\n",
      "2018-10-26T18:19:53.804104: step 21408, loss 4.47141e-05, acc 1\n",
      "2018-10-26T18:19:53.973649: step 21409, loss 1.12686e-06, acc 1\n",
      "2018-10-26T18:19:54.145191: step 21410, loss 6.94751e-07, acc 1\n",
      "2018-10-26T18:19:54.311745: step 21411, loss 0, acc 1\n",
      "2018-10-26T18:19:54.489271: step 21412, loss 1.11758e-07, acc 1\n",
      "2018-10-26T18:19:54.660813: step 21413, loss 0, acc 1\n",
      "2018-10-26T18:19:54.833351: step 21414, loss 1.82955e-05, acc 1\n",
      "2018-10-26T18:19:54.999907: step 21415, loss 2.38418e-07, acc 1\n",
      "2018-10-26T18:19:55.184414: step 21416, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:19:55.350969: step 21417, loss 3.19838e-05, acc 1\n",
      "2018-10-26T18:19:55.524505: step 21418, loss 1.64277e-06, acc 1\n",
      "2018-10-26T18:19:55.692057: step 21419, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:55.868586: step 21420, loss 6.87306e-07, acc 1\n",
      "2018-10-26T18:19:56.036138: step 21421, loss 0, acc 1\n",
      "2018-10-26T18:19:56.220644: step 21422, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:19:56.398171: step 21423, loss 2.49042e-05, acc 1\n",
      "2018-10-26T18:19:56.582679: step 21424, loss 0, acc 1\n",
      "2018-10-26T18:19:56.761203: step 21425, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:19:56.972635: step 21426, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:19:57.142183: step 21427, loss 1.08776e-06, acc 1\n",
      "2018-10-26T18:19:57.316717: step 21428, loss 2.19074e-05, acc 1\n",
      "2018-10-26T18:19:57.480280: step 21429, loss 0, acc 1\n",
      "2018-10-26T18:19:57.691714: step 21430, loss 0, acc 1\n",
      "2018-10-26T18:19:57.854280: step 21431, loss 5.06631e-07, acc 1\n",
      "2018-10-26T18:19:58.030808: step 21432, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:19:58.198361: step 21433, loss 6.57427e-05, acc 1\n",
      "2018-10-26T18:19:58.379876: step 21434, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:58.556405: step 21435, loss 4.19547e-05, acc 1\n",
      "2018-10-26T18:19:58.728943: step 21436, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:19:58.902480: step 21437, loss 1.9744e-07, acc 1\n",
      "2018-10-26T18:19:59.069035: step 21438, loss 0.0031338, acc 1\n",
      "2018-10-26T18:19:59.247558: step 21439, loss 7.99055e-07, acc 1\n",
      "2018-10-26T18:19:59.417105: step 21440, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:19:59.596625: step 21441, loss 8.01764e-05, acc 1\n",
      "2018-10-26T18:19:59.762183: step 21442, loss 3.57223e-06, acc 1\n",
      "2018-10-26T18:19:59.939716: step 21443, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:20:00.116236: step 21444, loss 0, acc 1\n",
      "2018-10-26T18:20:00.289773: step 21445, loss 1.53289e-06, acc 1\n",
      "2018-10-26T18:20:00.456328: step 21446, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:20:00.635849: step 21447, loss 6.01728e-06, acc 1\n",
      "2018-10-26T18:20:00.805395: step 21448, loss 1.17752e-05, acc 1\n",
      "2018-10-26T18:20:00.982922: step 21449, loss 0, acc 1\n",
      "2018-10-26T18:20:01.155460: step 21450, loss 4.7955e-06, acc 1\n",
      "2018-10-26T18:20:01.339967: step 21451, loss 0, acc 1\n",
      "2018-10-26T18:20:01.504527: step 21452, loss 6.29562e-07, acc 1\n",
      "2018-10-26T18:20:01.689035: step 21453, loss 3.03608e-07, acc 1\n",
      "2018-10-26T18:20:01.867558: step 21454, loss 9.12695e-08, acc 1\n",
      "2018-10-26T18:20:02.044086: step 21455, loss 0.0882133, acc 0.984375\n",
      "2018-10-26T18:20:02.214631: step 21456, loss 2.19527e-05, acc 1\n",
      "2018-10-26T18:20:02.391159: step 21457, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:20:02.565699: step 21458, loss 2.1568e-06, acc 1\n",
      "2018-10-26T18:20:02.740227: step 21459, loss 0, acc 1\n",
      "2018-10-26T18:20:02.911767: step 21460, loss 4.25758e-06, acc 1\n",
      "2018-10-26T18:20:03.094280: step 21461, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:20:03.266819: step 21462, loss 1.22931e-06, acc 1\n",
      "2018-10-26T18:20:03.442350: step 21463, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:20:03.607908: step 21464, loss 0.000109408, acc 1\n",
      "2018-10-26T18:20:03.795407: step 21465, loss 7.63154e-05, acc 1\n",
      "2018-10-26T18:20:03.963956: step 21466, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:20:04.138490: step 21467, loss 0, acc 1\n",
      "2018-10-26T18:20:04.312027: step 21468, loss 5.91867e-05, acc 1\n",
      "2018-10-26T18:20:04.491547: step 21469, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:20:04.667078: step 21470, loss 1.41561e-07, acc 1\n",
      "2018-10-26T18:20:04.845600: step 21471, loss 0.0115322, acc 0.984375\n",
      "2018-10-26T18:20:05.022128: step 21472, loss 7.03992e-06, acc 1\n",
      "2018-10-26T18:20:05.191676: step 21473, loss 0, acc 1\n",
      "2018-10-26T18:20:05.370201: step 21474, loss 0.000131442, acc 1\n",
      "2018-10-26T18:20:05.537751: step 21475, loss 0.0525872, acc 0.984375\n",
      "2018-10-26T18:20:05.721261: step 21476, loss 4.02116e-06, acc 1\n",
      "2018-10-26T18:20:05.894797: step 21477, loss 6.94753e-07, acc 1\n",
      "2018-10-26T18:20:06.074318: step 21478, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:20:06.240873: step 21479, loss 2.18846e-06, acc 1\n",
      "2018-10-26T18:20:06.423385: step 21480, loss 0, acc 1\n",
      "2018-10-26T18:20:06.594926: step 21481, loss 1.27773e-06, acc 1\n",
      "2018-10-26T18:20:06.779434: step 21482, loss 1.38948e-05, acc 1\n",
      "2018-10-26T18:20:06.947983: step 21483, loss 1.29099e-05, acc 1\n",
      "2018-10-26T18:20:07.126506: step 21484, loss 4.37715e-07, acc 1\n",
      "2018-10-26T18:20:07.299045: step 21485, loss 6.83578e-07, acc 1\n",
      "2018-10-26T18:20:07.477568: step 21486, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:20:07.645121: step 21487, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:20:07.828631: step 21488, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:20:07.998178: step 21489, loss 1.59518e-05, acc 1\n",
      "2018-10-26T18:20:08.179693: step 21490, loss 1.01511e-06, acc 1\n",
      "2018-10-26T18:20:08.345250: step 21491, loss 0, acc 1\n",
      "2018-10-26T18:20:08.525768: step 21492, loss 5.05824e-05, acc 1\n",
      "2018-10-26T18:20:08.692323: step 21493, loss 0.000157797, acc 1\n",
      "2018-10-26T18:20:08.867854: step 21494, loss 2.23228e-05, acc 1\n",
      "2018-10-26T18:20:09.040393: step 21495, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:20:09.215924: step 21496, loss 7.46907e-07, acc 1\n",
      "2018-10-26T18:20:09.392452: step 21497, loss 2.19791e-07, acc 1\n",
      "2018-10-26T18:20:09.566985: step 21498, loss 2.66857e-05, acc 1\n",
      "2018-10-26T18:20:09.749498: step 21499, loss 5.35474e-05, acc 1\n",
      "2018-10-26T18:20:09.918047: step 21500, loss 1.17346e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:20:10.377819: step 21500, loss 7.80575, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21500\n",
      "\n",
      "2018-10-26T18:20:10.787471: step 21501, loss 8.43367e-05, acc 1\n",
      "2018-10-26T18:20:10.962005: step 21502, loss 0.0416256, acc 0.984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:20:11.141525: step 21503, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:20:11.323040: step 21504, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:20:11.506551: step 21505, loss 0.00518805, acc 1\n",
      "2018-10-26T18:20:11.683079: step 21506, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:20:11.848636: step 21507, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:20:12.027159: step 21508, loss 0.000632372, acc 1\n",
      "2018-10-26T18:20:12.191719: step 21509, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:20:12.360270: step 21510, loss 1.20322e-06, acc 1\n",
      "2018-10-26T18:20:12.526824: step 21511, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:20:12.700360: step 21512, loss 1.51053e-06, acc 1\n",
      "2018-10-26T18:20:12.867913: step 21513, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:20:13.045439: step 21514, loss 8.60523e-07, acc 1\n",
      "2018-10-26T18:20:13.210996: step 21515, loss 0, acc 1\n",
      "2018-10-26T18:20:13.396500: step 21516, loss 6.81281e-06, acc 1\n",
      "2018-10-26T18:20:13.568042: step 21517, loss 2.77883e-06, acc 1\n",
      "2018-10-26T18:20:13.739584: step 21518, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:20:13.906139: step 21519, loss 0, acc 1\n",
      "2018-10-26T18:20:14.085659: step 21520, loss 1.23227e-05, acc 1\n",
      "2018-10-26T18:20:14.256204: step 21521, loss 5.58784e-07, acc 1\n",
      "2018-10-26T18:20:14.428742: step 21522, loss 1.56461e-07, acc 1\n",
      "2018-10-26T18:20:14.597292: step 21523, loss 7.00571e-06, acc 1\n",
      "2018-10-26T18:20:14.773820: step 21524, loss 0, acc 1\n",
      "2018-10-26T18:20:14.948354: step 21525, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:20:15.122888: step 21526, loss 0.0185018, acc 0.984375\n",
      "2018-10-26T18:20:15.293432: step 21527, loss 2.99672e-06, acc 1\n",
      "2018-10-26T18:20:15.464973: step 21528, loss 0, acc 1\n",
      "2018-10-26T18:20:15.646489: step 21529, loss 0.000193008, acc 1\n",
      "2018-10-26T18:20:15.811049: step 21530, loss 1.99644e-05, acc 1\n",
      "2018-10-26T18:20:15.984586: step 21531, loss 0, acc 1\n",
      "2018-10-26T18:20:16.153135: step 21532, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:20:16.337642: step 21533, loss 1.25418e-05, acc 1\n",
      "2018-10-26T18:20:16.510181: step 21534, loss 0.00815748, acc 1\n",
      "2018-10-26T18:20:16.685712: step 21535, loss 1.05795e-06, acc 1\n",
      "2018-10-26T18:20:16.850272: step 21536, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:20:17.023809: step 21537, loss 6.89219e-06, acc 1\n",
      "2018-10-26T18:20:17.197345: step 21538, loss 4.24657e-05, acc 1\n",
      "2018-10-26T18:20:17.382849: step 21539, loss 1.72293e-05, acc 1\n",
      "2018-10-26T18:20:17.560375: step 21540, loss 2.48644e-06, acc 1\n",
      "2018-10-26T18:20:17.724935: step 21541, loss 0.000161938, acc 1\n",
      "2018-10-26T18:20:17.904456: step 21542, loss 2.75205e-05, acc 1\n",
      "2018-10-26T18:20:18.074003: step 21543, loss 2.27096e-05, acc 1\n",
      "2018-10-26T18:20:18.250531: step 21544, loss 4.80556e-07, acc 1\n",
      "2018-10-26T18:20:18.418083: step 21545, loss 5.43053e-06, acc 1\n",
      "2018-10-26T18:20:18.597603: step 21546, loss 2.21084e-06, acc 1\n",
      "2018-10-26T18:20:18.767151: step 21547, loss 0, acc 1\n",
      "2018-10-26T18:20:18.945674: step 21548, loss 2.52228e-05, acc 1\n",
      "2018-10-26T18:20:19.120209: step 21549, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:20:19.298730: step 21550, loss 0, acc 1\n",
      "2018-10-26T18:20:19.471269: step 21551, loss 0.000194223, acc 1\n",
      "2018-10-26T18:20:19.640817: step 21552, loss 0, acc 1\n",
      "2018-10-26T18:20:19.826321: step 21553, loss 4.91734e-07, acc 1\n",
      "2018-10-26T18:20:19.998859: step 21554, loss 2.99883e-07, acc 1\n",
      "2018-10-26T18:20:20.172396: step 21555, loss 1.76042e-05, acc 1\n",
      "2018-10-26T18:20:20.340946: step 21556, loss 7.36063e-05, acc 1\n",
      "2018-10-26T18:20:20.516477: step 21557, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:20:20.695997: step 21558, loss 2.25185e-06, acc 1\n",
      "2018-10-26T18:20:20.894468: step 21559, loss 4.50891e-06, acc 1\n",
      "2018-10-26T18:20:21.064013: step 21560, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:20:21.238548: step 21561, loss 0.00374587, acc 1\n",
      "2018-10-26T18:20:21.404105: step 21562, loss 4.87886e-05, acc 1\n",
      "2018-10-26T18:20:21.577642: step 21563, loss 5.63721e-06, acc 1\n",
      "2018-10-26T18:20:21.766138: step 21564, loss 4.87566e-06, acc 1\n",
      "2018-10-26T18:20:21.941669: step 21565, loss 4.39578e-07, acc 1\n",
      "2018-10-26T18:20:22.128170: step 21566, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:20:22.295724: step 21567, loss 8.89673e-05, acc 1\n",
      "2018-10-26T18:20:22.474246: step 21568, loss 2.0489e-07, acc 1\n",
      "2018-10-26T18:20:22.646785: step 21569, loss 0, acc 1\n",
      "2018-10-26T18:20:22.818327: step 21570, loss 2.91809e-05, acc 1\n",
      "2018-10-26T18:20:22.987873: step 21571, loss 0, acc 1\n",
      "2018-10-26T18:20:23.167394: step 21572, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:20:23.338936: step 21573, loss 3.4086e-07, acc 1\n",
      "2018-10-26T18:20:23.549374: step 21574, loss 2.88708e-07, acc 1\n",
      "2018-10-26T18:20:23.729891: step 21575, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:20:23.906419: step 21576, loss 7.3014e-07, acc 1\n",
      "2018-10-26T18:20:24.085940: step 21577, loss 9.27509e-06, acc 1\n",
      "2018-10-26T18:20:24.254490: step 21578, loss 2.7618e-05, acc 1\n",
      "2018-10-26T18:20:24.428026: step 21579, loss 2.58239e-05, acc 1\n",
      "2018-10-26T18:20:24.601562: step 21580, loss 3.11386e-05, acc 1\n",
      "2018-10-26T18:20:24.791057: step 21581, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:20:24.961600: step 21582, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:20:25.182011: step 21583, loss 3.26862e-06, acc 1\n",
      "2018-10-26T18:20:25.391451: step 21584, loss 6.35228e-06, acc 1\n",
      "2018-10-26T18:20:25.565985: step 21585, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:20:25.781410: step 21586, loss 1.86264e-07, acc 1\n",
      "2018-10-26T18:20:25.973896: step 21587, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:20:26.155410: step 21588, loss 8.88044e-06, acc 1\n",
      "2018-10-26T18:20:26.395768: step 21589, loss 0.000676559, acc 1\n",
      "2018-10-26T18:20:26.605209: step 21590, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:20:26.835593: step 21591, loss 3.83701e-07, acc 1\n",
      "2018-10-26T18:20:27.059994: step 21592, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:20:27.241509: step 21593, loss 8.79455e-06, acc 1\n",
      "2018-10-26T18:20:27.484858: step 21594, loss 0.000166094, acc 1\n",
      "2018-10-26T18:20:27.679338: step 21595, loss 0, acc 1\n",
      "2018-10-26T18:20:27.899750: step 21596, loss 1.3038e-06, acc 1\n",
      "2018-10-26T18:20:28.105201: step 21597, loss 8.20442e-05, acc 1\n",
      "2018-10-26T18:20:28.280732: step 21598, loss 3.66697e-05, acc 1\n",
      "2018-10-26T18:20:28.504135: step 21599, loss 6.71154e-06, acc 1\n",
      "2018-10-26T18:20:28.687645: step 21600, loss 4.56969e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:20:29.228200: step 21600, loss 7.87552, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21600\n",
      "\n",
      "2018-10-26T18:20:29.620640: step 21601, loss 5.6065e-07, acc 1\n",
      "2018-10-26T18:20:29.835067: step 21602, loss 2.43252e-06, acc 1\n",
      "2018-10-26T18:20:30.046502: step 21603, loss 1.36526e-06, acc 1\n",
      "2018-10-26T18:20:30.252951: step 21604, loss 1.45796e-05, acc 1\n",
      "2018-10-26T18:20:30.547165: step 21605, loss 1.04305e-06, acc 1\n",
      "2018-10-26T18:20:30.761591: step 21606, loss 1.78656e-05, acc 1\n",
      "2018-10-26T18:20:30.953080: step 21607, loss 0, acc 1\n",
      "2018-10-26T18:20:31.181469: step 21608, loss 0.0106503, acc 1\n",
      "2018-10-26T18:20:31.365977: step 21609, loss 1.22557e-06, acc 1\n",
      "2018-10-26T18:20:31.575417: step 21610, loss 2.34966e-05, acc 1\n",
      "2018-10-26T18:20:31.766907: step 21611, loss 9.51578e-06, acc 1\n",
      "2018-10-26T18:20:31.949418: step 21612, loss 5.49471e-07, acc 1\n",
      "2018-10-26T18:20:32.153872: step 21613, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:20:32.382261: step 21614, loss 6.59054e-06, acc 1\n",
      "2018-10-26T18:20:32.568764: step 21615, loss 0.0684704, acc 0.984375\n",
      "2018-10-26T18:20:32.789174: step 21616, loss 1.71362e-07, acc 1\n",
      "2018-10-26T18:20:32.958722: step 21617, loss 1.30194e-06, acc 1\n",
      "2018-10-26T18:20:33.180129: step 21618, loss 1.27873e-05, acc 1\n",
      "2018-10-26T18:20:33.349677: step 21619, loss 1.63913e-07, acc 1\n",
      "2018-10-26T18:20:33.536179: step 21620, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:20:33.772547: step 21621, loss 0, acc 1\n",
      "2018-10-26T18:20:33.968024: step 21622, loss 0.00106855, acc 1\n",
      "2018-10-26T18:20:34.185443: step 21623, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:20:34.422809: step 21624, loss 5.99766e-07, acc 1\n",
      "2018-10-26T18:20:34.601332: step 21625, loss 4.89869e-07, acc 1\n",
      "2018-10-26T18:20:34.806783: step 21626, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:20:35.003259: step 21627, loss 1.07551e-05, acc 1\n",
      "2018-10-26T18:20:35.212699: step 21628, loss 0.000104818, acc 1\n",
      "2018-10-26T18:20:35.396209: step 21629, loss 7.26415e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:20:35.577723: step 21630, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:20:35.783176: step 21631, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:20:35.956711: step 21632, loss 9.70409e-07, acc 1\n",
      "2018-10-26T18:20:36.135234: step 21633, loss 5.71914e-06, acc 1\n",
      "2018-10-26T18:20:36.335699: step 21634, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:20:36.511230: step 21635, loss 0.000128031, acc 1\n",
      "2018-10-26T18:20:36.686761: step 21636, loss 0, acc 1\n",
      "2018-10-26T18:20:36.877251: step 21637, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:20:37.064751: step 21638, loss 0, acc 1\n",
      "2018-10-26T18:20:37.249259: step 21639, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:20:37.420799: step 21640, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:20:37.629242: step 21641, loss 4.99971e-05, acc 1\n",
      "2018-10-26T18:20:37.797792: step 21642, loss 0.00043061, acc 1\n",
      "2018-10-26T18:20:37.975318: step 21643, loss 8.03009e-06, acc 1\n",
      "2018-10-26T18:20:38.143867: step 21644, loss 4.74967e-07, acc 1\n",
      "2018-10-26T18:20:38.340342: step 21645, loss 2.52001e-06, acc 1\n",
      "2018-10-26T18:20:38.509890: step 21646, loss 0.185155, acc 0.984375\n",
      "2018-10-26T18:20:38.691405: step 21647, loss 5.16241e-06, acc 1\n",
      "2018-10-26T18:20:38.869928: step 21648, loss 1.64182e-05, acc 1\n",
      "2018-10-26T18:20:39.046456: step 21649, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:20:39.220989: step 21650, loss 0.000792868, acc 1\n",
      "2018-10-26T18:20:39.396521: step 21651, loss 2.19032e-06, acc 1\n",
      "2018-10-26T18:20:39.576041: step 21652, loss 2.4138e-06, acc 1\n",
      "2018-10-26T18:20:39.746586: step 21653, loss 1.22932e-06, acc 1\n",
      "2018-10-26T18:20:39.923113: step 21654, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:20:40.086676: step 21655, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:20:40.271183: step 21656, loss 4.34123e-06, acc 1\n",
      "2018-10-26T18:20:40.435744: step 21657, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:20:40.618256: step 21658, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:20:40.787803: step 21659, loss 2.0882e-05, acc 1\n",
      "2018-10-26T18:20:40.961340: step 21660, loss 5.20424e-05, acc 1\n",
      "2018-10-26T18:20:41.126898: step 21661, loss 3.9336e-06, acc 1\n",
      "2018-10-26T18:20:41.302428: step 21662, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:20:41.469981: step 21663, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:20:41.646509: step 21664, loss 8.52831e-05, acc 1\n",
      "2018-10-26T18:20:41.813064: step 21665, loss 9.77858e-07, acc 1\n",
      "2018-10-26T18:20:41.996574: step 21666, loss 1.52538e-05, acc 1\n",
      "2018-10-26T18:20:42.168115: step 21667, loss 7.09519e-05, acc 1\n",
      "2018-10-26T18:20:42.341652: step 21668, loss 1.65208e-06, acc 1\n",
      "2018-10-26T18:20:42.510200: step 21669, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:20:42.686730: step 21670, loss 4.11639e-07, acc 1\n",
      "2018-10-26T18:20:42.856277: step 21671, loss 7.43213e-05, acc 1\n",
      "2018-10-26T18:20:43.038789: step 21672, loss 0, acc 1\n",
      "2018-10-26T18:20:43.207338: step 21673, loss 0, acc 1\n",
      "2018-10-26T18:20:43.386859: step 21674, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:20:43.558400: step 21675, loss 2.1177e-06, acc 1\n",
      "2018-10-26T18:20:43.739916: step 21676, loss 4.44417e-05, acc 1\n",
      "2018-10-26T18:20:43.916444: step 21677, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:20:44.094967: step 21678, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:20:44.277479: step 21679, loss 0.000536583, acc 1\n",
      "2018-10-26T18:20:44.440044: step 21680, loss 2.46812e-05, acc 1\n",
      "2018-10-26T18:20:44.616574: step 21681, loss 6.96292e-06, acc 1\n",
      "2018-10-26T18:20:44.782131: step 21682, loss 0.00405646, acc 1\n",
      "2018-10-26T18:20:44.966638: step 21683, loss 0.14725, acc 0.984375\n",
      "2018-10-26T18:20:45.133194: step 21684, loss 1.24794e-06, acc 1\n",
      "2018-10-26T18:20:45.316704: step 21685, loss 2.78275e-05, acc 1\n",
      "2018-10-26T18:20:45.484255: step 21686, loss 4.44929e-05, acc 1\n",
      "2018-10-26T18:20:45.664784: step 21687, loss 8.51206e-07, acc 1\n",
      "2018-10-26T18:20:45.837312: step 21688, loss 3.75859e-06, acc 1\n",
      "2018-10-26T18:20:46.010848: step 21689, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:20:46.187377: step 21690, loss 7.63666e-07, acc 1\n",
      "2018-10-26T18:20:46.365899: step 21691, loss 4.6752e-07, acc 1\n",
      "2018-10-26T18:20:46.539436: step 21692, loss 0, acc 1\n",
      "2018-10-26T18:20:46.716961: step 21693, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:20:46.888503: step 21694, loss 0.00290401, acc 1\n",
      "2018-10-26T18:20:47.071015: step 21695, loss 2.27231e-06, acc 1\n",
      "2018-10-26T18:20:47.239565: step 21696, loss 5.03578e-06, acc 1\n",
      "2018-10-26T18:20:47.412104: step 21697, loss 1.10416e-05, acc 1\n",
      "2018-10-26T18:20:47.579656: step 21698, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:20:47.760175: step 21699, loss 5.78063e-05, acc 1\n",
      "2018-10-26T18:20:47.931715: step 21700, loss 7.05927e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:20:48.384506: step 21700, loss 7.59609, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21700\n",
      "\n",
      "2018-10-26T18:20:48.726610: step 21701, loss 1.21639e-05, acc 1\n",
      "2018-10-26T18:20:48.900147: step 21702, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:20:49.088643: step 21703, loss 2.53836e-05, acc 1\n",
      "2018-10-26T18:20:49.265172: step 21704, loss 2.43314e-05, acc 1\n",
      "2018-10-26T18:20:49.465635: step 21705, loss 4.44737e-06, acc 1\n",
      "2018-10-26T18:20:49.712974: step 21706, loss 1.01325e-06, acc 1\n",
      "2018-10-26T18:20:49.885513: step 21707, loss 8.22953e-06, acc 1\n",
      "2018-10-26T18:20:50.068025: step 21708, loss 0, acc 1\n",
      "2018-10-26T18:20:50.243557: step 21709, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:20:50.419087: step 21710, loss 3.59488e-07, acc 1\n",
      "2018-10-26T18:20:50.600602: step 21711, loss 3.73744e-05, acc 1\n",
      "2018-10-26T18:20:50.767157: step 21712, loss 2.10465e-06, acc 1\n",
      "2018-10-26T18:20:50.942689: step 21713, loss 0.00060248, acc 1\n",
      "2018-10-26T18:20:51.118220: step 21714, loss 1.37579e-05, acc 1\n",
      "2018-10-26T18:20:51.295746: step 21715, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:20:51.461304: step 21716, loss 0.000335647, acc 1\n",
      "2018-10-26T18:20:51.644812: step 21717, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:20:51.817352: step 21718, loss 0.00168915, acc 1\n",
      "2018-10-26T18:20:51.995875: step 21719, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:20:52.168414: step 21720, loss 1.05984e-05, acc 1\n",
      "2018-10-26T18:20:52.346937: step 21721, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:20:52.510500: step 21722, loss 3.48311e-07, acc 1\n",
      "2018-10-26T18:20:52.687028: step 21723, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:20:52.851589: step 21724, loss 5.57207e-06, acc 1\n",
      "2018-10-26T18:20:53.028116: step 21725, loss 3.24098e-07, acc 1\n",
      "2018-10-26T18:20:53.199659: step 21726, loss 1.20039e-05, acc 1\n",
      "2018-10-26T18:20:53.394139: step 21727, loss 8.32134e-05, acc 1\n",
      "2018-10-26T18:20:53.561692: step 21728, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:20:53.741212: step 21729, loss 2.08615e-07, acc 1\n",
      "2018-10-26T18:20:53.915745: step 21730, loss 0.00332706, acc 1\n",
      "2018-10-26T18:20:54.102247: step 21731, loss 1.20509e-06, acc 1\n",
      "2018-10-26T18:20:54.270797: step 21732, loss 1.86998e-06, acc 1\n",
      "2018-10-26T18:20:54.446328: step 21733, loss 0.00592124, acc 1\n",
      "2018-10-26T18:20:54.612883: step 21734, loss 2.07063e-05, acc 1\n",
      "2018-10-26T18:20:54.789411: step 21735, loss 2.21654e-07, acc 1\n",
      "2018-10-26T18:20:54.952974: step 21736, loss 3.7625e-07, acc 1\n",
      "2018-10-26T18:20:55.137482: step 21737, loss 0.000392424, acc 1\n",
      "2018-10-26T18:20:55.302041: step 21738, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:20:55.489540: step 21739, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:20:55.655098: step 21740, loss 8.51069e-05, acc 1\n",
      "2018-10-26T18:20:55.833621: step 21741, loss 8.73704e-05, acc 1\n",
      "2018-10-26T18:20:55.995189: step 21742, loss 3.04151e-05, acc 1\n",
      "2018-10-26T18:20:56.176704: step 21743, loss 3.63191e-06, acc 1\n",
      "2018-10-26T18:20:56.349243: step 21744, loss 1.43603e-06, acc 1\n",
      "2018-10-26T18:20:56.532753: step 21745, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:20:56.698311: step 21746, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:20:56.882818: step 21747, loss 5.37718e-05, acc 1\n",
      "2018-10-26T18:20:57.052365: step 21748, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:20:57.229890: step 21749, loss 2.45783e-05, acc 1\n",
      "2018-10-26T18:20:57.393453: step 21750, loss 1.283e-05, acc 1\n",
      "2018-10-26T18:20:57.580952: step 21751, loss 2.53319e-07, acc 1\n",
      "2018-10-26T18:20:57.746511: step 21752, loss 3.6783e-06, acc 1\n",
      "2018-10-26T18:20:57.931018: step 21753, loss 1.70982e-06, acc 1\n",
      "2018-10-26T18:20:58.106548: step 21754, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:20:58.291055: step 21755, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:20:58.471574: step 21756, loss 1.47027e-05, acc 1\n",
      "2018-10-26T18:20:58.650097: step 21757, loss 2.42142e-07, acc 1\n",
      "2018-10-26T18:20:58.815653: step 21758, loss 7.45056e-08, acc 1\n",
      "2018-10-26T18:20:58.993180: step 21759, loss 1.11198e-06, acc 1\n",
      "2018-10-26T18:20:59.157740: step 21760, loss 2.40031e-05, acc 1\n",
      "2018-10-26T18:20:59.329283: step 21761, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:20:59.498831: step 21762, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:20:59.679346: step 21763, loss 1.77819e-05, acc 1\n",
      "2018-10-26T18:20:59.856871: step 21764, loss 8.02068e-05, acc 1\n",
      "2018-10-26T18:21:00.038387: step 21765, loss 5.68308e-05, acc 1\n",
      "2018-10-26T18:21:00.208931: step 21766, loss 6.57476e-05, acc 1\n",
      "2018-10-26T18:21:00.377481: step 21767, loss 0.00129061, acc 1\n",
      "2018-10-26T18:21:00.560991: step 21768, loss 0.000185197, acc 1\n",
      "2018-10-26T18:21:00.728543: step 21769, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:21:00.904073: step 21770, loss 2.08615e-07, acc 1\n",
      "2018-10-26T18:21:01.077610: step 21771, loss 0.00177449, acc 1\n",
      "2018-10-26T18:21:01.248154: step 21772, loss 1.47148e-07, acc 1\n",
      "2018-10-26T18:21:01.421691: step 21773, loss 0.175208, acc 0.984375\n",
      "2018-10-26T18:21:01.594230: step 21774, loss 9.98431e-06, acc 1\n",
      "2018-10-26T18:21:01.763777: step 21775, loss 0, acc 1\n",
      "2018-10-26T18:21:01.943297: step 21776, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:21:02.112844: step 21777, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:21:02.290369: step 21778, loss 0.000590264, acc 1\n",
      "2018-10-26T18:21:02.456925: step 21779, loss 7.41326e-07, acc 1\n",
      "2018-10-26T18:21:02.633452: step 21780, loss 1.93146e-06, acc 1\n",
      "2018-10-26T18:21:02.800008: step 21781, loss 1.76941e-06, acc 1\n",
      "2018-10-26T18:21:02.972546: step 21782, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:21:03.139102: step 21783, loss 1.53288e-06, acc 1\n",
      "2018-10-26T18:21:03.314633: step 21784, loss 1.88126e-07, acc 1\n",
      "2018-10-26T18:21:03.493156: step 21785, loss 0.000148371, acc 1\n",
      "2018-10-26T18:21:03.671679: step 21786, loss 7.97725e-05, acc 1\n",
      "2018-10-26T18:21:03.836239: step 21787, loss 0, acc 1\n",
      "2018-10-26T18:21:04.021744: step 21788, loss 4.22815e-07, acc 1\n",
      "2018-10-26T18:21:04.189296: step 21789, loss 1.69308e-06, acc 1\n",
      "2018-10-26T18:21:04.360859: step 21790, loss 1.4156e-07, acc 1\n",
      "2018-10-26T18:21:04.526396: step 21791, loss 6.34247e-05, acc 1\n",
      "2018-10-26T18:21:04.702924: step 21792, loss 9.6484e-07, acc 1\n",
      "2018-10-26T18:21:04.868482: step 21793, loss 1.26283e-06, acc 1\n",
      "2018-10-26T18:21:05.048002: step 21794, loss 1.58152e-05, acc 1\n",
      "2018-10-26T18:21:05.216551: step 21795, loss 1.31207e-05, acc 1\n",
      "2018-10-26T18:21:05.389090: step 21796, loss 2.42143e-07, acc 1\n",
      "2018-10-26T18:21:05.562626: step 21797, loss 2.74902e-06, acc 1\n",
      "2018-10-26T18:21:05.735166: step 21798, loss 5.38762e-05, acc 1\n",
      "2018-10-26T18:21:05.903715: step 21799, loss 0.00368262, acc 1\n",
      "2018-10-26T18:21:06.076255: step 21800, loss 4.37761e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:21:06.527050: step 21800, loss 7.89945, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21800\n",
      "\n",
      "2018-10-26T18:21:06.919620: step 21801, loss 0, acc 1\n",
      "2018-10-26T18:21:07.103129: step 21802, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:21:07.275668: step 21803, loss 5.28564e-05, acc 1\n",
      "2018-10-26T18:21:07.440229: step 21804, loss 3.84817e-05, acc 1\n",
      "2018-10-26T18:21:07.645680: step 21805, loss 6.29171e-05, acc 1\n",
      "2018-10-26T18:21:07.870080: step 21806, loss 6.93688e-06, acc 1\n",
      "2018-10-26T18:21:08.054589: step 21807, loss 0.0169506, acc 0.984375\n",
      "2018-10-26T18:21:08.236102: step 21808, loss 1.86443e-06, acc 1\n",
      "2018-10-26T18:21:08.400662: step 21809, loss 1.21813e-06, acc 1\n",
      "2018-10-26T18:21:08.574199: step 21810, loss 5.68001e-05, acc 1\n",
      "2018-10-26T18:21:08.745740: step 21811, loss 2.22012e-06, acc 1\n",
      "2018-10-26T18:21:08.922268: step 21812, loss 3.53899e-07, acc 1\n",
      "2018-10-26T18:21:09.090818: step 21813, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:21:09.265353: step 21814, loss 8.81007e-07, acc 1\n",
      "2018-10-26T18:21:09.441880: step 21815, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:21:09.621401: step 21816, loss 0.000322867, acc 1\n",
      "2018-10-26T18:21:09.788953: step 21817, loss 2.36554e-07, acc 1\n",
      "2018-10-26T18:21:09.975456: step 21818, loss 8.41893e-07, acc 1\n",
      "2018-10-26T18:21:10.143008: step 21819, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:21:10.322528: step 21820, loss 4.47029e-07, acc 1\n",
      "2018-10-26T18:21:10.489082: step 21821, loss 1.58988e-05, acc 1\n",
      "2018-10-26T18:21:10.667605: step 21822, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:21:10.840145: step 21823, loss 0, acc 1\n",
      "2018-10-26T18:21:11.022657: step 21824, loss 6.20614e-05, acc 1\n",
      "2018-10-26T18:21:11.193202: step 21825, loss 5.28985e-07, acc 1\n",
      "2018-10-26T18:21:11.374717: step 21826, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:21:11.546258: step 21827, loss 5.67821e-06, acc 1\n",
      "2018-10-26T18:21:11.729768: step 21828, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:11.896322: step 21829, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:21:12.073868: step 21830, loss 4.05259e-06, acc 1\n",
      "2018-10-26T18:21:12.239405: step 21831, loss 0.000477886, acc 1\n",
      "2018-10-26T18:21:12.417929: step 21832, loss 2.20152e-06, acc 1\n",
      "2018-10-26T18:21:12.583487: step 21833, loss 0.000222248, acc 1\n",
      "2018-10-26T18:21:12.764004: step 21834, loss 5.1781e-07, acc 1\n",
      "2018-10-26T18:21:12.931556: step 21835, loss 5.55061e-07, acc 1\n",
      "2018-10-26T18:21:13.113071: step 21836, loss 5.98125e-05, acc 1\n",
      "2018-10-26T18:21:13.278629: step 21837, loss 9.3132e-08, acc 1\n",
      "2018-10-26T18:21:13.458149: step 21838, loss 0.000215108, acc 1\n",
      "2018-10-26T18:21:13.622709: step 21839, loss 2.35634e-05, acc 1\n",
      "2018-10-26T18:21:13.801234: step 21840, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:21:13.969782: step 21841, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:21:14.148306: step 21842, loss 5.40226e-05, acc 1\n",
      "2018-10-26T18:21:14.318852: step 21843, loss 5.13096e-06, acc 1\n",
      "2018-10-26T18:21:14.496375: step 21844, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:21:14.668914: step 21845, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:21:14.847438: step 21846, loss 3.18523e-05, acc 1\n",
      "2018-10-26T18:21:15.025961: step 21847, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:21:15.205481: step 21848, loss 1.46211e-06, acc 1\n",
      "2018-10-26T18:21:15.380014: step 21849, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:21:15.551556: step 21850, loss 5.24619e-06, acc 1\n",
      "2018-10-26T18:21:15.728084: step 21851, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:21:15.896635: step 21852, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:21:16.076155: step 21853, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:21:16.241712: step 21854, loss 1.17973e-05, acc 1\n",
      "2018-10-26T18:21:16.424225: step 21855, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:21:16.591777: step 21856, loss 0, acc 1\n",
      "2018-10-26T18:21:16.768306: step 21857, loss 4.86086e-06, acc 1\n",
      "2018-10-26T18:21:16.944838: step 21858, loss 0.000382232, acc 1\n",
      "2018-10-26T18:21:17.128343: step 21859, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:21:17.292913: step 21860, loss 2.44005e-07, acc 1\n",
      "2018-10-26T18:21:17.470429: step 21861, loss 1.15294e-06, acc 1\n",
      "2018-10-26T18:21:17.645960: step 21862, loss 0, acc 1\n",
      "2018-10-26T18:21:17.820494: step 21863, loss 0.0454105, acc 0.984375\n",
      "2018-10-26T18:21:17.995028: step 21864, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:18.184522: step 21865, loss 0.00120107, acc 1\n",
      "2018-10-26T18:21:18.368031: step 21866, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:18.546554: step 21867, loss 1.2293e-06, acc 1\n",
      "2018-10-26T18:21:18.732058: step 21868, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:21:18.899611: step 21869, loss 1.00532e-05, acc 1\n",
      "2018-10-26T18:21:19.084119: step 21870, loss 3.7625e-07, acc 1\n",
      "2018-10-26T18:21:19.255659: step 21871, loss 1.38957e-05, acc 1\n",
      "2018-10-26T18:21:19.437175: step 21872, loss 7.59941e-07, acc 1\n",
      "2018-10-26T18:21:19.606722: step 21873, loss 2.34692e-07, acc 1\n",
      "2018-10-26T18:21:19.790231: step 21874, loss 0, acc 1\n",
      "2018-10-26T18:21:19.955789: step 21875, loss 0, acc 1\n",
      "2018-10-26T18:21:20.139299: step 21876, loss 0.00109358, acc 1\n",
      "2018-10-26T18:21:20.307849: step 21877, loss 5.32707e-07, acc 1\n",
      "2018-10-26T18:21:20.489363: step 21878, loss 0, acc 1\n",
      "2018-10-26T18:21:20.660905: step 21879, loss 0.000191933, acc 1\n",
      "2018-10-26T18:21:20.849402: step 21880, loss 6.36709e-06, acc 1\n",
      "2018-10-26T18:21:21.015956: step 21881, loss 0.000331363, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:21:21.201461: step 21882, loss 0.161761, acc 0.984375\n",
      "2018-10-26T18:21:21.368017: step 21883, loss 4.88312e-05, acc 1\n",
      "2018-10-26T18:21:21.544545: step 21884, loss 8.56815e-08, acc 1\n",
      "2018-10-26T18:21:21.712096: step 21885, loss 0.000171631, acc 1\n",
      "2018-10-26T18:21:21.883638: step 21886, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:21:22.051190: step 21887, loss 6.25472e-05, acc 1\n",
      "2018-10-26T18:21:22.226722: step 21888, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:21:22.395271: step 21889, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:21:22.575790: step 21890, loss 8.1161e-05, acc 1\n",
      "2018-10-26T18:21:22.742343: step 21891, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:22.917874: step 21892, loss 0.00160719, acc 1\n",
      "2018-10-26T18:21:23.085428: step 21893, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:21:23.257966: step 21894, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:23.430506: step 21895, loss 6.2051e-06, acc 1\n",
      "2018-10-26T18:21:23.615012: step 21896, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:21:23.787551: step 21897, loss 6.64958e-07, acc 1\n",
      "2018-10-26T18:21:23.968069: step 21898, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:21:24.145594: step 21899, loss 0.00481385, acc 1\n",
      "2018-10-26T18:21:24.316139: step 21900, loss 2.84393e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:21:24.765937: step 21900, loss 7.5949, acc 0.730769\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-21900\n",
      "\n",
      "2018-10-26T18:21:25.195789: step 21901, loss 8.21406e-07, acc 1\n",
      "2018-10-26T18:21:25.419193: step 21902, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:25.633619: step 21903, loss 4.86143e-07, acc 1\n",
      "2018-10-26T18:21:25.853036: step 21904, loss 0.00483862, acc 1\n",
      "2018-10-26T18:21:26.115332: step 21905, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:21:26.295849: step 21906, loss 3.96738e-07, acc 1\n",
      "2018-10-26T18:21:26.478361: step 21907, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:21:26.651903: step 21908, loss 3.55725e-06, acc 1\n",
      "2018-10-26T18:21:26.826433: step 21909, loss 1.93703e-06, acc 1\n",
      "2018-10-26T18:21:26.988998: step 21910, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:21:27.167520: step 21911, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:27.337067: step 21912, loss 1.03321e-05, acc 1\n",
      "2018-10-26T18:21:27.522572: step 21913, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:21:27.691121: step 21914, loss 1.62049e-07, acc 1\n",
      "2018-10-26T18:21:27.867651: step 21915, loss 0.0153365, acc 0.984375\n",
      "2018-10-26T18:21:28.034204: step 21916, loss 6.08093e-05, acc 1\n",
      "2018-10-26T18:21:28.211731: step 21917, loss 8.56257e-05, acc 1\n",
      "2018-10-26T18:21:28.389256: step 21918, loss 5.45324e-06, acc 1\n",
      "2018-10-26T18:21:28.564788: step 21919, loss 0.00185244, acc 1\n",
      "2018-10-26T18:21:28.737326: step 21920, loss 0.000312502, acc 1\n",
      "2018-10-26T18:21:28.915850: step 21921, loss 6.2445e-05, acc 1\n",
      "2018-10-26T18:21:29.092378: step 21922, loss 9.87307e-05, acc 1\n",
      "2018-10-26T18:21:29.269903: step 21923, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:21:29.438454: step 21924, loss 1.55152e-06, acc 1\n",
      "2018-10-26T18:21:29.618970: step 21925, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:21:29.789515: step 21926, loss 1.05799e-05, acc 1\n",
      "2018-10-26T18:21:29.969035: step 21927, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:21:30.135590: step 21928, loss 5.88302e-06, acc 1\n",
      "2018-10-26T18:21:30.321094: step 21929, loss 2.07494e-05, acc 1\n",
      "2018-10-26T18:21:30.494631: step 21930, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:21:30.676147: step 21931, loss 0.000141305, acc 1\n",
      "2018-10-26T18:21:30.916503: step 21932, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:21:31.129934: step 21933, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:21:31.315438: step 21934, loss 4.47286e-05, acc 1\n",
      "2018-10-26T18:21:31.544824: step 21935, loss 3.01919e-06, acc 1\n",
      "2018-10-26T18:21:31.718361: step 21936, loss 3.89617e-06, acc 1\n",
      "2018-10-26T18:21:31.938773: step 21937, loss 0.000197657, acc 1\n",
      "2018-10-26T18:21:32.151205: step 21938, loss 0.000194814, acc 1\n",
      "2018-10-26T18:21:32.373612: step 21939, loss 4.90732e-06, acc 1\n",
      "2018-10-26T18:21:32.604995: step 21940, loss 0.0146522, acc 0.984375\n",
      "2018-10-26T18:21:32.781520: step 21941, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:21:33.017889: step 21942, loss 3.57624e-07, acc 1\n",
      "2018-10-26T18:21:33.194417: step 21943, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:21:33.407847: step 21944, loss 2.54882e-05, acc 1\n",
      "2018-10-26T18:21:33.631258: step 21945, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:21:33.800798: step 21946, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:21:34.011235: step 21947, loss 3.3133e-06, acc 1\n",
      "2018-10-26T18:21:34.211700: step 21948, loss 6.14663e-07, acc 1\n",
      "2018-10-26T18:21:34.393214: step 21949, loss 8.92187e-07, acc 1\n",
      "2018-10-26T18:21:34.569742: step 21950, loss 1.57013e-06, acc 1\n",
      "2018-10-26T18:21:34.803119: step 21951, loss 1.66106e-05, acc 1\n",
      "2018-10-26T18:21:35.008570: step 21952, loss 0.000720351, acc 1\n",
      "2018-10-26T18:21:35.187093: step 21953, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:21:35.428448: step 21954, loss 7.50746e-05, acc 1\n",
      "2018-10-26T18:21:35.626919: step 21955, loss 1.31931e-05, acc 1\n",
      "2018-10-26T18:21:35.814418: step 21956, loss 3.68391e-06, acc 1\n",
      "2018-10-26T18:21:35.987954: step 21957, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:21:36.228311: step 21958, loss 9.68574e-08, acc 1\n",
      "2018-10-26T18:21:36.419800: step 21959, loss 0, acc 1\n",
      "2018-10-26T18:21:36.608295: step 21960, loss 4.14595e-06, acc 1\n",
      "2018-10-26T18:21:36.819731: step 21961, loss 0.0252685, acc 0.984375\n",
      "2018-10-26T18:21:37.013213: step 21962, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:21:37.222655: step 21963, loss 1.79176e-06, acc 1\n",
      "2018-10-26T18:21:37.406164: step 21964, loss 5.58434e-05, acc 1\n",
      "2018-10-26T18:21:37.586682: step 21965, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:21:37.803103: step 21966, loss 0.000246944, acc 1\n",
      "2018-10-26T18:21:37.981627: step 21967, loss 0.000676607, acc 1\n",
      "2018-10-26T18:21:38.164140: step 21968, loss 1.82538e-07, acc 1\n",
      "2018-10-26T18:21:38.373579: step 21969, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:21:38.544124: step 21970, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:21:38.716662: step 21971, loss 8.34455e-07, acc 1\n",
      "2018-10-26T18:21:38.906156: step 21972, loss 3.00605e-06, acc 1\n",
      "2018-10-26T18:21:39.095652: step 21973, loss 2.95082e-05, acc 1\n",
      "2018-10-26T18:21:39.279159: step 21974, loss 0, acc 1\n",
      "2018-10-26T18:21:39.454692: step 21975, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:21:39.673107: step 21976, loss 0.0138599, acc 0.984375\n",
      "2018-10-26T18:21:39.884542: step 21977, loss 0, acc 1\n",
      "2018-10-26T18:21:40.091988: step 21978, loss 5.13262e-06, acc 1\n",
      "2018-10-26T18:21:40.294449: step 21979, loss 2.63356e-06, acc 1\n",
      "2018-10-26T18:21:40.517855: step 21980, loss 0, acc 1\n",
      "2018-10-26T18:21:40.688394: step 21981, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:21:40.913792: step 21982, loss 7.7858e-05, acc 1\n",
      "2018-10-26T18:21:41.110268: step 21983, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:21:41.320705: step 21984, loss 5.01198e-06, acc 1\n",
      "2018-10-26T18:21:41.502220: step 21985, loss 0.000244263, acc 1\n",
      "2018-10-26T18:21:41.683735: step 21986, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:21:41.903149: step 21987, loss 7.64351e-05, acc 1\n",
      "2018-10-26T18:21:42.076685: step 21988, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:21:42.256205: step 21989, loss 0.0467889, acc 0.984375\n",
      "2018-10-26T18:21:42.447694: step 21990, loss 0, acc 1\n",
      "2018-10-26T18:21:42.629209: step 21991, loss 1.97244e-05, acc 1\n",
      "2018-10-26T18:21:42.808730: step 21992, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:21:42.984260: step 21993, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:21:43.199684: step 21994, loss 0.00126469, acc 1\n",
      "2018-10-26T18:21:43.365242: step 21995, loss 7.8043e-07, acc 1\n",
      "2018-10-26T18:21:43.552741: step 21996, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:43.729270: step 21997, loss 0, acc 1\n",
      "2018-10-26T18:21:43.956664: step 21998, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:21:44.131197: step 21999, loss 1.64514e-05, acc 1\n",
      "2018-10-26T18:21:44.314706: step 22000, loss 7.50471e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:21:44.804398: step 22000, loss 7.62156, acc 0.728893\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22000\n",
      "\n",
      "2018-10-26T18:21:45.187575: step 22001, loss 0.00132847, acc 1\n",
      "2018-10-26T18:21:45.373078: step 22002, loss 1.52548e-06, acc 1\n",
      "2018-10-26T18:21:45.543623: step 22003, loss 2.21654e-07, acc 1\n",
      "2018-10-26T18:21:45.725138: step 22004, loss 3.42723e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:21:45.951533: step 22005, loss 0.025318, acc 0.984375\n",
      "2018-10-26T18:21:46.165962: step 22006, loss 1.4845e-06, acc 1\n",
      "2018-10-26T18:21:46.335507: step 22007, loss 4.15365e-07, acc 1\n",
      "2018-10-26T18:21:46.514029: step 22008, loss 1.80853e-06, acc 1\n",
      "2018-10-26T18:21:46.691557: step 22009, loss 0.000178272, acc 1\n",
      "2018-10-26T18:21:46.866090: step 22010, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:21:47.041620: step 22011, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:21:47.212165: step 22012, loss 0.000923377, acc 1\n",
      "2018-10-26T18:21:47.387695: step 22013, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:21:47.552256: step 22014, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:21:47.727786: step 22015, loss 0.000127174, acc 1\n",
      "2018-10-26T18:21:47.899329: step 22016, loss 0.000185816, acc 1\n",
      "2018-10-26T18:21:48.080844: step 22017, loss 9.25708e-07, acc 1\n",
      "2018-10-26T18:21:48.251388: step 22018, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:21:48.423927: step 22019, loss 7.26423e-07, acc 1\n",
      "2018-10-26T18:21:48.597464: step 22020, loss 4.59449e-06, acc 1\n",
      "2018-10-26T18:21:48.771997: step 22021, loss 1.77674e-05, acc 1\n",
      "2018-10-26T18:21:48.940547: step 22022, loss 0.000137058, acc 1\n",
      "2018-10-26T18:21:49.124056: step 22023, loss 1.29628e-05, acc 1\n",
      "2018-10-26T18:21:49.290611: step 22024, loss 0.00064591, acc 1\n",
      "2018-10-26T18:21:49.468137: step 22025, loss 1.96815e-05, acc 1\n",
      "2018-10-26T18:21:49.632697: step 22026, loss 0, acc 1\n",
      "2018-10-26T18:21:49.805236: step 22027, loss 0.00021755, acc 1\n",
      "2018-10-26T18:21:49.976778: step 22028, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:21:50.154304: step 22029, loss 1.04739e-05, acc 1\n",
      "2018-10-26T18:21:50.323851: step 22030, loss 0.0882565, acc 0.984375\n",
      "2018-10-26T18:21:50.509355: step 22031, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:21:50.675910: step 22032, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:21:50.851442: step 22033, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:21:51.015004: step 22034, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:21:51.194524: step 22035, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:21:51.366066: step 22036, loss 4.6752e-07, acc 1\n",
      "2018-10-26T18:21:51.540600: step 22037, loss 1.59621e-06, acc 1\n",
      "2018-10-26T18:21:51.714135: step 22038, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:21:51.895652: step 22039, loss 0.00237494, acc 1\n",
      "2018-10-26T18:21:52.074175: step 22040, loss 2.96505e-06, acc 1\n",
      "2018-10-26T18:21:52.259679: step 22041, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:21:52.429226: step 22042, loss 3.37298e-05, acc 1\n",
      "2018-10-26T18:21:52.603759: step 22043, loss 8.77292e-07, acc 1\n",
      "2018-10-26T18:21:52.771311: step 22044, loss 0.0041894, acc 1\n",
      "2018-10-26T18:21:52.948837: step 22045, loss 1.25548e-05, acc 1\n",
      "2018-10-26T18:21:53.114395: step 22046, loss 0.162931, acc 0.984375\n",
      "2018-10-26T18:21:53.293915: step 22047, loss 0.0151704, acc 0.984375\n",
      "2018-10-26T18:21:53.468450: step 22048, loss 2.46409e-06, acc 1\n",
      "2018-10-26T18:21:53.657943: step 22049, loss 4.3241e-05, acc 1\n",
      "2018-10-26T18:21:53.840457: step 22050, loss 0, acc 1\n",
      "2018-10-26T18:21:54.022967: step 22051, loss 2.07485e-06, acc 1\n",
      "2018-10-26T18:21:54.197513: step 22052, loss 2.07858e-06, acc 1\n",
      "2018-10-26T18:21:54.372035: step 22053, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:21:54.562525: step 22054, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:21:54.729080: step 22055, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:21:54.901620: step 22056, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:55.068174: step 22057, loss 3.77699e-06, acc 1\n",
      "2018-10-26T18:21:55.239717: step 22058, loss 7.46184e-06, acc 1\n",
      "2018-10-26T18:21:55.412256: step 22059, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:21:55.587787: step 22060, loss 7.24594e-06, acc 1\n",
      "2018-10-26T18:21:55.757333: step 22061, loss 0.0034047, acc 1\n",
      "2018-10-26T18:21:55.935856: step 22062, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:21:56.102411: step 22063, loss 1.86067e-06, acc 1\n",
      "2018-10-26T18:21:56.281931: step 22064, loss 0.00154798, acc 1\n",
      "2018-10-26T18:21:56.452476: step 22065, loss 0.000224009, acc 1\n",
      "2018-10-26T18:21:56.625015: step 22066, loss 0.204968, acc 0.984375\n",
      "2018-10-26T18:21:56.793565: step 22067, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:21:56.968099: step 22068, loss 0, acc 1\n",
      "2018-10-26T18:21:57.132658: step 22069, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:21:57.307192: step 22070, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:21:57.472749: step 22071, loss 0.000150587, acc 1\n",
      "2018-10-26T18:21:57.650275: step 22072, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:21:57.824810: step 22073, loss 2.07045e-05, acc 1\n",
      "2018-10-26T18:21:58.005327: step 22074, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:21:58.169888: step 22075, loss 0, acc 1\n",
      "2018-10-26T18:21:58.348411: step 22076, loss 1.22063e-05, acc 1\n",
      "2018-10-26T18:21:58.517958: step 22077, loss 3.8184e-07, acc 1\n",
      "2018-10-26T18:21:58.689499: step 22078, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:21:58.857051: step 22079, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:21:59.028593: step 22080, loss 6.32081e-05, acc 1\n",
      "2018-10-26T18:21:59.198140: step 22081, loss 1.2417e-05, acc 1\n",
      "2018-10-26T18:21:59.385641: step 22082, loss 2.84982e-07, acc 1\n",
      "2018-10-26T18:21:59.554189: step 22083, loss 1.2405e-06, acc 1\n",
      "2018-10-26T18:21:59.727724: step 22084, loss 3.08424e-06, acc 1\n",
      "2018-10-26T18:21:59.899267: step 22085, loss 0.000229544, acc 1\n",
      "2018-10-26T18:22:00.079785: step 22086, loss 9.38757e-07, acc 1\n",
      "2018-10-26T18:22:00.242350: step 22087, loss 1.01388e-05, acc 1\n",
      "2018-10-26T18:22:00.417880: step 22088, loss 9.31795e-06, acc 1\n",
      "2018-10-26T18:22:00.587428: step 22089, loss 0.00684375, acc 1\n",
      "2018-10-26T18:22:00.762959: step 22090, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:22:00.928517: step 22091, loss 0, acc 1\n",
      "2018-10-26T18:22:01.112027: step 22092, loss 8.19552e-07, acc 1\n",
      "2018-10-26T18:22:01.280577: step 22093, loss 9.35023e-07, acc 1\n",
      "2018-10-26T18:22:01.455110: step 22094, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:22:01.623659: step 22095, loss 6.89165e-07, acc 1\n",
      "2018-10-26T18:22:01.805175: step 22096, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:22:01.972727: step 22097, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:02.151250: step 22098, loss 0, acc 1\n",
      "2018-10-26T18:22:02.326781: step 22099, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:22:02.506300: step 22100, loss 0.092949, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:22:02.953107: step 22100, loss 7.73484, acc 0.725141\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22100\n",
      "\n",
      "2018-10-26T18:22:03.307066: step 22101, loss 0, acc 1\n",
      "2018-10-26T18:22:03.468634: step 22102, loss 5.38295e-07, acc 1\n",
      "2018-10-26T18:22:03.650149: step 22103, loss 2.37512e-05, acc 1\n",
      "2018-10-26T18:22:03.823685: step 22104, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:04.019163: step 22105, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:22:04.262512: step 22106, loss 3.43621e-06, acc 1\n",
      "2018-10-26T18:22:04.429068: step 22107, loss 4.15366e-07, acc 1\n",
      "2018-10-26T18:22:04.614572: step 22108, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:04.784120: step 22109, loss 3.05471e-07, acc 1\n",
      "2018-10-26T18:22:04.963639: step 22110, loss 7.58078e-07, acc 1\n",
      "2018-10-26T18:22:05.130195: step 22111, loss 0.00622984, acc 1\n",
      "2018-10-26T18:22:05.311709: step 22112, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:05.477267: step 22113, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:22:05.654793: step 22114, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:22:05.822345: step 22115, loss 8.13968e-07, acc 1\n",
      "2018-10-26T18:22:06.003860: step 22116, loss 0.000370206, acc 1\n",
      "2018-10-26T18:22:06.175401: step 22117, loss 5.66237e-07, acc 1\n",
      "2018-10-26T18:22:06.361904: step 22118, loss 1.3609e-05, acc 1\n",
      "2018-10-26T18:22:06.529456: step 22119, loss 0.000333243, acc 1\n",
      "2018-10-26T18:22:06.707979: step 22120, loss 5.5888e-06, acc 1\n",
      "2018-10-26T18:22:06.880520: step 22121, loss 3.75653e-06, acc 1\n",
      "2018-10-26T18:22:07.064027: step 22122, loss 0, acc 1\n",
      "2018-10-26T18:22:07.231581: step 22123, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:22:07.412098: step 22124, loss 0.000405154, acc 1\n",
      "2018-10-26T18:22:07.585633: step 22125, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:22:07.774129: step 22126, loss 0.000209987, acc 1\n",
      "2018-10-26T18:22:07.937693: step 22127, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:22:08.113224: step 22128, loss 3.56816e-05, acc 1\n",
      "2018-10-26T18:22:08.283769: step 22129, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:22:08.467278: step 22130, loss 3.3341e-07, acc 1\n",
      "2018-10-26T18:22:08.631838: step 22131, loss 5.67105e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:22:08.835295: step 22132, loss 1.66884e-06, acc 1\n",
      "2018-10-26T18:22:09.038751: step 22133, loss 0, acc 1\n",
      "2018-10-26T18:22:09.243205: step 22134, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:22:09.422726: step 22135, loss 0, acc 1\n",
      "2018-10-26T18:22:09.592273: step 22136, loss 0, acc 1\n",
      "2018-10-26T18:22:09.777371: step 22137, loss 1.86817e-05, acc 1\n",
      "2018-10-26T18:22:09.968859: step 22138, loss 1.04677e-06, acc 1\n",
      "2018-10-26T18:22:10.150375: step 22139, loss 6.63731e-05, acc 1\n",
      "2018-10-26T18:22:10.327901: step 22140, loss 4.62066e-06, acc 1\n",
      "2018-10-26T18:22:10.504429: step 22141, loss 6.34102e-06, acc 1\n",
      "2018-10-26T18:22:10.674972: step 22142, loss 0.000555572, acc 1\n",
      "2018-10-26T18:22:10.850504: step 22143, loss 1.23705e-05, acc 1\n",
      "2018-10-26T18:22:11.025038: step 22144, loss 2.4773e-07, acc 1\n",
      "2018-10-26T18:22:11.204558: step 22145, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:22:11.379091: step 22146, loss 0.00035155, acc 1\n",
      "2018-10-26T18:22:11.564596: step 22147, loss 8.38188e-08, acc 1\n",
      "2018-10-26T18:22:11.738132: step 22148, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:22:11.916655: step 22149, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:12.086203: step 22150, loss 1.36154e-06, acc 1\n",
      "2018-10-26T18:22:12.256747: step 22151, loss 2.66357e-07, acc 1\n",
      "2018-10-26T18:22:12.424300: step 22152, loss 6.80649e-06, acc 1\n",
      "2018-10-26T18:22:12.598834: step 22153, loss 0.00206988, acc 1\n",
      "2018-10-26T18:22:12.770374: step 22154, loss 0.00265599, acc 1\n",
      "2018-10-26T18:22:12.943920: step 22155, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:22:13.122435: step 22156, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:22:13.301955: step 22157, loss 0.000536591, acc 1\n",
      "2018-10-26T18:22:13.470505: step 22158, loss 4.43303e-07, acc 1\n",
      "2018-10-26T18:22:13.646035: step 22159, loss 6.29569e-07, acc 1\n",
      "2018-10-26T18:22:13.813587: step 22160, loss 0.00394734, acc 1\n",
      "2018-10-26T18:22:13.993108: step 22161, loss 3.73364e-05, acc 1\n",
      "2018-10-26T18:22:14.162655: step 22162, loss 1.15595e-05, acc 1\n",
      "2018-10-26T18:22:14.335194: step 22163, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:22:14.514714: step 22164, loss 0.00383556, acc 1\n",
      "2018-10-26T18:22:14.689248: step 22165, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:22:14.866772: step 22166, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:22:15.033329: step 22167, loss 0.000167172, acc 1\n",
      "2018-10-26T18:22:15.207862: step 22168, loss 7.67036e-06, acc 1\n",
      "2018-10-26T18:22:15.372422: step 22169, loss 0, acc 1\n",
      "2018-10-26T18:22:15.557948: step 22170, loss 8.76028e-05, acc 1\n",
      "2018-10-26T18:22:15.739442: step 22171, loss 8.23268e-07, acc 1\n",
      "2018-10-26T18:22:15.917115: step 22172, loss 0.000128031, acc 1\n",
      "2018-10-26T18:22:16.085665: step 22173, loss 1.26683e-05, acc 1\n",
      "2018-10-26T18:22:16.266181: step 22174, loss 4.42133e-06, acc 1\n",
      "2018-10-26T18:22:16.443707: step 22175, loss 2.4773e-07, acc 1\n",
      "2018-10-26T18:22:16.625221: step 22176, loss 8.66825e-06, acc 1\n",
      "2018-10-26T18:22:16.793772: step 22177, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:16.969303: step 22178, loss 0.00133699, acc 1\n",
      "2018-10-26T18:22:17.134860: step 22179, loss 4.73109e-07, acc 1\n",
      "2018-10-26T18:22:17.315378: step 22180, loss 0, acc 1\n",
      "2018-10-26T18:22:17.481933: step 22181, loss 0.00147246, acc 1\n",
      "2018-10-26T18:22:17.656468: step 22182, loss 2.55181e-07, acc 1\n",
      "2018-10-26T18:22:17.824020: step 22183, loss 1.47328e-06, acc 1\n",
      "2018-10-26T18:22:18.010521: step 22184, loss 1.73226e-07, acc 1\n",
      "2018-10-26T18:22:18.174083: step 22185, loss 6.95957e-06, acc 1\n",
      "2018-10-26T18:22:18.350613: step 22186, loss 9.66684e-07, acc 1\n",
      "2018-10-26T18:22:18.517168: step 22187, loss 1.38123e-05, acc 1\n",
      "2018-10-26T18:22:18.700677: step 22188, loss 8.86115e-05, acc 1\n",
      "2018-10-26T18:22:18.871222: step 22189, loss 1.03189e-06, acc 1\n",
      "2018-10-26T18:22:19.056727: step 22190, loss 0, acc 1\n",
      "2018-10-26T18:22:19.220289: step 22191, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:22:19.402801: step 22192, loss 0.000134366, acc 1\n",
      "2018-10-26T18:22:19.568359: step 22193, loss 0.0159389, acc 0.984375\n",
      "2018-10-26T18:22:19.742893: step 22194, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:22:19.916429: step 22195, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:20.098941: step 22196, loss 8.72222e-06, acc 1\n",
      "2018-10-26T18:22:20.267491: step 22197, loss 0, acc 1\n",
      "2018-10-26T18:22:20.441028: step 22198, loss 0.000484471, acc 1\n",
      "2018-10-26T18:22:20.610574: step 22199, loss 1.29263e-06, acc 1\n",
      "2018-10-26T18:22:20.788099: step 22200, loss 5.96046e-09, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:22:21.233908: step 22200, loss 7.91084, acc 0.731707\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22200\n",
      "\n",
      "2018-10-26T18:22:21.600775: step 22201, loss 8.5121e-07, acc 1\n",
      "2018-10-26T18:22:21.765335: step 22202, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:22:21.941864: step 22203, loss 1.06912e-06, acc 1\n",
      "2018-10-26T18:22:22.111410: step 22204, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:22.308884: step 22205, loss 8.22701e-06, acc 1\n",
      "2018-10-26T18:22:22.544255: step 22206, loss 2.17502e-05, acc 1\n",
      "2018-10-26T18:22:22.714799: step 22207, loss 2.68121e-05, acc 1\n",
      "2018-10-26T18:22:22.910277: step 22208, loss 3.20528e-06, acc 1\n",
      "2018-10-26T18:22:23.079823: step 22209, loss 1.41561e-07, acc 1\n",
      "2018-10-26T18:22:23.260341: step 22210, loss 0, acc 1\n",
      "2018-10-26T18:22:23.427894: step 22211, loss 7.52497e-07, acc 1\n",
      "2018-10-26T18:22:23.621377: step 22212, loss 2.51455e-07, acc 1\n",
      "2018-10-26T18:22:23.790937: step 22213, loss 1.3839e-06, acc 1\n",
      "2018-10-26T18:22:23.968450: step 22214, loss 5.87212e-06, acc 1\n",
      "2018-10-26T18:22:24.134027: step 22215, loss 0, acc 1\n",
      "2018-10-26T18:22:24.314525: step 22216, loss 0.000111778, acc 1\n",
      "2018-10-26T18:22:24.491053: step 22217, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:22:24.669577: step 22218, loss 0.004038, acc 1\n",
      "2018-10-26T18:22:24.836131: step 22219, loss 6.30937e-06, acc 1\n",
      "2018-10-26T18:22:25.014655: step 22220, loss 4.35815e-06, acc 1\n",
      "2018-10-26T18:22:25.185198: step 22221, loss 0, acc 1\n",
      "2018-10-26T18:22:25.362724: step 22222, loss 1.32657e-05, acc 1\n",
      "2018-10-26T18:22:25.532270: step 22223, loss 1.39282e-05, acc 1\n",
      "2018-10-26T18:22:25.711792: step 22224, loss 0.00140683, acc 1\n",
      "2018-10-26T18:22:25.881338: step 22225, loss 9.34878e-05, acc 1\n",
      "2018-10-26T18:22:26.059862: step 22226, loss 0.000211709, acc 1\n",
      "2018-10-26T18:22:26.231402: step 22227, loss 8.28855e-07, acc 1\n",
      "2018-10-26T18:22:26.406934: step 22228, loss 0, acc 1\n",
      "2018-10-26T18:22:26.576481: step 22229, loss 4.10063e-05, acc 1\n",
      "2018-10-26T18:22:26.752013: step 22230, loss 1.82531e-06, acc 1\n",
      "2018-10-26T18:22:26.921559: step 22231, loss 7.82491e-06, acc 1\n",
      "2018-10-26T18:22:27.101079: step 22232, loss 2.05892e-05, acc 1\n",
      "2018-10-26T18:22:27.266637: step 22233, loss 3.40028e-05, acc 1\n",
      "2018-10-26T18:22:27.443166: step 22234, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:22:27.617699: step 22235, loss 8.75419e-07, acc 1\n",
      "2018-10-26T18:22:27.851076: step 22236, loss 0.0684591, acc 0.984375\n",
      "2018-10-26T18:22:28.036580: step 22237, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:22:28.228068: step 22238, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:22:28.409583: step 22239, loss 0, acc 1\n",
      "2018-10-26T18:22:28.588107: step 22240, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:22:28.761643: step 22241, loss 0, acc 1\n",
      "2018-10-26T18:22:28.950139: step 22242, loss 0.000156814, acc 1\n",
      "2018-10-26T18:22:29.110710: step 22243, loss 1.19019e-06, acc 1\n",
      "2018-10-26T18:22:29.289233: step 22244, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:22:29.455788: step 22245, loss 2.27241e-07, acc 1\n",
      "2018-10-26T18:22:29.636306: step 22246, loss 3.20372e-07, acc 1\n",
      "2018-10-26T18:22:29.803858: step 22247, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:22:29.985373: step 22248, loss 5.51334e-07, acc 1\n",
      "2018-10-26T18:22:30.152925: step 22249, loss 1.23304e-06, acc 1\n",
      "2018-10-26T18:22:30.334440: step 22250, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:22:30.507978: step 22251, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:22:30.687496: step 22252, loss 5.09537e-06, acc 1\n",
      "2018-10-26T18:22:30.861034: step 22253, loss 9.38758e-07, acc 1\n",
      "2018-10-26T18:22:31.041551: step 22254, loss 1.62601e-06, acc 1\n",
      "2018-10-26T18:22:31.207109: step 22255, loss 5.02088e-06, acc 1\n",
      "2018-10-26T18:22:31.398598: step 22256, loss 2.37319e-05, acc 1\n",
      "2018-10-26T18:22:31.567147: step 22257, loss 6.291e-06, acc 1\n",
      "2018-10-26T18:22:31.744673: step 22258, loss 0, acc 1\n",
      "2018-10-26T18:22:31.912224: step 22259, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:22:32.093740: step 22260, loss 0, acc 1\n",
      "2018-10-26T18:22:32.260295: step 22261, loss 5.03765e-06, acc 1\n",
      "2018-10-26T18:22:32.432833: step 22262, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:22:32.595399: step 22263, loss 0.0608341, acc 0.984375\n",
      "2018-10-26T18:22:32.769933: step 22264, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:22:32.940477: step 22265, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:22:33.121994: step 22266, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:22:33.287550: step 22267, loss 4.06051e-07, acc 1\n",
      "2018-10-26T18:22:33.461086: step 22268, loss 5.50501e-06, acc 1\n",
      "2018-10-26T18:22:33.636617: step 22269, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:22:33.807162: step 22270, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:22:33.984688: step 22271, loss 9.08707e-06, acc 1\n",
      "2018-10-26T18:22:34.151243: step 22272, loss 0, acc 1\n",
      "2018-10-26T18:22:34.325777: step 22273, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:22:34.493328: step 22274, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:22:34.668860: step 22275, loss 8.94069e-08, acc 1\n",
      "2018-10-26T18:22:34.840401: step 22276, loss 1.51753e-05, acc 1\n",
      "2018-10-26T18:22:35.013937: step 22277, loss 0.00576572, acc 1\n",
      "2018-10-26T18:22:35.183484: step 22278, loss 9.3502e-07, acc 1\n",
      "2018-10-26T18:22:35.359035: step 22279, loss 7.09654e-07, acc 1\n",
      "2018-10-26T18:22:35.526567: step 22280, loss 0, acc 1\n",
      "2018-10-26T18:22:35.703096: step 22281, loss 1.559e-06, acc 1\n",
      "2018-10-26T18:22:35.873641: step 22282, loss 0.193751, acc 0.984375\n",
      "2018-10-26T18:22:36.049171: step 22283, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:22:36.215727: step 22284, loss 9.61095e-07, acc 1\n",
      "2018-10-26T18:22:36.395246: step 22285, loss 1.45219e-05, acc 1\n",
      "2018-10-26T18:22:36.591722: step 22286, loss 2.66717e-06, acc 1\n",
      "2018-10-26T18:22:36.773237: step 22287, loss 1.14736e-06, acc 1\n",
      "2018-10-26T18:22:36.992651: step 22288, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:22:37.166186: step 22289, loss 0.0128962, acc 0.984375\n",
      "2018-10-26T18:22:37.350694: step 22290, loss 2.6656e-05, acc 1\n",
      "2018-10-26T18:22:37.557142: step 22291, loss 2.16527e-05, acc 1\n",
      "2018-10-26T18:22:37.778550: step 22292, loss 9.51783e-07, acc 1\n",
      "2018-10-26T18:22:37.990984: step 22293, loss 2.60769e-07, acc 1\n",
      "2018-10-26T18:22:38.216381: step 22294, loss 1.4991e-05, acc 1\n",
      "2018-10-26T18:22:38.444771: step 22295, loss 0, acc 1\n",
      "2018-10-26T18:22:38.622297: step 22296, loss 5.52086e-05, acc 1\n",
      "2018-10-26T18:22:38.862654: step 22297, loss 0, acc 1\n",
      "2018-10-26T18:22:39.033199: step 22298, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:22:39.269566: step 22299, loss 0.000803525, acc 1\n",
      "2018-10-26T18:22:39.479007: step 22300, loss 3.20373e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:22:39.991638: step 22300, loss 7.88747, acc 0.733584\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22300\n",
      "\n",
      "2018-10-26T18:22:40.369628: step 22301, loss 8.95862e-06, acc 1\n",
      "2018-10-26T18:22:40.583057: step 22302, loss 0.0639216, acc 0.984375\n",
      "2018-10-26T18:22:40.804467: step 22303, loss 3.27823e-07, acc 1\n",
      "2018-10-26T18:22:41.068760: step 22304, loss 2.47527e-06, acc 1\n",
      "2018-10-26T18:22:41.359982: step 22305, loss 4.31262e-05, acc 1\n",
      "2018-10-26T18:22:41.593358: step 22306, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:22:41.803796: step 22307, loss 9.87201e-08, acc 1\n",
      "2018-10-26T18:22:42.001269: step 22308, loss 0.000299311, acc 1\n",
      "2018-10-26T18:22:42.210709: step 22309, loss 1.00417e-05, acc 1\n",
      "2018-10-26T18:22:42.442090: step 22310, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:22:42.656518: step 22311, loss 3.68409e-06, acc 1\n",
      "2018-10-26T18:22:42.849004: step 22312, loss 5.42025e-07, acc 1\n",
      "2018-10-26T18:22:43.059441: step 22313, loss 0, acc 1\n",
      "2018-10-26T18:22:43.239959: step 22314, loss 5.09357e-05, acc 1\n",
      "2018-10-26T18:22:43.417486: step 22315, loss 2.08417e-06, acc 1\n",
      "2018-10-26T18:22:43.616951: step 22316, loss 3.22206e-06, acc 1\n",
      "2018-10-26T18:22:43.793480: step 22317, loss 0, acc 1\n",
      "2018-10-26T18:22:43.969021: step 22318, loss 1.15852e-06, acc 1\n",
      "2018-10-26T18:22:44.160498: step 22319, loss 0, acc 1\n",
      "2018-10-26T18:22:44.338024: step 22320, loss 2.87754e-06, acc 1\n",
      "2018-10-26T18:22:44.514554: step 22321, loss 0, acc 1\n",
      "2018-10-26T18:22:44.697069: step 22322, loss 8.11408e-06, acc 1\n",
      "2018-10-26T18:22:44.909498: step 22323, loss 2.0432e-06, acc 1\n",
      "2018-10-26T18:22:45.086027: step 22324, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:22:45.263552: step 22325, loss 1.08961e-06, acc 1\n",
      "2018-10-26T18:22:45.433099: step 22326, loss 8.19563e-08, acc 1\n",
      "2018-10-26T18:22:45.657499: step 22327, loss 0.00258204, acc 1\n",
      "2018-10-26T18:22:45.825052: step 22328, loss 0.079974, acc 0.984375\n",
      "2018-10-26T18:22:46.029505: step 22329, loss 3.08189e-05, acc 1\n",
      "2018-10-26T18:22:46.219996: step 22330, loss 5.81135e-07, acc 1\n",
      "2018-10-26T18:22:46.443400: step 22331, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:22:46.656829: step 22332, loss 1.23631e-05, acc 1\n",
      "2018-10-26T18:22:46.838344: step 22333, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:22:47.063742: step 22334, loss 0.000213338, acc 1\n",
      "2018-10-26T18:22:47.237278: step 22335, loss 0, acc 1\n",
      "2018-10-26T18:22:47.448713: step 22336, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:22:47.620256: step 22337, loss 2.77533e-07, acc 1\n",
      "2018-10-26T18:22:47.798778: step 22338, loss 4.90916e-06, acc 1\n",
      "2018-10-26T18:22:48.014202: step 22339, loss 0.00766741, acc 1\n",
      "2018-10-26T18:22:48.192726: step 22340, loss 4.74971e-07, acc 1\n",
      "2018-10-26T18:22:48.371248: step 22341, loss 1.46397e-06, acc 1\n",
      "2018-10-26T18:22:48.583681: step 22342, loss 0, acc 1\n",
      "2018-10-26T18:22:48.760210: step 22343, loss 1.25165e-06, acc 1\n",
      "2018-10-26T18:22:48.940728: step 22344, loss 0.00183847, acc 1\n",
      "2018-10-26T18:22:49.145181: step 22345, loss 0.000144224, acc 1\n",
      "2018-10-26T18:22:49.328691: step 22346, loss 5.58694e-06, acc 1\n",
      "2018-10-26T18:22:49.506216: step 22347, loss 0.000857088, acc 1\n",
      "2018-10-26T18:22:49.686734: step 22348, loss 2.62616e-06, acc 1\n",
      "2018-10-26T18:22:49.893182: step 22349, loss 1.93714e-07, acc 1\n",
      "2018-10-26T18:22:50.071706: step 22350, loss 1.96985e-05, acc 1\n",
      "2018-10-26T18:22:50.253221: step 22351, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:22:50.424762: step 22352, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:22:50.640186: step 22353, loss 1.34664e-06, acc 1\n",
      "2018-10-26T18:22:50.827685: step 22354, loss 0, acc 1\n",
      "2018-10-26T18:22:51.012193: step 22355, loss 0.000905934, acc 1\n",
      "2018-10-26T18:22:51.200689: step 22356, loss 9.23852e-07, acc 1\n",
      "2018-10-26T18:22:51.380209: step 22357, loss 0.07774, acc 0.984375\n",
      "2018-10-26T18:22:51.563719: step 22358, loss 4.97807e-06, acc 1\n",
      "2018-10-26T18:22:51.730274: step 22359, loss 0.0136452, acc 0.984375\n",
      "2018-10-26T18:22:51.913784: step 22360, loss 7.93099e-06, acc 1\n",
      "2018-10-26T18:22:52.081336: step 22361, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:22:52.262851: step 22362, loss 3.81838e-07, acc 1\n",
      "2018-10-26T18:22:52.429406: step 22363, loss 5.94629e-06, acc 1\n",
      "2018-10-26T18:22:52.607928: step 22364, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:22:52.775480: step 22365, loss 1.08962e-06, acc 1\n",
      "2018-10-26T18:22:52.955002: step 22366, loss 0.000113586, acc 1\n",
      "2018-10-26T18:22:53.130533: step 22367, loss 4.72485e-06, acc 1\n",
      "2018-10-26T18:22:53.312047: step 22368, loss 7.45257e-06, acc 1\n",
      "2018-10-26T18:22:53.476608: step 22369, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:22:53.656128: step 22370, loss 3.73605e-06, acc 1\n",
      "2018-10-26T18:22:53.820688: step 22371, loss 3.2514e-05, acc 1\n",
      "2018-10-26T18:22:54.001207: step 22372, loss 2.56596e-05, acc 1\n",
      "2018-10-26T18:22:54.171751: step 22373, loss 0.000788348, acc 1\n",
      "2018-10-26T18:22:54.352269: step 22374, loss 2.42498e-06, acc 1\n",
      "2018-10-26T18:22:54.519820: step 22375, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:22:54.701336: step 22376, loss 7.99646e-06, acc 1\n",
      "2018-10-26T18:22:54.863901: step 22377, loss 5.14192e-06, acc 1\n",
      "2018-10-26T18:22:55.046413: step 22378, loss 0.0011129, acc 1\n",
      "2018-10-26T18:22:55.211971: step 22379, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:55.394484: step 22380, loss 1.36381e-05, acc 1\n",
      "2018-10-26T18:22:55.565028: step 22381, loss 0.00524065, acc 1\n",
      "2018-10-26T18:22:55.743551: step 22382, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:22:55.912101: step 22383, loss 0.0017349, acc 1\n",
      "2018-10-26T18:22:56.097605: step 22384, loss 1.43791e-06, acc 1\n",
      "2018-10-26T18:22:56.262166: step 22385, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:22:56.446672: step 22386, loss 0.000167512, acc 1\n",
      "2018-10-26T18:22:56.619211: step 22387, loss 2.30967e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:22:56.797734: step 22388, loss 2.57956e-06, acc 1\n",
      "2018-10-26T18:22:56.972268: step 22389, loss 0.00020683, acc 1\n",
      "2018-10-26T18:22:57.152786: step 22390, loss 5.33409e-06, acc 1\n",
      "2018-10-26T18:22:57.317346: step 22391, loss 5.23364e-05, acc 1\n",
      "2018-10-26T18:22:57.501854: step 22392, loss 1.22747e-06, acc 1\n",
      "2018-10-26T18:22:57.674392: step 22393, loss 0.000441481, acc 1\n",
      "2018-10-26T18:22:57.857903: step 22394, loss 7.00822e-05, acc 1\n",
      "2018-10-26T18:22:58.030442: step 22395, loss 2.26148e-05, acc 1\n",
      "2018-10-26T18:22:58.200986: step 22396, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:22:58.370532: step 22397, loss 2.10133e-05, acc 1\n",
      "2018-10-26T18:22:58.547061: step 22398, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:22:58.723589: step 22399, loss 7.55121e-06, acc 1\n",
      "2018-10-26T18:22:58.896128: step 22400, loss 1.77314e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:22:59.355899: step 22400, loss 7.80665, acc 0.733584\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22400\n",
      "\n",
      "2018-10-26T18:22:59.702916: step 22401, loss 0, acc 1\n",
      "2018-10-26T18:22:59.872462: step 22402, loss 3.22235e-07, acc 1\n",
      "2018-10-26T18:23:00.058962: step 22403, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:23:00.227514: step 22404, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:23:00.422990: step 22405, loss 0.000541591, acc 1\n",
      "2018-10-26T18:23:00.653376: step 22406, loss 3.70663e-07, acc 1\n",
      "2018-10-26T18:23:00.822922: step 22407, loss 0.00486448, acc 1\n",
      "2018-10-26T18:23:01.005435: step 22408, loss 9.35342e-06, acc 1\n",
      "2018-10-26T18:23:01.175978: step 22409, loss 2.1234e-07, acc 1\n",
      "2018-10-26T18:23:01.347520: step 22410, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:01.514075: step 22411, loss 9.68574e-08, acc 1\n",
      "2018-10-26T18:23:01.694592: step 22412, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:23:01.862145: step 22413, loss 0.000952467, acc 1\n",
      "2018-10-26T18:23:02.042663: step 22414, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:23:02.205228: step 22415, loss 1.78621e-06, acc 1\n",
      "2018-10-26T18:23:02.383752: step 22416, loss 1.10062e-05, acc 1\n",
      "2018-10-26T18:23:02.551304: step 22417, loss 4.90937e-06, acc 1\n",
      "2018-10-26T18:23:02.725837: step 22418, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:23:02.893390: step 22419, loss 2.5518e-07, acc 1\n",
      "2018-10-26T18:23:03.070916: step 22420, loss 0.000837072, acc 1\n",
      "2018-10-26T18:23:03.236473: step 22421, loss 1.0859e-06, acc 1\n",
      "2018-10-26T18:23:03.411006: step 22422, loss 0.000104295, acc 1\n",
      "2018-10-26T18:23:03.579556: step 22423, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:03.758082: step 22424, loss 8.45629e-07, acc 1\n",
      "2018-10-26T18:23:03.927627: step 22425, loss 3.59486e-07, acc 1\n",
      "2018-10-26T18:23:04.101162: step 22426, loss 9.87199e-08, acc 1\n",
      "2018-10-26T18:23:04.273702: step 22427, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:23:04.448235: step 22428, loss 2.81257e-07, acc 1\n",
      "2018-10-26T18:23:04.625761: step 22429, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:23:04.799297: step 22430, loss 5.90448e-07, acc 1\n",
      "2018-10-26T18:23:04.975825: step 22431, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:23:05.155346: step 22432, loss 4.94667e-06, acc 1\n",
      "2018-10-26T18:23:05.323896: step 22433, loss 0.000357489, acc 1\n",
      "2018-10-26T18:23:05.496434: step 22434, loss 5.17811e-07, acc 1\n",
      "2018-10-26T18:23:05.663987: step 22435, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:05.838521: step 22436, loss 0.000391237, acc 1\n",
      "2018-10-26T18:23:06.003081: step 22437, loss 4.42324e-06, acc 1\n",
      "2018-10-26T18:23:06.184597: step 22438, loss 0, acc 1\n",
      "2018-10-26T18:23:06.348159: step 22439, loss 2.44005e-07, acc 1\n",
      "2018-10-26T18:23:06.521696: step 22440, loss 0.000165482, acc 1\n",
      "2018-10-26T18:23:06.703211: step 22441, loss 3.2146e-06, acc 1\n",
      "2018-10-26T18:23:06.882730: step 22442, loss 0, acc 1\n",
      "2018-10-26T18:23:07.056267: step 22443, loss 1.85695e-06, acc 1\n",
      "2018-10-26T18:23:07.234791: step 22444, loss 0.00131703, acc 1\n",
      "2018-10-26T18:23:07.409323: step 22445, loss 2.97066e-06, acc 1\n",
      "2018-10-26T18:23:07.577874: step 22446, loss 6.82138e-06, acc 1\n",
      "2018-10-26T18:23:07.761383: step 22447, loss 8.01181e-06, acc 1\n",
      "2018-10-26T18:23:07.935917: step 22448, loss 3.94876e-07, acc 1\n",
      "2018-10-26T18:23:08.110450: step 22449, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:23:08.280995: step 22450, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:23:08.457523: step 22451, loss 0, acc 1\n",
      "2018-10-26T18:23:08.626073: step 22452, loss 7.25532e-06, acc 1\n",
      "2018-10-26T18:23:08.810581: step 22453, loss 1.08589e-06, acc 1\n",
      "2018-10-26T18:23:08.977134: step 22454, loss 0.00111657, acc 1\n",
      "2018-10-26T18:23:09.156655: step 22455, loss 0.000161535, acc 1\n",
      "2018-10-26T18:23:09.323210: step 22456, loss 7.04065e-07, acc 1\n",
      "2018-10-26T18:23:09.503727: step 22457, loss 5.45746e-07, acc 1\n",
      "2018-10-26T18:23:09.677264: step 22458, loss 2.42143e-07, acc 1\n",
      "2018-10-26T18:23:09.851798: step 22459, loss 1.62171e-05, acc 1\n",
      "2018-10-26T18:23:10.023340: step 22460, loss 0.000233673, acc 1\n",
      "2018-10-26T18:23:10.203857: step 22461, loss 5.30251e-05, acc 1\n",
      "2018-10-26T18:23:10.371410: step 22462, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:23:10.547938: step 22463, loss 2.14376e-06, acc 1\n",
      "2018-10-26T18:23:10.718481: step 22464, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:23:10.900995: step 22465, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:23:11.066552: step 22466, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:23:11.252057: step 22467, loss 0.0935007, acc 0.984375\n",
      "2018-10-26T18:23:11.422602: step 22468, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:23:11.602121: step 22469, loss 1.91096e-06, acc 1\n",
      "2018-10-26T18:23:11.770671: step 22470, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:23:11.952186: step 22471, loss 8.77281e-07, acc 1\n",
      "2018-10-26T18:23:12.127716: step 22472, loss 3.70254e-06, acc 1\n",
      "2018-10-26T18:23:12.314242: step 22473, loss 0.00175275, acc 1\n",
      "2018-10-26T18:23:12.482769: step 22474, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:23:12.657302: step 22475, loss 1.95381e-06, acc 1\n",
      "2018-10-26T18:23:12.824854: step 22476, loss 0.0444983, acc 0.984375\n",
      "2018-10-26T18:23:13.007367: step 22477, loss 3.18298e-05, acc 1\n",
      "2018-10-26T18:23:13.177911: step 22478, loss 4.58888e-06, acc 1\n",
      "2018-10-26T18:23:13.354440: step 22479, loss 4.04329e-06, acc 1\n",
      "2018-10-26T18:23:13.519997: step 22480, loss 0.000121474, acc 1\n",
      "2018-10-26T18:23:13.700515: step 22481, loss 0.000132057, acc 1\n",
      "2018-10-26T18:23:13.874075: step 22482, loss 0, acc 1\n",
      "2018-10-26T18:23:14.052574: step 22483, loss 0.000115973, acc 1\n",
      "2018-10-26T18:23:14.232111: step 22484, loss 3.79975e-07, acc 1\n",
      "2018-10-26T18:23:14.408623: step 22485, loss 0.0053487, acc 1\n",
      "2018-10-26T18:23:14.590138: step 22486, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:23:14.759684: step 22487, loss 4.01535e-06, acc 1\n",
      "2018-10-26T18:23:14.944192: step 22488, loss 0, acc 1\n",
      "2018-10-26T18:23:15.116731: step 22489, loss 4.1755e-05, acc 1\n",
      "2018-10-26T18:23:15.292262: step 22490, loss 0.000351305, acc 1\n",
      "2018-10-26T18:23:15.461809: step 22491, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:23:15.641329: step 22492, loss 3.67999e-05, acc 1\n",
      "2018-10-26T18:23:15.809878: step 22493, loss 6.79101e-05, acc 1\n",
      "2018-10-26T18:23:15.990397: step 22494, loss 1.7234e-05, acc 1\n",
      "2018-10-26T18:23:16.157950: step 22495, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:23:16.337469: step 22496, loss 2.49593e-07, acc 1\n",
      "2018-10-26T18:23:16.523971: step 22497, loss 0.000290173, acc 1\n",
      "2018-10-26T18:23:16.713464: step 22498, loss 0.000163036, acc 1\n",
      "2018-10-26T18:23:16.882014: step 22499, loss 5.97263e-06, acc 1\n",
      "2018-10-26T18:23:17.059540: step 22500, loss 9.33804e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:23:17.524298: step 22500, loss 7.81151, acc 0.738274\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22500\n",
      "\n",
      "2018-10-26T18:23:17.898972: step 22501, loss 0, acc 1\n",
      "2018-10-26T18:23:18.083620: step 22502, loss 1.29452e-06, acc 1\n",
      "2018-10-26T18:23:18.245188: step 22503, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:18.429696: step 22504, loss 0.000281468, acc 1\n",
      "2018-10-26T18:23:18.618192: step 22505, loss 0, acc 1\n",
      "2018-10-26T18:23:18.866528: step 22506, loss 0.00141336, acc 1\n",
      "2018-10-26T18:23:19.042060: step 22507, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:23:19.231553: step 22508, loss 1.48346e-05, acc 1\n",
      "2018-10-26T18:23:19.412088: step 22509, loss 0.000187598, acc 1\n",
      "2018-10-26T18:23:19.593586: step 22510, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:23:19.763133: step 22511, loss 4.78068e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:23:19.948645: step 22512, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:23:20.116190: step 22513, loss 7.75041e-06, acc 1\n",
      "2018-10-26T18:23:20.288729: step 22514, loss 0, acc 1\n",
      "2018-10-26T18:23:20.457278: step 22515, loss 1.06353e-06, acc 1\n",
      "2018-10-26T18:23:20.633806: step 22516, loss 3.20103e-05, acc 1\n",
      "2018-10-26T18:23:20.807344: step 22517, loss 0, acc 1\n",
      "2018-10-26T18:23:20.985866: step 22518, loss 5.56325e-06, acc 1\n",
      "2018-10-26T18:23:21.158404: step 22519, loss 1.3634e-06, acc 1\n",
      "2018-10-26T18:23:21.335931: step 22520, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:23:21.500492: step 22521, loss 1.16971e-06, acc 1\n",
      "2018-10-26T18:23:21.674028: step 22522, loss 9.88387e-05, acc 1\n",
      "2018-10-26T18:23:21.842576: step 22523, loss 1.11423e-05, acc 1\n",
      "2018-10-26T18:23:22.027085: step 22524, loss 5.45746e-07, acc 1\n",
      "2018-10-26T18:23:22.195634: step 22525, loss 8.75442e-08, acc 1\n",
      "2018-10-26T18:23:22.368173: step 22526, loss 0.000963999, acc 1\n",
      "2018-10-26T18:23:22.536723: step 22527, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:23:22.709262: step 22528, loss 3.51651e-05, acc 1\n",
      "2018-10-26T18:23:22.874818: step 22529, loss 6.47518e-06, acc 1\n",
      "2018-10-26T18:23:23.050350: step 22530, loss 0.000112108, acc 1\n",
      "2018-10-26T18:23:23.212915: step 22531, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:23:23.395428: step 22532, loss 5.2712e-07, acc 1\n",
      "2018-10-26T18:23:23.562981: step 22533, loss 0, acc 1\n",
      "2018-10-26T18:23:23.736516: step 22534, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:23:23.912068: step 22535, loss 0.0001203, acc 1\n",
      "2018-10-26T18:23:24.089574: step 22536, loss 6.43974e-06, acc 1\n",
      "2018-10-26T18:23:24.264107: step 22537, loss 1.12947e-05, acc 1\n",
      "2018-10-26T18:23:24.448615: step 22538, loss 4.15366e-07, acc 1\n",
      "2018-10-26T18:23:24.618160: step 22539, loss 0.000195913, acc 1\n",
      "2018-10-26T18:23:24.793693: step 22540, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:24.965234: step 22541, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:23:25.138769: step 22542, loss 0, acc 1\n",
      "2018-10-26T18:23:25.306323: step 22543, loss 2.50328e-06, acc 1\n",
      "2018-10-26T18:23:25.480857: step 22544, loss 0, acc 1\n",
      "2018-10-26T18:23:25.647410: step 22545, loss 4.17735e-06, acc 1\n",
      "2018-10-26T18:23:25.822942: step 22546, loss 1.24656e-05, acc 1\n",
      "2018-10-26T18:23:25.991492: step 22547, loss 2.25926e-06, acc 1\n",
      "2018-10-26T18:23:26.168020: step 22548, loss 0, acc 1\n",
      "2018-10-26T18:23:26.337568: step 22549, loss 2.1151e-05, acc 1\n",
      "2018-10-26T18:23:26.523072: step 22550, loss 0.00938348, acc 1\n",
      "2018-10-26T18:23:26.696608: step 22551, loss 0, acc 1\n",
      "2018-10-26T18:23:26.875130: step 22552, loss 2.79395e-07, acc 1\n",
      "2018-10-26T18:23:27.049664: step 22553, loss 1.11758e-07, acc 1\n",
      "2018-10-26T18:23:27.222203: step 22554, loss 0.00078315, acc 1\n",
      "2018-10-26T18:23:27.398732: step 22555, loss 1.90723e-06, acc 1\n",
      "2018-10-26T18:23:27.569276: step 22556, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:23:27.751789: step 22557, loss 0.000179024, acc 1\n",
      "2018-10-26T18:23:27.919341: step 22558, loss 3.41386e-06, acc 1\n",
      "2018-10-26T18:23:28.095869: step 22559, loss 2.79186e-06, acc 1\n",
      "2018-10-26T18:23:28.264419: step 22560, loss 7.89751e-07, acc 1\n",
      "2018-10-26T18:23:28.440946: step 22561, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:23:28.612488: step 22562, loss 6.07213e-07, acc 1\n",
      "2018-10-26T18:23:28.797993: step 22563, loss 1.22254e-05, acc 1\n",
      "2018-10-26T18:23:28.974522: step 22564, loss 3.07333e-07, acc 1\n",
      "2018-10-26T18:23:29.152048: step 22565, loss 0, acc 1\n",
      "2018-10-26T18:23:29.326581: step 22566, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:23:29.502112: step 22567, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:23:29.678640: step 22568, loss 8.62272e-06, acc 1\n",
      "2018-10-26T18:23:29.852176: step 22569, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:23:30.098518: step 22570, loss 2.53989e-05, acc 1\n",
      "2018-10-26T18:23:30.288014: step 22571, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:23:30.463543: step 22572, loss 7.72979e-07, acc 1\n",
      "2018-10-26T18:23:30.632093: step 22573, loss 0, acc 1\n",
      "2018-10-26T18:23:30.818594: step 22574, loss 2.96158e-07, acc 1\n",
      "2018-10-26T18:23:30.992131: step 22575, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:31.166664: step 22576, loss 0.0796891, acc 0.984375\n",
      "2018-10-26T18:23:31.335214: step 22577, loss 1.58317e-06, acc 1\n",
      "2018-10-26T18:23:31.507753: step 22578, loss 2.16066e-07, acc 1\n",
      "2018-10-26T18:23:31.675305: step 22579, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:23:31.849839: step 22580, loss 1.68188e-06, acc 1\n",
      "2018-10-26T18:23:32.019386: step 22581, loss 0.0017884, acc 1\n",
      "2018-10-26T18:23:32.195914: step 22582, loss 1.20716e-05, acc 1\n",
      "2018-10-26T18:23:32.368453: step 22583, loss 2.9802e-07, acc 1\n",
      "2018-10-26T18:23:32.547974: step 22584, loss 6.35155e-07, acc 1\n",
      "2018-10-26T18:23:32.716524: step 22585, loss 3.49078e-05, acc 1\n",
      "2018-10-26T18:23:32.893052: step 22586, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:23:33.060604: step 22587, loss 0, acc 1\n",
      "2018-10-26T18:23:33.235137: step 22588, loss 1.31311e-06, acc 1\n",
      "2018-10-26T18:23:33.400695: step 22589, loss 7.17728e-06, acc 1\n",
      "2018-10-26T18:23:33.574232: step 22590, loss 1.57575e-06, acc 1\n",
      "2018-10-26T18:23:33.749763: step 22591, loss 0, acc 1\n",
      "2018-10-26T18:23:33.933733: step 22592, loss 9.49947e-08, acc 1\n",
      "2018-10-26T18:23:34.105275: step 22593, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:23:34.284795: step 22594, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:23:34.455340: step 22595, loss 6.43225e-06, acc 1\n",
      "2018-10-26T18:23:34.638849: step 22596, loss 8.30718e-07, acc 1\n",
      "2018-10-26T18:23:34.810390: step 22597, loss 2.70081e-07, acc 1\n",
      "2018-10-26T18:23:34.989911: step 22598, loss 1.46666e-05, acc 1\n",
      "2018-10-26T18:23:35.158460: step 22599, loss 0, acc 1\n",
      "2018-10-26T18:23:35.332994: step 22600, loss 6.51912e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:23:35.792766: step 22600, loss 8.38408, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22600\n",
      "\n",
      "2018-10-26T18:23:36.164883: step 22601, loss 1.29076e-06, acc 1\n",
      "2018-10-26T18:23:36.348393: step 22602, loss 0.0162089, acc 0.984375\n",
      "2018-10-26T18:23:36.520932: step 22603, loss 8.75442e-08, acc 1\n",
      "2018-10-26T18:23:36.697460: step 22604, loss 1.60184e-06, acc 1\n",
      "2018-10-26T18:23:36.883962: step 22605, loss 4.88771e-05, acc 1\n",
      "2018-10-26T18:23:37.128309: step 22606, loss 0.000872173, acc 1\n",
      "2018-10-26T18:23:37.298853: step 22607, loss 1.33224e-05, acc 1\n",
      "2018-10-26T18:23:37.483360: step 22608, loss 2.4883e-06, acc 1\n",
      "2018-10-26T18:23:37.653904: step 22609, loss 9.70408e-07, acc 1\n",
      "2018-10-26T18:23:37.837414: step 22610, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:23:38.006961: step 22611, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:23:38.181496: step 22612, loss 9.04986e-06, acc 1\n",
      "2018-10-26T18:23:38.348050: step 22613, loss 4.6014e-05, acc 1\n",
      "2018-10-26T18:23:38.530563: step 22614, loss 6.05767e-05, acc 1\n",
      "2018-10-26T18:23:38.696149: step 22615, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:23:38.882627: step 22616, loss 0.000488543, acc 1\n",
      "2018-10-26T18:23:39.054163: step 22617, loss 1.72659e-06, acc 1\n",
      "2018-10-26T18:23:39.230692: step 22618, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:39.397247: step 22619, loss 0.000132443, acc 1\n",
      "2018-10-26T18:23:39.575770: step 22620, loss 1.7359e-06, acc 1\n",
      "2018-10-26T18:23:39.744320: step 22621, loss 3.09925e-06, acc 1\n",
      "2018-10-26T18:23:39.930823: step 22622, loss 2.25379e-07, acc 1\n",
      "2018-10-26T18:23:40.095381: step 22623, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:23:40.271910: step 22624, loss 5.99846e-06, acc 1\n",
      "2018-10-26T18:23:40.446443: step 22625, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:23:40.631948: step 22626, loss 0, acc 1\n",
      "2018-10-26T18:23:40.797505: step 22627, loss 0.00062921, acc 1\n",
      "2018-10-26T18:23:40.977026: step 22628, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:23:41.147570: step 22629, loss 1.15108e-06, acc 1\n",
      "2018-10-26T18:23:41.327103: step 22630, loss 2.26535e-05, acc 1\n",
      "2018-10-26T18:23:41.491651: step 22631, loss 8.87673e-06, acc 1\n",
      "2018-10-26T18:23:41.676157: step 22632, loss 7.42105e-06, acc 1\n",
      "2018-10-26T18:23:41.844707: step 22633, loss 3.72525e-07, acc 1\n",
      "2018-10-26T18:23:42.033204: step 22634, loss 0, acc 1\n",
      "2018-10-26T18:23:42.210729: step 22635, loss 0, acc 1\n",
      "2018-10-26T18:23:42.429146: step 22636, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:23:42.619637: step 22637, loss 0.000294986, acc 1\n",
      "2018-10-26T18:23:42.818106: step 22638, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:43.038518: step 22639, loss 5.55064e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:23:43.215046: step 22640, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:23:43.453410: step 22641, loss 3.5616e-05, acc 1\n",
      "2018-10-26T18:23:43.657865: step 22642, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:23:43.844365: step 22643, loss 5.42494e-06, acc 1\n",
      "2018-10-26T18:23:44.074749: step 22644, loss 1.47458e-05, acc 1\n",
      "2018-10-26T18:23:44.250280: step 22645, loss 1.11758e-07, acc 1\n",
      "2018-10-26T18:23:44.467701: step 22646, loss 5.02324e-06, acc 1\n",
      "2018-10-26T18:23:44.659205: step 22647, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:23:44.833722: step 22648, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:23:45.045183: step 22649, loss 6.7012e-05, acc 1\n",
      "2018-10-26T18:23:45.247617: step 22650, loss 7.25175e-07, acc 1\n",
      "2018-10-26T18:23:45.424143: step 22651, loss 4.28406e-07, acc 1\n",
      "2018-10-26T18:23:45.636576: step 22652, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:23:45.810112: step 22653, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:23:46.022546: step 22654, loss 9.44336e-07, acc 1\n",
      "2018-10-26T18:23:46.251932: step 22655, loss 3.88695e-05, acc 1\n",
      "2018-10-26T18:23:46.427462: step 22656, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:23:46.643884: step 22657, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:46.847342: step 22658, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:23:47.073736: step 22659, loss 0.000271419, acc 1\n",
      "2018-10-26T18:23:47.265226: step 22660, loss 0.0587928, acc 0.984375\n",
      "2018-10-26T18:23:47.436766: step 22661, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:23:47.610302: step 22662, loss 9.61095e-07, acc 1\n",
      "2018-10-26T18:23:47.812763: step 22663, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:23:48.028187: step 22664, loss 1.7303e-06, acc 1\n",
      "2018-10-26T18:23:48.240618: step 22665, loss 2.36725e-06, acc 1\n",
      "2018-10-26T18:23:48.426123: step 22666, loss 4.24677e-07, acc 1\n",
      "2018-10-26T18:23:48.630578: step 22667, loss 0.0049143, acc 1\n",
      "2018-10-26T18:23:48.803115: step 22668, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:23:49.029511: step 22669, loss 0.0020885, acc 1\n",
      "2018-10-26T18:23:49.196065: step 22670, loss 1.17751e-05, acc 1\n",
      "2018-10-26T18:23:49.370600: step 22671, loss 1.02789e-05, acc 1\n",
      "2018-10-26T18:23:49.581038: step 22672, loss 1.42113e-06, acc 1\n",
      "2018-10-26T18:23:49.753576: step 22673, loss 8.8846e-07, acc 1\n",
      "2018-10-26T18:23:49.929108: step 22674, loss 0.0620511, acc 0.984375\n",
      "2018-10-26T18:23:50.129572: step 22675, loss 5.98409e-06, acc 1\n",
      "2018-10-26T18:23:50.306100: step 22676, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:23:50.484624: step 22677, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:23:50.653172: step 22678, loss 1.20355e-05, acc 1\n",
      "2018-10-26T18:23:50.853637: step 22679, loss 4.71372e-06, acc 1\n",
      "2018-10-26T18:23:51.030165: step 22680, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:51.208688: step 22681, loss 1.89989e-07, acc 1\n",
      "2018-10-26T18:23:51.387213: step 22682, loss 0.000267839, acc 1\n",
      "2018-10-26T18:23:51.599644: step 22683, loss 7.525e-07, acc 1\n",
      "2018-10-26T18:23:51.779164: step 22684, loss 1.53369e-05, acc 1\n",
      "2018-10-26T18:23:51.964669: step 22685, loss 1.6205e-07, acc 1\n",
      "2018-10-26T18:23:52.178100: step 22686, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:52.391529: step 22687, loss 0.000323619, acc 1\n",
      "2018-10-26T18:23:52.595983: step 22688, loss 2.77585e-05, acc 1\n",
      "2018-10-26T18:23:52.830355: step 22689, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:23:53.000899: step 22690, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:53.219316: step 22691, loss 1.32989e-06, acc 1\n",
      "2018-10-26T18:23:53.386868: step 22692, loss 2.04515e-05, acc 1\n",
      "2018-10-26T18:23:53.592320: step 22693, loss 3.92439e-06, acc 1\n",
      "2018-10-26T18:23:53.766853: step 22694, loss 0.000297881, acc 1\n",
      "2018-10-26T18:23:53.947373: step 22695, loss 2.70081e-07, acc 1\n",
      "2018-10-26T18:23:54.147836: step 22696, loss 0.00278531, acc 1\n",
      "2018-10-26T18:23:54.325361: step 22697, loss 9.27322e-06, acc 1\n",
      "2018-10-26T18:23:54.508870: step 22698, loss 4.33588e-06, acc 1\n",
      "2018-10-26T18:23:54.714323: step 22699, loss 0.000655396, acc 1\n",
      "2018-10-26T18:23:54.887858: step 22700, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:23:55.363587: step 22700, loss 8.08168, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22700\n",
      "\n",
      "2018-10-26T18:23:55.711027: step 22701, loss 4.3275e-05, acc 1\n",
      "2018-10-26T18:23:55.882570: step 22702, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:23:56.069071: step 22703, loss 0, acc 1\n",
      "2018-10-26T18:23:56.241610: step 22704, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:23:56.439082: step 22705, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:23:56.702379: step 22706, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:23:56.880902: step 22707, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:23:57.070396: step 22708, loss 0.00039424, acc 1\n",
      "2018-10-26T18:23:57.233959: step 22709, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:23:57.414477: step 22710, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:23:57.593001: step 22711, loss 0.000167043, acc 1\n",
      "2018-10-26T18:23:57.772520: step 22712, loss 1.038e-05, acc 1\n",
      "2018-10-26T18:23:57.955033: step 22713, loss 4.13476e-06, acc 1\n",
      "2018-10-26T18:23:58.119593: step 22714, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:23:58.298116: step 22715, loss 8.56953e-05, acc 1\n",
      "2018-10-26T18:23:58.468660: step 22716, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:23:58.646190: step 22717, loss 1.01511e-06, acc 1\n",
      "2018-10-26T18:23:58.816730: step 22718, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:23:58.991265: step 22719, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:23:59.159814: step 22720, loss 0.000191373, acc 1\n",
      "2018-10-26T18:23:59.338337: step 22721, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:23:59.504892: step 22722, loss 8.912e-06, acc 1\n",
      "2018-10-26T18:23:59.690396: step 22723, loss 1.55896e-06, acc 1\n",
      "2018-10-26T18:23:59.859943: step 22724, loss 1.816e-06, acc 1\n",
      "2018-10-26T18:24:00.051432: step 22725, loss 5.96035e-07, acc 1\n",
      "2018-10-26T18:24:00.218984: step 22726, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:24:00.397507: step 22727, loss 1.00393e-06, acc 1\n",
      "2018-10-26T18:24:00.567054: step 22728, loss 1.88676e-06, acc 1\n",
      "2018-10-26T18:24:00.745577: step 22729, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:24:00.917118: step 22730, loss 2.38215e-06, acc 1\n",
      "2018-10-26T18:24:01.092649: step 22731, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:24:01.263194: step 22732, loss 0.000348502, acc 1\n",
      "2018-10-26T18:24:01.442715: step 22733, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:24:01.613258: step 22734, loss 7.74482e-06, acc 1\n",
      "2018-10-26T18:24:01.792779: step 22735, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:24:01.960331: step 22736, loss 3.33411e-07, acc 1\n",
      "2018-10-26T18:24:02.141846: step 22737, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:24:02.320369: step 22738, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:24:02.497895: step 22739, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:24:02.671431: step 22740, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:24:02.852328: step 22741, loss 2.66356e-07, acc 1\n",
      "2018-10-26T18:24:03.022872: step 22742, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:03.201395: step 22743, loss 4.51626e-06, acc 1\n",
      "2018-10-26T18:24:03.366952: step 22744, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:03.546473: step 22745, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:24:03.723001: step 22746, loss 7.47304e-06, acc 1\n",
      "2018-10-26T18:24:03.908506: step 22747, loss 1.52002e-05, acc 1\n",
      "2018-10-26T18:24:04.078052: step 22748, loss 5.81136e-07, acc 1\n",
      "2018-10-26T18:24:04.258571: step 22749, loss 1.73217e-06, acc 1\n",
      "2018-10-26T18:24:04.427142: step 22750, loss 0.000858035, acc 1\n",
      "2018-10-26T18:24:04.610630: step 22751, loss 3.54692e-05, acc 1\n",
      "2018-10-26T18:24:04.783169: step 22752, loss 0.000157097, acc 1\n",
      "2018-10-26T18:24:04.965680: step 22753, loss 1.72721e-05, acc 1\n",
      "2018-10-26T18:24:05.132236: step 22754, loss 9.37188e-06, acc 1\n",
      "2018-10-26T18:24:05.315746: step 22755, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:05.489283: step 22756, loss 0, acc 1\n",
      "2018-10-26T18:24:05.671795: step 22757, loss 0, acc 1\n",
      "2018-10-26T18:24:05.835357: step 22758, loss 0.000191371, acc 1\n",
      "2018-10-26T18:24:06.019865: step 22759, loss 1.09453e-05, acc 1\n",
      "2018-10-26T18:24:06.186419: step 22760, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:24:06.362947: step 22761, loss 5.87554e-06, acc 1\n",
      "2018-10-26T18:24:06.529502: step 22762, loss 0, acc 1\n",
      "2018-10-26T18:24:06.711018: step 22763, loss 0.000581325, acc 1\n",
      "2018-10-26T18:24:06.878570: step 22764, loss 0, acc 1\n",
      "2018-10-26T18:24:07.058090: step 22765, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:24:07.222651: step 22766, loss 5.92315e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:24:07.404166: step 22767, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:24:07.585681: step 22768, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:07.766198: step 22769, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:24:07.941730: step 22770, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:24:08.115266: step 22771, loss 1.08963e-06, acc 1\n",
      "2018-10-26T18:24:08.286807: step 22772, loss 4.84049e-05, acc 1\n",
      "2018-10-26T18:24:08.457352: step 22773, loss 2.8971e-05, acc 1\n",
      "2018-10-26T18:24:08.630889: step 22774, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:24:08.807417: step 22775, loss 0, acc 1\n",
      "2018-10-26T18:24:08.979956: step 22776, loss 4.52897e-05, acc 1\n",
      "2018-10-26T18:24:09.157481: step 22777, loss 0.000170189, acc 1\n",
      "2018-10-26T18:24:09.326031: step 22778, loss 0.000276434, acc 1\n",
      "2018-10-26T18:24:09.497572: step 22779, loss 0.000149331, acc 1\n",
      "2018-10-26T18:24:09.664127: step 22780, loss 9.25299e-05, acc 1\n",
      "2018-10-26T18:24:09.853649: step 22781, loss 0, acc 1\n",
      "2018-10-26T18:24:10.022200: step 22782, loss 2.99114e-06, acc 1\n",
      "2018-10-26T18:24:10.199725: step 22783, loss 7.45056e-08, acc 1\n",
      "2018-10-26T18:24:10.363288: step 22784, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:24:10.540814: step 22785, loss 9.49948e-08, acc 1\n",
      "2018-10-26T18:24:10.712355: step 22786, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:24:10.883897: step 22787, loss 2.17928e-07, acc 1\n",
      "2018-10-26T18:24:11.058431: step 22788, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:11.233961: step 22789, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:24:11.405503: step 22790, loss 0.000763086, acc 1\n",
      "2018-10-26T18:24:11.582032: step 22791, loss 2.38587e-06, acc 1\n",
      "2018-10-26T18:24:11.747589: step 22792, loss 3.1104e-06, acc 1\n",
      "2018-10-26T18:24:11.928106: step 22793, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:24:12.108625: step 22794, loss 0, acc 1\n",
      "2018-10-26T18:24:12.293151: step 22795, loss 6.92132e-05, acc 1\n",
      "2018-10-26T18:24:12.469659: step 22796, loss 0, acc 1\n",
      "2018-10-26T18:24:12.644194: step 22797, loss 0, acc 1\n",
      "2018-10-26T18:24:12.823716: step 22798, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:24:13.000243: step 22799, loss 4.44214e-06, acc 1\n",
      "2018-10-26T18:24:13.169789: step 22800, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:24:13.617593: step 22800, loss 8.02031, acc 0.725141\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22800\n",
      "\n",
      "2018-10-26T18:24:14.001699: step 22801, loss 4.36171e-06, acc 1\n",
      "2018-10-26T18:24:14.181220: step 22802, loss 3.10548e-05, acc 1\n",
      "2018-10-26T18:24:14.353759: step 22803, loss 8.38188e-08, acc 1\n",
      "2018-10-26T18:24:14.533279: step 22804, loss 0, acc 1\n",
      "2018-10-26T18:24:14.721775: step 22805, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:24:14.962133: step 22806, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:15.136667: step 22807, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:24:15.318182: step 22808, loss 4.68389e-06, acc 1\n",
      "2018-10-26T18:24:15.489724: step 22809, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:15.658273: step 22810, loss 0.0001574, acc 1\n",
      "2018-10-26T18:24:15.835809: step 22811, loss 0.000105773, acc 1\n",
      "2018-10-26T18:24:16.006344: step 22812, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:24:16.181874: step 22813, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:24:16.353416: step 22814, loss 0, acc 1\n",
      "2018-10-26T18:24:16.532936: step 22815, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:24:16.707471: step 22816, loss 0.0121275, acc 0.984375\n",
      "2018-10-26T18:24:16.882004: step 22817, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:24:17.045568: step 22818, loss 0.000816213, acc 1\n",
      "2018-10-26T18:24:17.222095: step 22819, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:24:17.393637: step 22820, loss 0.000139085, acc 1\n",
      "2018-10-26T18:24:17.567174: step 22821, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:24:17.733728: step 22822, loss 0.105466, acc 0.984375\n",
      "2018-10-26T18:24:17.912251: step 22823, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:24:18.080801: step 22824, loss 0.116537, acc 0.984375\n",
      "2018-10-26T18:24:18.262316: step 22825, loss 0.000679796, acc 1\n",
      "2018-10-26T18:24:18.428870: step 22826, loss 4.12919e-06, acc 1\n",
      "2018-10-26T18:24:18.611383: step 22827, loss 2.99884e-07, acc 1\n",
      "2018-10-26T18:24:18.778936: step 22828, loss 3.26498e-06, acc 1\n",
      "2018-10-26T18:24:18.966436: step 22829, loss 5.60651e-07, acc 1\n",
      "2018-10-26T18:24:19.136979: step 22830, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:24:19.319493: step 22831, loss 8.88524e-06, acc 1\n",
      "2018-10-26T18:24:19.487044: step 22832, loss 0.0377638, acc 0.984375\n",
      "2018-10-26T18:24:19.673545: step 22833, loss 4.2891e-05, acc 1\n",
      "2018-10-26T18:24:19.842096: step 22834, loss 0.00978761, acc 1\n",
      "2018-10-26T18:24:20.020618: step 22835, loss 0, acc 1\n",
      "2018-10-26T18:24:20.197146: step 22836, loss 0.00645071, acc 1\n",
      "2018-10-26T18:24:20.376666: step 22837, loss 6.12799e-07, acc 1\n",
      "2018-10-26T18:24:20.555204: step 22838, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:24:20.721744: step 22839, loss 3.71714e-05, acc 1\n",
      "2018-10-26T18:24:20.896278: step 22840, loss 0.00524472, acc 1\n",
      "2018-10-26T18:24:21.069815: step 22841, loss 0.00030029, acc 1\n",
      "2018-10-26T18:24:21.246356: step 22842, loss 0.0246987, acc 0.984375\n",
      "2018-10-26T18:24:21.412898: step 22843, loss 1.99291e-06, acc 1\n",
      "2018-10-26T18:24:21.590424: step 22844, loss 1.77527e-05, acc 1\n",
      "2018-10-26T18:24:21.758974: step 22845, loss 5.69357e-05, acc 1\n",
      "2018-10-26T18:24:21.933506: step 22846, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:24:22.102057: step 22847, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:24:22.276590: step 22848, loss 3.31548e-07, acc 1\n",
      "2018-10-26T18:24:22.445140: step 22849, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:24:22.626654: step 22850, loss 0, acc 1\n",
      "2018-10-26T18:24:22.801189: step 22851, loss 1.01146e-05, acc 1\n",
      "2018-10-26T18:24:22.974725: step 22852, loss 0.0136503, acc 0.984375\n",
      "2018-10-26T18:24:23.148262: step 22853, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:24:23.321804: step 22854, loss 4.31329e-06, acc 1\n",
      "2018-10-26T18:24:23.490348: step 22855, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:24:23.673857: step 22856, loss 5.0976e-06, acc 1\n",
      "2018-10-26T18:24:23.846396: step 22857, loss 4.10475e-05, acc 1\n",
      "2018-10-26T18:24:24.021927: step 22858, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:24:24.191474: step 22859, loss 2.73787e-06, acc 1\n",
      "2018-10-26T18:24:24.366009: step 22860, loss 1.74823e-05, acc 1\n",
      "2018-10-26T18:24:24.538546: step 22861, loss 0.00186224, acc 1\n",
      "2018-10-26T18:24:24.720061: step 22862, loss 5.02833e-06, acc 1\n",
      "2018-10-26T18:24:24.885621: step 22863, loss 3.82018e-05, acc 1\n",
      "2018-10-26T18:24:25.068132: step 22864, loss 0.0537053, acc 0.984375\n",
      "2018-10-26T18:24:25.242666: step 22865, loss 5.74337e-05, acc 1\n",
      "2018-10-26T18:24:25.424180: step 22866, loss 0.0987406, acc 0.984375\n",
      "2018-10-26T18:24:25.588741: step 22867, loss 1.46968e-05, acc 1\n",
      "2018-10-26T18:24:25.774245: step 22868, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:24:25.942794: step 22869, loss 1.14922e-06, acc 1\n",
      "2018-10-26T18:24:26.124309: step 22870, loss 5.81137e-07, acc 1\n",
      "2018-10-26T18:24:26.294854: step 22871, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:24:26.473377: step 22872, loss 8.38188e-08, acc 1\n",
      "2018-10-26T18:24:26.637938: step 22873, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:24:26.819452: step 22874, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:24:26.988003: step 22875, loss 0.0354727, acc 0.984375\n",
      "2018-10-26T18:24:27.178493: step 22876, loss 4.99183e-07, acc 1\n",
      "2018-10-26T18:24:27.358014: step 22877, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:27.538531: step 22878, loss 1.96869e-06, acc 1\n",
      "2018-10-26T18:24:27.723038: step 22879, loss 3.57624e-07, acc 1\n",
      "2018-10-26T18:24:27.889593: step 22880, loss 6.77994e-07, acc 1\n",
      "2018-10-26T18:24:28.076095: step 22881, loss 9.72744e-06, acc 1\n",
      "2018-10-26T18:24:28.243647: step 22882, loss 2.73807e-07, acc 1\n",
      "2018-10-26T18:24:28.431146: step 22883, loss 2.39148e-05, acc 1\n",
      "2018-10-26T18:24:28.600693: step 22884, loss 3.19531e-05, acc 1\n",
      "2018-10-26T18:24:28.831077: step 22885, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:24:29.026555: step 22886, loss 4.19416e-06, acc 1\n",
      "2018-10-26T18:24:29.226023: step 22887, loss 0, acc 1\n",
      "2018-10-26T18:24:29.421500: step 22888, loss 1.38017e-06, acc 1\n",
      "2018-10-26T18:24:29.604013: step 22889, loss 1.46772e-06, acc 1\n",
      "2018-10-26T18:24:29.786525: step 22890, loss 0.000174208, acc 1\n",
      "2018-10-26T18:24:29.993970: step 22891, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:24:30.171496: step 22892, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:24:30.362985: step 22893, loss 2.49592e-07, acc 1\n",
      "2018-10-26T18:24:30.532532: step 22894, loss 0.000486198, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:24:30.717039: step 22895, loss 1.0593e-05, acc 1\n",
      "2018-10-26T18:24:30.892570: step 22896, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:24:31.066105: step 22897, loss 2.76581e-06, acc 1\n",
      "2018-10-26T18:24:31.234656: step 22898, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:24:31.417168: step 22899, loss 2.6317e-06, acc 1\n",
      "2018-10-26T18:24:31.584721: step 22900, loss 4.89868e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:24:32.064438: step 22900, loss 8.16849, acc 0.727955\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-22900\n",
      "\n",
      "2018-10-26T18:24:32.424642: step 22901, loss 1.11572e-05, acc 1\n",
      "2018-10-26T18:24:32.598178: step 22902, loss 5.10357e-07, acc 1\n",
      "2018-10-26T18:24:32.764734: step 22903, loss 1.46987e-05, acc 1\n",
      "2018-10-26T18:24:32.944253: step 22904, loss 4.86144e-07, acc 1\n",
      "2018-10-26T18:24:33.122776: step 22905, loss 8.21405e-07, acc 1\n",
      "2018-10-26T18:24:33.363134: step 22906, loss 2.46222e-06, acc 1\n",
      "2018-10-26T18:24:33.534676: step 22907, loss 7.90927e-06, acc 1\n",
      "2018-10-26T18:24:33.715199: step 22908, loss 9.06161e-06, acc 1\n",
      "2018-10-26T18:24:33.882746: step 22909, loss 5.55063e-07, acc 1\n",
      "2018-10-26T18:24:34.062266: step 22910, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:24:34.231813: step 22911, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:24:34.404352: step 22912, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:24:34.567916: step 22913, loss 0.000842466, acc 1\n",
      "2018-10-26T18:24:34.744444: step 22914, loss 0.00743198, acc 1\n",
      "2018-10-26T18:24:34.911996: step 22915, loss 8.43931e-06, acc 1\n",
      "2018-10-26T18:24:35.084535: step 22916, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:24:35.258072: step 22917, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:24:35.434600: step 22918, loss 0.0522741, acc 0.984375\n",
      "2018-10-26T18:24:35.599161: step 22919, loss 8.3819e-08, acc 1\n",
      "2018-10-26T18:24:35.776686: step 22920, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:24:35.952217: step 22921, loss 0.00232032, acc 1\n",
      "2018-10-26T18:24:36.126750: step 22922, loss 1.07471e-06, acc 1\n",
      "2018-10-26T18:24:36.297294: step 22923, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:24:36.478809: step 22924, loss 8.06053e-06, acc 1\n",
      "2018-10-26T18:24:36.648356: step 22925, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:36.822891: step 22926, loss 2.97437e-06, acc 1\n",
      "2018-10-26T18:24:37.020363: step 22927, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:24:37.197890: step 22928, loss 6.0535e-07, acc 1\n",
      "2018-10-26T18:24:37.372422: step 22929, loss 5.21889e-06, acc 1\n",
      "2018-10-26T18:24:37.555932: step 22930, loss 1.51974e-05, acc 1\n",
      "2018-10-26T18:24:37.757394: step 22931, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:24:37.942897: step 22932, loss 1.9091e-06, acc 1\n",
      "2018-10-26T18:24:38.119426: step 22933, loss 4.89872e-07, acc 1\n",
      "2018-10-26T18:24:38.299944: step 22934, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:38.481459: step 22935, loss 2.35951e-05, acc 1\n",
      "2018-10-26T18:24:38.649012: step 22936, loss 0.00492744, acc 1\n",
      "2018-10-26T18:24:38.830526: step 22937, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:24:39.002068: step 22938, loss 7.18965e-07, acc 1\n",
      "2018-10-26T18:24:39.182585: step 22939, loss 0.000562774, acc 1\n",
      "2018-10-26T18:24:39.349140: step 22940, loss 0.00269888, acc 1\n",
      "2018-10-26T18:24:39.524672: step 22941, loss 1.41561e-07, acc 1\n",
      "2018-10-26T18:24:39.695215: step 22942, loss 2.00164e-05, acc 1\n",
      "2018-10-26T18:24:39.874736: step 22943, loss 1.82158e-06, acc 1\n",
      "2018-10-26T18:24:40.048273: step 22944, loss 0.000262392, acc 1\n",
      "2018-10-26T18:24:40.224801: step 22945, loss 2.6822e-07, acc 1\n",
      "2018-10-26T18:24:40.394348: step 22946, loss 1.61372e-05, acc 1\n",
      "2018-10-26T18:24:40.568881: step 22947, loss 2.14203e-07, acc 1\n",
      "2018-10-26T18:24:40.737431: step 22948, loss 2.96159e-07, acc 1\n",
      "2018-10-26T18:24:40.918946: step 22949, loss 7.63684e-08, acc 1\n",
      "2018-10-26T18:24:41.080515: step 22950, loss 1.27363e-05, acc 1\n",
      "2018-10-26T18:24:41.259037: step 22951, loss 1.22365e-05, acc 1\n",
      "2018-10-26T18:24:41.423598: step 22952, loss 7.07805e-08, acc 1\n",
      "2018-10-26T18:24:41.603119: step 22953, loss 0.1488, acc 0.984375\n",
      "2018-10-26T18:24:41.770670: step 22954, loss 2.42557e-05, acc 1\n",
      "2018-10-26T18:24:41.952186: step 22955, loss 3.73788e-06, acc 1\n",
      "2018-10-26T18:24:42.120736: step 22956, loss 2.30967e-07, acc 1\n",
      "2018-10-26T18:24:42.299276: step 22957, loss 2.65237e-05, acc 1\n",
      "2018-10-26T18:24:42.468806: step 22958, loss 1.82538e-07, acc 1\n",
      "2018-10-26T18:24:42.647328: step 22959, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:24:42.814880: step 22960, loss 2.48458e-06, acc 1\n",
      "2018-10-26T18:24:42.991409: step 22961, loss 1.12766e-05, acc 1\n",
      "2018-10-26T18:24:43.171927: step 22962, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:24:43.350449: step 22963, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:43.527976: step 22964, loss 6.22791e-06, acc 1\n",
      "2018-10-26T18:24:43.697523: step 22965, loss 0.000781591, acc 1\n",
      "2018-10-26T18:24:43.876045: step 22966, loss 0, acc 1\n",
      "2018-10-26T18:24:44.057560: step 22967, loss 6.41035e-06, acc 1\n",
      "2018-10-26T18:24:44.234101: step 22968, loss 0, acc 1\n",
      "2018-10-26T18:24:44.412612: step 22969, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:24:44.585151: step 22970, loss 2.84029e-06, acc 1\n",
      "2018-10-26T18:24:44.766666: step 22971, loss 1.4584e-06, acc 1\n",
      "2018-10-26T18:24:44.942197: step 22972, loss 0, acc 1\n",
      "2018-10-26T18:24:45.126704: step 22973, loss 0, acc 1\n",
      "2018-10-26T18:24:45.291264: step 22974, loss 1.0472e-05, acc 1\n",
      "2018-10-26T18:24:45.466796: step 22975, loss 5.33756e-06, acc 1\n",
      "2018-10-26T18:24:45.635344: step 22976, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:24:45.814866: step 22977, loss 0.0143555, acc 0.984375\n",
      "2018-10-26T18:24:45.986407: step 22978, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:46.164930: step 22979, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:24:46.332483: step 22980, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:24:46.509010: step 22981, loss 2.10477e-07, acc 1\n",
      "2018-10-26T18:24:46.675565: step 22982, loss 5.00971e-06, acc 1\n",
      "2018-10-26T18:24:46.855086: step 22983, loss 1.27688e-05, acc 1\n",
      "2018-10-26T18:24:47.020644: step 22984, loss 1.40276e-05, acc 1\n",
      "2018-10-26T18:24:47.201161: step 22985, loss 0.00034173, acc 1\n",
      "2018-10-26T18:24:47.366719: step 22986, loss 5.75882e-06, acc 1\n",
      "2018-10-26T18:24:47.553220: step 22987, loss 4.56345e-07, acc 1\n",
      "2018-10-26T18:24:47.720773: step 22988, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:24:47.905280: step 22989, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:24:48.120704: step 22990, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:24:48.300224: step 22991, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:24:48.497697: step 22992, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:24:48.693174: step 22993, loss 0.0019246, acc 1\n",
      "2018-10-26T18:24:48.879676: step 22994, loss 4.63924e-06, acc 1\n",
      "2018-10-26T18:24:49.107069: step 22995, loss 0.000864542, acc 1\n",
      "2018-10-26T18:24:49.310526: step 22996, loss 1.58654e-05, acc 1\n",
      "2018-10-26T18:24:49.502013: step 22997, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:24:49.736388: step 22998, loss 9.34559e-05, acc 1\n",
      "2018-10-26T18:24:49.946825: step 22999, loss 0, acc 1\n",
      "2018-10-26T18:24:50.157263: step 23000, loss 0.000111696, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:24:50.681860: step 23000, loss 8.76493, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23000\n",
      "\n",
      "2018-10-26T18:24:51.132089: step 23001, loss 1.27267e-05, acc 1\n",
      "2018-10-26T18:24:51.319587: step 23002, loss 0.000324802, acc 1\n",
      "2018-10-26T18:24:51.573908: step 23003, loss 0.000634361, acc 1\n",
      "2018-10-26T18:24:51.784346: step 23004, loss 3.07133e-06, acc 1\n",
      "2018-10-26T18:24:52.089531: step 23005, loss 0.000461806, acc 1\n",
      "2018-10-26T18:24:52.314929: step 23006, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:24:52.566257: step 23007, loss 0.000128743, acc 1\n",
      "2018-10-26T18:24:52.762733: step 23008, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:24:52.992119: step 23009, loss 0, acc 1\n",
      "2018-10-26T18:24:53.183608: step 23010, loss 0, acc 1\n",
      "2018-10-26T18:24:53.371106: step 23011, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:24:53.609471: step 23012, loss 0.000105759, acc 1\n",
      "2018-10-26T18:24:53.812927: step 23013, loss 0, acc 1\n",
      "2018-10-26T18:24:54.038324: step 23014, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:54.249759: step 23015, loss 0.0142237, acc 0.984375\n",
      "2018-10-26T18:24:54.435263: step 23016, loss 0.0174279, acc 0.984375\n",
      "2018-10-26T18:24:54.649690: step 23017, loss 0.0665908, acc 0.984375\n",
      "2018-10-26T18:24:54.829210: step 23018, loss 9.49946e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:24:55.009729: step 23019, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:24:55.222162: step 23020, loss 4.12543e-06, acc 1\n",
      "2018-10-26T18:24:55.396694: step 23021, loss 0.00025601, acc 1\n",
      "2018-10-26T18:24:55.569247: step 23022, loss 2.00409e-06, acc 1\n",
      "2018-10-26T18:24:55.777677: step 23023, loss 2.38417e-07, acc 1\n",
      "2018-10-26T18:24:55.958195: step 23024, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:24:56.141704: step 23025, loss 0.000212501, acc 1\n",
      "2018-10-26T18:24:56.306264: step 23026, loss 0, acc 1\n",
      "2018-10-26T18:24:56.517699: step 23027, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:24:56.685251: step 23028, loss 4.62799e-06, acc 1\n",
      "2018-10-26T18:24:56.863774: step 23029, loss 5.39079e-05, acc 1\n",
      "2018-10-26T18:24:57.045311: step 23030, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:24:57.270687: step 23031, loss 5.57204e-06, acc 1\n",
      "2018-10-26T18:24:57.439238: step 23032, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:24:57.610778: step 23033, loss 0.0013568, acc 1\n",
      "2018-10-26T18:24:57.777333: step 23034, loss 2.35421e-06, acc 1\n",
      "2018-10-26T18:24:57.991761: step 23035, loss 0, acc 1\n",
      "2018-10-26T18:24:58.191230: step 23036, loss 4.32843e-05, acc 1\n",
      "2018-10-26T18:24:58.386706: step 23037, loss 0.000240279, acc 1\n",
      "2018-10-26T18:24:58.583181: step 23038, loss 7.04069e-07, acc 1\n",
      "2018-10-26T18:24:58.768684: step 23039, loss 0, acc 1\n",
      "2018-10-26T18:24:58.960173: step 23040, loss 1.91851e-07, acc 1\n",
      "2018-10-26T18:24:59.153657: step 23041, loss 0.000448516, acc 1\n",
      "2018-10-26T18:24:59.324201: step 23042, loss 4.1723e-07, acc 1\n",
      "2018-10-26T18:24:59.516686: step 23043, loss 0, acc 1\n",
      "2018-10-26T18:24:59.691220: step 23044, loss 0.000137278, acc 1\n",
      "2018-10-26T18:24:59.893680: step 23045, loss 0.00694153, acc 1\n",
      "2018-10-26T18:25:00.077189: step 23046, loss 0.000178468, acc 1\n",
      "2018-10-26T18:25:00.252720: step 23047, loss 4.67864e-06, acc 1\n",
      "2018-10-26T18:25:00.459169: step 23048, loss 0, acc 1\n",
      "2018-10-26T18:25:00.633701: step 23049, loss 1.35595e-06, acc 1\n",
      "2018-10-26T18:25:00.810230: step 23050, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:25:01.022662: step 23051, loss 4.97249e-06, acc 1\n",
      "2018-10-26T18:25:01.194204: step 23052, loss 1.62264e-05, acc 1\n",
      "2018-10-26T18:25:01.377714: step 23053, loss 8.70886e-05, acc 1\n",
      "2018-10-26T18:25:01.545267: step 23054, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:25:01.759694: step 23055, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:25:01.933229: step 23056, loss 0.00532889, acc 1\n",
      "2018-10-26T18:25:02.121726: step 23057, loss 1.53477e-06, acc 1\n",
      "2018-10-26T18:25:02.287299: step 23058, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:25:02.501711: step 23059, loss 2.43537e-05, acc 1\n",
      "2018-10-26T18:25:02.675277: step 23060, loss 4.2654e-07, acc 1\n",
      "2018-10-26T18:25:02.849780: step 23061, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:25:03.025311: step 23062, loss 2.64494e-07, acc 1\n",
      "2018-10-26T18:25:03.209819: step 23063, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:25:03.385349: step 23064, loss 0, acc 1\n",
      "2018-10-26T18:25:03.555894: step 23065, loss 5.33985e-05, acc 1\n",
      "2018-10-26T18:25:03.727435: step 23066, loss 7.12113e-06, acc 1\n",
      "2018-10-26T18:25:03.899976: step 23067, loss 4.11544e-05, acc 1\n",
      "2018-10-26T18:25:04.075507: step 23068, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:25:04.242061: step 23069, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:25:04.416595: step 23070, loss 2.26005e-05, acc 1\n",
      "2018-10-26T18:25:04.587139: step 23071, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:25:04.763668: step 23072, loss 1.46026e-06, acc 1\n",
      "2018-10-26T18:25:04.941192: step 23073, loss 2.76031e-06, acc 1\n",
      "2018-10-26T18:25:05.119715: step 23074, loss 7.41157e-06, acc 1\n",
      "2018-10-26T18:25:05.296271: step 23075, loss 8.23613e-05, acc 1\n",
      "2018-10-26T18:25:05.469780: step 23076, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:25:05.637332: step 23077, loss 0.15648, acc 0.984375\n",
      "2018-10-26T18:25:05.812866: step 23078, loss 2.64006e-05, acc 1\n",
      "2018-10-26T18:25:05.982411: step 23079, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:25:06.157942: step 23080, loss 1.19205e-06, acc 1\n",
      "2018-10-26T18:25:06.333472: step 23081, loss 1.9147e-06, acc 1\n",
      "2018-10-26T18:25:06.518977: step 23082, loss 7.54363e-07, acc 1\n",
      "2018-10-26T18:25:06.695505: step 23083, loss 0.000193389, acc 1\n",
      "2018-10-26T18:25:06.871036: step 23084, loss 7.15278e-06, acc 1\n",
      "2018-10-26T18:25:07.038590: step 23085, loss 8.67381e-06, acc 1\n",
      "2018-10-26T18:25:07.220104: step 23086, loss 8.15823e-07, acc 1\n",
      "2018-10-26T18:25:07.387656: step 23087, loss 3.16647e-07, acc 1\n",
      "2018-10-26T18:25:07.579146: step 23088, loss 0.000148365, acc 1\n",
      "2018-10-26T18:25:07.745700: step 23089, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:25:07.918238: step 23090, loss 9.92773e-07, acc 1\n",
      "2018-10-26T18:25:08.090777: step 23091, loss 3.78262e-06, acc 1\n",
      "2018-10-26T18:25:08.266308: step 23092, loss 0.00980293, acc 1\n",
      "2018-10-26T18:25:08.439846: step 23093, loss 0.000558534, acc 1\n",
      "2018-10-26T18:25:08.619366: step 23094, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:08.785920: step 23095, loss 5.73032e-06, acc 1\n",
      "2018-10-26T18:25:08.965440: step 23096, loss 2.42142e-07, acc 1\n",
      "2018-10-26T18:25:09.130001: step 23097, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:09.305533: step 23098, loss 1.31313e-06, acc 1\n",
      "2018-10-26T18:25:09.474081: step 23099, loss 1.70423e-06, acc 1\n",
      "2018-10-26T18:25:09.651609: step 23100, loss 6.21365e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:25:10.105396: step 23100, loss 8.34506, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23100\n",
      "\n",
      "2018-10-26T18:25:10.484712: step 23101, loss 0, acc 1\n",
      "2018-10-26T18:25:10.666227: step 23102, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:25:10.837769: step 23103, loss 0.000377892, acc 1\n",
      "2018-10-26T18:25:11.016293: step 23104, loss 0, acc 1\n",
      "2018-10-26T18:25:11.214763: step 23105, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:25:11.461103: step 23106, loss 4.60067e-07, acc 1\n",
      "2018-10-26T18:25:11.626661: step 23107, loss 2.6077e-07, acc 1\n",
      "2018-10-26T18:25:11.819147: step 23108, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:25:11.989692: step 23109, loss 2.38216e-06, acc 1\n",
      "2018-10-26T18:25:12.167218: step 23110, loss 9.68574e-08, acc 1\n",
      "2018-10-26T18:25:12.339757: step 23111, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:25:12.518281: step 23112, loss 0.00403995, acc 1\n",
      "2018-10-26T18:25:12.690818: step 23113, loss 0, acc 1\n",
      "2018-10-26T18:25:12.868344: step 23114, loss 0, acc 1\n",
      "2018-10-26T18:25:13.042878: step 23115, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:25:13.217411: step 23116, loss 3.07382e-05, acc 1\n",
      "2018-10-26T18:25:13.387955: step 23117, loss 5.77673e-05, acc 1\n",
      "2018-10-26T18:25:13.562490: step 23118, loss 5.15946e-07, acc 1\n",
      "2018-10-26T18:25:13.730041: step 23119, loss 2.36554e-07, acc 1\n",
      "2018-10-26T18:25:13.916543: step 23120, loss 3.16648e-07, acc 1\n",
      "2018-10-26T18:25:14.086090: step 23121, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:25:14.261621: step 23122, loss 2.06752e-07, acc 1\n",
      "2018-10-26T18:25:14.427179: step 23123, loss 1.74285e-05, acc 1\n",
      "2018-10-26T18:25:14.607697: step 23124, loss 0.000121575, acc 1\n",
      "2018-10-26T18:25:14.778241: step 23125, loss 0, acc 1\n",
      "2018-10-26T18:25:14.962748: step 23126, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:25:15.136285: step 23127, loss 3.27822e-07, acc 1\n",
      "2018-10-26T18:25:15.332760: step 23128, loss 0, acc 1\n",
      "2018-10-26T18:25:15.503304: step 23129, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:25:15.686814: step 23130, loss 1.56462e-07, acc 1\n",
      "2018-10-26T18:25:15.861347: step 23131, loss 5.5837e-06, acc 1\n",
      "2018-10-26T18:25:16.040868: step 23132, loss 8.92538e-06, acc 1\n",
      "2018-10-26T18:25:16.211412: step 23133, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:25:16.396916: step 23134, loss 1.51245e-06, acc 1\n",
      "2018-10-26T18:25:16.567462: step 23135, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:25:16.760944: step 23136, loss 7.82872e-06, acc 1\n",
      "2018-10-26T18:25:16.929493: step 23137, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:25:17.111008: step 23138, loss 0, acc 1\n",
      "2018-10-26T18:25:17.276566: step 23139, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:25:17.460076: step 23140, loss 4.74969e-07, acc 1\n",
      "2018-10-26T18:25:17.632615: step 23141, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:25:17.809143: step 23142, loss 7.10997e-06, acc 1\n",
      "2018-10-26T18:25:17.974701: step 23143, loss 1.23352e-05, acc 1\n",
      "2018-10-26T18:25:18.156216: step 23144, loss 6.66318e-05, acc 1\n",
      "2018-10-26T18:25:18.323769: step 23145, loss 2.98564e-06, acc 1\n",
      "2018-10-26T18:25:18.503289: step 23146, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:18.677823: step 23147, loss 0.000147305, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:25:18.857342: step 23148, loss 0, acc 1\n",
      "2018-10-26T18:25:19.038857: step 23149, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:25:19.217380: step 23150, loss 0, acc 1\n",
      "2018-10-26T18:25:19.389920: step 23151, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:25:19.559467: step 23152, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:25:19.735994: step 23153, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:25:19.906539: step 23154, loss 9.68572e-08, acc 1\n",
      "2018-10-26T18:25:20.084065: step 23155, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:25:20.264582: step 23156, loss 0, acc 1\n",
      "2018-10-26T18:25:20.432136: step 23157, loss 4.09777e-07, acc 1\n",
      "2018-10-26T18:25:20.604674: step 23158, loss 0.0559064, acc 0.984375\n",
      "2018-10-26T18:25:20.780204: step 23159, loss 0, acc 1\n",
      "2018-10-26T18:25:20.961720: step 23160, loss 2.87752e-06, acc 1\n",
      "2018-10-26T18:25:21.130281: step 23161, loss 0, acc 1\n",
      "2018-10-26T18:25:21.303806: step 23162, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:25:21.474350: step 23163, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:25:21.644894: step 23164, loss 3.00057e-06, acc 1\n",
      "2018-10-26T18:25:21.814442: step 23165, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:21.987978: step 23166, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:25:22.163509: step 23167, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:25:22.333056: step 23168, loss 4.07287e-05, acc 1\n",
      "2018-10-26T18:25:22.500607: step 23169, loss 1.17342e-06, acc 1\n",
      "2018-10-26T18:25:22.677136: step 23170, loss 7.60622e-06, acc 1\n",
      "2018-10-26T18:25:22.868625: step 23171, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:23.048146: step 23172, loss 0, acc 1\n",
      "2018-10-26T18:25:23.228663: step 23173, loss 8.54931e-07, acc 1\n",
      "2018-10-26T18:25:23.405192: step 23174, loss 2.64495e-07, acc 1\n",
      "2018-10-26T18:25:23.582717: step 23175, loss 6.46332e-07, acc 1\n",
      "2018-10-26T18:25:23.755256: step 23176, loss 4.7184e-05, acc 1\n",
      "2018-10-26T18:25:23.928793: step 23177, loss 2.61866e-06, acc 1\n",
      "2018-10-26T18:25:24.097342: step 23178, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:25:24.270879: step 23179, loss 1.70937e-05, acc 1\n",
      "2018-10-26T18:25:24.442420: step 23180, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:25:24.618948: step 23181, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:25:24.783508: step 23182, loss 2.38164e-05, acc 1\n",
      "2018-10-26T18:25:24.959039: step 23183, loss 9.31297e-07, acc 1\n",
      "2018-10-26T18:25:25.132576: step 23184, loss 0.000147621, acc 1\n",
      "2018-10-26T18:25:25.309104: step 23185, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:25:25.485633: step 23186, loss 4.74733e-06, acc 1\n",
      "2018-10-26T18:25:25.657174: step 23187, loss 8.35321e-06, acc 1\n",
      "2018-10-26T18:25:25.834699: step 23188, loss 1.18309e-05, acc 1\n",
      "2018-10-26T18:25:26.009234: step 23189, loss 0.000171836, acc 1\n",
      "2018-10-26T18:25:26.177783: step 23190, loss 0.00107576, acc 1\n",
      "2018-10-26T18:25:26.354311: step 23191, loss 0.000281854, acc 1\n",
      "2018-10-26T18:25:26.527848: step 23192, loss 4.7844e-06, acc 1\n",
      "2018-10-26T18:25:26.712355: step 23193, loss 4.18998e-05, acc 1\n",
      "2018-10-26T18:25:26.882899: step 23194, loss 6.2398e-07, acc 1\n",
      "2018-10-26T18:25:27.059428: step 23195, loss 0, acc 1\n",
      "2018-10-26T18:25:27.226980: step 23196, loss 0.00181905, acc 1\n",
      "2018-10-26T18:25:27.400517: step 23197, loss 9.25844e-06, acc 1\n",
      "2018-10-26T18:25:27.568069: step 23198, loss 0.000862114, acc 1\n",
      "2018-10-26T18:25:27.739611: step 23199, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:27.907163: step 23200, loss 0.000479554, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:25:28.363942: step 23200, loss 8.77017, acc 0.707317\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23200\n",
      "\n",
      "2018-10-26T18:25:28.740172: step 23201, loss 0, acc 1\n",
      "2018-10-26T18:25:28.915703: step 23202, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:25:29.090236: step 23203, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:25:29.271751: step 23204, loss 7.05927e-07, acc 1\n",
      "2018-10-26T18:25:29.504136: step 23205, loss 6.83581e-07, acc 1\n",
      "2018-10-26T18:25:29.701603: step 23206, loss 0, acc 1\n",
      "2018-10-26T18:25:29.877134: step 23207, loss 1.12688e-06, acc 1\n",
      "2018-10-26T18:25:30.068622: step 23208, loss 6.03356e-05, acc 1\n",
      "2018-10-26T18:25:30.242159: step 23209, loss 0.0363963, acc 0.984375\n",
      "2018-10-26T18:25:30.415695: step 23210, loss 2.76393e-06, acc 1\n",
      "2018-10-26T18:25:30.584244: step 23211, loss 0, acc 1\n",
      "2018-10-26T18:25:30.766757: step 23212, loss 2.53318e-07, acc 1\n",
      "2018-10-26T18:25:30.933312: step 23213, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:25:31.115824: step 23214, loss 4.90066e-05, acc 1\n",
      "2018-10-26T18:25:31.292352: step 23215, loss 0, acc 1\n",
      "2018-10-26T18:25:31.472870: step 23216, loss 1.11756e-06, acc 1\n",
      "2018-10-26T18:25:31.650396: step 23217, loss 5.12146e-06, acc 1\n",
      "2018-10-26T18:25:31.813959: step 23218, loss 7.72063e-06, acc 1\n",
      "2018-10-26T18:25:31.998466: step 23219, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:25:32.163027: step 23220, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:25:32.346535: step 23221, loss 4.67456e-06, acc 1\n",
      "2018-10-26T18:25:32.515086: step 23222, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:32.701588: step 23223, loss 0.324984, acc 0.984375\n",
      "2018-10-26T18:25:32.868142: step 23224, loss 5.43883e-07, acc 1\n",
      "2018-10-26T18:25:33.045668: step 23225, loss 3.66348e-06, acc 1\n",
      "2018-10-26T18:25:33.215218: step 23226, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:25:33.436624: step 23227, loss 7.86027e-07, acc 1\n",
      "2018-10-26T18:25:33.689947: step 23228, loss 3.22637e-05, acc 1\n",
      "2018-10-26T18:25:33.946263: step 23229, loss 2.65405e-06, acc 1\n",
      "2018-10-26T18:25:34.184625: step 23230, loss 0.0544192, acc 0.984375\n",
      "2018-10-26T18:25:34.412018: step 23231, loss 0, acc 1\n",
      "2018-10-26T18:25:34.661351: step 23232, loss 4.90917e-06, acc 1\n",
      "2018-10-26T18:25:34.881762: step 23233, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:25:35.104169: step 23234, loss 0, acc 1\n",
      "2018-10-26T18:25:35.351508: step 23235, loss 1.166e-06, acc 1\n",
      "2018-10-26T18:25:35.560948: step 23236, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:25:35.775375: step 23237, loss 0.0021568, acc 1\n",
      "2018-10-26T18:25:36.050639: step 23238, loss 0.000299776, acc 1\n",
      "2018-10-26T18:25:36.280026: step 23239, loss 1.5329e-06, acc 1\n",
      "2018-10-26T18:25:36.529360: step 23240, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:25:36.774706: step 23241, loss 7.93467e-07, acc 1\n",
      "2018-10-26T18:25:36.996113: step 23242, loss 2.77532e-07, acc 1\n",
      "2018-10-26T18:25:37.228493: step 23243, loss 0.000325318, acc 1\n",
      "2018-10-26T18:25:37.440927: step 23244, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:25:37.645378: step 23245, loss 0.000736227, acc 1\n",
      "2018-10-26T18:25:37.842852: step 23246, loss 2.58003e-05, acc 1\n",
      "2018-10-26T18:25:38.044312: step 23247, loss 0.0331901, acc 0.984375\n",
      "2018-10-26T18:25:38.252759: step 23248, loss 7.11515e-07, acc 1\n",
      "2018-10-26T18:25:38.441253: step 23249, loss 5.43623e-06, acc 1\n",
      "2018-10-26T18:25:38.635732: step 23250, loss 9.00006e-07, acc 1\n",
      "2018-10-26T18:25:38.841186: step 23251, loss 0.000493039, acc 1\n",
      "2018-10-26T18:25:39.057606: step 23252, loss 4.22815e-07, acc 1\n",
      "2018-10-26T18:25:39.253083: step 23253, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:39.462525: step 23254, loss 1.11382e-06, acc 1\n",
      "2018-10-26T18:25:39.659996: step 23255, loss 1.90916e-06, acc 1\n",
      "2018-10-26T18:25:39.853479: step 23256, loss 0.0014356, acc 1\n",
      "2018-10-26T18:25:40.048956: step 23257, loss 2.34309e-06, acc 1\n",
      "2018-10-26T18:25:40.272359: step 23258, loss 3.97252e-06, acc 1\n",
      "2018-10-26T18:25:40.474818: step 23259, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:25:40.683261: step 23260, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:25:40.879736: step 23261, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:25:41.097155: step 23262, loss 0.000270666, acc 1\n",
      "2018-10-26T18:25:41.293630: step 23263, loss 0, acc 1\n",
      "2018-10-26T18:25:41.522021: step 23264, loss 3.51072e-06, acc 1\n",
      "2018-10-26T18:25:41.742431: step 23265, loss 0, acc 1\n",
      "2018-10-26T18:25:41.953867: step 23266, loss 4.91731e-07, acc 1\n",
      "2018-10-26T18:25:42.235115: step 23267, loss 1.03187e-06, acc 1\n",
      "2018-10-26T18:25:42.403665: step 23268, loss 3.11781e-06, acc 1\n",
      "2018-10-26T18:25:42.589169: step 23269, loss 5.06726e-05, acc 1\n",
      "2018-10-26T18:25:42.806588: step 23270, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:42.999074: step 23271, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:25:43.188567: step 23272, loss 6.51924e-08, acc 1\n",
      "2018-10-26T18:25:43.377065: step 23273, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:25:43.558579: step 23274, loss 0.00431009, acc 1\n",
      "2018-10-26T18:25:43.744084: step 23275, loss 0.000565419, acc 1\n",
      "2018-10-26T18:25:43.933577: step 23276, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:25:44.116089: step 23277, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:25:44.288628: step 23278, loss 0.0967391, acc 0.984375\n",
      "2018-10-26T18:25:44.458176: step 23279, loss 0, acc 1\n",
      "2018-10-26T18:25:44.639691: step 23280, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:44.814224: step 23281, loss 3.81838e-07, acc 1\n",
      "2018-10-26T18:25:44.999729: step 23282, loss 2.18596e-05, acc 1\n",
      "2018-10-26T18:25:45.177255: step 23283, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:25:45.348795: step 23284, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:25:45.520337: step 23285, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:25:45.693873: step 23286, loss 0.00400299, acc 1\n",
      "2018-10-26T18:25:45.860429: step 23287, loss 3.53088e-05, acc 1\n",
      "2018-10-26T18:25:46.040947: step 23288, loss 1.12127e-06, acc 1\n",
      "2018-10-26T18:25:46.210493: step 23289, loss 0.000172723, acc 1\n",
      "2018-10-26T18:25:46.382035: step 23290, loss 2.2984e-06, acc 1\n",
      "2018-10-26T18:25:46.551582: step 23291, loss 1.66512e-06, acc 1\n",
      "2018-10-26T18:25:46.727112: step 23292, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:25:46.893669: step 23293, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:47.076180: step 23294, loss 5.31925e-06, acc 1\n",
      "2018-10-26T18:25:47.243733: step 23295, loss 1.69864e-06, acc 1\n",
      "2018-10-26T18:25:47.425247: step 23296, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:25:47.594795: step 23297, loss 4.00466e-07, acc 1\n",
      "2018-10-26T18:25:47.769329: step 23298, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:25:47.935883: step 23299, loss 4.38418e-06, acc 1\n",
      "2018-10-26T18:25:48.111414: step 23300, loss 0.174458, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:25:48.564204: step 23300, loss 8.32833, acc 0.728893\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23300\n",
      "\n",
      "2018-10-26T18:25:48.928963: step 23301, loss 5.20153e-06, acc 1\n",
      "2018-10-26T18:25:49.095518: step 23302, loss 4.73821e-06, acc 1\n",
      "2018-10-26T18:25:49.270052: step 23303, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:25:49.433615: step 23304, loss 3.91153e-07, acc 1\n",
      "2018-10-26T18:25:49.632085: step 23305, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:49.870448: step 23306, loss 0, acc 1\n",
      "2018-10-26T18:25:50.041989: step 23307, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:50.220512: step 23308, loss 5.24433e-06, acc 1\n",
      "2018-10-26T18:25:50.394049: step 23309, loss 0, acc 1\n",
      "2018-10-26T18:25:50.570577: step 23310, loss 2.7634e-05, acc 1\n",
      "2018-10-26T18:25:50.737132: step 23311, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:25:50.915655: step 23312, loss 2.4436e-06, acc 1\n",
      "2018-10-26T18:25:51.083208: step 23313, loss 0.000611103, acc 1\n",
      "2018-10-26T18:25:51.267715: step 23314, loss 4.06376e-06, acc 1\n",
      "2018-10-26T18:25:51.440254: step 23315, loss 0.00010198, acc 1\n",
      "2018-10-26T18:25:51.616782: step 23316, loss 0.000723779, acc 1\n",
      "2018-10-26T18:25:51.789343: step 23317, loss 2.18586e-05, acc 1\n",
      "2018-10-26T18:25:51.962858: step 23318, loss 2.17647e-05, acc 1\n",
      "2018-10-26T18:25:52.134399: step 23319, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:25:52.323893: step 23320, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:25:52.503413: step 23321, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:25:52.675952: step 23322, loss 2.46036e-06, acc 1\n",
      "2018-10-26T18:25:52.843505: step 23323, loss 9.872e-08, acc 1\n",
      "2018-10-26T18:25:53.021031: step 23324, loss 0.00525246, acc 1\n",
      "2018-10-26T18:25:53.192573: step 23325, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:25:53.370097: step 23326, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:53.538647: step 23327, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:25:53.761053: step 23328, loss 0, acc 1\n",
      "2018-10-26T18:25:53.956530: step 23329, loss 0, acc 1\n",
      "2018-10-26T18:25:54.137048: step 23330, loss 0, acc 1\n",
      "2018-10-26T18:25:54.358457: step 23331, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:25:54.529998: step 23332, loss 0.000148008, acc 1\n",
      "2018-10-26T18:25:54.727471: step 23333, loss 2.79396e-07, acc 1\n",
      "2018-10-26T18:25:54.905006: step 23334, loss 3.98601e-07, acc 1\n",
      "2018-10-26T18:25:55.082522: step 23335, loss 4.85702e-06, acc 1\n",
      "2018-10-26T18:25:55.324882: step 23336, loss 4.11642e-07, acc 1\n",
      "2018-10-26T18:25:55.525339: step 23337, loss 2.94296e-07, acc 1\n",
      "2018-10-26T18:25:55.704859: step 23338, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:55.945217: step 23339, loss 0, acc 1\n",
      "2018-10-26T18:25:56.113767: step 23340, loss 1.51053e-06, acc 1\n",
      "2018-10-26T18:25:56.329192: step 23341, loss 1.91852e-07, acc 1\n",
      "2018-10-26T18:25:56.531651: step 23342, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:25:56.757048: step 23343, loss 0, acc 1\n",
      "2018-10-26T18:25:56.943550: step 23344, loss 7.68918e-05, acc 1\n",
      "2018-10-26T18:25:57.163960: step 23345, loss 2.61037e-05, acc 1\n",
      "2018-10-26T18:25:57.403321: step 23346, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:25:57.615754: step 23347, loss 0, acc 1\n",
      "2018-10-26T18:25:57.818213: step 23348, loss 2.1717e-06, acc 1\n",
      "2018-10-26T18:25:58.069542: step 23349, loss 2.25178e-06, acc 1\n",
      "2018-10-26T18:25:58.249061: step 23350, loss 0, acc 1\n",
      "2018-10-26T18:25:58.465484: step 23351, loss 2.12707e-06, acc 1\n",
      "2018-10-26T18:25:58.643009: step 23352, loss 4.26115e-06, acc 1\n",
      "2018-10-26T18:25:58.868407: step 23353, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:25:59.034962: step 23354, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:25:59.214481: step 23355, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:25:59.392008: step 23356, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:25:59.613437: step 23357, loss 6.426e-07, acc 1\n",
      "2018-10-26T18:25:59.816874: step 23358, loss 0.000380317, acc 1\n",
      "2018-10-26T18:26:00.015342: step 23359, loss 1.81312e-05, acc 1\n",
      "2018-10-26T18:26:00.207829: step 23360, loss 0.000272248, acc 1\n",
      "2018-10-26T18:26:00.404303: step 23361, loss 2.79396e-07, acc 1\n",
      "2018-10-26T18:26:00.596789: step 23362, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:26:00.774315: step 23363, loss 0.000605637, acc 1\n",
      "2018-10-26T18:26:00.994726: step 23364, loss 1.56462e-07, acc 1\n",
      "2018-10-26T18:26:01.161281: step 23365, loss 2.36554e-07, acc 1\n",
      "2018-10-26T18:26:01.356758: step 23366, loss 0, acc 1\n",
      "2018-10-26T18:26:01.539271: step 23367, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:26:01.718791: step 23368, loss 0, acc 1\n",
      "2018-10-26T18:26:01.887341: step 23369, loss 0, acc 1\n",
      "2018-10-26T18:26:02.105757: step 23370, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:26:02.282286: step 23371, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:02.463801: step 23372, loss 0.00022126, acc 1\n",
      "2018-10-26T18:26:02.663267: step 23373, loss 2.39155e-06, acc 1\n",
      "2018-10-26T18:26:02.866723: step 23374, loss 0.00013088, acc 1\n",
      "2018-10-26T18:26:03.042255: step 23375, loss 1.72285e-06, acc 1\n",
      "2018-10-26T18:26:03.225764: step 23376, loss 9.54828e-06, acc 1\n",
      "2018-10-26T18:26:03.440192: step 23377, loss 5.63388e-05, acc 1\n",
      "2018-10-26T18:26:03.610736: step 23378, loss 0.0001484, acc 1\n",
      "2018-10-26T18:26:03.832145: step 23379, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:26:04.039590: step 23380, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:26:04.267980: step 23381, loss 8.05208e-06, acc 1\n",
      "2018-10-26T18:26:04.464456: step 23382, loss 0, acc 1\n",
      "2018-10-26T18:26:04.642978: step 23383, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:26:04.857405: step 23384, loss 1.58131e-06, acc 1\n",
      "2018-10-26T18:26:05.027950: step 23385, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:26:05.250355: step 23386, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:26:05.419903: step 23387, loss 7.07796e-07, acc 1\n",
      "2018-10-26T18:26:05.603412: step 23388, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:26:05.810858: step 23389, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:26:05.992373: step 23390, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:26:06.179873: step 23391, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:06.393302: step 23392, loss 0.000223801, acc 1\n",
      "2018-10-26T18:26:06.573819: step 23393, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:26:06.752342: step 23394, loss 5.12223e-07, acc 1\n",
      "2018-10-26T18:26:06.963778: step 23395, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:26:07.136317: step 23396, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:26:07.321821: step 23397, loss 5.89623e-06, acc 1\n",
      "2018-10-26T18:26:07.500343: step 23398, loss 5.89975e-06, acc 1\n",
      "2018-10-26T18:26:07.716765: step 23399, loss 4.46265e-05, acc 1\n",
      "2018-10-26T18:26:07.890302: step 23400, loss 9.93411e-09, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:26:08.353065: step 23400, loss 8.43237, acc 0.725141\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23400\n",
      "\n",
      "2018-10-26T18:26:08.723077: step 23401, loss 0.000280756, acc 1\n",
      "2018-10-26T18:26:08.899605: step 23402, loss 3.76023e-06, acc 1\n",
      "2018-10-26T18:26:09.071146: step 23403, loss 2.06752e-07, acc 1\n",
      "2018-10-26T18:26:09.241691: step 23404, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:26:09.435173: step 23405, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:26:09.669548: step 23406, loss 0.000115676, acc 1\n",
      "2018-10-26T18:26:09.842087: step 23407, loss 2.96158e-07, acc 1\n",
      "2018-10-26T18:26:10.021607: step 23408, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:26:10.192152: step 23409, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:26:10.367683: step 23410, loss 0.000220475, acc 1\n",
      "2018-10-26T18:26:10.534238: step 23411, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:26:10.707774: step 23412, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:26:10.888293: step 23413, loss 0, acc 1\n",
      "2018-10-26T18:26:11.063823: step 23414, loss 2.46051e-05, acc 1\n",
      "2018-10-26T18:26:11.233370: step 23415, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:26:11.407903: step 23416, loss 3.13266e-06, acc 1\n",
      "2018-10-26T18:26:11.585429: step 23417, loss 1.41742e-06, acc 1\n",
      "2018-10-26T18:26:11.753978: step 23418, loss 5.3457e-07, acc 1\n",
      "2018-10-26T18:26:11.928512: step 23419, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:26:12.098059: step 23420, loss 2.79942e-06, acc 1\n",
      "2018-10-26T18:26:12.272593: step 23421, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:26:12.446129: step 23422, loss 5.95936e-06, acc 1\n",
      "2018-10-26T18:26:12.630637: step 23423, loss 1.43418e-06, acc 1\n",
      "2018-10-26T18:26:12.806167: step 23424, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:12.976711: step 23425, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:26:13.151245: step 23426, loss 1.13803e-06, acc 1\n",
      "2018-10-26T18:26:13.325779: step 23427, loss 4.12517e-05, acc 1\n",
      "2018-10-26T18:26:13.497320: step 23428, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:26:13.673849: step 23429, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:26:13.847386: step 23430, loss 5.28982e-07, acc 1\n",
      "2018-10-26T18:26:14.023914: step 23431, loss 2.30632e-05, acc 1\n",
      "2018-10-26T18:26:14.204431: step 23432, loss 4.31329e-06, acc 1\n",
      "2018-10-26T18:26:14.368991: step 23433, loss 2.01527e-06, acc 1\n",
      "2018-10-26T18:26:14.541530: step 23434, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:26:14.708085: step 23435, loss 1.49164e-05, acc 1\n",
      "2018-10-26T18:26:14.880625: step 23436, loss 0, acc 1\n",
      "2018-10-26T18:26:15.048176: step 23437, loss 0, acc 1\n",
      "2018-10-26T18:26:15.223708: step 23438, loss 0.00141671, acc 1\n",
      "2018-10-26T18:26:15.389266: step 23439, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:26:15.564797: step 23440, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:26:15.731351: step 23441, loss 9.27583e-07, acc 1\n",
      "2018-10-26T18:26:15.908878: step 23442, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:26:16.087400: step 23443, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:26:16.263929: step 23444, loss 2.42143e-07, acc 1\n",
      "2018-10-26T18:26:16.448436: step 23445, loss 1.79977e-05, acc 1\n",
      "2018-10-26T18:26:16.617982: step 23446, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:16.793514: step 23447, loss 2.13445e-06, acc 1\n",
      "2018-10-26T18:26:16.962063: step 23448, loss 4.21231e-05, acc 1\n",
      "2018-10-26T18:26:17.147568: step 23449, loss 0, acc 1\n",
      "2018-10-26T18:26:17.314123: step 23450, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:26:17.493643: step 23451, loss 5.46597e-06, acc 1\n",
      "2018-10-26T18:26:17.667180: step 23452, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:26:17.839719: step 23453, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:18.020236: step 23454, loss 0, acc 1\n",
      "2018-10-26T18:26:18.190781: step 23455, loss 0.0184243, acc 0.984375\n",
      "2018-10-26T18:26:18.372296: step 23456, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:26:18.543837: step 23457, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:26:18.714381: step 23458, loss 0.000645257, acc 1\n",
      "2018-10-26T18:26:18.884926: step 23459, loss 2.10477e-07, acc 1\n",
      "2018-10-26T18:26:19.064453: step 23460, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:26:19.233992: step 23461, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:26:19.412516: step 23462, loss 0.000462652, acc 1\n",
      "2018-10-26T18:26:19.586053: step 23463, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:26:19.758591: step 23464, loss 2.58564e-05, acc 1\n",
      "2018-10-26T18:26:19.931131: step 23465, loss 1.16038e-06, acc 1\n",
      "2018-10-26T18:26:20.120624: step 23466, loss 2.81257e-07, acc 1\n",
      "2018-10-26T18:26:20.290171: step 23467, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:26:20.465702: step 23468, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:26:20.631261: step 23469, loss 5.94179e-07, acc 1\n",
      "2018-10-26T18:26:20.810780: step 23470, loss 1.07471e-06, acc 1\n",
      "2018-10-26T18:26:20.979330: step 23471, loss 9.47491e-06, acc 1\n",
      "2018-10-26T18:26:21.153864: step 23472, loss 1.97688e-05, acc 1\n",
      "2018-10-26T18:26:21.327400: step 23473, loss 1.31051e-05, acc 1\n",
      "2018-10-26T18:26:21.509912: step 23474, loss 5.49756e-06, acc 1\n",
      "2018-10-26T18:26:21.683448: step 23475, loss 7.45038e-05, acc 1\n",
      "2018-10-26T18:26:21.861972: step 23476, loss 3.58409e-05, acc 1\n",
      "2018-10-26T18:26:22.033513: step 23477, loss 1.40065e-05, acc 1\n",
      "2018-10-26T18:26:22.208047: step 23478, loss 2.08615e-07, acc 1\n",
      "2018-10-26T18:26:22.380586: step 23479, loss 2.2556e-06, acc 1\n",
      "2018-10-26T18:26:22.559108: step 23480, loss 8.51435e-06, acc 1\n",
      "2018-10-26T18:26:22.727659: step 23481, loss 2.73806e-07, acc 1\n",
      "2018-10-26T18:26:22.904187: step 23482, loss 0, acc 1\n",
      "2018-10-26T18:26:23.070743: step 23483, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:26:23.246272: step 23484, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:23.410833: step 23485, loss 6.84931e-06, acc 1\n",
      "2018-10-26T18:26:23.591350: step 23486, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:26:23.757906: step 23487, loss 1.06171e-07, acc 1\n",
      "2018-10-26T18:26:23.934434: step 23488, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:26:24.110963: step 23489, loss 0.0258116, acc 0.984375\n",
      "2018-10-26T18:26:24.294472: step 23490, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:24.469006: step 23491, loss 1.46401e-06, acc 1\n",
      "2018-10-26T18:26:24.647528: step 23492, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:24.827050: step 23493, loss 5.36433e-07, acc 1\n",
      "2018-10-26T18:26:25.022528: step 23494, loss 3.63174e-06, acc 1\n",
      "2018-10-26T18:26:25.202047: step 23495, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:26:25.380571: step 23496, loss 5.34575e-07, acc 1\n",
      "2018-10-26T18:26:25.577045: step 23497, loss 4.33991e-07, acc 1\n",
      "2018-10-26T18:26:25.792470: step 23498, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:26:26.002908: step 23499, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:26:26.219329: step 23500, loss 0.000244876, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:26:26.699047: step 23500, loss 8.54277, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23500\n",
      "\n",
      "2018-10-26T18:26:27.052524: step 23501, loss 0.00206141, acc 1\n",
      "2018-10-26T18:26:27.229053: step 23502, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:26:27.412562: step 23503, loss 0, acc 1\n",
      "2018-10-26T18:26:27.588093: step 23504, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:26:27.772601: step 23505, loss 0.000397891, acc 1\n",
      "2018-10-26T18:26:28.014952: step 23506, loss 3.68109e-05, acc 1\n",
      "2018-10-26T18:26:28.177519: step 23507, loss 5.18474e-06, acc 1\n",
      "2018-10-26T18:26:28.363022: step 23508, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:28.536559: step 23509, loss 8.09167e-05, acc 1\n",
      "2018-10-26T18:26:28.715083: step 23510, loss 2.86827e-05, acc 1\n",
      "2018-10-26T18:26:28.883632: step 23511, loss 4.34918e-05, acc 1\n",
      "2018-10-26T18:26:29.069136: step 23512, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:26:29.235691: step 23513, loss 0.00021392, acc 1\n",
      "2018-10-26T18:26:29.432166: step 23514, loss 0.000221511, acc 1\n",
      "2018-10-26T18:26:29.596726: step 23515, loss 0, acc 1\n",
      "2018-10-26T18:26:29.792204: step 23516, loss 0, acc 1\n",
      "2018-10-26T18:26:29.966738: step 23517, loss 0, acc 1\n",
      "2018-10-26T18:26:30.148608: step 23518, loss 0, acc 1\n",
      "2018-10-26T18:26:30.318155: step 23519, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:26:30.500667: step 23520, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:26:30.669217: step 23521, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:26:30.853724: step 23522, loss 1.82538e-07, acc 1\n",
      "2018-10-26T18:26:31.027261: step 23523, loss 0.000471271, acc 1\n",
      "2018-10-26T18:26:31.210771: step 23524, loss 0.000119979, acc 1\n",
      "2018-10-26T18:26:31.384306: step 23525, loss 5.15007e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:26:31.578788: step 23526, loss 7.29804e-06, acc 1\n",
      "2018-10-26T18:26:31.766287: step 23527, loss 7.2083e-07, acc 1\n",
      "2018-10-26T18:26:31.949797: step 23528, loss 0, acc 1\n",
      "2018-10-26T18:26:32.139290: step 23529, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:26:32.309834: step 23530, loss 3.0964e-05, acc 1\n",
      "2018-10-26T18:26:32.484367: step 23531, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:32.660895: step 23532, loss 5.21151e-05, acc 1\n",
      "2018-10-26T18:26:32.830443: step 23533, loss 2.49593e-07, acc 1\n",
      "2018-10-26T18:26:32.997996: step 23534, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:26:33.169537: step 23535, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:26:33.342076: step 23536, loss 2.0489e-07, acc 1\n",
      "2018-10-26T18:26:33.516610: step 23537, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:26:33.689149: step 23538, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:26:33.860690: step 23539, loss 1.35349e-05, acc 1\n",
      "2018-10-26T18:26:34.034227: step 23540, loss 0.000200148, acc 1\n",
      "2018-10-26T18:26:34.210755: step 23541, loss 0, acc 1\n",
      "2018-10-26T18:26:34.375315: step 23542, loss 0, acc 1\n",
      "2018-10-26T18:26:34.557827: step 23543, loss 0, acc 1\n",
      "2018-10-26T18:26:34.727375: step 23544, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:26:34.909886: step 23545, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:26:35.079461: step 23546, loss 1.89989e-07, acc 1\n",
      "2018-10-26T18:26:35.256960: step 23547, loss 7.47478e-05, acc 1\n",
      "2018-10-26T18:26:35.429513: step 23548, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:35.616997: step 23549, loss 1.11569e-06, acc 1\n",
      "2018-10-26T18:26:35.792530: step 23550, loss 1.2049e-05, acc 1\n",
      "2018-10-26T18:26:35.968063: step 23551, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:26:36.144589: step 23552, loss 0, acc 1\n",
      "2018-10-26T18:26:36.320119: step 23553, loss 9.0526e-06, acc 1\n",
      "2018-10-26T18:26:36.533549: step 23554, loss 0, acc 1\n",
      "2018-10-26T18:26:36.707085: step 23555, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:36.879624: step 23556, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:26:37.048174: step 23557, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:26:37.221710: step 23558, loss 2.71929e-06, acc 1\n",
      "2018-10-26T18:26:37.394249: step 23559, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:26:37.569780: step 23560, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:26:37.742319: step 23561, loss 2.1234e-07, acc 1\n",
      "2018-10-26T18:26:37.921840: step 23562, loss 7.42505e-06, acc 1\n",
      "2018-10-26T18:26:38.085402: step 23563, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:26:38.261930: step 23564, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:26:38.436479: step 23565, loss 0.000106911, acc 1\n",
      "2018-10-26T18:26:38.614987: step 23566, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:26:38.792514: step 23567, loss 0, acc 1\n",
      "2018-10-26T18:26:38.966049: step 23568, loss 3.91151e-07, acc 1\n",
      "2018-10-26T18:26:39.138589: step 23569, loss 0, acc 1\n",
      "2018-10-26T18:26:39.309132: step 23570, loss 2.46595e-06, acc 1\n",
      "2018-10-26T18:26:39.491645: step 23571, loss 3.96741e-07, acc 1\n",
      "2018-10-26T18:26:39.663187: step 23572, loss 2.71178e-06, acc 1\n",
      "2018-10-26T18:26:39.837720: step 23573, loss 3.40488e-05, acc 1\n",
      "2018-10-26T18:26:40.004275: step 23574, loss 8.54534e-06, acc 1\n",
      "2018-10-26T18:26:40.178809: step 23575, loss 0.00513867, acc 1\n",
      "2018-10-26T18:26:40.352346: step 23576, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:26:40.528874: step 23577, loss 1.74135e-05, acc 1\n",
      "2018-10-26T18:26:40.699419: step 23578, loss 3.16646e-07, acc 1\n",
      "2018-10-26T18:26:40.882928: step 23579, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:26:41.053478: step 23580, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:26:41.230998: step 23581, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:26:41.396556: step 23582, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:26:41.576076: step 23583, loss 0, acc 1\n",
      "2018-10-26T18:26:41.741634: step 23584, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:26:41.925143: step 23585, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:26:42.089703: step 23586, loss 8.9777e-07, acc 1\n",
      "2018-10-26T18:26:42.270221: step 23587, loss 3.11403e-05, acc 1\n",
      "2018-10-26T18:26:42.435779: step 23588, loss 0.000300252, acc 1\n",
      "2018-10-26T18:26:42.613305: step 23589, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:26:42.782851: step 23590, loss 4.6938e-07, acc 1\n",
      "2018-10-26T18:26:42.961375: step 23591, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:26:43.129924: step 23592, loss 3.92127e-05, acc 1\n",
      "2018-10-26T18:26:43.307450: step 23593, loss 0.000518033, acc 1\n",
      "2018-10-26T18:26:43.477994: step 23594, loss 8.34456e-07, acc 1\n",
      "2018-10-26T18:26:43.653525: step 23595, loss 8.0278e-07, acc 1\n",
      "2018-10-26T18:26:43.822075: step 23596, loss 0, acc 1\n",
      "2018-10-26T18:26:44.005585: step 23597, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:26:44.173137: step 23598, loss 2.46967e-06, acc 1\n",
      "2018-10-26T18:26:44.345676: step 23599, loss 2.21654e-07, acc 1\n",
      "2018-10-26T18:26:44.515224: step 23600, loss 6.33298e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:26:44.970008: step 23600, loss 8.44947, acc 0.729831\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23600\n",
      "\n",
      "2018-10-26T18:26:45.360666: step 23601, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:26:45.545174: step 23602, loss 0, acc 1\n",
      "2018-10-26T18:26:45.710731: step 23603, loss 4.37716e-07, acc 1\n",
      "2018-10-26T18:26:45.892247: step 23604, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:26:46.090716: step 23605, loss 2.86845e-07, acc 1\n",
      "2018-10-26T18:26:46.315116: step 23606, loss 6.19639e-06, acc 1\n",
      "2018-10-26T18:26:46.489649: step 23607, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:26:46.676151: step 23608, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:26:46.843704: step 23609, loss 6.72401e-07, acc 1\n",
      "2018-10-26T18:26:47.030207: step 23610, loss 5.62977e-06, acc 1\n",
      "2018-10-26T18:26:47.206734: step 23611, loss 0.000422149, acc 1\n",
      "2018-10-26T18:26:47.388249: step 23612, loss 0.000195371, acc 1\n",
      "2018-10-26T18:26:47.562782: step 23613, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:26:47.738313: step 23614, loss 1.74339e-05, acc 1\n",
      "2018-10-26T18:26:47.904869: step 23615, loss 7.54353e-07, acc 1\n",
      "2018-10-26T18:26:48.092367: step 23616, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:26:48.262912: step 23617, loss 5.59069e-06, acc 1\n",
      "2018-10-26T18:26:48.441453: step 23618, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:26:48.611979: step 23619, loss 1.07295e-05, acc 1\n",
      "2018-10-26T18:26:48.815435: step 23620, loss 1.80676e-07, acc 1\n",
      "2018-10-26T18:26:49.014902: step 23621, loss 7.58659e-06, acc 1\n",
      "2018-10-26T18:26:49.220354: step 23622, loss 0, acc 1\n",
      "2018-10-26T18:26:49.409847: step 23623, loss 0, acc 1\n",
      "2018-10-26T18:26:49.609316: step 23624, loss 2.34307e-06, acc 1\n",
      "2018-10-26T18:26:49.793821: step 23625, loss 9.05433e-05, acc 1\n",
      "2018-10-26T18:26:49.980324: step 23626, loss 0, acc 1\n",
      "2018-10-26T18:26:50.162836: step 23627, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:26:50.344351: step 23628, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:26:50.508911: step 23629, loss 1.29635e-06, acc 1\n",
      "2018-10-26T18:26:50.687434: step 23630, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:26:50.856982: step 23631, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:26:51.052459: step 23632, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:26:51.229985: step 23633, loss 3.36544e-06, acc 1\n",
      "2018-10-26T18:26:51.414491: step 23634, loss 1.35411e-05, acc 1\n",
      "2018-10-26T18:26:51.597004: step 23635, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:26:51.764560: step 23636, loss 1.06913e-06, acc 1\n",
      "2018-10-26T18:26:51.948066: step 23637, loss 0.00213479, acc 1\n",
      "2018-10-26T18:26:52.122599: step 23638, loss 5.07302e-06, acc 1\n",
      "2018-10-26T18:26:52.303117: step 23639, loss 2.45296e-06, acc 1\n",
      "2018-10-26T18:26:52.481639: step 23640, loss 7.02642e-06, acc 1\n",
      "2018-10-26T18:26:52.661161: step 23641, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:26:52.842676: step 23642, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:26:53.007236: step 23643, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:53.195732: step 23644, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:26:53.360292: step 23645, loss 0, acc 1\n",
      "2018-10-26T18:26:53.539813: step 23646, loss 3.18509e-07, acc 1\n",
      "2018-10-26T18:26:53.705370: step 23647, loss 1.04118e-06, acc 1\n",
      "2018-10-26T18:26:53.885888: step 23648, loss 3.59488e-07, acc 1\n",
      "2018-10-26T18:26:54.057429: step 23649, loss 6.22111e-07, acc 1\n",
      "2018-10-26T18:26:54.246923: step 23650, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:26:54.416471: step 23651, loss 0, acc 1\n",
      "2018-10-26T18:26:54.596988: step 23652, loss 0, acc 1\n",
      "2018-10-26T18:26:54.768530: step 23653, loss 2.14755e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:26:54.956029: step 23654, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:26:55.129759: step 23655, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:26:55.311273: step 23656, loss 2.65411e-06, acc 1\n",
      "2018-10-26T18:26:55.476831: step 23657, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:55.659344: step 23658, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:26:55.828889: step 23659, loss 0, acc 1\n",
      "2018-10-26T18:26:56.005418: step 23660, loss 7.07375e-05, acc 1\n",
      "2018-10-26T18:26:56.177957: step 23661, loss 1.21071e-07, acc 1\n",
      "2018-10-26T18:26:56.357478: step 23662, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:26:56.524033: step 23663, loss 0.000846262, acc 1\n",
      "2018-10-26T18:26:56.695574: step 23664, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:26:56.867128: step 23665, loss 1.03631e-05, acc 1\n",
      "2018-10-26T18:26:57.044641: step 23666, loss 1.3932e-05, acc 1\n",
      "2018-10-26T18:26:57.230147: step 23667, loss 4.46601e-06, acc 1\n",
      "2018-10-26T18:26:57.405677: step 23668, loss 0, acc 1\n",
      "2018-10-26T18:26:57.584200: step 23669, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:26:57.749758: step 23670, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:57.930275: step 23671, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:26:58.096829: step 23672, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:26:58.271364: step 23673, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:26:58.454874: step 23674, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:26:58.639382: step 23675, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:26:58.823888: step 23676, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:26:58.999418: step 23677, loss 3.50938e-05, acc 1\n",
      "2018-10-26T18:26:59.178939: step 23678, loss 2.44551e-06, acc 1\n",
      "2018-10-26T18:26:59.390374: step 23679, loss 3.12339e-06, acc 1\n",
      "2018-10-26T18:26:59.586849: step 23680, loss 2.21653e-07, acc 1\n",
      "2018-10-26T18:26:59.770358: step 23681, loss 1.28145e-06, acc 1\n",
      "2018-10-26T18:26:59.980796: step 23682, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:27:00.161314: step 23683, loss 5.47613e-07, acc 1\n",
      "2018-10-26T18:27:00.371751: step 23684, loss 1.64277e-06, acc 1\n",
      "2018-10-26T18:27:00.539304: step 23685, loss 3.29685e-07, acc 1\n",
      "2018-10-26T18:27:00.710845: step 23686, loss 6.94242e-06, acc 1\n",
      "2018-10-26T18:27:00.922281: step 23687, loss 5.26856e-06, acc 1\n",
      "2018-10-26T18:27:01.120750: step 23688, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:27:01.333183: step 23689, loss 5.99119e-06, acc 1\n",
      "2018-10-26T18:27:01.553595: step 23690, loss 0, acc 1\n",
      "2018-10-26T18:27:01.737104: step 23691, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:01.948539: step 23692, loss 0, acc 1\n",
      "2018-10-26T18:27:02.170945: step 23693, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:27:02.397342: step 23694, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:27:02.628721: step 23695, loss 2.72212e-05, acc 1\n",
      "2018-10-26T18:27:02.838161: step 23696, loss 0, acc 1\n",
      "2018-10-26T18:27:03.065555: step 23697, loss 9.20922e-05, acc 1\n",
      "2018-10-26T18:27:03.281976: step 23698, loss 2.03027e-07, acc 1\n",
      "2018-10-26T18:27:03.494410: step 23699, loss 7.0321e-06, acc 1\n",
      "2018-10-26T18:27:03.694872: step 23700, loss 0.000422855, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:27:04.218474: step 23700, loss 8.45786, acc 0.731707\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23700\n",
      "\n",
      "2018-10-26T18:27:04.601861: step 23701, loss 1.86818e-06, acc 1\n",
      "2018-10-26T18:27:04.784373: step 23702, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:04.959903: step 23703, loss 5.979e-07, acc 1\n",
      "2018-10-26T18:27:05.158374: step 23704, loss 2.72677e-05, acc 1\n",
      "2018-10-26T18:27:05.412694: step 23705, loss 0.000230755, acc 1\n",
      "2018-10-26T18:27:05.630113: step 23706, loss 1.42093e-05, acc 1\n",
      "2018-10-26T18:27:05.853517: step 23707, loss 5.04769e-07, acc 1\n",
      "2018-10-26T18:27:06.047000: step 23708, loss 4.32469e-06, acc 1\n",
      "2018-10-26T18:27:06.287358: step 23709, loss 5.04697e-06, acc 1\n",
      "2018-10-26T18:27:06.458899: step 23710, loss 1.04255e-05, acc 1\n",
      "2018-10-26T18:27:06.668339: step 23711, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:27:06.844867: step 23712, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:27:07.030372: step 23713, loss 1.70424e-06, acc 1\n",
      "2018-10-26T18:27:07.230836: step 23714, loss 0.000101697, acc 1\n",
      "2018-10-26T18:27:07.407364: step 23715, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:27:07.592869: step 23716, loss 8.2028e-06, acc 1\n",
      "2018-10-26T18:27:07.789345: step 23717, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:27:07.968864: step 23718, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:27:08.151377: step 23719, loss 0, acc 1\n",
      "2018-10-26T18:27:08.345857: step 23720, loss 9.85308e-07, acc 1\n",
      "2018-10-26T18:27:08.538343: step 23721, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:08.702905: step 23722, loss 0.0294167, acc 0.984375\n",
      "2018-10-26T18:27:08.881425: step 23723, loss 0, acc 1\n",
      "2018-10-26T18:27:09.097848: step 23724, loss 1.50869e-06, acc 1\n",
      "2018-10-26T18:27:09.276371: step 23725, loss 0, acc 1\n",
      "2018-10-26T18:27:09.452900: step 23726, loss 4.98746e-06, acc 1\n",
      "2018-10-26T18:27:09.638403: step 23727, loss 7.99056e-07, acc 1\n",
      "2018-10-26T18:27:09.854825: step 23728, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:10.059279: step 23729, loss 6.17345e-06, acc 1\n",
      "2018-10-26T18:27:10.253760: step 23730, loss 0, acc 1\n",
      "2018-10-26T18:27:10.463200: step 23731, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:27:10.646710: step 23732, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:27:10.869115: step 23733, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:27:11.071575: step 23734, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:27:11.265057: step 23735, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:27:11.465522: step 23736, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:27:11.677955: step 23737, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:11.868446: step 23738, loss 1.4262e-05, acc 1\n",
      "2018-10-26T18:27:12.063923: step 23739, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:27:12.244440: step 23740, loss 0.000286678, acc 1\n",
      "2018-10-26T18:27:12.438920: step 23741, loss 2.37177e-05, acc 1\n",
      "2018-10-26T18:27:12.620437: step 23742, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:12.840848: step 23743, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:27:13.018373: step 23744, loss 4.70064e-06, acc 1\n",
      "2018-10-26T18:27:13.230805: step 23745, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:13.415312: step 23746, loss 1.41561e-07, acc 1\n",
      "2018-10-26T18:27:13.606802: step 23747, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:13.806267: step 23748, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:27:13.999750: step 23749, loss 0.000143486, acc 1\n",
      "2018-10-26T18:27:14.205202: step 23750, loss 0, acc 1\n",
      "2018-10-26T18:27:14.379735: step 23751, loss 0, acc 1\n",
      "2018-10-26T18:27:14.598152: step 23752, loss 1.72219e-05, acc 1\n",
      "2018-10-26T18:27:14.766702: step 23753, loss 0.108964, acc 0.984375\n",
      "2018-10-26T18:27:14.955199: step 23754, loss 2.1829e-05, acc 1\n",
      "2018-10-26T18:27:15.124745: step 23755, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:27:15.303268: step 23756, loss 3.50174e-07, acc 1\n",
      "2018-10-26T18:27:15.474810: step 23757, loss 6.76249e-06, acc 1\n",
      "2018-10-26T18:27:15.656324: step 23758, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:27:15.821883: step 23759, loss 0, acc 1\n",
      "2018-10-26T18:27:16.005392: step 23760, loss 0, acc 1\n",
      "2018-10-26T18:27:16.171947: step 23761, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:16.353462: step 23762, loss 0, acc 1\n",
      "2018-10-26T18:27:16.532982: step 23763, loss 0, acc 1\n",
      "2018-10-26T18:27:16.714794: step 23764, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:27:16.898305: step 23765, loss 8.32581e-07, acc 1\n",
      "2018-10-26T18:27:17.075830: step 23766, loss 0, acc 1\n",
      "2018-10-26T18:27:17.245377: step 23767, loss 2.25364e-06, acc 1\n",
      "2018-10-26T18:27:17.428887: step 23768, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:27:17.599431: step 23769, loss 1.32056e-06, acc 1\n",
      "2018-10-26T18:27:17.780946: step 23770, loss 4.88005e-07, acc 1\n",
      "2018-10-26T18:27:17.956477: step 23771, loss 0, acc 1\n",
      "2018-10-26T18:27:18.138989: step 23772, loss 2.53683e-06, acc 1\n",
      "2018-10-26T18:27:18.307559: step 23773, loss 1.04965e-05, acc 1\n",
      "2018-10-26T18:27:18.489053: step 23774, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:18.662607: step 23775, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:27:18.851087: step 23776, loss 0, acc 1\n",
      "2018-10-26T18:27:19.029610: step 23777, loss 0.00229093, acc 1\n",
      "2018-10-26T18:27:19.215114: step 23778, loss 8.16286e-06, acc 1\n",
      "2018-10-26T18:27:19.399621: step 23779, loss 4.57559e-05, acc 1\n",
      "2018-10-26T18:27:19.592107: step 23780, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:27:19.773622: step 23781, loss 4.09782e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:27:19.957132: step 23782, loss 2.49764e-06, acc 1\n",
      "2018-10-26T18:27:20.137649: step 23783, loss 0, acc 1\n",
      "2018-10-26T18:27:20.317170: step 23784, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:27:20.492701: step 23785, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:27:20.661251: step 23786, loss 0.000543356, acc 1\n",
      "2018-10-26T18:27:20.833789: step 23787, loss 5.93702e-06, acc 1\n",
      "2018-10-26T18:27:20.998350: step 23788, loss 3.65074e-07, acc 1\n",
      "2018-10-26T18:27:21.179865: step 23789, loss 7.3759e-07, acc 1\n",
      "2018-10-26T18:27:21.355395: step 23790, loss 0, acc 1\n",
      "2018-10-26T18:27:21.530926: step 23791, loss 2.92434e-07, acc 1\n",
      "2018-10-26T18:27:21.705461: step 23792, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:27:21.877001: step 23793, loss 6.7637e-06, acc 1\n",
      "2018-10-26T18:27:22.046549: step 23794, loss 9.99476e-06, acc 1\n",
      "2018-10-26T18:27:22.215098: step 23795, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:27:22.387637: step 23796, loss 9.43518e-06, acc 1\n",
      "2018-10-26T18:27:22.556186: step 23797, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:27:22.732715: step 23798, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:27:22.903260: step 23799, loss 0, acc 1\n",
      "2018-10-26T18:27:23.078791: step 23800, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:27:23.530583: step 23800, loss 8.54686, acc 0.718574\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23800\n",
      "\n",
      "2018-10-26T18:27:23.877196: step 23801, loss 2.04962e-05, acc 1\n",
      "2018-10-26T18:27:24.063698: step 23802, loss 1.93714e-07, acc 1\n",
      "2018-10-26T18:27:24.245212: step 23803, loss 1.61483e-06, acc 1\n",
      "2018-10-26T18:27:24.418749: step 23804, loss 6.09991e-05, acc 1\n",
      "2018-10-26T18:27:24.615223: step 23805, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:24.854594: step 23806, loss 1.86264e-07, acc 1\n",
      "2018-10-26T18:27:25.036099: step 23807, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:25.222611: step 23808, loss 3.07334e-07, acc 1\n",
      "2018-10-26T18:27:25.390153: step 23809, loss 2.62052e-06, acc 1\n",
      "2018-10-26T18:27:25.561695: step 23810, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:27:25.735231: step 23811, loss 7.54748e-06, acc 1\n",
      "2018-10-26T18:27:25.913754: step 23812, loss 0, acc 1\n",
      "2018-10-26T18:27:26.083301: step 23813, loss 4.04905e-06, acc 1\n",
      "2018-10-26T18:27:26.267807: step 23814, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:27:26.435360: step 23815, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:26.605905: step 23816, loss 0.0701124, acc 0.984375\n",
      "2018-10-26T18:27:26.779441: step 23817, loss 1.57387e-06, acc 1\n",
      "2018-10-26T18:27:26.959959: step 23818, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:27:27.135490: step 23819, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:27:27.314012: step 23820, loss 9.71363e-05, acc 1\n",
      "2018-10-26T18:27:27.487550: step 23821, loss 6.42605e-07, acc 1\n",
      "2018-10-26T18:27:27.657096: step 23822, loss 2.45698e-05, acc 1\n",
      "2018-10-26T18:27:27.832626: step 23823, loss 1.41741e-06, acc 1\n",
      "2018-10-26T18:27:28.002173: step 23824, loss 0.000227725, acc 1\n",
      "2018-10-26T18:27:28.178727: step 23825, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:27:28.354234: step 23826, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:27:28.533754: step 23827, loss 0.00232251, acc 1\n",
      "2018-10-26T18:27:28.705296: step 23828, loss 3.37135e-07, acc 1\n",
      "2018-10-26T18:27:28.887808: step 23829, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:27:29.070320: step 23830, loss 6.63094e-07, acc 1\n",
      "2018-10-26T18:27:29.240865: step 23831, loss 2.36554e-07, acc 1\n",
      "2018-10-26T18:27:29.417393: step 23832, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:27:29.581953: step 23833, loss 3.33412e-07, acc 1\n",
      "2018-10-26T18:27:29.756487: step 23834, loss 0.000608078, acc 1\n",
      "2018-10-26T18:27:29.928030: step 23835, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:30.103584: step 23836, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:27:30.270115: step 23837, loss 0.152298, acc 0.984375\n",
      "2018-10-26T18:27:30.452626: step 23838, loss 4.52931e-06, acc 1\n",
      "2018-10-26T18:27:30.617187: step 23839, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:27:30.795710: step 23840, loss 8.23272e-07, acc 1\n",
      "2018-10-26T18:27:30.967251: step 23841, loss 1.50913e-05, acc 1\n",
      "2018-10-26T18:27:31.147770: step 23842, loss 0.000793795, acc 1\n",
      "2018-10-26T18:27:31.313326: step 23843, loss 1.64706e-05, acc 1\n",
      "2018-10-26T18:27:31.494842: step 23844, loss 0.000174049, acc 1\n",
      "2018-10-26T18:27:31.663392: step 23845, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:31.844907: step 23846, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:27:32.013457: step 23847, loss 1.75679e-05, acc 1\n",
      "2018-10-26T18:27:32.190982: step 23848, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:32.357537: step 23849, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:27:32.530075: step 23850, loss 1.28072e-05, acc 1\n",
      "2018-10-26T18:27:32.702616: step 23851, loss 3.74388e-07, acc 1\n",
      "2018-10-26T18:27:32.881138: step 23852, loss 0.0357017, acc 0.984375\n",
      "2018-10-26T18:27:33.053677: step 23853, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:27:33.232200: step 23854, loss 8.69831e-07, acc 1\n",
      "2018-10-26T18:27:33.394766: step 23855, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:27:33.582264: step 23856, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:27:33.754804: step 23857, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:27:33.930335: step 23858, loss 1.75825e-06, acc 1\n",
      "2018-10-26T18:27:34.101876: step 23859, loss 1.98599e-05, acc 1\n",
      "2018-10-26T18:27:34.282394: step 23860, loss 2.88707e-07, acc 1\n",
      "2018-10-26T18:27:34.449946: step 23861, loss 0.0361699, acc 0.984375\n",
      "2018-10-26T18:27:34.631461: step 23862, loss 3.02284e-06, acc 1\n",
      "2018-10-26T18:27:34.803004: step 23863, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:27:34.990502: step 23864, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:27:35.164039: step 23865, loss 3.06003e-06, acc 1\n",
      "2018-10-26T18:27:35.339570: step 23866, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:27:35.510114: step 23867, loss 1.03728e-05, acc 1\n",
      "2018-10-26T18:27:35.691629: step 23868, loss 4.41442e-07, acc 1\n",
      "2018-10-26T18:27:35.866163: step 23869, loss 0, acc 1\n",
      "2018-10-26T18:27:36.046680: step 23870, loss 8.96794e-06, acc 1\n",
      "2018-10-26T18:27:36.220217: step 23871, loss 9.38151e-05, acc 1\n",
      "2018-10-26T18:27:36.431652: step 23872, loss 4.98583e-06, acc 1\n",
      "2018-10-26T18:27:36.606186: step 23873, loss 0, acc 1\n",
      "2018-10-26T18:27:36.784708: step 23874, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:27:36.950265: step 23875, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:27:37.127792: step 23876, loss 5.76755e-06, acc 1\n",
      "2018-10-26T18:27:37.298337: step 23877, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:27:37.475862: step 23878, loss 5.82998e-07, acc 1\n",
      "2018-10-26T18:27:37.649398: step 23879, loss 1.7601e-06, acc 1\n",
      "2018-10-26T18:27:37.836898: step 23880, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:27:38.013426: step 23881, loss 0.000100401, acc 1\n",
      "2018-10-26T18:27:38.190951: step 23882, loss 0.000295381, acc 1\n",
      "2018-10-26T18:27:38.377452: step 23883, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:27:38.544008: step 23884, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:27:38.731508: step 23885, loss 8.36306e-07, acc 1\n",
      "2018-10-26T18:27:38.900056: step 23886, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:27:39.077610: step 23887, loss 4.63752e-05, acc 1\n",
      "2018-10-26T18:27:39.246131: step 23888, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:27:39.426650: step 23889, loss 6.33718e-05, acc 1\n",
      "2018-10-26T18:27:39.595199: step 23890, loss 0.00117614, acc 1\n",
      "2018-10-26T18:27:39.781701: step 23891, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:27:39.951248: step 23892, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:40.133760: step 23893, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:27:40.301312: step 23894, loss 1.04492e-06, acc 1\n",
      "2018-10-26T18:27:40.483824: step 23895, loss 2.85895e-06, acc 1\n",
      "2018-10-26T18:27:40.655367: step 23896, loss 4.22578e-06, acc 1\n",
      "2018-10-26T18:27:40.844860: step 23897, loss 1.8644e-06, acc 1\n",
      "2018-10-26T18:27:41.016402: step 23898, loss 0.000624452, acc 1\n",
      "2018-10-26T18:27:41.195922: step 23899, loss 1.07438e-05, acc 1\n",
      "2018-10-26T18:27:41.368461: step 23900, loss 1.12129e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:27:41.841198: step 23900, loss 8.51496, acc 0.727017\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-23900\n",
      "\n",
      "2018-10-26T18:27:42.220099: step 23901, loss 0.000496542, acc 1\n",
      "2018-10-26T18:27:42.404606: step 23902, loss 6.22113e-07, acc 1\n",
      "2018-10-26T18:27:42.578142: step 23903, loss 0.00311744, acc 1\n",
      "2018-10-26T18:27:42.763647: step 23904, loss 1.86265e-09, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:27:42.995030: step 23905, loss 0.000197059, acc 1\n",
      "2018-10-26T18:27:43.193498: step 23906, loss 0, acc 1\n",
      "2018-10-26T18:27:43.371023: step 23907, loss 0.0494183, acc 0.984375\n",
      "2018-10-26T18:27:43.548550: step 23908, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:27:43.718096: step 23909, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:27:43.887644: step 23910, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:27:44.071155: step 23911, loss 9.96488e-07, acc 1\n",
      "2018-10-26T18:27:44.242695: step 23912, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:27:44.422216: step 23913, loss 0, acc 1\n",
      "2018-10-26T18:27:44.589767: step 23914, loss 0, acc 1\n",
      "2018-10-26T18:27:44.760312: step 23915, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:27:44.932852: step 23916, loss 3.14563e-05, acc 1\n",
      "2018-10-26T18:27:45.106388: step 23917, loss 0, acc 1\n",
      "2018-10-26T18:27:45.272942: step 23918, loss 3.03609e-07, acc 1\n",
      "2018-10-26T18:27:45.446478: step 23919, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:27:45.620015: step 23920, loss 0, acc 1\n",
      "2018-10-26T18:27:45.799535: step 23921, loss 0, acc 1\n",
      "2018-10-26T18:27:45.971077: step 23922, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:27:46.144613: step 23923, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:27:46.315158: step 23924, loss 1.76238e-05, acc 1\n",
      "2018-10-26T18:27:46.492684: step 23925, loss 4.34709e-06, acc 1\n",
      "2018-10-26T18:27:46.663228: step 23926, loss 1.86264e-07, acc 1\n",
      "2018-10-26T18:27:46.840753: step 23927, loss 0, acc 1\n",
      "2018-10-26T18:27:47.007308: step 23928, loss 6.8707e-05, acc 1\n",
      "2018-10-26T18:27:47.186828: step 23929, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:27:47.357373: step 23930, loss 0.000254779, acc 1\n",
      "2018-10-26T18:27:47.536894: step 23931, loss 8.72033e-06, acc 1\n",
      "2018-10-26T18:27:47.704446: step 23932, loss 5.01905e-06, acc 1\n",
      "2018-10-26T18:27:47.875987: step 23933, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:27:48.043540: step 23934, loss 6.38571e-06, acc 1\n",
      "2018-10-26T18:27:48.225054: step 23935, loss 5.9231e-07, acc 1\n",
      "2018-10-26T18:27:48.389615: step 23936, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:27:48.561156: step 23937, loss 0, acc 1\n",
      "2018-10-26T18:27:48.731701: step 23938, loss 5.02118e-05, acc 1\n",
      "2018-10-26T18:27:48.908229: step 23939, loss 4.1723e-07, acc 1\n",
      "2018-10-26T18:27:49.073787: step 23940, loss 6.61858e-06, acc 1\n",
      "2018-10-26T18:27:49.254305: step 23941, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:27:49.419863: step 23942, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:27:49.595393: step 23943, loss 0.0931526, acc 0.984375\n",
      "2018-10-26T18:27:49.762946: step 23944, loss 0.00049592, acc 1\n",
      "2018-10-26T18:27:49.948453: step 23945, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:27:50.116002: step 23946, loss 0.000337415, acc 1\n",
      "2018-10-26T18:27:50.291533: step 23947, loss 0, acc 1\n",
      "2018-10-26T18:27:50.461080: step 23948, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:27:50.638605: step 23949, loss 8.09888e-06, acc 1\n",
      "2018-10-26T18:27:50.807156: step 23950, loss 4.93709e-06, acc 1\n",
      "2018-10-26T18:27:50.995651: step 23951, loss 8.2886e-06, acc 1\n",
      "2018-10-26T18:27:51.160212: step 23952, loss 0.0116092, acc 0.984375\n",
      "2018-10-26T18:27:51.338735: step 23953, loss 8.95916e-07, acc 1\n",
      "2018-10-26T18:27:51.503296: step 23954, loss 0.000797719, acc 1\n",
      "2018-10-26T18:27:51.681819: step 23955, loss 0.00116797, acc 1\n",
      "2018-10-26T18:27:51.859344: step 23956, loss 0.033303, acc 0.984375\n",
      "2018-10-26T18:27:52.040043: step 23957, loss 0.113353, acc 0.984375\n",
      "2018-10-26T18:27:52.213579: step 23958, loss 0.000826173, acc 1\n",
      "2018-10-26T18:27:52.388113: step 23959, loss 2.03388e-06, acc 1\n",
      "2018-10-26T18:27:52.557659: step 23960, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:52.733191: step 23961, loss 0, acc 1\n",
      "2018-10-26T18:27:52.902737: step 23962, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:27:53.087245: step 23963, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:27:53.258786: step 23964, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:27:53.435314: step 23965, loss 0.015501, acc 0.984375\n",
      "2018-10-26T18:27:53.612840: step 23966, loss 2.2183e-06, acc 1\n",
      "2018-10-26T18:27:53.782387: step 23967, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:27:53.962905: step 23968, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:27:54.140431: step 23969, loss 6.76553e-06, acc 1\n",
      "2018-10-26T18:27:54.321946: step 23970, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:27:54.495482: step 23971, loss 7.42104e-05, acc 1\n",
      "2018-10-26T18:27:54.678991: step 23972, loss 0.000102111, acc 1\n",
      "2018-10-26T18:27:54.863499: step 23973, loss 5.36734e-06, acc 1\n",
      "2018-10-26T18:27:55.041025: step 23974, loss 8.87637e-06, acc 1\n",
      "2018-10-26T18:27:55.218550: step 23975, loss 0, acc 1\n",
      "2018-10-26T18:27:55.399067: step 23976, loss 1.69305e-06, acc 1\n",
      "2018-10-26T18:27:55.567618: step 23977, loss 0.00074343, acc 1\n",
      "2018-10-26T18:27:55.751127: step 23978, loss 5.32707e-07, acc 1\n",
      "2018-10-26T18:27:55.926658: step 23979, loss 0, acc 1\n",
      "2018-10-26T18:27:56.113160: step 23980, loss 1.82538e-07, acc 1\n",
      "2018-10-26T18:27:56.283703: step 23981, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:27:56.463224: step 23982, loss 0, acc 1\n",
      "2018-10-26T18:27:56.630777: step 23983, loss 3.87428e-07, acc 1\n",
      "2018-10-26T18:27:56.806307: step 23984, loss 3.01391e-05, acc 1\n",
      "2018-10-26T18:27:56.973859: step 23985, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:27:57.151386: step 23986, loss 7.59941e-07, acc 1\n",
      "2018-10-26T18:27:57.321931: step 23987, loss 8.71774e-06, acc 1\n",
      "2018-10-26T18:27:57.504444: step 23988, loss 4.22036e-05, acc 1\n",
      "2018-10-26T18:27:57.670997: step 23989, loss 6.91083e-06, acc 1\n",
      "2018-10-26T18:27:57.855504: step 23990, loss 3.86266e-06, acc 1\n",
      "2018-10-26T18:27:58.033030: step 23991, loss 3.29686e-07, acc 1\n",
      "2018-10-26T18:27:58.218534: step 23992, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:27:58.386087: step 23993, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:27:58.566605: step 23994, loss 1.39135e-06, acc 1\n",
      "2018-10-26T18:27:58.734157: step 23995, loss 1.27959e-06, acc 1\n",
      "2018-10-26T18:27:58.914674: step 23996, loss 0, acc 1\n",
      "2018-10-26T18:27:59.084222: step 23997, loss 0, acc 1\n",
      "2018-10-26T18:27:59.269786: step 23998, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:27:59.432292: step 23999, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:27:59.605828: step 24000, loss 2.38418e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:28:00.069589: step 24000, loss 8.73288, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24000\n",
      "\n",
      "2018-10-26T18:28:00.442866: step 24001, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:28:00.613411: step 24002, loss 8.00917e-07, acc 1\n",
      "2018-10-26T18:28:00.795924: step 24003, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:28:00.963476: step 24004, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:28:01.161946: step 24005, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:28:01.409285: step 24006, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:28:01.576837: step 24007, loss 7.58078e-07, acc 1\n",
      "2018-10-26T18:28:01.763339: step 24008, loss 0.000519949, acc 1\n",
      "2018-10-26T18:28:01.932886: step 24009, loss 0, acc 1\n",
      "2018-10-26T18:28:02.112406: step 24010, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:28:02.285943: step 24011, loss 4.30055e-06, acc 1\n",
      "2018-10-26T18:28:02.465464: step 24012, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:02.635009: step 24013, loss 6.57501e-07, acc 1\n",
      "2018-10-26T18:28:02.815527: step 24014, loss 0, acc 1\n",
      "2018-10-26T18:28:02.992056: step 24015, loss 5.71822e-07, acc 1\n",
      "2018-10-26T18:28:03.176563: step 24016, loss 1.43749e-05, acc 1\n",
      "2018-10-26T18:28:03.339128: step 24017, loss 0.00116358, acc 1\n",
      "2018-10-26T18:28:03.519646: step 24018, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:03.686202: step 24019, loss 0.000166965, acc 1\n",
      "2018-10-26T18:28:03.874698: step 24020, loss 2.57222e-06, acc 1\n",
      "2018-10-26T18:28:04.051226: step 24021, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:28:04.236730: step 24022, loss 4.02326e-07, acc 1\n",
      "2018-10-26T18:28:04.406278: step 24023, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:28:04.582806: step 24024, loss 0.000121722, acc 1\n",
      "2018-10-26T18:28:04.746369: step 24025, loss 0.000187577, acc 1\n",
      "2018-10-26T18:28:04.944838: step 24026, loss 8.2084e-06, acc 1\n",
      "2018-10-26T18:28:05.126353: step 24027, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:28:05.344769: step 24028, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:28:05.533267: step 24029, loss 0, acc 1\n",
      "2018-10-26T18:28:05.725752: step 24030, loss 9.872e-08, acc 1\n",
      "2018-10-26T18:28:05.915247: step 24031, loss 2.06752e-07, acc 1\n",
      "2018-10-26T18:28:06.107731: step 24032, loss 4.17227e-07, acc 1\n",
      "2018-10-26T18:28:06.285257: step 24033, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:28:06.491705: step 24034, loss 2.94643e-06, acc 1\n",
      "2018-10-26T18:28:06.673220: step 24035, loss 0.0630332, acc 0.984375\n",
      "2018-10-26T18:28:06.870693: step 24036, loss 4.76833e-07, acc 1\n",
      "2018-10-26T18:28:07.089122: step 24037, loss 9.60091e-06, acc 1\n",
      "2018-10-26T18:28:07.305530: step 24038, loss 5.08444e-06, acc 1\n",
      "2018-10-26T18:28:07.524946: step 24039, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:28:07.742364: step 24040, loss 9.44454e-06, acc 1\n",
      "2018-10-26T18:28:07.922882: step 24041, loss 1.06602e-05, acc 1\n",
      "2018-10-26T18:28:08.139303: step 24042, loss 7.9983e-05, acc 1\n",
      "2018-10-26T18:28:08.351736: step 24043, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:28:08.564168: step 24044, loss 1.15294e-06, acc 1\n",
      "2018-10-26T18:28:08.742691: step 24045, loss 1.08722e-05, acc 1\n",
      "2018-10-26T18:28:08.922211: step 24046, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:28:09.146611: step 24047, loss 0, acc 1\n",
      "2018-10-26T18:28:09.323141: step 24048, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:28:09.499698: step 24049, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:28:09.717087: step 24050, loss 9.04612e-06, acc 1\n",
      "2018-10-26T18:28:09.927476: step 24051, loss 0.000710822, acc 1\n",
      "2018-10-26T18:28:10.110985: step 24052, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:28:10.331397: step 24053, loss 8.99633e-07, acc 1\n",
      "2018-10-26T18:28:10.535851: step 24054, loss 0, acc 1\n",
      "2018-10-26T18:28:10.722353: step 24055, loss 8.02782e-07, acc 1\n",
      "2018-10-26T18:28:10.939770: step 24056, loss 2.64494e-07, acc 1\n",
      "2018-10-26T18:28:11.113307: step 24057, loss 6.22372e-06, acc 1\n",
      "2018-10-26T18:28:11.345686: step 24058, loss 8.94044e-07, acc 1\n",
      "2018-10-26T18:28:11.562109: step 24059, loss 4.88682e-06, acc 1\n",
      "2018-10-26T18:28:11.735644: step 24060, loss 9.89683e-06, acc 1\n",
      "2018-10-26T18:28:11.959047: step 24061, loss 1.3038e-06, acc 1\n",
      "2018-10-26T18:28:12.129591: step 24062, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:28:12.326066: step 24063, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:28:12.509576: step 24064, loss 0.00896626, acc 1\n",
      "2018-10-26T18:28:12.721012: step 24065, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:28:12.925466: step 24066, loss 7.31879e-06, acc 1\n",
      "2018-10-26T18:28:13.099998: step 24067, loss 4.26543e-07, acc 1\n",
      "2018-10-26T18:28:13.313429: step 24068, loss 9.12693e-08, acc 1\n",
      "2018-10-26T18:28:13.483973: step 24069, loss 0.002606, acc 1\n",
      "2018-10-26T18:28:13.692416: step 24070, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:28:13.864955: step 24071, loss 5.68005e-06, acc 1\n",
      "2018-10-26T18:28:14.049462: step 24072, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:28:14.255910: step 24073, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:28:14.434433: step 24074, loss 1.67638e-07, acc 1\n",
      "2018-10-26T18:28:14.609963: step 24075, loss 0.000104487, acc 1\n",
      "2018-10-26T18:28:14.830375: step 24076, loss 1.12313e-06, acc 1\n",
      "2018-10-26T18:28:14.998925: step 24077, loss 9.08663e-06, acc 1\n",
      "2018-10-26T18:28:15.171464: step 24078, loss 0, acc 1\n",
      "2018-10-26T18:28:15.346995: step 24079, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:28:15.544468: step 24080, loss 3.09196e-07, acc 1\n",
      "2018-10-26T18:28:15.716009: step 24081, loss 0.000239045, acc 1\n",
      "2018-10-26T18:28:15.911488: step 24082, loss 2.87753e-06, acc 1\n",
      "2018-10-26T18:28:16.145861: step 24083, loss 5.15724e-06, acc 1\n",
      "2018-10-26T18:28:16.332362: step 24084, loss 0.00400203, acc 1\n",
      "2018-10-26T18:28:16.518864: step 24085, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:28:16.695392: step 24086, loss 1.87557e-06, acc 1\n",
      "2018-10-26T18:28:16.895859: step 24087, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:28:17.124609: step 24088, loss 1.62791e-06, acc 1\n",
      "2018-10-26T18:28:17.308120: step 24089, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:28:17.506589: step 24090, loss 0, acc 1\n",
      "2018-10-26T18:28:17.720019: step 24091, loss 0.00265283, acc 1\n",
      "2018-10-26T18:28:17.924489: step 24092, loss 0, acc 1\n",
      "2018-10-26T18:28:18.116959: step 24093, loss 2.50604e-05, acc 1\n",
      "2018-10-26T18:28:18.330388: step 24094, loss 1.93714e-07, acc 1\n",
      "2018-10-26T18:28:18.507913: step 24095, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:28:18.715359: step 24096, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:28:18.886901: step 24097, loss 0, acc 1\n",
      "2018-10-26T18:28:19.109307: step 24098, loss 0.000303791, acc 1\n",
      "2018-10-26T18:28:19.285836: step 24099, loss 7.96415e-05, acc 1\n",
      "2018-10-26T18:28:19.462364: step 24100, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:28:19.955047: step 24100, loss 8.59196, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24100\n",
      "\n",
      "2018-10-26T18:28:20.341696: step 24101, loss 3.66024e-05, acc 1\n",
      "2018-10-26T18:28:20.526164: step 24102, loss 1.50134e-05, acc 1\n",
      "2018-10-26T18:28:20.705685: step 24103, loss 0, acc 1\n",
      "2018-10-26T18:28:20.882213: step 24104, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:28:21.132545: step 24105, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:28:21.315057: step 24106, loss 0, acc 1\n",
      "2018-10-26T18:28:21.492583: step 24107, loss 3.21463e-06, acc 1\n",
      "2018-10-26T18:28:21.683074: step 24108, loss 2.14203e-07, acc 1\n",
      "2018-10-26T18:28:21.847634: step 24109, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:28:22.025160: step 24110, loss 0, acc 1\n",
      "2018-10-26T18:28:22.198695: step 24111, loss 0.000670355, acc 1\n",
      "2018-10-26T18:28:22.379213: step 24112, loss 5.43887e-07, acc 1\n",
      "2018-10-26T18:28:22.550756: step 24113, loss 0, acc 1\n",
      "2018-10-26T18:28:22.730275: step 24114, loss 0, acc 1\n",
      "2018-10-26T18:28:22.898826: step 24115, loss 1.36901e-06, acc 1\n",
      "2018-10-26T18:28:23.075353: step 24116, loss 7.89742e-07, acc 1\n",
      "2018-10-26T18:28:23.242906: step 24117, loss 0.000481198, acc 1\n",
      "2018-10-26T18:28:23.422426: step 24118, loss 1.8067e-06, acc 1\n",
      "2018-10-26T18:28:23.594964: step 24119, loss 3.76582e-06, acc 1\n",
      "2018-10-26T18:28:23.775483: step 24120, loss 2.51032e-05, acc 1\n",
      "2018-10-26T18:28:23.948022: step 24121, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:28:24.125548: step 24122, loss 1.76941e-06, acc 1\n",
      "2018-10-26T18:28:24.293100: step 24123, loss 9.96484e-07, acc 1\n",
      "2018-10-26T18:28:24.474615: step 24124, loss 4.34867e-06, acc 1\n",
      "2018-10-26T18:28:24.643164: step 24125, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:28:24.835650: step 24126, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:28:25.001208: step 24127, loss 3.21646e-06, acc 1\n",
      "2018-10-26T18:28:25.182724: step 24128, loss 3.82492e-05, acc 1\n",
      "2018-10-26T18:28:25.348281: step 24129, loss 2.84984e-07, acc 1\n",
      "2018-10-26T18:28:25.527801: step 24130, loss 0.00100196, acc 1\n",
      "2018-10-26T18:28:25.697348: step 24131, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:28:25.876868: step 24132, loss 1.12315e-05, acc 1\n",
      "2018-10-26T18:28:26.041429: step 24133, loss 5.1738e-06, acc 1\n",
      "2018-10-26T18:28:26.219952: step 24134, loss 6.01626e-07, acc 1\n",
      "2018-10-26T18:28:26.387504: step 24135, loss 2.03027e-07, acc 1\n",
      "2018-10-26T18:28:26.564033: step 24136, loss 7.45056e-08, acc 1\n",
      "2018-10-26T18:28:26.728593: step 24137, loss 0.000271659, acc 1\n",
      "2018-10-26T18:28:26.909111: step 24138, loss 2.76677e-05, acc 1\n",
      "2018-10-26T18:28:27.075689: step 24139, loss 1.96984e-05, acc 1\n",
      "2018-10-26T18:28:27.258177: step 24140, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:28:27.426728: step 24141, loss 3.07333e-07, acc 1\n",
      "2018-10-26T18:28:27.614226: step 24142, loss 8.00732e-06, acc 1\n",
      "2018-10-26T18:28:27.787762: step 24143, loss 2.10477e-07, acc 1\n",
      "2018-10-26T18:28:27.967283: step 24144, loss 9.08945e-07, acc 1\n",
      "2018-10-26T18:28:28.134836: step 24145, loss 1.21071e-07, acc 1\n",
      "2018-10-26T18:28:28.319342: step 24146, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:28.486895: step 24147, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:28:28.664421: step 24148, loss 3.1106e-07, acc 1\n",
      "2018-10-26T18:28:28.830975: step 24149, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:29.013488: step 24150, loss 5.83825e-06, acc 1\n",
      "2018-10-26T18:28:29.181041: step 24151, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:28:29.366545: step 24152, loss 1.1479e-05, acc 1\n",
      "2018-10-26T18:28:29.540081: step 24153, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:29.724587: step 24154, loss 2.12328e-06, acc 1\n",
      "2018-10-26T18:28:29.902115: step 24155, loss 4.53675e-06, acc 1\n",
      "2018-10-26T18:28:30.091607: step 24156, loss 9.49947e-08, acc 1\n",
      "2018-10-26T18:28:30.277111: step 24157, loss 1.03156e-05, acc 1\n",
      "2018-10-26T18:28:30.460621: step 24158, loss 1.89718e-05, acc 1\n",
      "2018-10-26T18:28:30.628174: step 24159, loss 0.000374035, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:28:30.812681: step 24160, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:28:30.984222: step 24161, loss 0, acc 1\n",
      "2018-10-26T18:28:31.159753: step 24162, loss 4.57772e-06, acc 1\n",
      "2018-10-26T18:28:31.332292: step 24163, loss 0.000295689, acc 1\n",
      "2018-10-26T18:28:31.504831: step 24164, loss 1.78991e-05, acc 1\n",
      "2018-10-26T18:28:31.680362: step 24165, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:28:31.859883: step 24166, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:28:32.028432: step 24167, loss 4.92034e-06, acc 1\n",
      "2018-10-26T18:28:32.198976: step 24168, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:28:32.367527: step 24169, loss 1.97615e-05, acc 1\n",
      "2018-10-26T18:28:32.542060: step 24170, loss 8.09955e-05, acc 1\n",
      "2018-10-26T18:28:32.711607: step 24171, loss 1.06955e-05, acc 1\n",
      "2018-10-26T18:28:32.882152: step 24172, loss 0.00161326, acc 1\n",
      "2018-10-26T18:28:33.047731: step 24173, loss 0, acc 1\n",
      "2018-10-26T18:28:33.221245: step 24174, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:33.397774: step 24175, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:28:33.572307: step 24176, loss 2.04683e-05, acc 1\n",
      "2018-10-26T18:28:33.742851: step 24177, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:28:33.919380: step 24178, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:28:34.083940: step 24179, loss 7.32142e-05, acc 1\n",
      "2018-10-26T18:28:34.258474: step 24180, loss 0, acc 1\n",
      "2018-10-26T18:28:34.429018: step 24181, loss 0, acc 1\n",
      "2018-10-26T18:28:34.600560: step 24182, loss 0, acc 1\n",
      "2018-10-26T18:28:34.767115: step 24183, loss 0, acc 1\n",
      "2018-10-26T18:28:34.937677: step 24184, loss 7.00738e-05, acc 1\n",
      "2018-10-26T18:28:35.111195: step 24185, loss 0, acc 1\n",
      "2018-10-26T18:28:35.287723: step 24186, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:28:35.458269: step 24187, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:28:35.637789: step 24188, loss 0, acc 1\n",
      "2018-10-26T18:28:35.809331: step 24189, loss 1.11383e-06, acc 1\n",
      "2018-10-26T18:28:35.986856: step 24190, loss 2.68384e-06, acc 1\n",
      "2018-10-26T18:28:36.152414: step 24191, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:36.354872: step 24192, loss 0, acc 1\n",
      "2018-10-26T18:28:36.529406: step 24193, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:28:36.704938: step 24194, loss 2.53113e-06, acc 1\n",
      "2018-10-26T18:28:36.874484: step 24195, loss 1.45331e-05, acc 1\n",
      "2018-10-26T18:28:37.049018: step 24196, loss 0.000162787, acc 1\n",
      "2018-10-26T18:28:37.221557: step 24197, loss 2.01165e-07, acc 1\n",
      "2018-10-26T18:28:37.396090: step 24198, loss 1.17343e-06, acc 1\n",
      "2018-10-26T18:28:37.566635: step 24199, loss 8.54938e-07, acc 1\n",
      "2018-10-26T18:28:37.747153: step 24200, loss 5.58794e-09, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:28:38.201937: step 24200, loss 8.61978, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24200\n",
      "\n",
      "2018-10-26T18:28:38.553429: step 24201, loss 1.92958e-05, acc 1\n",
      "2018-10-26T18:28:38.721979: step 24202, loss 2.77533e-07, acc 1\n",
      "2018-10-26T18:28:38.898507: step 24203, loss 0, acc 1\n",
      "2018-10-26T18:28:39.069051: step 24204, loss 4.52637e-05, acc 1\n",
      "2018-10-26T18:28:39.259543: step 24205, loss 2.99805e-05, acc 1\n",
      "2018-10-26T18:28:39.499901: step 24206, loss 0.00027304, acc 1\n",
      "2018-10-26T18:28:39.672439: step 24207, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:28:39.855949: step 24208, loss 1.20323e-06, acc 1\n",
      "2018-10-26T18:28:40.028488: step 24209, loss 1.84396e-06, acc 1\n",
      "2018-10-26T18:28:40.208008: step 24210, loss 0.000167687, acc 1\n",
      "2018-10-26T18:28:40.377556: step 24211, loss 0, acc 1\n",
      "2018-10-26T18:28:40.555081: step 24212, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:28:40.726623: step 24213, loss 0.000896119, acc 1\n",
      "2018-10-26T18:28:40.904148: step 24214, loss 0, acc 1\n",
      "2018-10-26T18:28:41.074693: step 24215, loss 0, acc 1\n",
      "2018-10-26T18:28:41.247231: step 24216, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:28:41.420767: step 24217, loss 1.20138e-06, acc 1\n",
      "2018-10-26T18:28:41.600288: step 24218, loss 1.25231e-05, acc 1\n",
      "2018-10-26T18:28:41.771830: step 24219, loss 3.68801e-07, acc 1\n",
      "2018-10-26T18:28:41.954343: step 24220, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:28:42.122892: step 24221, loss 4.82418e-07, acc 1\n",
      "2018-10-26T18:28:42.300417: step 24222, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:28:42.473954: step 24223, loss 0.000736059, acc 1\n",
      "2018-10-26T18:28:42.651480: step 24224, loss 0.000271057, acc 1\n",
      "2018-10-26T18:28:42.822024: step 24225, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:28:43.006532: step 24226, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:28:43.178073: step 24227, loss 6.5328e-06, acc 1\n",
      "2018-10-26T18:28:43.357593: step 24228, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:28:43.528137: step 24229, loss 1.05608e-06, acc 1\n",
      "2018-10-26T18:28:43.706661: step 24230, loss 2.41008e-06, acc 1\n",
      "2018-10-26T18:28:43.873215: step 24231, loss 4.78693e-07, acc 1\n",
      "2018-10-26T18:28:44.057723: step 24232, loss 2.28585e-05, acc 1\n",
      "2018-10-26T18:28:44.226272: step 24233, loss 1.88127e-07, acc 1\n",
      "2018-10-26T18:28:44.404796: step 24234, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:28:44.571350: step 24235, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:28:44.757851: step 24236, loss 4.66223e-05, acc 1\n",
      "2018-10-26T18:28:44.921414: step 24237, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:28:45.101932: step 24238, loss 1.28879e-05, acc 1\n",
      "2018-10-26T18:28:45.274472: step 24239, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:28:45.456983: step 24240, loss 0, acc 1\n",
      "2018-10-26T18:28:45.631517: step 24241, loss 1.57386e-06, acc 1\n",
      "2018-10-26T18:28:45.814030: step 24242, loss 0.00117845, acc 1\n",
      "2018-10-26T18:28:45.984574: step 24243, loss 1.45274e-05, acc 1\n",
      "2018-10-26T18:28:46.161103: step 24244, loss 0, acc 1\n",
      "2018-10-26T18:28:46.336633: step 24245, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:28:46.515157: step 24246, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:28:46.695675: step 24247, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:28:46.877189: step 24248, loss 3.52037e-07, acc 1\n",
      "2018-10-26T18:28:47.046737: step 24249, loss 1.79735e-06, acc 1\n",
      "2018-10-26T18:28:47.228251: step 24250, loss 0, acc 1\n",
      "2018-10-26T18:28:47.396800: step 24251, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:28:47.582306: step 24252, loss 2.94295e-07, acc 1\n",
      "2018-10-26T18:28:47.756839: step 24253, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:28:47.935361: step 24254, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:28:48.109897: step 24255, loss 0.00133559, acc 1\n",
      "2018-10-26T18:28:48.290136: step 24256, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:28:48.458686: step 24257, loss 1.29076e-06, acc 1\n",
      "2018-10-26T18:28:48.648179: step 24258, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:28:48.819738: step 24259, loss 3.48313e-07, acc 1\n",
      "2018-10-26T18:28:48.995252: step 24260, loss 0.000226563, acc 1\n",
      "2018-10-26T18:28:49.162803: step 24261, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:28:49.351310: step 24262, loss 0.06011, acc 0.984375\n",
      "2018-10-26T18:28:49.517856: step 24263, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:28:49.708346: step 24264, loss 0, acc 1\n",
      "2018-10-26T18:28:49.880886: step 24265, loss 0.000108947, acc 1\n",
      "2018-10-26T18:28:50.061404: step 24266, loss 0.000154511, acc 1\n",
      "2018-10-26T18:28:50.229953: step 24267, loss 0.000569437, acc 1\n",
      "2018-10-26T18:28:50.409473: step 24268, loss 2.81257e-07, acc 1\n",
      "2018-10-26T18:28:50.578024: step 24269, loss 8.15069e-06, acc 1\n",
      "2018-10-26T18:28:50.756547: step 24270, loss 2.39696e-05, acc 1\n",
      "2018-10-26T18:28:50.928087: step 24271, loss 6.40744e-07, acc 1\n",
      "2018-10-26T18:28:51.107610: step 24272, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:28:51.276158: step 24273, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:28:51.454681: step 24274, loss 1.2808e-05, acc 1\n",
      "2018-10-26T18:28:51.621236: step 24275, loss 9.3132e-08, acc 1\n",
      "2018-10-26T18:28:51.801756: step 24276, loss 3.21832e-06, acc 1\n",
      "2018-10-26T18:28:51.976288: step 24277, loss 4.09779e-07, acc 1\n",
      "2018-10-26T18:28:52.154810: step 24278, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:52.325354: step 24279, loss 3.39899e-06, acc 1\n",
      "2018-10-26T18:28:52.509862: step 24280, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:28:52.681403: step 24281, loss 7.81204e-06, acc 1\n",
      "2018-10-26T18:28:52.864913: step 24282, loss 1.53102e-06, acc 1\n",
      "2018-10-26T18:28:53.036455: step 24283, loss 1.28146e-06, acc 1\n",
      "2018-10-26T18:28:53.219964: step 24284, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:28:53.389511: step 24285, loss 1.50685e-06, acc 1\n",
      "2018-10-26T18:28:53.581264: step 24286, loss 4.27145e-05, acc 1\n",
      "2018-10-26T18:28:53.753804: step 24287, loss 1.82539e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:28:53.930332: step 24288, loss 0.480851, acc 0.984375\n",
      "2018-10-26T18:28:54.098881: step 24289, loss 9.01494e-07, acc 1\n",
      "2018-10-26T18:28:54.273415: step 24290, loss 0.000172684, acc 1\n",
      "2018-10-26T18:28:54.451938: step 24291, loss 4.38434e-06, acc 1\n",
      "2018-10-26T18:28:54.625475: step 24292, loss 4.33305e-05, acc 1\n",
      "2018-10-26T18:28:54.802002: step 24293, loss 3.14168e-05, acc 1\n",
      "2018-10-26T18:28:54.968558: step 24294, loss 0, acc 1\n",
      "2018-10-26T18:28:55.141097: step 24295, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:28:55.307652: step 24296, loss 4.95456e-07, acc 1\n",
      "2018-10-26T18:28:55.485177: step 24297, loss 5.07675e-06, acc 1\n",
      "2018-10-26T18:28:55.652729: step 24298, loss 6.98482e-07, acc 1\n",
      "2018-10-26T18:28:55.832263: step 24299, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:28:55.993818: step 24300, loss 3.31796e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:28:56.442619: step 24300, loss 8.62519, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24300\n",
      "\n",
      "2018-10-26T18:28:56.822237: step 24301, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:57.007744: step 24302, loss 0, acc 1\n",
      "2018-10-26T18:28:57.176292: step 24303, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:28:57.343844: step 24304, loss 2.84214e-06, acc 1\n",
      "2018-10-26T18:28:57.543311: step 24305, loss 0.000771853, acc 1\n",
      "2018-10-26T18:28:57.768710: step 24306, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:28:57.943245: step 24307, loss 0.000235181, acc 1\n",
      "2018-10-26T18:28:58.120769: step 24308, loss 4.32128e-07, acc 1\n",
      "2018-10-26T18:28:58.285328: step 24309, loss 2.78452e-06, acc 1\n",
      "2018-10-26T18:28:58.462854: step 24310, loss 5.14944e-06, acc 1\n",
      "2018-10-26T18:28:58.632401: step 24311, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:28:58.815912: step 24312, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:28:58.985458: step 24313, loss 0, acc 1\n",
      "2018-10-26T18:28:59.156002: step 24314, loss 0.000583736, acc 1\n",
      "2018-10-26T18:28:59.324553: step 24315, loss 0, acc 1\n",
      "2018-10-26T18:28:59.499085: step 24316, loss 2.29105e-07, acc 1\n",
      "2018-10-26T18:28:59.672624: step 24317, loss 7.76704e-07, acc 1\n",
      "2018-10-26T18:28:59.857130: step 24318, loss 3.91293e-06, acc 1\n",
      "2018-10-26T18:29:00.031662: step 24319, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:29:00.214175: step 24320, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:29:00.387712: step 24321, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:29:00.563243: step 24322, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:29:00.738774: step 24323, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:29:00.920288: step 24324, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:29:01.085846: step 24325, loss 4.99565e-05, acc 1\n",
      "2018-10-26T18:29:01.266365: step 24326, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:01.430923: step 24327, loss 6.44462e-07, acc 1\n",
      "2018-10-26T18:29:01.604460: step 24328, loss 1.27856e-05, acc 1\n",
      "2018-10-26T18:29:01.774007: step 24329, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:29:01.952531: step 24330, loss 1.64333e-05, acc 1\n",
      "2018-10-26T18:29:02.121081: step 24331, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:29:02.298606: step 24332, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:29:02.464163: step 24333, loss 1.58317e-06, acc 1\n",
      "2018-10-26T18:29:02.637700: step 24334, loss 0, acc 1\n",
      "2018-10-26T18:29:02.813231: step 24335, loss 7.28278e-07, acc 1\n",
      "2018-10-26T18:29:02.987765: step 24336, loss 0, acc 1\n",
      "2018-10-26T18:29:03.154319: step 24337, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:29:03.329850: step 24338, loss 0.000979851, acc 1\n",
      "2018-10-26T18:29:03.496406: step 24339, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:29:03.670939: step 24340, loss 6.64954e-07, acc 1\n",
      "2018-10-26T18:29:03.847467: step 24341, loss 4.14011e-06, acc 1\n",
      "2018-10-26T18:29:04.024993: step 24342, loss 0, acc 1\n",
      "2018-10-26T18:29:04.192546: step 24343, loss 0, acc 1\n",
      "2018-10-26T18:29:04.372066: step 24344, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:29:04.552584: step 24345, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:29:04.727117: step 24346, loss 2.39087e-05, acc 1\n",
      "2018-10-26T18:29:04.898659: step 24347, loss 1.02286e-05, acc 1\n",
      "2018-10-26T18:29:05.079177: step 24348, loss 0, acc 1\n",
      "2018-10-26T18:29:05.252713: step 24349, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:29:05.430239: step 24350, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:29:05.595796: step 24351, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:29:05.785289: step 24352, loss 0, acc 1\n",
      "2018-10-26T18:29:05.959824: step 24353, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:29:06.141338: step 24354, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:06.309888: step 24355, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:29:06.488411: step 24356, loss 3.83701e-07, acc 1\n",
      "2018-10-26T18:29:06.669926: step 24357, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:06.844460: step 24358, loss 0, acc 1\n",
      "2018-10-26T18:29:07.019991: step 24359, loss 1.63092e-05, acc 1\n",
      "2018-10-26T18:29:07.191533: step 24360, loss 3.6746e-06, acc 1\n",
      "2018-10-26T18:29:07.372050: step 24361, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:29:07.542595: step 24362, loss 3.21101e-06, acc 1\n",
      "2018-10-26T18:29:07.729096: step 24363, loss 0.00114352, acc 1\n",
      "2018-10-26T18:29:07.899640: step 24364, loss 0, acc 1\n",
      "2018-10-26T18:29:08.075173: step 24365, loss 2.6857e-06, acc 1\n",
      "2018-10-26T18:29:08.238735: step 24366, loss 0.000239938, acc 1\n",
      "2018-10-26T18:29:08.415263: step 24367, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:08.586805: step 24368, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:29:08.778292: step 24369, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:29:08.975765: step 24370, loss 3.90407e-05, acc 1\n",
      "2018-10-26T18:29:09.179223: step 24371, loss 8.56954e-06, acc 1\n",
      "2018-10-26T18:29:09.373702: step 24372, loss 5.23395e-07, acc 1\n",
      "2018-10-26T18:29:09.554220: step 24373, loss 0.00399706, acc 1\n",
      "2018-10-26T18:29:09.728775: step 24374, loss 0.000100618, acc 1\n",
      "2018-10-26T18:29:09.925229: step 24375, loss 3.59489e-07, acc 1\n",
      "2018-10-26T18:29:10.104749: step 24376, loss 4.71932e-06, acc 1\n",
      "2018-10-26T18:29:10.285267: step 24377, loss 6.72401e-07, acc 1\n",
      "2018-10-26T18:29:10.472766: step 24378, loss 5.5506e-07, acc 1\n",
      "2018-10-26T18:29:10.640318: step 24379, loss 5.03589e-06, acc 1\n",
      "2018-10-26T18:29:10.821833: step 24380, loss 6.26993e-05, acc 1\n",
      "2018-10-26T18:29:10.992378: step 24381, loss 4.91733e-07, acc 1\n",
      "2018-10-26T18:29:11.182868: step 24382, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:29:11.387322: step 24383, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:29:11.620700: step 24384, loss 0.00235503, acc 1\n",
      "2018-10-26T18:29:11.801217: step 24385, loss 4.99495e-06, acc 1\n",
      "2018-10-26T18:29:12.010658: step 24386, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:29:12.178209: step 24387, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:29:12.392636: step 24388, loss 3.39713e-06, acc 1\n",
      "2018-10-26T18:29:12.564178: step 24389, loss 4.15157e-05, acc 1\n",
      "2018-10-26T18:29:12.742701: step 24390, loss 5.28982e-07, acc 1\n",
      "2018-10-26T18:29:12.975079: step 24391, loss 4.04189e-07, acc 1\n",
      "2018-10-26T18:29:13.207459: step 24392, loss 0, acc 1\n",
      "2018-10-26T18:29:13.422883: step 24393, loss 6.54463e-06, acc 1\n",
      "2018-10-26T18:29:13.647285: step 24394, loss 3.8929e-07, acc 1\n",
      "2018-10-26T18:29:13.868693: step 24395, loss 4.74968e-07, acc 1\n",
      "2018-10-26T18:29:14.117028: step 24396, loss 1.7303e-06, acc 1\n",
      "2018-10-26T18:29:14.293557: step 24397, loss 1.21071e-07, acc 1\n",
      "2018-10-26T18:29:14.519952: step 24398, loss 0.000133501, acc 1\n",
      "2018-10-26T18:29:14.700469: step 24399, loss 4.90094e-05, acc 1\n",
      "2018-10-26T18:29:14.897942: step 24400, loss 2.08615e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:29:15.429522: step 24400, loss 8.67922, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24400\n",
      "\n",
      "2018-10-26T18:29:15.855581: step 24401, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:29:16.034104: step 24402, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:29:16.224595: step 24403, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:29:16.398131: step 24404, loss 3.50174e-07, acc 1\n",
      "2018-10-26T18:29:16.686363: step 24405, loss 0.00613105, acc 1\n",
      "2018-10-26T18:29:16.895802: step 24406, loss 0, acc 1\n",
      "2018-10-26T18:29:17.120203: step 24407, loss 2.00596e-05, acc 1\n",
      "2018-10-26T18:29:17.302715: step 24408, loss 5.84861e-07, acc 1\n",
      "2018-10-26T18:29:17.552049: step 24409, loss 7.97194e-07, acc 1\n",
      "2018-10-26T18:29:17.725586: step 24410, loss 1.22986e-05, acc 1\n",
      "2018-10-26T18:29:17.959960: step 24411, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:29:18.125516: step 24412, loss 7.45058e-09, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:29:18.339944: step 24413, loss 0.000108864, acc 1\n",
      "2018-10-26T18:29:18.509491: step 24414, loss 9.29246e-06, acc 1\n",
      "2018-10-26T18:29:18.701977: step 24415, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:18.868532: step 24416, loss 0, acc 1\n",
      "2018-10-26T18:29:19.085951: step 24417, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:29:19.263476: step 24418, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:29:19.455962: step 24419, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:19.654432: step 24420, loss 3.42723e-07, acc 1\n",
      "2018-10-26T18:29:19.831957: step 24421, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:29:20.019457: step 24422, loss 2.60754e-06, acc 1\n",
      "2018-10-26T18:29:20.212939: step 24423, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:29:20.385478: step 24424, loss 0.000808238, acc 1\n",
      "2018-10-26T18:29:20.551036: step 24425, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:29:20.759479: step 24426, loss 0, acc 1\n",
      "2018-10-26T18:29:20.936007: step 24427, loss 1.13803e-05, acc 1\n",
      "2018-10-26T18:29:21.111539: step 24428, loss 1.14828e-05, acc 1\n",
      "2018-10-26T18:29:21.303027: step 24429, loss 2.50702e-06, acc 1\n",
      "2018-10-26T18:29:21.528424: step 24430, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:21.717918: step 24431, loss 0.0365734, acc 0.984375\n",
      "2018-10-26T18:29:21.915391: step 24432, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:22.122836: step 24433, loss 0.000269528, acc 1\n",
      "2018-10-26T18:29:22.319311: step 24434, loss 0.000612744, acc 1\n",
      "2018-10-26T18:29:22.505813: step 24435, loss 0, acc 1\n",
      "2018-10-26T18:29:22.724230: step 24436, loss 4.93595e-07, acc 1\n",
      "2018-10-26T18:29:22.906742: step 24437, loss 0, acc 1\n",
      "2018-10-26T18:29:23.111196: step 24438, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:23.284732: step 24439, loss 2.62631e-07, acc 1\n",
      "2018-10-26T18:29:23.482204: step 24440, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:23.663720: step 24441, loss 0.00011005, acc 1\n",
      "2018-10-26T18:29:23.842243: step 24442, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:24.044701: step 24443, loss 9.78932e-05, acc 1\n",
      "2018-10-26T18:29:24.231203: step 24444, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:29:24.411722: step 24445, loss 3.61286e-05, acc 1\n",
      "2018-10-26T18:29:24.626148: step 24446, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:29:24.798687: step 24447, loss 3.15869e-05, acc 1\n",
      "2018-10-26T18:29:24.982197: step 24448, loss 2.5423e-06, acc 1\n",
      "2018-10-26T18:29:25.173686: step 24449, loss 0.000417245, acc 1\n",
      "2018-10-26T18:29:25.358191: step 24450, loss 6.35782e-08, acc 1\n",
      "2018-10-26T18:29:25.536715: step 24451, loss 1.31392e-05, acc 1\n",
      "2018-10-26T18:29:25.714241: step 24452, loss 0, acc 1\n",
      "2018-10-26T18:29:25.914706: step 24453, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:29:26.091234: step 24454, loss 1.04493e-06, acc 1\n",
      "2018-10-26T18:29:26.273746: step 24455, loss 8.94069e-08, acc 1\n",
      "2018-10-26T18:29:26.445287: step 24456, loss 2.49581e-06, acc 1\n",
      "2018-10-26T18:29:26.620819: step 24457, loss 1.31373e-05, acc 1\n",
      "2018-10-26T18:29:26.789369: step 24458, loss 0.000314037, acc 1\n",
      "2018-10-26T18:29:26.968889: step 24459, loss 0.000371664, acc 1\n",
      "2018-10-26T18:29:27.137439: step 24460, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:29:27.310975: step 24461, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:27.487502: step 24462, loss 1.73033e-06, acc 1\n",
      "2018-10-26T18:29:27.664031: step 24463, loss 0, acc 1\n",
      "2018-10-26T18:29:27.829589: step 24464, loss 4.87378e-06, acc 1\n",
      "2018-10-26T18:29:28.010107: step 24465, loss 0.000178675, acc 1\n",
      "2018-10-26T18:29:28.179653: step 24466, loss 4.06052e-07, acc 1\n",
      "2018-10-26T18:29:28.366155: step 24467, loss 0.000197842, acc 1\n",
      "2018-10-26T18:29:28.540689: step 24468, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:29:28.722204: step 24469, loss 3.45237e-05, acc 1\n",
      "2018-10-26T18:29:28.895741: step 24470, loss 9.88581e-06, acc 1\n",
      "2018-10-26T18:29:29.065288: step 24471, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:29:29.239821: step 24472, loss 2.99012e-05, acc 1\n",
      "2018-10-26T18:29:29.408371: step 24473, loss 4.48893e-07, acc 1\n",
      "2018-10-26T18:29:29.582905: step 24474, loss 4.477e-05, acc 1\n",
      "2018-10-26T18:29:29.751454: step 24475, loss 8.43355e-05, acc 1\n",
      "2018-10-26T18:29:29.928980: step 24476, loss 0.00818659, acc 1\n",
      "2018-10-26T18:29:30.103514: step 24477, loss 1.67815e-06, acc 1\n",
      "2018-10-26T18:29:30.280041: step 24478, loss 0.00050913, acc 1\n",
      "2018-10-26T18:29:30.451583: step 24479, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:29:30.621131: step 24480, loss 5.05579e-05, acc 1\n",
      "2018-10-26T18:29:30.800651: step 24481, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:30.972192: step 24482, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:29:31.152709: step 24483, loss 1.21256e-06, acc 1\n",
      "2018-10-26T18:29:31.322257: step 24484, loss 2.38037e-05, acc 1\n",
      "2018-10-26T18:29:31.498786: step 24485, loss 7.78394e-06, acc 1\n",
      "2018-10-26T18:29:31.675313: step 24486, loss 0, acc 1\n",
      "2018-10-26T18:29:31.858825: step 24487, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:32.027382: step 24488, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:29:32.203902: step 24489, loss 2.05755e-05, acc 1\n",
      "2018-10-26T18:29:32.372451: step 24490, loss 9.68573e-08, acc 1\n",
      "2018-10-26T18:29:32.546984: step 24491, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:29:32.720521: step 24492, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:29:32.902036: step 24493, loss 1.8675e-05, acc 1\n",
      "2018-10-26T18:29:33.069588: step 24494, loss 2.66113e-05, acc 1\n",
      "2018-10-26T18:29:33.248112: step 24495, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:33.418656: step 24496, loss 2.74709e-05, acc 1\n",
      "2018-10-26T18:29:33.595184: step 24497, loss 4.37716e-07, acc 1\n",
      "2018-10-26T18:29:33.762737: step 24498, loss 0, acc 1\n",
      "2018-10-26T18:29:33.943255: step 24499, loss 7.26431e-08, acc 1\n",
      "2018-10-26T18:29:34.112801: step 24500, loss 3.70663e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:29:34.571576: step 24500, loss 8.54834, acc 0.728893\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24500\n",
      "\n",
      "2018-10-26T18:29:34.929910: step 24501, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:29:35.112421: step 24502, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:35.280970: step 24503, loss 0.0200823, acc 0.984375\n",
      "2018-10-26T18:29:35.461489: step 24504, loss 1.56461e-07, acc 1\n",
      "2018-10-26T18:29:35.653974: step 24505, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:29:35.903309: step 24506, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:29:36.085821: step 24507, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:29:36.280300: step 24508, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:36.445859: step 24509, loss 3.14229e-05, acc 1\n",
      "2018-10-26T18:29:36.627374: step 24510, loss 2.98021e-07, acc 1\n",
      "2018-10-26T18:29:36.800909: step 24511, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:29:36.978436: step 24512, loss 2.05629e-06, acc 1\n",
      "2018-10-26T18:29:37.149977: step 24513, loss 3.11059e-07, acc 1\n",
      "2018-10-26T18:29:37.333486: step 24514, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:29:37.500041: step 24515, loss 3.76396e-06, acc 1\n",
      "2018-10-26T18:29:37.684549: step 24516, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:29:37.852101: step 24517, loss 9.44796e-05, acc 1\n",
      "2018-10-26T18:29:38.038604: step 24518, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:29:38.210145: step 24519, loss 0, acc 1\n",
      "2018-10-26T18:29:38.393655: step 24520, loss 1.03696e-05, acc 1\n",
      "2018-10-26T18:29:38.559213: step 24521, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:29:38.751698: step 24522, loss 3.2444e-06, acc 1\n",
      "2018-10-26T18:29:38.924237: step 24523, loss 1.47917e-05, acc 1\n",
      "2018-10-26T18:29:39.152626: step 24524, loss 4.46813e-06, acc 1\n",
      "2018-10-26T18:29:39.320179: step 24525, loss 0.0774257, acc 0.984375\n",
      "2018-10-26T18:29:39.508675: step 24526, loss 0.000801498, acc 1\n",
      "2018-10-26T18:29:39.676227: step 24527, loss 1.5283e-05, acc 1\n",
      "2018-10-26T18:29:39.856745: step 24528, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:29:40.024298: step 24529, loss 2.1234e-07, acc 1\n",
      "2018-10-26T18:29:40.202821: step 24530, loss 3.42723e-07, acc 1\n",
      "2018-10-26T18:29:40.398299: step 24531, loss 3.89288e-07, acc 1\n",
      "2018-10-26T18:29:40.599760: step 24532, loss 0.000302519, acc 1\n",
      "2018-10-26T18:29:40.789254: step 24533, loss 6.68675e-07, acc 1\n",
      "2018-10-26T18:29:40.969773: step 24534, loss 0.00125147, acc 1\n",
      "2018-10-26T18:29:41.156274: step 24535, loss 2.01165e-07, acc 1\n",
      "2018-10-26T18:29:41.336791: step 24536, loss 1.4156e-07, acc 1\n",
      "2018-10-26T18:29:41.520300: step 24537, loss 0.000125747, acc 1\n",
      "2018-10-26T18:29:41.685858: step 24538, loss 4.41441e-07, acc 1\n",
      "2018-10-26T18:29:41.865385: step 24539, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:29:42.035922: step 24540, loss 4.28408e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:29:42.215443: step 24541, loss 0, acc 1\n",
      "2018-10-26T18:29:42.390974: step 24542, loss 0, acc 1\n",
      "2018-10-26T18:29:42.561519: step 24543, loss 0, acc 1\n",
      "2018-10-26T18:29:42.733060: step 24544, loss 2.98554e-06, acc 1\n",
      "2018-10-26T18:29:42.908591: step 24545, loss 9.14664e-06, acc 1\n",
      "2018-10-26T18:29:43.075146: step 24546, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:43.249680: step 24547, loss 1.19164e-05, acc 1\n",
      "2018-10-26T18:29:43.423216: step 24548, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:29:43.607830: step 24549, loss 0.000806638, acc 1\n",
      "2018-10-26T18:29:43.780366: step 24550, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:29:43.961881: step 24551, loss 8.39195e-06, acc 1\n",
      "2018-10-26T18:29:44.128436: step 24552, loss 2.84962e-06, acc 1\n",
      "2018-10-26T18:29:44.300975: step 24553, loss 9.35813e-06, acc 1\n",
      "2018-10-26T18:29:44.466532: step 24554, loss 5.28984e-07, acc 1\n",
      "2018-10-26T18:29:44.641067: step 24555, loss 7.65179e-06, acc 1\n",
      "2018-10-26T18:29:44.813605: step 24556, loss 3.35273e-07, acc 1\n",
      "2018-10-26T18:29:44.997116: step 24557, loss 0.00412101, acc 1\n",
      "2018-10-26T18:29:45.165665: step 24558, loss 1.82156e-06, acc 1\n",
      "2018-10-26T18:29:45.337206: step 24559, loss 0.00901715, acc 1\n",
      "2018-10-26T18:29:45.504759: step 24560, loss 0, acc 1\n",
      "2018-10-26T18:29:45.681287: step 24561, loss 0.000497805, acc 1\n",
      "2018-10-26T18:29:45.857816: step 24562, loss 5.569e-05, acc 1\n",
      "2018-10-26T18:29:46.042325: step 24563, loss 5.97901e-07, acc 1\n",
      "2018-10-26T18:29:46.219849: step 24564, loss 0.00662564, acc 1\n",
      "2018-10-26T18:29:46.401365: step 24565, loss 0, acc 1\n",
      "2018-10-26T18:29:46.569913: step 24566, loss 3.16943e-05, acc 1\n",
      "2018-10-26T18:29:46.741453: step 24567, loss 4.20955e-07, acc 1\n",
      "2018-10-26T18:29:46.916985: step 24568, loss 1.82767e-05, acc 1\n",
      "2018-10-26T18:29:47.089525: step 24569, loss 0.00526475, acc 1\n",
      "2018-10-26T18:29:47.270043: step 24570, loss 2.55162e-06, acc 1\n",
      "2018-10-26T18:29:47.435600: step 24571, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:29:47.613125: step 24572, loss 1.95007e-06, acc 1\n",
      "2018-10-26T18:29:47.778683: step 24573, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:29:47.954214: step 24574, loss 4.7405e-05, acc 1\n",
      "2018-10-26T18:29:48.123761: step 24575, loss 0.00020049, acc 1\n",
      "2018-10-26T18:29:48.308268: step 24576, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:29:48.478812: step 24577, loss 0, acc 1\n",
      "2018-10-26T18:29:48.658332: step 24578, loss 2.65946e-05, acc 1\n",
      "2018-10-26T18:29:48.826882: step 24579, loss 2.29012e-05, acc 1\n",
      "2018-10-26T18:29:48.999422: step 24580, loss 4.54479e-07, acc 1\n",
      "2018-10-26T18:29:49.166974: step 24581, loss 0.00271497, acc 1\n",
      "2018-10-26T18:29:49.348489: step 24582, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:49.520030: step 24583, loss 4.00464e-07, acc 1\n",
      "2018-10-26T18:29:49.691572: step 24584, loss 5.79272e-07, acc 1\n",
      "2018-10-26T18:29:49.857129: step 24585, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:29:50.032660: step 24586, loss 1.20881e-06, acc 1\n",
      "2018-10-26T18:29:50.204202: step 24587, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:29:50.382726: step 24588, loss 1.65168e-05, acc 1\n",
      "2018-10-26T18:29:50.555265: step 24589, loss 4.61496e-06, acc 1\n",
      "2018-10-26T18:29:50.727803: step 24590, loss 0.000111955, acc 1\n",
      "2018-10-26T18:29:50.896354: step 24591, loss 1.43824e-05, acc 1\n",
      "2018-10-26T18:29:51.084849: step 24592, loss 0, acc 1\n",
      "2018-10-26T18:29:51.249409: step 24593, loss 5.04769e-07, acc 1\n",
      "2018-10-26T18:29:51.430925: step 24594, loss 2.73599e-06, acc 1\n",
      "2018-10-26T18:29:51.604461: step 24595, loss 8.82881e-07, acc 1\n",
      "2018-10-26T18:29:51.778995: step 24596, loss 0.000347591, acc 1\n",
      "2018-10-26T18:29:51.943556: step 24597, loss 0.000326056, acc 1\n",
      "2018-10-26T18:29:52.123076: step 24598, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:29:52.292623: step 24599, loss 1.00022e-06, acc 1\n",
      "2018-10-26T18:29:52.464163: step 24600, loss 3.33783e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:29:52.921942: step 24600, loss 8.53922, acc 0.727955\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24600\n",
      "\n",
      "2018-10-26T18:29:53.270914: step 24601, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:29:53.440187: step 24602, loss 4.80558e-07, acc 1\n",
      "2018-10-26T18:29:53.623696: step 24603, loss 0, acc 1\n",
      "2018-10-26T18:29:53.799227: step 24604, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:29:54.009665: step 24605, loss 1.38724e-05, acc 1\n",
      "2018-10-26T18:29:54.248028: step 24606, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:29:54.414584: step 24607, loss 4.54607e-06, acc 1\n",
      "2018-10-26T18:29:54.594104: step 24608, loss 1.42486e-06, acc 1\n",
      "2018-10-26T18:29:54.765645: step 24609, loss 0.000240285, acc 1\n",
      "2018-10-26T18:29:54.943171: step 24610, loss 1.53286e-05, acc 1\n",
      "2018-10-26T18:29:55.112718: step 24611, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:29:55.291241: step 24612, loss 4.37716e-07, acc 1\n",
      "2018-10-26T18:29:55.461786: step 24613, loss 1.89989e-07, acc 1\n",
      "2018-10-26T18:29:55.638313: step 24614, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:29:55.807861: step 24615, loss 4.90948e-06, acc 1\n",
      "2018-10-26T18:29:55.990373: step 24616, loss 2.34304e-06, acc 1\n",
      "2018-10-26T18:29:56.162912: step 24617, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:29:56.341435: step 24618, loss 0.122297, acc 0.984375\n",
      "2018-10-26T18:29:56.506992: step 24619, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:29:56.690502: step 24620, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:29:56.861047: step 24621, loss 0, acc 1\n",
      "2018-10-26T18:29:57.038573: step 24622, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:29:57.210114: step 24623, loss 3.12922e-07, acc 1\n",
      "2018-10-26T18:29:57.392627: step 24624, loss 4.87457e-05, acc 1\n",
      "2018-10-26T18:29:57.563170: step 24625, loss 0.210732, acc 0.984375\n",
      "2018-10-26T18:29:57.743688: step 24626, loss 3.60217e-05, acc 1\n",
      "2018-10-26T18:29:57.909246: step 24627, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:29:58.092756: step 24628, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:58.263300: step 24629, loss 2.12341e-07, acc 1\n",
      "2018-10-26T18:29:58.440837: step 24630, loss 4.82418e-07, acc 1\n",
      "2018-10-26T18:29:58.606384: step 24631, loss 0.000421268, acc 1\n",
      "2018-10-26T18:29:58.788896: step 24632, loss 0.0742009, acc 0.984375\n",
      "2018-10-26T18:29:58.954454: step 24633, loss 9.09759e-05, acc 1\n",
      "2018-10-26T18:29:59.138960: step 24634, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:29:59.309505: step 24635, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:29:59.491020: step 24636, loss 1.63912e-07, acc 1\n",
      "2018-10-26T18:29:59.656577: step 24637, loss 3.59277e-05, acc 1\n",
      "2018-10-26T18:29:59.836097: step 24638, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:30:00.013623: step 24639, loss 0, acc 1\n",
      "2018-10-26T18:30:00.200125: step 24640, loss 1.58131e-06, acc 1\n",
      "2018-10-26T18:30:00.392612: step 24641, loss 1.72658e-06, acc 1\n",
      "2018-10-26T18:30:00.582104: step 24642, loss 0, acc 1\n",
      "2018-10-26T18:30:00.762623: step 24643, loss 0, acc 1\n",
      "2018-10-26T18:30:00.945135: step 24644, loss 0.00197628, acc 1\n",
      "2018-10-26T18:30:01.132633: step 24645, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:30:01.303178: step 24646, loss 0, acc 1\n",
      "2018-10-26T18:30:01.494667: step 24647, loss 0, acc 1\n",
      "2018-10-26T18:30:01.664214: step 24648, loss 9.3132e-08, acc 1\n",
      "2018-10-26T18:30:01.850715: step 24649, loss 0.000115128, acc 1\n",
      "2018-10-26T18:30:02.024251: step 24650, loss 4.17227e-07, acc 1\n",
      "2018-10-26T18:30:02.205767: step 24651, loss 0.00949804, acc 1\n",
      "2018-10-26T18:30:02.374315: step 24652, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:30:02.557826: step 24653, loss 0.000205272, acc 1\n",
      "2018-10-26T18:30:02.728370: step 24654, loss 1.14549e-06, acc 1\n",
      "2018-10-26T18:30:02.914873: step 24655, loss 0.00487331, acc 1\n",
      "2018-10-26T18:30:03.083421: step 24656, loss 2.22428e-05, acc 1\n",
      "2018-10-26T18:30:03.263939: step 24657, loss 2.83315e-05, acc 1\n",
      "2018-10-26T18:30:03.432489: step 24658, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:30:03.615000: step 24659, loss 0.000120906, acc 1\n",
      "2018-10-26T18:30:03.788538: step 24660, loss 1.86264e-07, acc 1\n",
      "2018-10-26T18:30:03.970052: step 24661, loss 4.39578e-07, acc 1\n",
      "2018-10-26T18:30:04.144585: step 24662, loss 1.03933e-06, acc 1\n",
      "2018-10-26T18:30:04.325104: step 24663, loss 6.44923e-06, acc 1\n",
      "2018-10-26T18:30:04.489664: step 24664, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:30:04.675168: step 24665, loss 4.54077e-06, acc 1\n",
      "2018-10-26T18:30:04.846710: step 24666, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:30:05.031219: step 24667, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:30:05.204754: step 24668, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:30:05.381282: step 24669, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:30:05.550842: step 24670, loss 1.22417e-05, acc 1\n",
      "2018-10-26T18:30:05.724365: step 24671, loss 0, acc 1\n",
      "2018-10-26T18:30:05.891923: step 24672, loss 1.62301e-05, acc 1\n",
      "2018-10-26T18:30:06.074431: step 24673, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:30:06.241982: step 24674, loss 5.87978e-05, acc 1\n",
      "2018-10-26T18:30:06.426489: step 24675, loss 2.59255e-05, acc 1\n",
      "2018-10-26T18:30:06.594041: step 24676, loss 0.00383236, acc 1\n",
      "2018-10-26T18:30:06.771567: step 24677, loss 4.83655e-06, acc 1\n",
      "2018-10-26T18:30:06.951088: step 24678, loss 0.000152289, acc 1\n",
      "2018-10-26T18:30:07.124624: step 24679, loss 1.03732e-05, acc 1\n",
      "2018-10-26T18:30:07.301152: step 24680, loss 8.04642e-07, acc 1\n",
      "2018-10-26T18:30:07.471697: step 24681, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:30:07.651217: step 24682, loss 2.66336e-06, acc 1\n",
      "2018-10-26T18:30:07.816774: step 24683, loss 1.43045e-06, acc 1\n",
      "2018-10-26T18:30:07.997292: step 24684, loss 4.63651e-05, acc 1\n",
      "2018-10-26T18:30:08.170829: step 24685, loss 0, acc 1\n",
      "2018-10-26T18:30:08.343368: step 24686, loss 1.74893e-06, acc 1\n",
      "2018-10-26T18:30:08.509923: step 24687, loss 9.54942e-05, acc 1\n",
      "2018-10-26T18:30:08.691439: step 24688, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:30:08.865971: step 24689, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:30:09.038511: step 24690, loss 0, acc 1\n",
      "2018-10-26T18:30:09.209055: step 24691, loss 0, acc 1\n",
      "2018-10-26T18:30:09.381594: step 24692, loss 2.06189e-05, acc 1\n",
      "2018-10-26T18:30:09.560118: step 24693, loss 0.000818551, acc 1\n",
      "2018-10-26T18:30:09.730661: step 24694, loss 0.000131894, acc 1\n",
      "2018-10-26T18:30:09.928321: step 24695, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:30:10.095873: step 24696, loss 5.68779e-06, acc 1\n",
      "2018-10-26T18:30:10.269410: step 24697, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:30:10.443945: step 24698, loss 0, acc 1\n",
      "2018-10-26T18:30:10.614488: step 24699, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:30:10.790019: step 24700, loss 1.37835e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:30:11.249791: step 24700, loss 8.46864, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24700\n",
      "\n",
      "2018-10-26T18:30:11.631752: step 24701, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:30:11.821245: step 24702, loss 9.14534e-07, acc 1\n",
      "2018-10-26T18:30:11.987799: step 24703, loss 0, acc 1\n",
      "2018-10-26T18:30:12.163329: step 24704, loss 0.00119125, acc 1\n",
      "2018-10-26T18:30:12.354819: step 24705, loss 2.00152e-05, acc 1\n",
      "2018-10-26T18:30:12.604152: step 24706, loss 6.7525e-06, acc 1\n",
      "2018-10-26T18:30:12.770707: step 24707, loss 2.39704e-06, acc 1\n",
      "2018-10-26T18:30:12.956212: step 24708, loss 0.000151947, acc 1\n",
      "2018-10-26T18:30:13.124761: step 24709, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:30:13.298298: step 24710, loss 1.68006e-06, acc 1\n",
      "2018-10-26T18:30:13.476820: step 24711, loss 4.50881e-06, acc 1\n",
      "2018-10-26T18:30:13.651355: step 24712, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:30:13.818920: step 24713, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:30:13.995435: step 24714, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:14.166979: step 24715, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:30:14.347495: step 24716, loss 1.57014e-06, acc 1\n",
      "2018-10-26T18:30:14.516044: step 24717, loss 1.91852e-07, acc 1\n",
      "2018-10-26T18:30:14.694567: step 24718, loss 4.91733e-07, acc 1\n",
      "2018-10-26T18:30:14.863116: step 24719, loss 0.000860768, acc 1\n",
      "2018-10-26T18:30:15.039645: step 24720, loss 1.47149e-07, acc 1\n",
      "2018-10-26T18:30:15.213181: step 24721, loss 4.88009e-07, acc 1\n",
      "2018-10-26T18:30:15.391704: step 24722, loss 0.0709784, acc 0.984375\n",
      "2018-10-26T18:30:15.570228: step 24723, loss 0.000259342, acc 1\n",
      "2018-10-26T18:30:15.739774: step 24724, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:30:15.920292: step 24725, loss 2.09556e-05, acc 1\n",
      "2018-10-26T18:30:16.093828: step 24726, loss 2.34692e-07, acc 1\n",
      "2018-10-26T18:30:16.271354: step 24727, loss 0.000383242, acc 1\n",
      "2018-10-26T18:30:16.442896: step 24728, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:30:16.617429: step 24729, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:30:16.793958: step 24730, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:30:17.010381: step 24731, loss 2.07304e-06, acc 1\n",
      "2018-10-26T18:30:17.224806: step 24732, loss 0.00027525, acc 1\n",
      "2018-10-26T18:30:17.407319: step 24733, loss 0, acc 1\n",
      "2018-10-26T18:30:17.613767: step 24734, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:30:17.792290: step 24735, loss 0.000211381, acc 1\n",
      "2018-10-26T18:30:18.014696: step 24736, loss 6.47725e-05, acc 1\n",
      "2018-10-26T18:30:18.222142: step 24737, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:30:18.407645: step 24738, loss 0, acc 1\n",
      "2018-10-26T18:30:18.618083: step 24739, loss 8.67972e-07, acc 1\n",
      "2018-10-26T18:30:18.829519: step 24740, loss 9.3132e-08, acc 1\n",
      "2018-10-26T18:30:19.060901: step 24741, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:19.240420: step 24742, loss 0.101101, acc 0.984375\n",
      "2018-10-26T18:30:19.474795: step 24743, loss 0.000362529, acc 1\n",
      "2018-10-26T18:30:19.681244: step 24744, loss 2.06864e-05, acc 1\n",
      "2018-10-26T18:30:19.932572: step 24745, loss 4.28722e-06, acc 1\n",
      "2018-10-26T18:30:20.135031: step 24746, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:30:20.364419: step 24747, loss 2.22388e-06, acc 1\n",
      "2018-10-26T18:30:20.594803: step 24748, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:30:20.808232: step 24749, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:30:21.055571: step 24750, loss 0.000105534, acc 1\n",
      "2018-10-26T18:30:21.279971: step 24751, loss 6.79933e-05, acc 1\n",
      "2018-10-26T18:30:21.527310: step 24752, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:21.701844: step 24753, loss 4.87012e-05, acc 1\n",
      "2018-10-26T18:30:21.926244: step 24754, loss 0.00172682, acc 1\n",
      "2018-10-26T18:30:22.096789: step 24755, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:30:22.279301: step 24756, loss 3.46228e-06, acc 1\n",
      "2018-10-26T18:30:22.474779: step 24757, loss 1.67976e-05, acc 1\n",
      "2018-10-26T18:30:22.675244: step 24758, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:22.877703: step 24759, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:30:23.125042: step 24760, loss 9.7132e-05, acc 1\n",
      "2018-10-26T18:30:23.342461: step 24761, loss 1.70006e-05, acc 1\n",
      "2018-10-26T18:30:23.537938: step 24762, loss 0.00419971, acc 1\n",
      "2018-10-26T18:30:23.779294: step 24763, loss 0.000182207, acc 1\n",
      "2018-10-26T18:30:23.953827: step 24764, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:30:24.136339: step 24765, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:24.344782: step 24766, loss 4.40082e-06, acc 1\n",
      "2018-10-26T18:30:24.524303: step 24767, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:30:24.710805: step 24768, loss 2.46829e-05, acc 1\n",
      "2018-10-26T18:30:24.916257: step 24769, loss 3.19281e-05, acc 1\n",
      "2018-10-26T18:30:25.091787: step 24770, loss 5.12389e-05, acc 1\n",
      "2018-10-26T18:30:25.276294: step 24771, loss 8.35359e-06, acc 1\n",
      "2018-10-26T18:30:25.461798: step 24772, loss 3.65916e-05, acc 1\n",
      "2018-10-26T18:30:25.654284: step 24773, loss 1.06859e-05, acc 1\n",
      "2018-10-26T18:30:25.826823: step 24774, loss 2.79394e-07, acc 1\n",
      "2018-10-26T18:30:26.004348: step 24775, loss 0.000105941, acc 1\n",
      "2018-10-26T18:30:26.208803: step 24776, loss 6.76126e-07, acc 1\n",
      "2018-10-26T18:30:26.390317: step 24777, loss 2.28905e-06, acc 1\n",
      "2018-10-26T18:30:26.573827: step 24778, loss 4.25779e-05, acc 1\n",
      "2018-10-26T18:30:26.758334: step 24779, loss 0.000850794, acc 1\n",
      "2018-10-26T18:30:26.983731: step 24780, loss 2.40279e-07, acc 1\n",
      "2018-10-26T18:30:27.151284: step 24781, loss 3.42142e-06, acc 1\n",
      "2018-10-26T18:30:27.366708: step 24782, loss 4.32128e-07, acc 1\n",
      "2018-10-26T18:30:27.564181: step 24783, loss 1.05422e-06, acc 1\n",
      "2018-10-26T18:30:27.750683: step 24784, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:30:27.971095: step 24785, loss 2.66336e-06, acc 1\n",
      "2018-10-26T18:30:28.150614: step 24786, loss 1.45283e-06, acc 1\n",
      "2018-10-26T18:30:28.363046: step 24787, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:30:28.541570: step 24788, loss 0, acc 1\n",
      "2018-10-26T18:30:28.757991: step 24789, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:30:28.940503: step 24790, loss 6.1927e-06, acc 1\n",
      "2018-10-26T18:30:29.136979: step 24791, loss 2.5833e-06, acc 1\n",
      "2018-10-26T18:30:29.365369: step 24792, loss 9.2757e-07, acc 1\n",
      "2018-10-26T18:30:29.542895: step 24793, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:30:29.721417: step 24794, loss 0.000601107, acc 1\n",
      "2018-10-26T18:30:29.948810: step 24795, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:30.123343: step 24796, loss 0.00603857, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:30:30.305856: step 24797, loss 0.000407767, acc 1\n",
      "2018-10-26T18:30:30.502330: step 24798, loss 0.00157786, acc 1\n",
      "2018-10-26T18:30:30.695814: step 24799, loss 9.75138e-05, acc 1\n",
      "2018-10-26T18:30:30.879323: step 24800, loss 1.63905e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:30:31.365025: step 24800, loss 8.52731, acc 0.725141\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24800\n",
      "\n",
      "2018-10-26T18:30:31.717975: step 24801, loss 9.49948e-08, acc 1\n",
      "2018-10-26T18:30:31.894503: step 24802, loss 4.82418e-07, acc 1\n",
      "2018-10-26T18:30:32.095971: step 24803, loss 0, acc 1\n",
      "2018-10-26T18:30:32.279473: step 24804, loss 4.61443e-05, acc 1\n",
      "2018-10-26T18:30:32.502878: step 24805, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:32.710324: step 24806, loss 4.05266e-06, acc 1\n",
      "2018-10-26T18:30:32.885854: step 24807, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:33.072356: step 24808, loss 4.66677e-05, acc 1\n",
      "2018-10-26T18:30:33.251876: step 24809, loss 0, acc 1\n",
      "2018-10-26T18:30:33.429402: step 24810, loss 1.19577e-06, acc 1\n",
      "2018-10-26T18:30:33.601940: step 24811, loss 1.58692e-06, acc 1\n",
      "2018-10-26T18:30:33.777472: step 24812, loss 8.64268e-05, acc 1\n",
      "2018-10-26T18:30:33.948016: step 24813, loss 3.15873e-06, acc 1\n",
      "2018-10-26T18:30:34.129531: step 24814, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:34.294092: step 24815, loss 0.000468878, acc 1\n",
      "2018-10-26T18:30:34.467628: step 24816, loss 1.00825e-05, acc 1\n",
      "2018-10-26T18:30:34.639170: step 24817, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:30:34.814700: step 24818, loss 2.56806e-05, acc 1\n",
      "2018-10-26T18:30:34.986242: step 24819, loss 7.26431e-08, acc 1\n",
      "2018-10-26T18:30:35.165763: step 24820, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:30:35.332317: step 24821, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:30:35.506852: step 24822, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:30:35.677396: step 24823, loss 1.58022e-05, acc 1\n",
      "2018-10-26T18:30:35.858910: step 24824, loss 4.00755e-05, acc 1\n",
      "2018-10-26T18:30:36.028458: step 24825, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:30:36.209972: step 24826, loss 8.56815e-08, acc 1\n",
      "2018-10-26T18:30:36.387498: step 24827, loss 0, acc 1\n",
      "2018-10-26T18:30:36.561034: step 24828, loss 0, acc 1\n",
      "2018-10-26T18:30:36.735569: step 24829, loss 5.01755e-06, acc 1\n",
      "2018-10-26T18:30:36.904118: step 24830, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:30:37.078652: step 24831, loss 0, acc 1\n",
      "2018-10-26T18:30:37.252188: step 24832, loss 0, acc 1\n",
      "2018-10-26T18:30:37.428715: step 24833, loss 1.38575e-06, acc 1\n",
      "2018-10-26T18:30:37.600258: step 24834, loss 4.93597e-07, acc 1\n",
      "2018-10-26T18:30:37.781774: step 24835, loss 6.27706e-07, acc 1\n",
      "2018-10-26T18:30:37.961294: step 24836, loss 5.64372e-07, acc 1\n",
      "2018-10-26T18:30:38.137822: step 24837, loss 0.293637, acc 0.984375\n",
      "2018-10-26T18:30:38.306371: step 24838, loss 0.00732758, acc 1\n",
      "2018-10-26T18:30:38.482900: step 24839, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:30:38.658431: step 24840, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:30:38.838956: step 24841, loss 2.25379e-07, acc 1\n",
      "2018-10-26T18:30:39.013482: step 24842, loss 0, acc 1\n",
      "2018-10-26T18:30:39.185024: step 24843, loss 2.66356e-07, acc 1\n",
      "2018-10-26T18:30:39.351578: step 24844, loss 0.00073148, acc 1\n",
      "2018-10-26T18:30:39.535088: step 24845, loss 5.472e-05, acc 1\n",
      "2018-10-26T18:30:39.705632: step 24846, loss 2.6041e-05, acc 1\n",
      "2018-10-26T18:30:39.892133: step 24847, loss 6.20722e-06, acc 1\n",
      "2018-10-26T18:30:40.062678: step 24848, loss 2.58328e-06, acc 1\n",
      "2018-10-26T18:30:40.241202: step 24849, loss 2.73807e-07, acc 1\n",
      "2018-10-26T18:30:40.408753: step 24850, loss 0, acc 1\n",
      "2018-10-26T18:30:40.588273: step 24851, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:30:40.752835: step 24852, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:30:40.935347: step 24853, loss 1.63178e-05, acc 1\n",
      "2018-10-26T18:30:41.101902: step 24854, loss 1.5577e-05, acc 1\n",
      "2018-10-26T18:30:41.277432: step 24855, loss 4.54481e-07, acc 1\n",
      "2018-10-26T18:30:41.455956: step 24856, loss 3.39e-07, acc 1\n",
      "2018-10-26T18:30:41.639465: step 24857, loss 4.03969e-06, acc 1\n",
      "2018-10-26T18:30:41.804026: step 24858, loss 2.81258e-07, acc 1\n",
      "2018-10-26T18:30:42.035408: step 24859, loss 0.000647832, acc 1\n",
      "2018-10-26T18:30:42.225899: step 24860, loss 5.78621e-05, acc 1\n",
      "2018-10-26T18:30:42.401430: step 24861, loss 9.07526e-06, acc 1\n",
      "2018-10-26T18:30:42.577957: step 24862, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:30:42.757478: step 24863, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:42.923036: step 24864, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:43.102557: step 24865, loss 0.000120655, acc 1\n",
      "2018-10-26T18:30:43.276093: step 24866, loss 0.0740429, acc 0.984375\n",
      "2018-10-26T18:30:43.459603: step 24867, loss 0, acc 1\n",
      "2018-10-26T18:30:43.646105: step 24868, loss 0.000381087, acc 1\n",
      "2018-10-26T18:30:43.813657: step 24869, loss 0.00151309, acc 1\n",
      "2018-10-26T18:30:43.996169: step 24870, loss 1.05422e-06, acc 1\n",
      "2018-10-26T18:30:44.166714: step 24871, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:44.346233: step 24872, loss 0, acc 1\n",
      "2018-10-26T18:30:44.510794: step 24873, loss 1.85509e-06, acc 1\n",
      "2018-10-26T18:30:44.690314: step 24874, loss 1.40624e-06, acc 1\n",
      "2018-10-26T18:30:44.860858: step 24875, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:30:45.038384: step 24876, loss 2.97954e-05, acc 1\n",
      "2018-10-26T18:30:45.210923: step 24877, loss 0.000801576, acc 1\n",
      "2018-10-26T18:30:45.394434: step 24878, loss 1.91841e-06, acc 1\n",
      "2018-10-26T18:30:45.562982: step 24879, loss 0, acc 1\n",
      "2018-10-26T18:30:45.743501: step 24880, loss 0.000138463, acc 1\n",
      "2018-10-26T18:30:45.922023: step 24881, loss 4.10117e-06, acc 1\n",
      "2018-10-26T18:30:46.106530: step 24882, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:30:46.286050: step 24883, loss 0.000107235, acc 1\n",
      "2018-10-26T18:30:46.462579: step 24884, loss 3.29095e-06, acc 1\n",
      "2018-10-26T18:30:46.630131: step 24885, loss 2.64494e-07, acc 1\n",
      "2018-10-26T18:30:46.813641: step 24886, loss 0.000239874, acc 1\n",
      "2018-10-26T18:30:46.994159: step 24887, loss 0, acc 1\n",
      "2018-10-26T18:30:47.175673: step 24888, loss 9.87199e-08, acc 1\n",
      "2018-10-26T18:30:47.356191: step 24889, loss 0, acc 1\n",
      "2018-10-26T18:30:47.522747: step 24890, loss 2.21653e-07, acc 1\n",
      "2018-10-26T18:30:47.705259: step 24891, loss 7.97196e-07, acc 1\n",
      "2018-10-26T18:30:47.880790: step 24892, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:30:48.062305: step 24893, loss 0.0512394, acc 0.984375\n",
      "2018-10-26T18:30:48.233846: step 24894, loss 0.000161837, acc 1\n",
      "2018-10-26T18:30:48.418354: step 24895, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:30:48.586903: step 24896, loss 4.14758e-06, acc 1\n",
      "2018-10-26T18:30:48.765426: step 24897, loss 0.00134896, acc 1\n",
      "2018-10-26T18:30:48.939959: step 24898, loss 0.000258811, acc 1\n",
      "2018-10-26T18:30:49.119480: step 24899, loss 3.20372e-07, acc 1\n",
      "2018-10-26T18:30:49.280051: step 24900, loss 7.35122e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:30:49.740820: step 24900, loss 8.60302, acc 0.723265\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-24900\n",
      "\n",
      "2018-10-26T18:30:50.103992: step 24901, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:30:50.283512: step 24902, loss 5.06633e-07, acc 1\n",
      "2018-10-26T18:30:50.471011: step 24903, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:30:50.638563: step 24904, loss 3.36746e-06, acc 1\n",
      "2018-10-26T18:30:50.869946: step 24905, loss 8.56815e-08, acc 1\n",
      "2018-10-26T18:30:51.064426: step 24906, loss 2.24626e-06, acc 1\n",
      "2018-10-26T18:30:51.244943: step 24907, loss 0, acc 1\n",
      "2018-10-26T18:30:51.435434: step 24908, loss 1.15296e-05, acc 1\n",
      "2018-10-26T18:30:51.617947: step 24909, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:30:51.799462: step 24910, loss 0, acc 1\n",
      "2018-10-26T18:30:51.970010: step 24911, loss 5.62513e-07, acc 1\n",
      "2018-10-26T18:30:52.149527: step 24912, loss 5.42521e-05, acc 1\n",
      "2018-10-26T18:30:52.321067: step 24913, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:30:52.503580: step 24914, loss 4.41443e-07, acc 1\n",
      "2018-10-26T18:30:52.670135: step 24915, loss 8.05759e-06, acc 1\n",
      "2018-10-26T18:30:52.854642: step 24916, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:30:53.025187: step 24917, loss 8.37221e-06, acc 1\n",
      "2018-10-26T18:30:53.213683: step 24918, loss 1.00394e-06, acc 1\n",
      "2018-10-26T18:30:53.379241: step 24919, loss 9.08708e-06, acc 1\n",
      "2018-10-26T18:30:53.569731: step 24920, loss 0, acc 1\n",
      "2018-10-26T18:30:53.736286: step 24921, loss 6.70551e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:30:53.919797: step 24922, loss 6.32455e-05, acc 1\n",
      "2018-10-26T18:30:54.094330: step 24923, loss 5.11373e-05, acc 1\n",
      "2018-10-26T18:30:54.276842: step 24924, loss 1.76951e-07, acc 1\n",
      "2018-10-26T18:30:54.454368: step 24925, loss 1.4677e-06, acc 1\n",
      "2018-10-26T18:30:54.627905: step 24926, loss 0.000385527, acc 1\n",
      "2018-10-26T18:30:54.792464: step 24927, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:30:54.970988: step 24928, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:55.139538: step 24929, loss 5.46824e-06, acc 1\n",
      "2018-10-26T18:30:55.319058: step 24930, loss 0, acc 1\n",
      "2018-10-26T18:30:55.482621: step 24931, loss 7.36333e-06, acc 1\n",
      "2018-10-26T18:30:55.661144: step 24932, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:30:55.828696: step 24933, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:30:56.003229: step 24934, loss 9.33727e-06, acc 1\n",
      "2018-10-26T18:30:56.174772: step 24935, loss 3.4645e-07, acc 1\n",
      "2018-10-26T18:30:56.345316: step 24936, loss 4.31911e-05, acc 1\n",
      "2018-10-26T18:30:56.517856: step 24937, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:30:56.693385: step 24938, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:30:56.868917: step 24939, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:30:57.046442: step 24940, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:30:57.221973: step 24941, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:30:57.393516: step 24942, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:30:57.570044: step 24943, loss 1.07844e-06, acc 1\n",
      "2018-10-26T18:30:57.743580: step 24944, loss 9.31307e-07, acc 1\n",
      "2018-10-26T18:30:57.924098: step 24945, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:30:58.107608: step 24946, loss 2.66357e-07, acc 1\n",
      "2018-10-26T18:30:58.276156: step 24947, loss 0.000408272, acc 1\n",
      "2018-10-26T18:30:58.449694: step 24948, loss 1.28842e-05, acc 1\n",
      "2018-10-26T18:30:58.618243: step 24949, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:30:58.793774: step 24950, loss 9.08957e-07, acc 1\n",
      "2018-10-26T18:30:58.959331: step 24951, loss 2.13258e-06, acc 1\n",
      "2018-10-26T18:30:59.130873: step 24952, loss 2.38417e-07, acc 1\n",
      "2018-10-26T18:30:59.297429: step 24953, loss 5.53417e-05, acc 1\n",
      "2018-10-26T18:30:59.471963: step 24954, loss 2.96158e-07, acc 1\n",
      "2018-10-26T18:30:59.638517: step 24955, loss 5.1513e-06, acc 1\n",
      "2018-10-26T18:30:59.816042: step 24956, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:30:59.996560: step 24957, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:31:00.189046: step 24958, loss 4.69503e-06, acc 1\n",
      "2018-10-26T18:31:00.368567: step 24959, loss 4.13502e-07, acc 1\n",
      "2018-10-26T18:31:00.540108: step 24960, loss 8.94476e-05, acc 1\n",
      "2018-10-26T18:31:00.717633: step 24961, loss 1.27874e-05, acc 1\n",
      "2018-10-26T18:31:00.892167: step 24962, loss 0.000161333, acc 1\n",
      "2018-10-26T18:31:01.074680: step 24963, loss 0, acc 1\n",
      "2018-10-26T18:31:01.235250: step 24964, loss 0.000180915, acc 1\n",
      "2018-10-26T18:31:01.411779: step 24965, loss 9.77847e-05, acc 1\n",
      "2018-10-26T18:31:01.592296: step 24966, loss 9.55563e-05, acc 1\n",
      "2018-10-26T18:31:01.765768: step 24967, loss 0.000140702, acc 1\n",
      "2018-10-26T18:31:01.946286: step 24968, loss 3.00805e-06, acc 1\n",
      "2018-10-26T18:31:02.115832: step 24969, loss 8.61242e-06, acc 1\n",
      "2018-10-26T18:31:02.290366: step 24970, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:31:02.467893: step 24971, loss 0, acc 1\n",
      "2018-10-26T18:31:02.643423: step 24972, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:31:02.823941: step 24973, loss 0.000338725, acc 1\n",
      "2018-10-26T18:31:02.999473: step 24974, loss 3.55762e-07, acc 1\n",
      "2018-10-26T18:31:03.175002: step 24975, loss 4.17229e-07, acc 1\n",
      "2018-10-26T18:31:03.340561: step 24976, loss 9.68573e-08, acc 1\n",
      "2018-10-26T18:31:03.518086: step 24977, loss 1.46586e-06, acc 1\n",
      "2018-10-26T18:31:03.690624: step 24978, loss 3.77886e-06, acc 1\n",
      "2018-10-26T18:31:03.874135: step 24979, loss 6.06038e-06, acc 1\n",
      "2018-10-26T18:31:04.045676: step 24980, loss 4.73105e-07, acc 1\n",
      "2018-10-26T18:31:04.217217: step 24981, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:31:04.393746: step 24982, loss 2.38417e-07, acc 1\n",
      "2018-10-26T18:31:04.570274: step 24983, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:31:04.738717: step 24984, loss 5.1781e-07, acc 1\n",
      "2018-10-26T18:31:04.918237: step 24985, loss 0.0308065, acc 0.984375\n",
      "2018-10-26T18:31:05.091773: step 24986, loss 0.000738133, acc 1\n",
      "2018-10-26T18:31:05.270296: step 24987, loss 0.000863357, acc 1\n",
      "2018-10-26T18:31:05.436851: step 24988, loss 0, acc 1\n",
      "2018-10-26T18:31:05.606398: step 24989, loss 2.4399e-06, acc 1\n",
      "2018-10-26T18:31:05.774969: step 24990, loss 0, acc 1\n",
      "2018-10-26T18:31:05.958457: step 24991, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:31:06.129002: step 24992, loss 1.52545e-06, acc 1\n",
      "2018-10-26T18:31:06.310518: step 24993, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:31:06.481061: step 24994, loss 9.18763e-05, acc 1\n",
      "2018-10-26T18:31:06.652602: step 24995, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:31:06.822150: step 24996, loss 2.10094e-06, acc 1\n",
      "2018-10-26T18:31:07.005659: step 24997, loss 0.000395488, acc 1\n",
      "2018-10-26T18:31:07.176204: step 24998, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:31:07.355725: step 24999, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:31:07.522279: step 25000, loss 6.03486e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:31:07.987037: step 25000, loss 8.58534, acc 0.723265\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25000\n",
      "\n",
      "2018-10-26T18:31:08.357764: step 25001, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:31:08.538282: step 25002, loss 1.71544e-06, acc 1\n",
      "2018-10-26T18:31:08.713813: step 25003, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:31:08.895328: step 25004, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:31:09.091803: step 25005, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:31:09.320194: step 25006, loss 0.00360446, acc 1\n",
      "2018-10-26T18:31:09.493729: step 25007, loss 2.8312e-07, acc 1\n",
      "2018-10-26T18:31:09.676242: step 25008, loss 8.75442e-08, acc 1\n",
      "2018-10-26T18:31:09.860749: step 25009, loss 1.32617e-06, acc 1\n",
      "2018-10-26T18:31:10.028301: step 25010, loss 0.000797968, acc 1\n",
      "2018-10-26T18:31:10.216798: step 25011, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:31:10.391331: step 25012, loss 1.56461e-07, acc 1\n",
      "2018-10-26T18:31:10.570852: step 25013, loss 2.3987e-05, acc 1\n",
      "2018-10-26T18:31:10.740398: step 25014, loss 6.27699e-07, acc 1\n",
      "2018-10-26T18:31:10.922911: step 25015, loss 0, acc 1\n",
      "2018-10-26T18:31:11.088470: step 25016, loss 2.21653e-07, acc 1\n",
      "2018-10-26T18:31:11.267990: step 25017, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:31:11.439530: step 25018, loss 2.43946e-05, acc 1\n",
      "2018-10-26T18:31:11.617056: step 25019, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:31:11.786603: step 25020, loss 4.27642e-06, acc 1\n",
      "2018-10-26T18:31:11.969115: step 25021, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:31:12.136667: step 25022, loss 2.38997e-05, acc 1\n",
      "2018-10-26T18:31:12.317186: step 25023, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:31:12.488727: step 25024, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:31:12.666253: step 25025, loss 4.89426e-06, acc 1\n",
      "2018-10-26T18:31:12.834802: step 25026, loss 1.23489e-06, acc 1\n",
      "2018-10-26T18:31:13.018312: step 25027, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:31:13.189854: step 25028, loss 0, acc 1\n",
      "2018-10-26T18:31:13.370372: step 25029, loss 9.8625e-06, acc 1\n",
      "2018-10-26T18:31:13.540917: step 25030, loss 1.67325e-05, acc 1\n",
      "2018-10-26T18:31:13.724426: step 25031, loss 2.7304e-06, acc 1\n",
      "2018-10-26T18:31:13.894971: step 25032, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:14.075488: step 25033, loss 1.44456e-05, acc 1\n",
      "2018-10-26T18:31:14.244037: step 25034, loss 0, acc 1\n",
      "2018-10-26T18:31:14.421563: step 25035, loss 0, acc 1\n",
      "2018-10-26T18:31:14.591110: step 25036, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:31:14.781601: step 25037, loss 2.21455e-05, acc 1\n",
      "2018-10-26T18:31:14.947159: step 25038, loss 5.3547e-05, acc 1\n",
      "2018-10-26T18:31:15.134659: step 25039, loss 7.79324e-06, acc 1\n",
      "2018-10-26T18:31:15.303207: step 25040, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:31:15.483724: step 25041, loss 1.03717e-05, acc 1\n",
      "2018-10-26T18:31:15.650280: step 25042, loss 0, acc 1\n",
      "2018-10-26T18:31:15.832792: step 25043, loss 7.07789e-07, acc 1\n",
      "2018-10-26T18:31:15.998350: step 25044, loss 1.2442e-06, acc 1\n",
      "2018-10-26T18:31:16.183855: step 25045, loss 0.000231337, acc 1\n",
      "2018-10-26T18:31:16.351407: step 25046, loss 8.89568e-05, acc 1\n",
      "2018-10-26T18:31:16.532921: step 25047, loss 7.41532e-05, acc 1\n",
      "2018-10-26T18:31:16.702469: step 25048, loss 3.35276e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:31:16.883984: step 25049, loss 0, acc 1\n",
      "2018-10-26T18:31:17.049541: step 25050, loss 0, acc 1\n",
      "2018-10-26T18:31:17.226071: step 25051, loss 2.82667e-05, acc 1\n",
      "2018-10-26T18:31:17.393622: step 25052, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:31:17.572145: step 25053, loss 9.70882e-06, acc 1\n",
      "2018-10-26T18:31:17.738702: step 25054, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:31:17.915229: step 25055, loss 0, acc 1\n",
      "2018-10-26T18:31:18.086770: step 25056, loss 0.000101079, acc 1\n",
      "2018-10-26T18:31:18.262302: step 25057, loss 0.00212546, acc 1\n",
      "2018-10-26T18:31:18.431848: step 25058, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:31:18.608376: step 25059, loss 0.000102696, acc 1\n",
      "2018-10-26T18:31:18.779918: step 25060, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:18.966420: step 25061, loss 0, acc 1\n",
      "2018-10-26T18:31:19.135967: step 25062, loss 0, acc 1\n",
      "2018-10-26T18:31:19.307509: step 25063, loss 7.43179e-07, acc 1\n",
      "2018-10-26T18:31:19.479050: step 25064, loss 0, acc 1\n",
      "2018-10-26T18:31:19.657573: step 25065, loss 0.000169777, acc 1\n",
      "2018-10-26T18:31:19.834101: step 25066, loss 3.11403e-06, acc 1\n",
      "2018-10-26T18:31:20.009633: step 25067, loss 4.2468e-07, acc 1\n",
      "2018-10-26T18:31:20.180177: step 25068, loss 1.1127e-05, acc 1\n",
      "2018-10-26T18:31:20.360694: step 25069, loss 6.77989e-07, acc 1\n",
      "2018-10-26T18:31:20.528247: step 25070, loss 8.95118e-06, acc 1\n",
      "2018-10-26T18:31:20.700786: step 25071, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:31:20.876317: step 25072, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:31:21.057832: step 25073, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:31:21.227378: step 25074, loss 4.03264e-05, acc 1\n",
      "2018-10-26T18:31:21.399918: step 25075, loss 1.86263e-07, acc 1\n",
      "2018-10-26T18:31:21.567470: step 25076, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:21.746990: step 25077, loss 0, acc 1\n",
      "2018-10-26T18:31:21.932497: step 25078, loss 3.47784e-05, acc 1\n",
      "2018-10-26T18:31:22.102061: step 25079, loss 3.34807e-05, acc 1\n",
      "2018-10-26T18:31:22.276576: step 25080, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:31:22.460087: step 25081, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:22.657557: step 25082, loss 1.3485e-06, acc 1\n",
      "2018-10-26T18:31:22.863010: step 25083, loss 1.01649e-05, acc 1\n",
      "2018-10-26T18:31:23.051506: step 25084, loss 5.82997e-07, acc 1\n",
      "2018-10-26T18:31:23.241996: step 25085, loss 4.13091e-06, acc 1\n",
      "2018-10-26T18:31:23.460412: step 25086, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:31:23.667859: step 25087, loss 6.09073e-07, acc 1\n",
      "2018-10-26T18:31:23.845385: step 25088, loss 1.17043e-05, acc 1\n",
      "2018-10-26T18:31:24.064798: step 25089, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:31:24.265262: step 25090, loss 9.75996e-07, acc 1\n",
      "2018-10-26T18:31:24.465726: step 25091, loss 0.000156776, acc 1\n",
      "2018-10-26T18:31:24.678159: step 25092, loss 6.7358e-06, acc 1\n",
      "2018-10-26T18:31:24.914528: step 25093, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:31:25.095045: step 25094, loss 1.50495e-06, acc 1\n",
      "2018-10-26T18:31:25.328422: step 25095, loss 6.29561e-07, acc 1\n",
      "2018-10-26T18:31:25.494976: step 25096, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:31:25.703420: step 25097, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:31:25.896903: step 25098, loss 1.66139e-06, acc 1\n",
      "2018-10-26T18:31:26.115321: step 25099, loss 0.0158171, acc 0.984375\n",
      "2018-10-26T18:31:26.320770: step 25100, loss 3.71559e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:31:26.888254: step 25100, loss 8.61585, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25100\n",
      "\n",
      "2018-10-26T18:31:27.331072: step 25101, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:31:27.539514: step 25102, loss 9.12695e-08, acc 1\n",
      "2018-10-26T18:31:27.754942: step 25103, loss 9.89033e-07, acc 1\n",
      "2018-10-26T18:31:27.968369: step 25104, loss 0, acc 1\n",
      "2018-10-26T18:31:28.249618: step 25105, loss 7.41315e-07, acc 1\n",
      "2018-10-26T18:31:28.462051: step 25106, loss 0, acc 1\n",
      "2018-10-26T18:31:28.689443: step 25107, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:31:28.947752: step 25108, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:31:29.159186: step 25109, loss 0.000121838, acc 1\n",
      "2018-10-26T18:31:29.381592: step 25110, loss 0, acc 1\n",
      "2018-10-26T18:31:29.591033: step 25111, loss 0.000327168, acc 1\n",
      "2018-10-26T18:31:29.770554: step 25112, loss 0, acc 1\n",
      "2018-10-26T18:31:29.986975: step 25113, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:31:30.159514: step 25114, loss 1.23605e-05, acc 1\n",
      "2018-10-26T18:31:30.345018: step 25115, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:31:30.555456: step 25116, loss 2.03292e-05, acc 1\n",
      "2018-10-26T18:31:30.737968: step 25117, loss 1.62414e-06, acc 1\n",
      "2018-10-26T18:31:30.915495: step 25118, loss 0.00610244, acc 1\n",
      "2018-10-26T18:31:31.136903: step 25119, loss 1.58318e-06, acc 1\n",
      "2018-10-26T18:31:31.319416: step 25120, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:31:31.501927: step 25121, loss 5.05882e-05, acc 1\n",
      "2018-10-26T18:31:31.676460: step 25122, loss 4.38447e-05, acc 1\n",
      "2018-10-26T18:31:31.866952: step 25123, loss 0, acc 1\n",
      "2018-10-26T18:31:32.049464: step 25124, loss 1.31311e-06, acc 1\n",
      "2018-10-26T18:31:32.242946: step 25125, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:31:32.438425: step 25126, loss 9.71533e-05, acc 1\n",
      "2018-10-26T18:31:32.662825: step 25127, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:32.855311: step 25128, loss 3.68799e-07, acc 1\n",
      "2018-10-26T18:31:33.051786: step 25129, loss 7.50099e-06, acc 1\n",
      "2018-10-26T18:31:33.276187: step 25130, loss 4.1089e-05, acc 1\n",
      "2018-10-26T18:31:33.444736: step 25131, loss 4.42169e-06, acc 1\n",
      "2018-10-26T18:31:33.644203: step 25132, loss 1.03746e-06, acc 1\n",
      "2018-10-26T18:31:33.837686: step 25133, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:31:34.026183: step 25134, loss 1.25725e-06, acc 1\n",
      "2018-10-26T18:31:34.228641: step 25135, loss 2.0498e-05, acc 1\n",
      "2018-10-26T18:31:34.415143: step 25136, loss 0, acc 1\n",
      "2018-10-26T18:31:34.596672: step 25137, loss 7.82302e-07, acc 1\n",
      "2018-10-26T18:31:34.819064: step 25138, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:31:34.990605: step 25139, loss 0.0118045, acc 0.984375\n",
      "2018-10-26T18:31:35.167135: step 25140, loss 0, acc 1\n",
      "2018-10-26T18:31:35.386549: step 25141, loss 0.000212595, acc 1\n",
      "2018-10-26T18:31:35.556095: step 25142, loss 8.1023e-07, acc 1\n",
      "2018-10-26T18:31:35.732624: step 25143, loss 8.64968e-06, acc 1\n",
      "2018-10-26T18:31:35.937078: step 25144, loss 1.44649e-05, acc 1\n",
      "2018-10-26T18:31:36.120586: step 25145, loss 8.75945e-06, acc 1\n",
      "2018-10-26T18:31:36.309083: step 25146, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:31:36.501569: step 25147, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:31:36.698056: step 25148, loss 3.7625e-07, acc 1\n",
      "2018-10-26T18:31:36.876566: step 25149, loss 4.80941e-05, acc 1\n",
      "2018-10-26T18:31:37.086007: step 25150, loss 1.26469e-06, acc 1\n",
      "2018-10-26T18:31:37.294450: step 25151, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:31:37.475965: step 25152, loss 1.85695e-06, acc 1\n",
      "2018-10-26T18:31:37.681416: step 25153, loss 2.29834e-06, acc 1\n",
      "2018-10-26T18:31:37.862931: step 25154, loss 5.40635e-06, acc 1\n",
      "2018-10-26T18:31:38.046441: step 25155, loss 7.10305e-05, acc 1\n",
      "2018-10-26T18:31:38.212997: step 25156, loss 3.79976e-07, acc 1\n",
      "2018-10-26T18:31:38.400496: step 25157, loss 1.51977e-05, acc 1\n",
      "2018-10-26T18:31:38.565056: step 25158, loss 0.00493994, acc 1\n",
      "2018-10-26T18:31:38.741584: step 25159, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:31:38.911131: step 25160, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:39.088656: step 25161, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:39.264187: step 25162, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:31:39.445702: step 25163, loss 0, acc 1\n",
      "2018-10-26T18:31:39.616247: step 25164, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:31:39.798759: step 25165, loss 3.1457e-06, acc 1\n",
      "2018-10-26T18:31:39.970300: step 25166, loss 1.60671e-05, acc 1\n",
      "2018-10-26T18:31:40.150819: step 25167, loss 0.00117991, acc 1\n",
      "2018-10-26T18:31:40.322360: step 25168, loss 1.29717e-05, acc 1\n",
      "2018-10-26T18:31:40.506867: step 25169, loss 0.000294197, acc 1\n",
      "2018-10-26T18:31:40.681401: step 25170, loss 0.000193256, acc 1\n",
      "2018-10-26T18:31:40.871892: step 25171, loss 1.76155e-05, acc 1\n",
      "2018-10-26T18:31:41.051412: step 25172, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:41.226944: step 25173, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:31:41.399483: step 25174, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:31:41.581994: step 25175, loss 3.44322e-05, acc 1\n",
      "2018-10-26T18:31:41.771488: step 25176, loss 1.46585e-06, acc 1\n",
      "2018-10-26T18:31:41.938043: step 25177, loss 6.05348e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:31:42.123547: step 25178, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:31:42.289105: step 25179, loss 2.60768e-07, acc 1\n",
      "2018-10-26T18:31:42.462642: step 25180, loss 0, acc 1\n",
      "2018-10-26T18:31:42.630194: step 25181, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:31:42.809714: step 25182, loss 7.48767e-07, acc 1\n",
      "2018-10-26T18:31:42.983250: step 25183, loss 1.34291e-06, acc 1\n",
      "2018-10-26T18:31:43.192692: step 25184, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:31:43.390164: step 25185, loss 1.30052e-05, acc 1\n",
      "2018-10-26T18:31:43.569683: step 25186, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:31:43.743221: step 25187, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:31:43.911770: step 25188, loss 0.00171423, acc 1\n",
      "2018-10-26T18:31:44.087301: step 25189, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:31:44.254853: step 25190, loss 1.88488e-06, acc 1\n",
      "2018-10-26T18:31:44.430384: step 25191, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:31:44.599931: step 25192, loss 6.83327e-06, acc 1\n",
      "2018-10-26T18:31:44.772470: step 25193, loss 0.000406823, acc 1\n",
      "2018-10-26T18:31:44.945033: step 25194, loss 8.91395e-06, acc 1\n",
      "2018-10-26T18:31:45.130513: step 25195, loss 0, acc 1\n",
      "2018-10-26T18:31:45.299063: step 25196, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:31:45.474594: step 25197, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:31:45.645138: step 25198, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:31:45.820669: step 25199, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:31:45.984233: step 25200, loss 2.83694e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:31:46.442010: step 25200, loss 8.62813, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25200\n",
      "\n",
      "2018-10-26T18:31:46.830533: step 25201, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:31:47.005067: step 25202, loss 0, acc 1\n",
      "2018-10-26T18:31:47.177606: step 25203, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:31:47.354134: step 25204, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:31:47.587513: step 25205, loss 3.63212e-07, acc 1\n",
      "2018-10-26T18:31:47.776008: step 25206, loss 1.58304e-05, acc 1\n",
      "2018-10-26T18:31:47.953533: step 25207, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:31:48.136045: step 25208, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:31:48.308584: step 25209, loss 0.00822824, acc 1\n",
      "2018-10-26T18:31:48.483118: step 25210, loss 5.65816e-05, acc 1\n",
      "2018-10-26T18:31:48.655657: step 25211, loss 4.38219e-06, acc 1\n",
      "2018-10-26T18:31:48.838169: step 25212, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:31:49.003727: step 25213, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:49.178260: step 25214, loss 1.0654e-06, acc 1\n",
      "2018-10-26T18:31:49.342820: step 25215, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:49.521343: step 25216, loss 4.82537e-06, acc 1\n",
      "2018-10-26T18:31:49.698870: step 25217, loss 1.90539e-06, acc 1\n",
      "2018-10-26T18:31:49.889362: step 25218, loss 0.00023011, acc 1\n",
      "2018-10-26T18:31:50.064892: step 25219, loss 0, acc 1\n",
      "2018-10-26T18:31:50.244412: step 25220, loss 8.03528e-06, acc 1\n",
      "2018-10-26T18:31:50.416951: step 25221, loss 8.94006e-06, acc 1\n",
      "2018-10-26T18:31:50.594477: step 25222, loss 1.86862e-05, acc 1\n",
      "2018-10-26T18:31:50.765021: step 25223, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:31:50.948533: step 25224, loss 6.6124e-05, acc 1\n",
      "2018-10-26T18:31:51.117080: step 25225, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:31:51.300589: step 25226, loss 0, acc 1\n",
      "2018-10-26T18:31:51.475125: step 25227, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:31:51.656638: step 25228, loss 2.09005e-05, acc 1\n",
      "2018-10-26T18:31:51.823194: step 25229, loss 3.49879e-05, acc 1\n",
      "2018-10-26T18:31:52.000719: step 25230, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:31:52.165280: step 25231, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:31:52.348789: step 25232, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:31:52.515344: step 25233, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:31:52.698854: step 25234, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:31:52.871393: step 25235, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:31:53.055900: step 25236, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:31:53.230434: step 25237, loss 0.000110769, acc 1\n",
      "2018-10-26T18:31:53.406962: step 25238, loss 7.70004e-05, acc 1\n",
      "2018-10-26T18:31:53.585485: step 25239, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:53.759022: step 25240, loss 2.04691e-06, acc 1\n",
      "2018-10-26T18:31:53.938542: step 25241, loss 0.000856219, acc 1\n",
      "2018-10-26T18:31:54.108090: step 25242, loss 3.69413e-05, acc 1\n",
      "2018-10-26T18:31:54.283619: step 25243, loss 8.08372e-07, acc 1\n",
      "2018-10-26T18:31:54.457155: step 25244, loss 0.000146555, acc 1\n",
      "2018-10-26T18:31:54.634694: step 25245, loss 4.58204e-07, acc 1\n",
      "2018-10-26T18:31:54.808217: step 25246, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:31:54.982752: step 25247, loss 0, acc 1\n",
      "2018-10-26T18:31:55.156289: step 25248, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:31:55.341793: step 25249, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:31:55.512337: step 25250, loss 0.0034563, acc 1\n",
      "2018-10-26T18:31:55.691857: step 25251, loss 1.9744e-07, acc 1\n",
      "2018-10-26T18:31:55.870381: step 25252, loss 2.93898e-06, acc 1\n",
      "2018-10-26T18:31:56.056882: step 25253, loss 1.53661e-06, acc 1\n",
      "2018-10-26T18:31:56.233410: step 25254, loss 6.03485e-07, acc 1\n",
      "2018-10-26T18:31:56.414925: step 25255, loss 5.92316e-05, acc 1\n",
      "2018-10-26T18:31:56.585471: step 25256, loss 1.93518e-06, acc 1\n",
      "2018-10-26T18:31:56.766986: step 25257, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:31:56.934537: step 25258, loss 4.01655e-05, acc 1\n",
      "2018-10-26T18:31:57.115055: step 25259, loss 1.28705e-06, acc 1\n",
      "2018-10-26T18:31:57.284601: step 25260, loss 1.45467e-06, acc 1\n",
      "2018-10-26T18:31:57.461130: step 25261, loss 8.52486e-06, acc 1\n",
      "2018-10-26T18:31:57.628682: step 25262, loss 7.00343e-07, acc 1\n",
      "2018-10-26T18:31:57.808202: step 25263, loss 0.0154573, acc 0.984375\n",
      "2018-10-26T18:31:57.983733: step 25264, loss 2.48272e-06, acc 1\n",
      "2018-10-26T18:31:58.164252: step 25265, loss 1.17323e-05, acc 1\n",
      "2018-10-26T18:31:58.337788: step 25266, loss 2.03023e-06, acc 1\n",
      "2018-10-26T18:31:58.519302: step 25267, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:31:58.701815: step 25268, loss 0.000364902, acc 1\n",
      "2018-10-26T18:31:58.876349: step 25269, loss 0.000196319, acc 1\n",
      "2018-10-26T18:31:59.053875: step 25270, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:31:59.224418: step 25271, loss 0, acc 1\n",
      "2018-10-26T18:31:59.405934: step 25272, loss 9.25504e-05, acc 1\n",
      "2018-10-26T18:31:59.575481: step 25273, loss 3.68799e-07, acc 1\n",
      "2018-10-26T18:31:59.757993: step 25274, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:31:59.926543: step 25275, loss 0.000177739, acc 1\n",
      "2018-10-26T18:32:00.114042: step 25276, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:32:00.297551: step 25277, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:32:00.477072: step 25278, loss 2.94133e-05, acc 1\n",
      "2018-10-26T18:32:00.657590: step 25279, loss 1.67637e-07, acc 1\n",
      "2018-10-26T18:32:00.832124: step 25280, loss 2.5518e-07, acc 1\n",
      "2018-10-26T18:32:01.011644: step 25281, loss 0, acc 1\n",
      "2018-10-26T18:32:01.177201: step 25282, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:01.355724: step 25283, loss 2.21653e-07, acc 1\n",
      "2018-10-26T18:32:01.530258: step 25284, loss 3.92969e-06, acc 1\n",
      "2018-10-26T18:32:01.714765: step 25285, loss 0.000107727, acc 1\n",
      "2018-10-26T18:32:01.879326: step 25286, loss 0, acc 1\n",
      "2018-10-26T18:32:02.058845: step 25287, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:32:02.232382: step 25288, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:32:02.409908: step 25289, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:32:02.581449: step 25290, loss 0.00080315, acc 1\n",
      "2018-10-26T18:32:02.761966: step 25291, loss 7.04064e-07, acc 1\n",
      "2018-10-26T18:32:02.934506: step 25292, loss 2.12169e-05, acc 1\n",
      "2018-10-26T18:32:03.123002: step 25293, loss 0, acc 1\n",
      "2018-10-26T18:32:03.294544: step 25294, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:32:03.473067: step 25295, loss 0, acc 1\n",
      "2018-10-26T18:32:03.637628: step 25296, loss 0, acc 1\n",
      "2018-10-26T18:32:03.829116: step 25297, loss 0, acc 1\n",
      "2018-10-26T18:32:03.998663: step 25298, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:32:04.187159: step 25299, loss 6.09918e-05, acc 1\n",
      "2018-10-26T18:32:04.364684: step 25300, loss 2.15084e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:32:04.823459: step 25300, loss 8.68139, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25300\n",
      "\n",
      "2018-10-26T18:32:05.221117: step 25301, loss 1.68908e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:32:05.409615: step 25302, loss 0.208175, acc 0.984375\n",
      "2018-10-26T18:32:05.573177: step 25303, loss 9.05232e-07, acc 1\n",
      "2018-10-26T18:32:05.746714: step 25304, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:32:05.989067: step 25305, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:32:06.176565: step 25306, loss 2.7155e-06, acc 1\n",
      "2018-10-26T18:32:06.356085: step 25307, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:32:06.539595: step 25308, loss 0.000144895, acc 1\n",
      "2018-10-26T18:32:06.710139: step 25309, loss 3.475e-05, acc 1\n",
      "2018-10-26T18:32:06.879687: step 25310, loss 0.000895363, acc 1\n",
      "2018-10-26T18:32:07.050231: step 25311, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:32:07.217782: step 25312, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:32:07.395309: step 25313, loss 5.94103e-05, acc 1\n",
      "2018-10-26T18:32:07.571837: step 25314, loss 6.73969e-06, acc 1\n",
      "2018-10-26T18:32:07.744376: step 25315, loss 0.0904492, acc 0.984375\n",
      "2018-10-26T18:32:07.916915: step 25316, loss 0.000139497, acc 1\n",
      "2018-10-26T18:32:08.100424: step 25317, loss 2.2257e-06, acc 1\n",
      "2018-10-26T18:32:08.264985: step 25318, loss 0, acc 1\n",
      "2018-10-26T18:32:08.438521: step 25319, loss 9.872e-08, acc 1\n",
      "2018-10-26T18:32:08.613055: step 25320, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:08.786592: step 25321, loss 8.47274e-06, acc 1\n",
      "2018-10-26T18:32:08.957136: step 25322, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:32:09.134661: step 25323, loss 2.00594e-06, acc 1\n",
      "2018-10-26T18:32:09.312187: step 25324, loss 4.63793e-07, acc 1\n",
      "2018-10-26T18:32:09.492705: step 25325, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:32:09.669233: step 25326, loss 0.00149345, acc 1\n",
      "2018-10-26T18:32:09.841771: step 25327, loss 3.46448e-07, acc 1\n",
      "2018-10-26T18:32:10.043273: step 25328, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:32:10.214815: step 25329, loss 0, acc 1\n",
      "2018-10-26T18:32:10.392340: step 25330, loss 0, acc 1\n",
      "2018-10-26T18:32:10.565877: step 25331, loss 0, acc 1\n",
      "2018-10-26T18:32:10.737418: step 25332, loss 1.36529e-05, acc 1\n",
      "2018-10-26T18:32:10.910954: step 25333, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:32:11.084491: step 25334, loss 7.65529e-07, acc 1\n",
      "2018-10-26T18:32:11.256032: step 25335, loss 0.000253618, acc 1\n",
      "2018-10-26T18:32:11.432560: step 25336, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:32:11.602110: step 25337, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:32:11.776642: step 25338, loss 2.93902e-06, acc 1\n",
      "2018-10-26T18:32:11.943196: step 25339, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:32:12.119725: step 25340, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:32:12.286279: step 25341, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:12.464803: step 25342, loss 0, acc 1\n",
      "2018-10-26T18:32:12.631357: step 25343, loss 2.46903e-05, acc 1\n",
      "2018-10-26T18:32:12.806889: step 25344, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:32:12.975440: step 25345, loss 2.9114e-05, acc 1\n",
      "2018-10-26T18:32:13.155956: step 25346, loss 6.35415e-06, acc 1\n",
      "2018-10-26T18:32:13.330490: step 25347, loss 0.0294132, acc 0.984375\n",
      "2018-10-26T18:32:13.510010: step 25348, loss 0.000119314, acc 1\n",
      "2018-10-26T18:32:13.677562: step 25349, loss 8.2572e-06, acc 1\n",
      "2018-10-26T18:32:13.856085: step 25350, loss 5.59759e-05, acc 1\n",
      "2018-10-26T18:32:14.030619: step 25351, loss 4.9546e-07, acc 1\n",
      "2018-10-26T18:32:14.205153: step 25352, loss 1.45254e-05, acc 1\n",
      "2018-10-26T18:32:14.382678: step 25353, loss 5.07675e-06, acc 1\n",
      "2018-10-26T18:32:14.557459: step 25354, loss 0, acc 1\n",
      "2018-10-26T18:32:14.729998: step 25355, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:32:14.907524: step 25356, loss 1.51924e-05, acc 1\n",
      "2018-10-26T18:32:15.080062: step 25357, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:32:15.258586: step 25358, loss 0.0356537, acc 0.984375\n",
      "2018-10-26T18:32:15.423146: step 25359, loss 4.73105e-07, acc 1\n",
      "2018-10-26T18:32:15.604661: step 25360, loss 7.31852e-05, acc 1\n",
      "2018-10-26T18:32:15.772214: step 25361, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:15.947745: step 25362, loss 0.00750385, acc 1\n",
      "2018-10-26T18:32:16.125270: step 25363, loss 2.29887e-05, acc 1\n",
      "2018-10-26T18:32:16.313767: step 25364, loss 1.85881e-06, acc 1\n",
      "2018-10-26T18:32:16.485308: step 25365, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:32:16.667821: step 25366, loss 0, acc 1\n",
      "2018-10-26T18:32:16.834376: step 25367, loss 5.06631e-07, acc 1\n",
      "2018-10-26T18:32:17.009907: step 25368, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:32:17.187434: step 25369, loss 0, acc 1\n",
      "2018-10-26T18:32:17.362963: step 25370, loss 4.95572e-06, acc 1\n",
      "2018-10-26T18:32:17.530516: step 25371, loss 3.89804e-06, acc 1\n",
      "2018-10-26T18:32:17.711033: step 25372, loss 4.41441e-07, acc 1\n",
      "2018-10-26T18:32:17.881578: step 25373, loss 1.05837e-05, acc 1\n",
      "2018-10-26T18:32:18.067082: step 25374, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:32:18.237626: step 25375, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:32:18.417146: step 25376, loss 1.56461e-07, acc 1\n",
      "2018-10-26T18:32:18.594672: step 25377, loss 3.46448e-07, acc 1\n",
      "2018-10-26T18:32:18.768208: step 25378, loss 0, acc 1\n",
      "2018-10-26T18:32:18.943739: step 25379, loss 4.34495e-06, acc 1\n",
      "2018-10-26T18:32:19.118273: step 25380, loss 0.000622906, acc 1\n",
      "2018-10-26T18:32:19.287821: step 25381, loss 0.068984, acc 0.984375\n",
      "2018-10-26T18:32:19.464348: step 25382, loss 5.1222e-07, acc 1\n",
      "2018-10-26T18:32:19.634893: step 25383, loss 1.98918e-06, acc 1\n",
      "2018-10-26T18:32:19.809427: step 25384, loss 0, acc 1\n",
      "2018-10-26T18:32:19.979971: step 25385, loss 7.7496e-06, acc 1\n",
      "2018-10-26T18:32:20.153507: step 25386, loss 1.03212e-05, acc 1\n",
      "2018-10-26T18:32:20.322057: step 25387, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:20.496591: step 25388, loss 0, acc 1\n",
      "2018-10-26T18:32:20.671125: step 25389, loss 0, acc 1\n",
      "2018-10-26T18:32:20.846656: step 25390, loss 3.83874e-05, acc 1\n",
      "2018-10-26T18:32:21.014209: step 25391, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:32:21.196720: step 25392, loss 0, acc 1\n",
      "2018-10-26T18:32:21.366267: step 25393, loss 0.00345199, acc 1\n",
      "2018-10-26T18:32:21.537809: step 25394, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:21.710348: step 25395, loss 0.00306238, acc 1\n",
      "2018-10-26T18:32:21.891863: step 25396, loss 0, acc 1\n",
      "2018-10-26T18:32:22.057420: step 25397, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:32:22.233949: step 25398, loss 0, acc 1\n",
      "2018-10-26T18:32:22.408482: step 25399, loss 5.02906e-07, acc 1\n",
      "2018-10-26T18:32:22.587005: step 25400, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:32:23.046776: step 25400, loss 8.64538, acc 0.730769\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25400\n",
      "\n",
      "2018-10-26T18:32:23.415680: step 25401, loss 0.00036563, acc 1\n",
      "2018-10-26T18:32:23.587222: step 25402, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:32:23.796662: step 25403, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:32:23.996130: step 25404, loss 6.37267e-06, acc 1\n",
      "2018-10-26T18:32:24.260424: step 25405, loss 2.63956e-05, acc 1\n",
      "2018-10-26T18:32:24.452910: step 25406, loss 1.97174e-05, acc 1\n",
      "2018-10-26T18:32:24.655369: step 25407, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:32:24.840873: step 25408, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:32:25.028371: step 25409, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:32:25.214873: step 25410, loss 0, acc 1\n",
      "2018-10-26T18:32:25.403371: step 25411, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:25.587877: step 25412, loss 0.00017832, acc 1\n",
      "2018-10-26T18:32:25.766399: step 25413, loss 3.12351e-06, acc 1\n",
      "2018-10-26T18:32:25.952902: step 25414, loss 0.00111506, acc 1\n",
      "2018-10-26T18:32:26.123446: step 25415, loss 0.000826883, acc 1\n",
      "2018-10-26T18:32:26.308950: step 25416, loss 1.8409e-05, acc 1\n",
      "2018-10-26T18:32:26.482486: step 25417, loss 2.48503e-05, acc 1\n",
      "2018-10-26T18:32:26.669985: step 25418, loss 2.50697e-05, acc 1\n",
      "2018-10-26T18:32:26.834546: step 25419, loss 2.48087e-06, acc 1\n",
      "2018-10-26T18:32:27.017058: step 25420, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:27.189596: step 25421, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:32:27.369117: step 25422, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:32:27.545646: step 25423, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:32:27.726164: step 25424, loss 6.63091e-07, acc 1\n",
      "2018-10-26T18:32:27.895710: step 25425, loss 0.135568, acc 0.984375\n",
      "2018-10-26T18:32:28.106153: step 25426, loss 2.10281e-06, acc 1\n",
      "2018-10-26T18:32:28.296640: step 25427, loss 1.80815e-05, acc 1\n",
      "2018-10-26T18:32:28.506080: step 25428, loss 1.60179e-06, acc 1\n",
      "2018-10-26T18:32:28.684603: step 25429, loss 5.83003e-07, acc 1\n",
      "2018-10-26T18:32:28.913990: step 25430, loss 0.000963939, acc 1\n",
      "2018-10-26T18:32:29.094507: step 25431, loss 0, acc 1\n",
      "2018-10-26T18:32:29.318907: step 25432, loss 1.84019e-06, acc 1\n",
      "2018-10-26T18:32:29.491447: step 25433, loss 1.93714e-07, acc 1\n",
      "2018-10-26T18:32:29.666978: step 25434, loss 3.15661e-05, acc 1\n",
      "2018-10-26T18:32:29.889384: step 25435, loss 1.31312e-06, acc 1\n",
      "2018-10-26T18:32:30.058930: step 25436, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:32:30.234462: step 25437, loss 7.88662e-05, acc 1\n",
      "2018-10-26T18:32:30.455870: step 25438, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:32:30.651347: step 25439, loss 0, acc 1\n",
      "2018-10-26T18:32:30.859791: step 25440, loss 0.00228477, acc 1\n",
      "2018-10-26T18:32:31.110121: step 25441, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:32:31.292635: step 25442, loss 5.45887e-06, acc 1\n",
      "2018-10-26T18:32:31.528006: step 25443, loss 0.000680524, acc 1\n",
      "2018-10-26T18:32:31.707525: step 25444, loss 9.68573e-08, acc 1\n",
      "2018-10-26T18:32:31.924945: step 25445, loss 1.90537e-06, acc 1\n",
      "2018-10-26T18:32:32.135383: step 25446, loss 2.04319e-06, acc 1\n",
      "2018-10-26T18:32:32.318892: step 25447, loss 0.00225939, acc 1\n",
      "2018-10-26T18:32:32.554264: step 25448, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:32.758718: step 25449, loss 2.84959e-06, acc 1\n",
      "2018-10-26T18:32:32.940231: step 25450, loss 1.16527e-05, acc 1\n",
      "2018-10-26T18:32:33.157662: step 25451, loss 1.29076e-06, acc 1\n",
      "2018-10-26T18:32:33.363102: step 25452, loss 1.15296e-06, acc 1\n",
      "2018-10-26T18:32:33.551599: step 25453, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:32:33.746079: step 25454, loss 6.86606e-06, acc 1\n",
      "2018-10-26T18:32:33.923605: step 25455, loss 4.73105e-07, acc 1\n",
      "2018-10-26T18:32:34.166955: step 25456, loss 0, acc 1\n",
      "2018-10-26T18:32:34.343482: step 25457, loss 1.27282e-05, acc 1\n",
      "2018-10-26T18:32:34.563893: step 25458, loss 6.8456e-06, acc 1\n",
      "2018-10-26T18:32:34.768348: step 25459, loss 5.43884e-07, acc 1\n",
      "2018-10-26T18:32:34.967815: step 25460, loss 2.36554e-07, acc 1\n",
      "2018-10-26T18:32:35.182241: step 25461, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:32:35.360765: step 25462, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:35.567213: step 25463, loss 6.79021e-05, acc 1\n",
      "2018-10-26T18:32:35.782637: step 25464, loss 5.79272e-07, acc 1\n",
      "2018-10-26T18:32:35.954180: step 25465, loss 9.70881e-06, acc 1\n",
      "2018-10-26T18:32:36.129710: step 25466, loss 5.82532e-06, acc 1\n",
      "2018-10-26T18:32:36.341147: step 25467, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:32:36.518671: step 25468, loss 0.000458846, acc 1\n",
      "2018-10-26T18:32:36.691211: step 25469, loss 5.34571e-07, acc 1\n",
      "2018-10-26T18:32:36.883695: step 25470, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:32:37.069214: step 25471, loss 0, acc 1\n",
      "2018-10-26T18:32:37.242737: step 25472, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:32:37.417271: step 25473, loss 9.20128e-07, acc 1\n",
      "2018-10-26T18:32:37.629703: step 25474, loss 7.94466e-05, acc 1\n",
      "2018-10-26T18:32:37.804237: step 25475, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:32:37.980765: step 25476, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:38.158291: step 25477, loss 5.89164e-05, acc 1\n",
      "2018-10-26T18:32:38.363742: step 25478, loss 0.000145634, acc 1\n",
      "2018-10-26T18:32:38.572185: step 25479, loss 7.56224e-07, acc 1\n",
      "2018-10-26T18:32:38.775642: step 25480, loss 0.000115185, acc 1\n",
      "2018-10-26T18:32:39.010014: step 25481, loss 8.40031e-07, acc 1\n",
      "2018-10-26T18:32:39.211476: step 25482, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:39.394986: step 25483, loss 1.04776e-05, acc 1\n",
      "2018-10-26T18:32:39.614401: step 25484, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:32:39.784944: step 25485, loss 6.45832e-06, acc 1\n",
      "2018-10-26T18:32:40.008347: step 25486, loss 4.37091e-05, acc 1\n",
      "2018-10-26T18:32:40.180886: step 25487, loss 7.39457e-07, acc 1\n",
      "2018-10-26T18:32:40.359409: step 25488, loss 1.8197e-06, acc 1\n",
      "2018-10-26T18:32:40.576829: step 25489, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:32:40.746376: step 25490, loss 3.68016e-06, acc 1\n",
      "2018-10-26T18:32:40.920909: step 25491, loss 0, acc 1\n",
      "2018-10-26T18:32:41.121374: step 25492, loss 0, acc 1\n",
      "2018-10-26T18:32:41.298899: step 25493, loss 9.47173e-05, acc 1\n",
      "2018-10-26T18:32:41.475428: step 25494, loss 3.9782e-05, acc 1\n",
      "2018-10-26T18:32:41.651957: step 25495, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:32:41.858405: step 25496, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:32:42.036927: step 25497, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:32:42.219439: step 25498, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:32:42.396966: step 25499, loss 4.61085e-05, acc 1\n",
      "2018-10-26T18:32:42.597431: step 25500, loss 0.000495946, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:32:43.057201: step 25500, loss 8.7397, acc 0.726079\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25500\n",
      "\n",
      "2018-10-26T18:32:43.435750: step 25501, loss 0.00022432, acc 1\n",
      "2018-10-26T18:32:43.613276: step 25502, loss 4.6193e-07, acc 1\n",
      "2018-10-26T18:32:43.796785: step 25503, loss 3.3001e-05, acc 1\n",
      "2018-10-26T18:32:43.963340: step 25504, loss 2.25378e-07, acc 1\n",
      "2018-10-26T18:32:44.181759: step 25505, loss 4.41442e-07, acc 1\n",
      "2018-10-26T18:32:44.389205: step 25506, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:32:44.572712: step 25507, loss 4.9732e-07, acc 1\n",
      "2018-10-26T18:32:44.821048: step 25508, loss 6.11409e-05, acc 1\n",
      "2018-10-26T18:32:45.016526: step 25509, loss 0, acc 1\n",
      "2018-10-26T18:32:45.194051: step 25510, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:32:45.387535: step 25511, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:32:45.556085: step 25512, loss 2.58973e-05, acc 1\n",
      "2018-10-26T18:32:45.733610: step 25513, loss 2.0956e-05, acc 1\n",
      "2018-10-26T18:32:45.905152: step 25514, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:32:46.083675: step 25515, loss 2.51693e-05, acc 1\n",
      "2018-10-26T18:32:46.253222: step 25516, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:46.430748: step 25517, loss 1.08216e-06, acc 1\n",
      "2018-10-26T18:32:46.601293: step 25518, loss 0, acc 1\n",
      "2018-10-26T18:32:46.778818: step 25519, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:32:46.954349: step 25520, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:32:47.134866: step 25521, loss 0, acc 1\n",
      "2018-10-26T18:32:47.302418: step 25522, loss 1.75451e-06, acc 1\n",
      "2018-10-26T18:32:47.481939: step 25523, loss 0.000372907, acc 1\n",
      "2018-10-26T18:32:47.647497: step 25524, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:32:47.826019: step 25525, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:48.004543: step 25526, loss 3.91872e-06, acc 1\n",
      "2018-10-26T18:32:48.185061: step 25527, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:32:48.351617: step 25528, loss 4.41165e-05, acc 1\n",
      "2018-10-26T18:32:48.534129: step 25529, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:32:48.704672: step 25530, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:32:48.879206: step 25531, loss 0.000170984, acc 1\n",
      "2018-10-26T18:32:49.047756: step 25532, loss 3.37475e-06, acc 1\n",
      "2018-10-26T18:32:49.231266: step 25533, loss 0, acc 1\n",
      "2018-10-26T18:32:49.399814: step 25534, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:32:49.581330: step 25535, loss 8.98115e-05, acc 1\n",
      "2018-10-26T18:32:49.751874: step 25536, loss 3.79129e-05, acc 1\n",
      "2018-10-26T18:32:49.930397: step 25537, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:32:50.105928: step 25538, loss 2.06741e-06, acc 1\n",
      "2018-10-26T18:32:50.287444: step 25539, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:32:50.453000: step 25540, loss 4.39895e-06, acc 1\n",
      "2018-10-26T18:32:50.635513: step 25541, loss 1.36472e-05, acc 1\n",
      "2018-10-26T18:32:50.811044: step 25542, loss 0, acc 1\n",
      "2018-10-26T18:32:50.989567: step 25543, loss 1.17717e-06, acc 1\n",
      "2018-10-26T18:32:51.161109: step 25544, loss 1.91851e-07, acc 1\n",
      "2018-10-26T18:32:51.339631: step 25545, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:51.508208: step 25546, loss 0, acc 1\n",
      "2018-10-26T18:32:51.688700: step 25547, loss 1.16225e-06, acc 1\n",
      "2018-10-26T18:32:51.866226: step 25548, loss 7.01167e-05, acc 1\n",
      "2018-10-26T18:32:52.044749: step 25549, loss 0.000662878, acc 1\n",
      "2018-10-26T18:32:52.216289: step 25550, loss 1.95755e-05, acc 1\n",
      "2018-10-26T18:32:52.396807: step 25551, loss 3.51458e-05, acc 1\n",
      "2018-10-26T18:32:52.569346: step 25552, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:52.751859: step 25553, loss 1.78618e-06, acc 1\n",
      "2018-10-26T18:32:52.922403: step 25554, loss 2.01158e-06, acc 1\n",
      "2018-10-26T18:32:53.102920: step 25555, loss 6.7878e-05, acc 1\n",
      "2018-10-26T18:32:53.272467: step 25556, loss 0.00101817, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:32:53.456974: step 25557, loss 3.91293e-06, acc 1\n",
      "2018-10-26T18:32:53.620537: step 25558, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:53.802052: step 25559, loss 7.93675e-06, acc 1\n",
      "2018-10-26T18:32:53.970602: step 25560, loss 2.34692e-07, acc 1\n",
      "2018-10-26T18:32:54.143142: step 25561, loss 5.56922e-07, acc 1\n",
      "2018-10-26T18:32:54.311691: step 25562, loss 1.96311e-06, acc 1\n",
      "2018-10-26T18:32:54.498192: step 25563, loss 9.59531e-06, acc 1\n",
      "2018-10-26T18:32:54.671729: step 25564, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:54.861222: step 25565, loss 0, acc 1\n",
      "2018-10-26T18:32:55.037751: step 25566, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:55.211287: step 25567, loss 6.7799e-07, acc 1\n",
      "2018-10-26T18:32:55.378841: step 25568, loss 2.74157e-06, acc 1\n",
      "2018-10-26T18:32:55.552376: step 25569, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:32:55.719929: step 25570, loss 1.72099e-06, acc 1\n",
      "2018-10-26T18:32:55.900446: step 25571, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:32:56.068996: step 25572, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:32:56.241535: step 25573, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:32:56.415072: step 25574, loss 0.000693286, acc 1\n",
      "2018-10-26T18:32:56.588607: step 25575, loss 0, acc 1\n",
      "2018-10-26T18:32:56.761146: step 25576, loss 1.10406e-05, acc 1\n",
      "2018-10-26T18:32:56.946651: step 25577, loss 0.00167133, acc 1\n",
      "2018-10-26T18:32:57.120187: step 25578, loss 2.43993e-06, acc 1\n",
      "2018-10-26T18:32:57.296716: step 25579, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:57.465265: step 25580, loss 5.68097e-07, acc 1\n",
      "2018-10-26T18:32:57.638801: step 25581, loss 1.73226e-07, acc 1\n",
      "2018-10-26T18:32:57.808349: step 25582, loss 1.90729e-06, acc 1\n",
      "2018-10-26T18:32:57.989863: step 25583, loss 2.20156e-06, acc 1\n",
      "2018-10-26T18:32:58.158413: step 25584, loss 1.03932e-06, acc 1\n",
      "2018-10-26T18:32:58.332946: step 25585, loss 0.000135102, acc 1\n",
      "2018-10-26T18:32:58.507480: step 25586, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:32:58.688996: step 25587, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:32:58.853556: step 25588, loss 1.32248e-07, acc 1\n",
      "2018-10-26T18:32:59.028089: step 25589, loss 1.51053e-06, acc 1\n",
      "2018-10-26T18:32:59.200628: step 25590, loss 2.11582e-06, acc 1\n",
      "2018-10-26T18:32:59.377156: step 25591, loss 1.76947e-06, acc 1\n",
      "2018-10-26T18:32:59.549696: step 25592, loss 3.26674e-06, acc 1\n",
      "2018-10-26T18:32:59.725234: step 25593, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:32:59.892779: step 25594, loss 0, acc 1\n",
      "2018-10-26T18:33:00.079281: step 25595, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:33:00.245835: step 25596, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:33:00.426354: step 25597, loss 2.21653e-07, acc 1\n",
      "2018-10-26T18:33:00.596897: step 25598, loss 6.53784e-07, acc 1\n",
      "2018-10-26T18:33:00.782403: step 25599, loss 1.08031e-06, acc 1\n",
      "2018-10-26T18:33:00.951949: step 25600, loss 2.98023e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:33:01.409726: step 25600, loss 8.70352, acc 0.725141\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25600\n",
      "\n",
      "2018-10-26T18:33:01.783774: step 25601, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:33:01.960302: step 25602, loss 6.2523e-05, acc 1\n",
      "2018-10-26T18:33:02.155779: step 25603, loss 2.10474e-05, acc 1\n",
      "2018-10-26T18:33:02.321338: step 25604, loss 3.57625e-07, acc 1\n",
      "2018-10-26T18:33:02.555711: step 25605, loss 0.01596, acc 0.984375\n",
      "2018-10-26T18:33:02.749193: step 25606, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:33:02.931707: step 25607, loss 1.58503e-06, acc 1\n",
      "2018-10-26T18:33:03.122197: step 25608, loss 3.42724e-07, acc 1\n",
      "2018-10-26T18:33:03.300720: step 25609, loss 3.0361e-07, acc 1\n",
      "2018-10-26T18:33:03.476252: step 25610, loss 0, acc 1\n",
      "2018-10-26T18:33:03.646796: step 25611, loss 0.000250304, acc 1\n",
      "2018-10-26T18:33:03.825319: step 25612, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:33:03.992871: step 25613, loss 7.52389e-05, acc 1\n",
      "2018-10-26T18:33:04.171394: step 25614, loss 0, acc 1\n",
      "2018-10-26T18:33:04.340941: step 25615, loss 1.33234e-05, acc 1\n",
      "2018-10-26T18:33:04.515474: step 25616, loss 1.55342e-06, acc 1\n",
      "2018-10-26T18:33:04.689011: step 25617, loss 5.3151e-06, acc 1\n",
      "2018-10-26T18:33:04.864543: step 25618, loss 0, acc 1\n",
      "2018-10-26T18:33:05.033092: step 25619, loss 1.94414e-05, acc 1\n",
      "2018-10-26T18:33:05.215604: step 25620, loss 1.4156e-07, acc 1\n",
      "2018-10-26T18:33:05.386149: step 25621, loss 1.96501e-06, acc 1\n",
      "2018-10-26T18:33:05.563674: step 25622, loss 0, acc 1\n",
      "2018-10-26T18:33:05.731241: step 25623, loss 6.75634e-06, acc 1\n",
      "2018-10-26T18:33:05.913739: step 25624, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:33:06.085280: step 25625, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:33:06.262806: step 25626, loss 2.94295e-07, acc 1\n",
      "2018-10-26T18:33:06.438337: step 25627, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:33:06.620849: step 25628, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:33:06.791394: step 25629, loss 2.43252e-06, acc 1\n",
      "2018-10-26T18:33:06.984877: step 25630, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:33:07.153427: step 25631, loss 0, acc 1\n",
      "2018-10-26T18:33:07.332947: step 25632, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:33:07.500499: step 25633, loss 4.47035e-08, acc 1\n",
      "2018-10-26T18:33:07.688997: step 25634, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:33:07.859540: step 25635, loss 5.15945e-07, acc 1\n",
      "2018-10-26T18:33:08.039061: step 25636, loss 3.02837e-06, acc 1\n",
      "2018-10-26T18:33:08.205616: step 25637, loss 0, acc 1\n",
      "2018-10-26T18:33:08.395109: step 25638, loss 3.37661e-06, acc 1\n",
      "2018-10-26T18:33:08.562661: step 25639, loss 2.85521e-06, acc 1\n",
      "2018-10-26T18:33:08.740188: step 25640, loss 0.00036863, acc 1\n",
      "2018-10-26T18:33:08.909734: step 25641, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:33:09.091249: step 25642, loss 6.31424e-07, acc 1\n",
      "2018-10-26T18:33:09.260796: step 25643, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:33:09.440316: step 25644, loss 2.42686e-06, acc 1\n",
      "2018-10-26T18:33:09.606872: step 25645, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:33:09.786391: step 25646, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:33:09.964915: step 25647, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:33:10.145432: step 25648, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:33:10.320964: step 25649, loss 4.04896e-06, acc 1\n",
      "2018-10-26T18:33:10.486521: step 25650, loss 0, acc 1\n",
      "2018-10-26T18:33:10.664046: step 25651, loss 9.3875e-07, acc 1\n",
      "2018-10-26T18:33:10.833594: step 25652, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:33:11.012118: step 25653, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:33:11.181664: step 25654, loss 1.47148e-07, acc 1\n",
      "2018-10-26T18:33:11.364177: step 25655, loss 0.000158126, acc 1\n",
      "2018-10-26T18:33:11.535717: step 25656, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:33:11.719227: step 25657, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:33:11.884785: step 25658, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:33:12.070298: step 25659, loss 0.000222671, acc 1\n",
      "2018-10-26T18:33:12.237843: step 25660, loss 1.79366e-06, acc 1\n",
      "2018-10-26T18:33:12.429331: step 25661, loss 1.51099e-05, acc 1\n",
      "2018-10-26T18:33:12.601869: step 25662, loss 2.62632e-07, acc 1\n",
      "2018-10-26T18:33:12.786377: step 25663, loss 2.23315e-06, acc 1\n",
      "2018-10-26T18:33:12.952932: step 25664, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:33:13.137438: step 25665, loss 2.90745e-05, acc 1\n",
      "2018-10-26T18:33:13.308980: step 25666, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:33:13.492490: step 25667, loss 3.45297e-06, acc 1\n",
      "2018-10-26T18:33:13.663034: step 25668, loss 0, acc 1\n",
      "2018-10-26T18:33:13.839562: step 25669, loss 0, acc 1\n",
      "2018-10-26T18:33:14.008112: step 25670, loss 0, acc 1\n",
      "2018-10-26T18:33:14.187633: step 25671, loss 1.63912e-07, acc 1\n",
      "2018-10-26T18:33:14.359175: step 25672, loss 2.24246e-06, acc 1\n",
      "2018-10-26T18:33:14.540689: step 25673, loss 0, acc 1\n",
      "2018-10-26T18:33:14.712231: step 25674, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:33:14.898733: step 25675, loss 7.97525e-05, acc 1\n",
      "2018-10-26T18:33:15.065287: step 25676, loss 1.3485e-06, acc 1\n",
      "2018-10-26T18:33:15.247800: step 25677, loss 0.000631376, acc 1\n",
      "2018-10-26T18:33:15.410365: step 25678, loss 0, acc 1\n",
      "2018-10-26T18:33:15.589885: step 25679, loss 8.39507e-05, acc 1\n",
      "2018-10-26T18:33:15.768409: step 25680, loss 5.7369e-07, acc 1\n",
      "2018-10-26T18:33:15.947928: step 25681, loss 1.02815e-06, acc 1\n",
      "2018-10-26T18:33:16.120469: step 25682, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:33:16.307967: step 25683, loss 5.02906e-07, acc 1\n",
      "2018-10-26T18:33:16.475520: step 25684, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:33:16.660026: step 25685, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:33:16.834560: step 25686, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:33:17.007099: step 25687, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:33:17.178640: step 25688, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:33:17.352178: step 25689, loss 2.70082e-07, acc 1\n",
      "2018-10-26T18:33:17.517734: step 25690, loss 3.3675e-06, acc 1\n",
      "2018-10-26T18:33:17.696257: step 25691, loss 1.4156e-07, acc 1\n",
      "2018-10-26T18:33:17.869794: step 25692, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:33:18.042333: step 25693, loss 0, acc 1\n",
      "2018-10-26T18:33:18.210882: step 25694, loss 6.44035e-06, acc 1\n",
      "2018-10-26T18:33:18.386414: step 25695, loss 1.01883e-06, acc 1\n",
      "2018-10-26T18:33:18.557956: step 25696, loss 1.97056e-05, acc 1\n",
      "2018-10-26T18:33:18.737475: step 25697, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:33:18.906026: step 25698, loss 0, acc 1\n",
      "2018-10-26T18:33:19.089536: step 25699, loss 1.14939e-05, acc 1\n",
      "2018-10-26T18:33:19.255093: step 25700, loss 5.38298e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:33:19.710876: step 25700, loss 8.75346, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25700\n",
      "\n",
      "2018-10-26T18:33:20.098386: step 25701, loss 0, acc 1\n",
      "2018-10-26T18:33:20.269929: step 25702, loss 3.93013e-07, acc 1\n",
      "2018-10-26T18:33:20.447453: step 25703, loss 6.14666e-07, acc 1\n",
      "2018-10-26T18:33:20.615006: step 25704, loss 0.000289799, acc 1\n",
      "2018-10-26T18:33:20.813476: step 25705, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:33:21.040868: step 25706, loss 3.53899e-07, acc 1\n",
      "2018-10-26T18:33:21.204431: step 25707, loss 3.68801e-07, acc 1\n",
      "2018-10-26T18:33:21.385946: step 25708, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:33:21.550506: step 25709, loss 0, acc 1\n",
      "2018-10-26T18:33:21.726038: step 25710, loss 5.21535e-07, acc 1\n",
      "2018-10-26T18:33:21.897579: step 25711, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:33:22.077099: step 25712, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:33:22.246648: step 25713, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:33:22.423176: step 25714, loss 0, acc 1\n",
      "2018-10-26T18:33:22.590727: step 25715, loss 6.25167e-06, acc 1\n",
      "2018-10-26T18:33:22.766258: step 25716, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:33:22.932813: step 25717, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:33:23.109342: step 25718, loss 1.11758e-07, acc 1\n",
      "2018-10-26T18:33:23.278889: step 25719, loss 2.94295e-07, acc 1\n",
      "2018-10-26T18:33:23.459406: step 25720, loss 8.39798e-05, acc 1\n",
      "2018-10-26T18:33:23.623966: step 25721, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:33:23.798501: step 25722, loss 0, acc 1\n",
      "2018-10-26T18:33:23.973034: step 25723, loss 2.8312e-07, acc 1\n",
      "2018-10-26T18:33:24.149562: step 25724, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:33:24.321103: step 25725, loss 5.96678e-06, acc 1\n",
      "2018-10-26T18:33:24.498630: step 25726, loss 3.22018e-06, acc 1\n",
      "2018-10-26T18:33:24.665184: step 25727, loss 0, acc 1\n",
      "2018-10-26T18:33:24.841714: step 25728, loss 4.45481e-06, acc 1\n",
      "2018-10-26T18:33:25.013254: step 25729, loss 3.06333e-05, acc 1\n",
      "2018-10-26T18:33:25.188785: step 25730, loss 1.00023e-06, acc 1\n",
      "2018-10-26T18:33:25.358332: step 25731, loss 3.79002e-06, acc 1\n",
      "2018-10-26T18:33:25.534861: step 25732, loss 0, acc 1\n",
      "2018-10-26T18:33:25.702413: step 25733, loss 7.72979e-07, acc 1\n",
      "2018-10-26T18:33:25.879939: step 25734, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:33:26.054472: step 25735, loss 1.26843e-06, acc 1\n",
      "2018-10-26T18:33:26.228010: step 25736, loss 6.23974e-07, acc 1\n",
      "2018-10-26T18:33:26.403541: step 25737, loss 4.58204e-07, acc 1\n",
      "2018-10-26T18:33:26.597023: step 25738, loss 0, acc 1\n",
      "2018-10-26T18:33:26.792500: step 25739, loss 0, acc 1\n",
      "2018-10-26T18:33:26.972021: step 25740, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:33:27.149546: step 25741, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:33:27.316102: step 25742, loss 0, acc 1\n",
      "2018-10-26T18:33:27.496619: step 25743, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:33:27.664172: step 25744, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:33:27.841697: step 25745, loss 0.01306, acc 0.984375\n",
      "2018-10-26T18:33:28.011255: step 25746, loss 3.53058e-05, acc 1\n",
      "2018-10-26T18:33:28.189767: step 25747, loss 0, acc 1\n",
      "2018-10-26T18:33:28.359314: step 25748, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:33:28.544819: step 25749, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:33:28.728328: step 25750, loss 2.89069e-06, acc 1\n",
      "2018-10-26T18:33:28.905855: step 25751, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:33:29.089363: step 25752, loss 1.62042e-06, acc 1\n",
      "2018-10-26T18:33:29.258911: step 25753, loss 3.94877e-07, acc 1\n",
      "2018-10-26T18:33:29.442420: step 25754, loss 9.01494e-07, acc 1\n",
      "2018-10-26T18:33:29.609973: step 25755, loss 0.0917803, acc 0.984375\n",
      "2018-10-26T18:33:29.789493: step 25756, loss 3.93013e-07, acc 1\n",
      "2018-10-26T18:33:29.965025: step 25757, loss 9.19971e-05, acc 1\n",
      "2018-10-26T18:33:30.147537: step 25758, loss 1.63912e-07, acc 1\n",
      "2018-10-26T18:33:30.323068: step 25759, loss 0, acc 1\n",
      "2018-10-26T18:33:30.498598: step 25760, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:33:30.675129: step 25761, loss 6.66813e-07, acc 1\n",
      "2018-10-26T18:33:30.843676: step 25762, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:33:31.022200: step 25763, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:33:31.193741: step 25764, loss 0, acc 1\n",
      "2018-10-26T18:33:31.372271: step 25765, loss 1.54188e-05, acc 1\n",
      "2018-10-26T18:33:31.538819: step 25766, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:33:31.718340: step 25767, loss 5.39176e-06, acc 1\n",
      "2018-10-26T18:33:31.882900: step 25768, loss 9.872e-08, acc 1\n",
      "2018-10-26T18:33:32.070399: step 25769, loss 6.33293e-07, acc 1\n",
      "2018-10-26T18:33:32.234959: step 25770, loss 0.000149449, acc 1\n",
      "2018-10-26T18:33:32.413482: step 25771, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:33:32.582032: step 25772, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:33:32.757562: step 25773, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:33:32.932097: step 25774, loss 0.000301479, acc 1\n",
      "2018-10-26T18:33:33.118598: step 25775, loss 6.8731e-07, acc 1\n",
      "2018-10-26T18:33:33.296124: step 25776, loss 0, acc 1\n",
      "2018-10-26T18:33:33.462679: step 25777, loss 7.59945e-07, acc 1\n",
      "2018-10-26T18:33:33.650178: step 25778, loss 1.50308e-06, acc 1\n",
      "2018-10-26T18:33:33.855629: step 25779, loss 0.00400773, acc 1\n",
      "2018-10-26T18:33:34.071054: step 25780, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:33:34.256557: step 25781, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:33:34.477971: step 25782, loss 1.13245e-06, acc 1\n",
      "2018-10-26T18:33:34.646516: step 25783, loss 3.76251e-07, acc 1\n",
      "2018-10-26T18:33:34.850970: step 25784, loss 9.82893e-05, acc 1\n",
      "2018-10-26T18:33:35.044453: step 25785, loss 3.83654e-05, acc 1\n",
      "2018-10-26T18:33:35.241926: step 25786, loss 1.97242e-05, acc 1\n",
      "2018-10-26T18:33:35.463335: step 25787, loss 5.35597e-05, acc 1\n",
      "2018-10-26T18:33:35.657814: step 25788, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:33:35.855288: step 25789, loss 0.0206271, acc 0.984375\n",
      "2018-10-26T18:33:36.043782: step 25790, loss 0.000226296, acc 1\n",
      "2018-10-26T18:33:36.241255: step 25791, loss 3.27824e-07, acc 1\n",
      "2018-10-26T18:33:36.464658: step 25792, loss 6.14597e-06, acc 1\n",
      "2018-10-26T18:33:36.667118: step 25793, loss 0, acc 1\n",
      "2018-10-26T18:33:36.900495: step 25794, loss 1.70023e-05, acc 1\n",
      "2018-10-26T18:33:37.122901: step 25795, loss 3.06748e-06, acc 1\n",
      "2018-10-26T18:33:37.343311: step 25796, loss 0, acc 1\n",
      "2018-10-26T18:33:37.579681: step 25797, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:33:37.768175: step 25798, loss 0, acc 1\n",
      "2018-10-26T18:33:37.997564: step 25799, loss 2.10989e-05, acc 1\n",
      "2018-10-26T18:33:38.184064: step 25800, loss 6.5565e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:33:38.755538: step 25800, loss 9.0336, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25800\n",
      "\n",
      "2018-10-26T18:33:39.162218: step 25801, loss 2.94643e-06, acc 1\n",
      "2018-10-26T18:33:39.342735: step 25802, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:33:39.541204: step 25803, loss 2.39519e-06, acc 1\n",
      "2018-10-26T18:33:39.736681: step 25804, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:33:40.062820: step 25805, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:33:40.287212: step 25806, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:33:40.488673: step 25807, loss 1.97439e-07, acc 1\n",
      "2018-10-26T18:33:40.718061: step 25808, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:33:40.939469: step 25809, loss 1.12686e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:33:41.124973: step 25810, loss 6.26875e-06, acc 1\n",
      "2018-10-26T18:33:41.345384: step 25811, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:33:41.543858: step 25812, loss 0.000272724, acc 1\n",
      "2018-10-26T18:33:41.758281: step 25813, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:33:41.926830: step 25814, loss 4.76218e-05, acc 1\n",
      "2018-10-26T18:33:42.122308: step 25815, loss 8.99213e-06, acc 1\n",
      "2018-10-26T18:33:42.296842: step 25816, loss 0.000104986, acc 1\n",
      "2018-10-26T18:33:42.476362: step 25817, loss 6.63089e-07, acc 1\n",
      "2018-10-26T18:33:42.676827: step 25818, loss 1.98953e-05, acc 1\n",
      "2018-10-26T18:33:42.853355: step 25819, loss 0, acc 1\n",
      "2018-10-26T18:33:43.035867: step 25820, loss 0.000262254, acc 1\n",
      "2018-10-26T18:33:43.245308: step 25821, loss 2.46463e-05, acc 1\n",
      "2018-10-26T18:33:43.422833: step 25822, loss 2.55181e-07, acc 1\n",
      "2018-10-26T18:33:43.612328: step 25823, loss 2.17294e-05, acc 1\n",
      "2018-10-26T18:33:43.808802: step 25824, loss 1.18152e-05, acc 1\n",
      "2018-10-26T18:33:44.003282: step 25825, loss 0.000205911, acc 1\n",
      "2018-10-26T18:33:44.196766: step 25826, loss 5.69959e-07, acc 1\n",
      "2018-10-26T18:33:44.385261: step 25827, loss 5.02907e-07, acc 1\n",
      "2018-10-26T18:33:44.599690: step 25828, loss 0.000488979, acc 1\n",
      "2018-10-26T18:33:44.801151: step 25829, loss 4.2174e-05, acc 1\n",
      "2018-10-26T18:33:44.991641: step 25830, loss 0, acc 1\n",
      "2018-10-26T18:33:45.205072: step 25831, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:33:45.386587: step 25832, loss 2.1234e-07, acc 1\n",
      "2018-10-26T18:33:45.583061: step 25833, loss 2.79006e-06, acc 1\n",
      "2018-10-26T18:33:45.776544: step 25834, loss 8.97837e-05, acc 1\n",
      "2018-10-26T18:33:46.013911: step 25835, loss 1.75495e-05, acc 1\n",
      "2018-10-26T18:33:46.226343: step 25836, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:33:46.405863: step 25837, loss 1.44907e-06, acc 1\n",
      "2018-10-26T18:33:46.626275: step 25838, loss 0, acc 1\n",
      "2018-10-26T18:33:46.797816: step 25839, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:33:46.977336: step 25840, loss 1.77164e-05, acc 1\n",
      "2018-10-26T18:33:47.196750: step 25841, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:33:47.369290: step 25842, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:33:47.545817: step 25843, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:33:47.753263: step 25844, loss 2.74903e-06, acc 1\n",
      "2018-10-26T18:33:47.931787: step 25845, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:33:48.112304: step 25846, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:33:48.291823: step 25847, loss 4.98208e-05, acc 1\n",
      "2018-10-26T18:33:48.495281: step 25848, loss 9.45328e-06, acc 1\n",
      "2018-10-26T18:33:48.679789: step 25849, loss 0.0181721, acc 0.984375\n",
      "2018-10-26T18:33:48.878257: step 25850, loss 0, acc 1\n",
      "2018-10-26T18:33:49.084705: step 25851, loss 1.31687e-06, acc 1\n",
      "2018-10-26T18:33:49.315091: step 25852, loss 0.000992195, acc 1\n",
      "2018-10-26T18:33:49.502589: step 25853, loss 3.90734e-06, acc 1\n",
      "2018-10-26T18:33:49.680115: step 25854, loss 3.50339e-05, acc 1\n",
      "2018-10-26T18:33:49.853651: step 25855, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:33:50.040153: step 25856, loss 9.12693e-08, acc 1\n",
      "2018-10-26T18:33:50.216681: step 25857, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:33:50.382238: step 25858, loss 7.64438e-06, acc 1\n",
      "2018-10-26T18:33:50.558767: step 25859, loss 4.32128e-07, acc 1\n",
      "2018-10-26T18:33:50.727316: step 25860, loss 1.24982e-06, acc 1\n",
      "2018-10-26T18:33:50.906842: step 25861, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:33:51.072394: step 25862, loss 1.12203e-05, acc 1\n",
      "2018-10-26T18:33:51.248923: step 25863, loss 1.21071e-07, acc 1\n",
      "2018-10-26T18:33:51.421462: step 25864, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:33:51.599985: step 25865, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:33:51.770529: step 25866, loss 3.36358e-06, acc 1\n",
      "2018-10-26T18:33:51.949054: step 25867, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:33:52.120595: step 25868, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:33:52.304103: step 25869, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:33:52.480633: step 25870, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:33:52.669130: step 25871, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:33:52.844659: step 25872, loss 0, acc 1\n",
      "2018-10-26T18:33:53.017198: step 25873, loss 2.8312e-07, acc 1\n",
      "2018-10-26T18:33:53.189737: step 25874, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:33:53.373247: step 25875, loss 3.70692e-05, acc 1\n",
      "2018-10-26T18:33:53.544788: step 25876, loss 7.7873e-05, acc 1\n",
      "2018-10-26T18:33:53.729296: step 25877, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:33:53.904827: step 25878, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:33:54.086341: step 25879, loss 5.60651e-07, acc 1\n",
      "2018-10-26T18:33:54.259878: step 25880, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:33:54.437405: step 25881, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:33:54.612935: step 25882, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:33:54.784476: step 25883, loss 5.2712e-07, acc 1\n",
      "2018-10-26T18:33:54.962999: step 25884, loss 0, acc 1\n",
      "2018-10-26T18:33:55.126562: step 25885, loss 1.88423e-05, acc 1\n",
      "2018-10-26T18:33:55.300100: step 25886, loss 0, acc 1\n",
      "2018-10-26T18:33:55.474633: step 25887, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:33:55.648170: step 25888, loss 3.37135e-07, acc 1\n",
      "2018-10-26T18:33:55.816718: step 25889, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:33:55.995242: step 25890, loss 6.37665e-06, acc 1\n",
      "2018-10-26T18:33:56.168778: step 25891, loss 1.50122e-06, acc 1\n",
      "2018-10-26T18:33:56.349295: step 25892, loss 1.89989e-07, acc 1\n",
      "2018-10-26T18:33:56.519840: step 25893, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:33:56.694374: step 25894, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:33:56.869905: step 25895, loss 6.68676e-07, acc 1\n",
      "2018-10-26T18:33:57.053414: step 25896, loss 0, acc 1\n",
      "2018-10-26T18:33:57.229943: step 25897, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:33:57.395500: step 25898, loss 4.01559e-06, acc 1\n",
      "2018-10-26T18:33:57.577016: step 25899, loss 2.75315e-05, acc 1\n",
      "2018-10-26T18:33:57.741576: step 25900, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:33:58.204340: step 25900, loss 8.75711, acc 0.727955\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-25900\n",
      "\n",
      "2018-10-26T18:33:58.533413: step 25901, loss 0, acc 1\n",
      "2018-10-26T18:33:58.700966: step 25902, loss 2.593e-05, acc 1\n",
      "2018-10-26T18:33:58.880485: step 25903, loss 1.19985e-05, acc 1\n",
      "2018-10-26T18:33:59.058011: step 25904, loss 0, acc 1\n",
      "2018-10-26T18:33:59.255483: step 25905, loss 1.25218e-05, acc 1\n",
      "2018-10-26T18:33:59.506813: step 25906, loss 1.89989e-07, acc 1\n",
      "2018-10-26T18:33:59.674364: step 25907, loss 0, acc 1\n",
      "2018-10-26T18:33:59.858871: step 25908, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:34:00.042381: step 25909, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:34:00.223896: step 25910, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:34:00.404414: step 25911, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:34:00.586928: step 25912, loss 1.62042e-06, acc 1\n",
      "2018-10-26T18:34:00.766446: step 25913, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:34:00.938985: step 25914, loss 9.86522e-05, acc 1\n",
      "2018-10-26T18:34:01.118506: step 25915, loss 0.330458, acc 0.984375\n",
      "2018-10-26T18:34:01.290048: step 25916, loss 6.75994e-06, acc 1\n",
      "2018-10-26T18:34:01.476549: step 25917, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:34:01.665046: step 25918, loss 9.86518e-06, acc 1\n",
      "2018-10-26T18:34:01.835590: step 25919, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:34:02.015111: step 25920, loss 6.93495e-06, acc 1\n",
      "2018-10-26T18:34:02.181665: step 25921, loss 1.9147e-06, acc 1\n",
      "2018-10-26T18:34:02.362183: step 25922, loss 1.0997e-05, acc 1\n",
      "2018-10-26T18:34:02.537714: step 25923, loss 2.00945e-05, acc 1\n",
      "2018-10-26T18:34:02.717234: step 25924, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:34:02.892766: step 25925, loss 2.45668e-06, acc 1\n",
      "2018-10-26T18:34:03.078269: step 25926, loss 0.00516681, acc 1\n",
      "2018-10-26T18:34:03.252803: step 25927, loss 1.80111e-06, acc 1\n",
      "2018-10-26T18:34:03.440331: step 25928, loss 7.65537e-07, acc 1\n",
      "2018-10-26T18:34:03.612841: step 25929, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:03.797348: step 25930, loss 2.8645e-06, acc 1\n",
      "2018-10-26T18:34:03.970885: step 25931, loss 1.28332e-06, acc 1\n",
      "2018-10-26T18:34:04.151402: step 25932, loss 0.0175939, acc 0.984375\n",
      "2018-10-26T18:34:04.324939: step 25933, loss 0.000107531, acc 1\n",
      "2018-10-26T18:34:04.511456: step 25934, loss 4.76831e-07, acc 1\n",
      "2018-10-26T18:34:04.681985: step 25935, loss 0.00596182, acc 1\n",
      "2018-10-26T18:34:04.864497: step 25936, loss 9.076e-06, acc 1\n",
      "2018-10-26T18:34:05.032050: step 25937, loss 0.00012285, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:34:05.216557: step 25938, loss 0.000171025, acc 1\n",
      "2018-10-26T18:34:05.385106: step 25939, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:05.568616: step 25940, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:34:05.736168: step 25941, loss 9.74133e-07, acc 1\n",
      "2018-10-26T18:34:05.911699: step 25942, loss 1.93714e-07, acc 1\n",
      "2018-10-26T18:34:06.077257: step 25943, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:06.255780: step 25944, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:34:06.422335: step 25945, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:34:06.603850: step 25946, loss 4.43434e-06, acc 1\n",
      "2018-10-26T18:34:06.777386: step 25947, loss 0, acc 1\n",
      "2018-10-26T18:34:06.949925: step 25948, loss 0.024021, acc 0.984375\n",
      "2018-10-26T18:34:07.118474: step 25949, loss 0, acc 1\n",
      "2018-10-26T18:34:07.286027: step 25950, loss 0, acc 1\n",
      "2018-10-26T18:34:07.464550: step 25951, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:07.650054: step 25952, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:34:07.835559: step 25953, loss 1.35068e-05, acc 1\n",
      "2018-10-26T18:34:08.007100: step 25954, loss 6.53775e-07, acc 1\n",
      "2018-10-26T18:34:08.186620: step 25955, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:34:08.357165: step 25956, loss 1.75332e-05, acc 1\n",
      "2018-10-26T18:34:08.532695: step 25957, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:08.699251: step 25958, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:34:08.876776: step 25959, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:34:09.049316: step 25960, loss 7.70327e-05, acc 1\n",
      "2018-10-26T18:34:09.224847: step 25961, loss 0, acc 1\n",
      "2018-10-26T18:34:09.398383: step 25962, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:34:09.569925: step 25963, loss 2.81809e-05, acc 1\n",
      "2018-10-26T18:34:09.739472: step 25964, loss 0, acc 1\n",
      "2018-10-26T18:34:09.912011: step 25965, loss 3.87564e-05, acc 1\n",
      "2018-10-26T18:34:10.094523: step 25966, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:10.269057: step 25967, loss 3.45715e-05, acc 1\n",
      "2018-10-26T18:34:10.450573: step 25968, loss 0.000104383, acc 1\n",
      "2018-10-26T18:34:10.615132: step 25969, loss 0, acc 1\n",
      "2018-10-26T18:34:10.792658: step 25970, loss 1.0058e-06, acc 1\n",
      "2018-10-26T18:34:10.963202: step 25971, loss 5.94173e-07, acc 1\n",
      "2018-10-26T18:34:11.140728: step 25972, loss 0.00653243, acc 1\n",
      "2018-10-26T18:34:11.311272: step 25973, loss 0, acc 1\n",
      "2018-10-26T18:34:11.487800: step 25974, loss 1.17529e-06, acc 1\n",
      "2018-10-26T18:34:11.662335: step 25975, loss 0.0750383, acc 0.984375\n",
      "2018-10-26T18:34:11.839860: step 25976, loss 8.67969e-07, acc 1\n",
      "2018-10-26T18:34:12.013397: step 25977, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:34:12.186932: step 25978, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:34:12.361466: step 25979, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:34:12.536001: step 25980, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:34:12.707542: step 25981, loss 1.3007e-05, acc 1\n",
      "2018-10-26T18:34:12.886064: step 25982, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:34:13.061595: step 25983, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:34:13.242114: step 25984, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:34:13.411660: step 25985, loss 6.34495e-06, acc 1\n",
      "2018-10-26T18:34:13.587191: step 25986, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:34:13.757736: step 25987, loss 0, acc 1\n",
      "2018-10-26T18:34:13.931271: step 25988, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:34:14.106803: step 25989, loss 1.47136e-05, acc 1\n",
      "2018-10-26T18:34:14.285326: step 25990, loss 8.41916e-06, acc 1\n",
      "2018-10-26T18:34:14.453876: step 25991, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:34:14.628415: step 25992, loss 1.06353e-06, acc 1\n",
      "2018-10-26T18:34:14.792970: step 25993, loss 0, acc 1\n",
      "2018-10-26T18:34:14.974484: step 25994, loss 1.91852e-07, acc 1\n",
      "2018-10-26T18:34:15.140042: step 25995, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:15.315573: step 25996, loss 0.00064027, acc 1\n",
      "2018-10-26T18:34:15.494096: step 25997, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:34:15.671622: step 25998, loss 1.06914e-06, acc 1\n",
      "2018-10-26T18:34:15.855133: step 25999, loss 0, acc 1\n",
      "2018-10-26T18:34:16.026673: step 26000, loss 1.86264e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:34:16.484449: step 26000, loss 8.99985, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26000\n",
      "\n",
      "2018-10-26T18:34:16.844565: step 26001, loss 7.68748e-06, acc 1\n",
      "2018-10-26T18:34:17.014112: step 26002, loss 5.55903e-06, acc 1\n",
      "2018-10-26T18:34:17.187648: step 26003, loss 3.39896e-06, acc 1\n",
      "2018-10-26T18:34:17.358193: step 26004, loss 3.81626e-06, acc 1\n",
      "2018-10-26T18:34:17.562646: step 26005, loss 4.91732e-07, acc 1\n",
      "2018-10-26T18:34:17.798017: step 26006, loss 4.61931e-07, acc 1\n",
      "2018-10-26T18:34:17.969559: step 26007, loss 1.91852e-07, acc 1\n",
      "2018-10-26T18:34:18.149080: step 26008, loss 1.67637e-07, acc 1\n",
      "2018-10-26T18:34:18.318627: step 26009, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:34:18.498147: step 26010, loss 0, acc 1\n",
      "2018-10-26T18:34:18.674675: step 26011, loss 2.0937e-05, acc 1\n",
      "2018-10-26T18:34:18.848211: step 26012, loss 2.33193e-06, acc 1\n",
      "2018-10-26T18:34:19.020751: step 26013, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:34:19.202265: step 26014, loss 1.29327e-05, acc 1\n",
      "2018-10-26T18:34:19.368821: step 26015, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:34:19.550335: step 26016, loss 5.01044e-07, acc 1\n",
      "2018-10-26T18:34:19.719882: step 26017, loss 0, acc 1\n",
      "2018-10-26T18:34:19.897409: step 26018, loss 0, acc 1\n",
      "2018-10-26T18:34:20.069947: step 26019, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:34:20.258443: step 26020, loss 1.18554e-05, acc 1\n",
      "2018-10-26T18:34:20.426993: step 26021, loss 8.67981e-05, acc 1\n",
      "2018-10-26T18:34:20.603521: step 26022, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:34:20.779052: step 26023, loss 9.68563e-07, acc 1\n",
      "2018-10-26T18:34:20.957575: step 26024, loss 1.26841e-06, acc 1\n",
      "2018-10-26T18:34:21.125128: step 26025, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:34:21.304649: step 26026, loss 2.38296e-05, acc 1\n",
      "2018-10-26T18:34:21.471203: step 26027, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:34:21.648728: step 26028, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:34:21.823263: step 26029, loss 0.0200839, acc 0.984375\n",
      "2018-10-26T18:34:22.012758: step 26030, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:34:22.191279: step 26031, loss 5.15367e-05, acc 1\n",
      "2018-10-26T18:34:22.370800: step 26032, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:34:22.537355: step 26033, loss 0, acc 1\n",
      "2018-10-26T18:34:22.720865: step 26034, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:22.891408: step 26035, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:34:23.068935: step 26036, loss 6.91261e-06, acc 1\n",
      "2018-10-26T18:34:23.236486: step 26037, loss 0.000173176, acc 1\n",
      "2018-10-26T18:34:23.417004: step 26038, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:34:23.585554: step 26039, loss 0, acc 1\n",
      "2018-10-26T18:34:23.767069: step 26040, loss 1.72658e-06, acc 1\n",
      "2018-10-26T18:34:23.938611: step 26041, loss 2.86846e-07, acc 1\n",
      "2018-10-26T18:34:24.120125: step 26042, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:34:24.288676: step 26043, loss 2.27279e-05, acc 1\n",
      "2018-10-26T18:34:24.470190: step 26044, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:24.643726: step 26045, loss 0.13816, acc 0.984375\n",
      "2018-10-26T18:34:24.831226: step 26046, loss 7.11514e-07, acc 1\n",
      "2018-10-26T18:34:24.999775: step 26047, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:25.178298: step 26048, loss 3.54622e-06, acc 1\n",
      "2018-10-26T18:34:25.344853: step 26049, loss 0.000506617, acc 1\n",
      "2018-10-26T18:34:25.532353: step 26050, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:34:25.699905: step 26051, loss 0, acc 1\n",
      "2018-10-26T18:34:25.888401: step 26052, loss 3.10659e-06, acc 1\n",
      "2018-10-26T18:34:26.058946: step 26053, loss 1.73495e-05, acc 1\n",
      "2018-10-26T18:34:26.238465: step 26054, loss 1.71362e-07, acc 1\n",
      "2018-10-26T18:34:26.409010: step 26055, loss 0.000331137, acc 1\n",
      "2018-10-26T18:34:26.586536: step 26056, loss 5.0302e-06, acc 1\n",
      "2018-10-26T18:34:26.761069: step 26057, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:34:26.954554: step 26058, loss 8.25131e-07, acc 1\n",
      "2018-10-26T18:34:27.140057: step 26059, loss 0, acc 1\n",
      "2018-10-26T18:34:27.323566: step 26060, loss 0.000114509, acc 1\n",
      "2018-10-26T18:34:27.502089: step 26061, loss 0.00386827, acc 1\n",
      "2018-10-26T18:34:27.686597: step 26062, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:34:27.856144: step 26063, loss 3.81178e-05, acc 1\n",
      "2018-10-26T18:34:28.038656: step 26064, loss 0, acc 1\n",
      "2018-10-26T18:34:28.209201: step 26065, loss 3.62255e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:34:28.392709: step 26066, loss 1.37085e-06, acc 1\n",
      "2018-10-26T18:34:28.566246: step 26067, loss 1.43792e-06, acc 1\n",
      "2018-10-26T18:34:28.737788: step 26068, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:34:28.907334: step 26069, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:34:29.086856: step 26070, loss 0.000102641, acc 1\n",
      "2018-10-26T18:34:29.260391: step 26071, loss 0.0142808, acc 0.984375\n",
      "2018-10-26T18:34:29.438914: step 26072, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:34:29.617437: step 26073, loss 3.66346e-06, acc 1\n",
      "2018-10-26T18:34:29.786985: step 26074, loss 0, acc 1\n",
      "2018-10-26T18:34:29.961518: step 26075, loss 0.00570195, acc 1\n",
      "2018-10-26T18:34:30.130068: step 26076, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:34:30.306597: step 26077, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:30.475145: step 26078, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:30.651674: step 26079, loss 6.56075e-06, acc 1\n",
      "2018-10-26T18:34:30.823216: step 26080, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:34:30.999744: step 26081, loss 0.00366742, acc 1\n",
      "2018-10-26T18:34:31.174279: step 26082, loss 0, acc 1\n",
      "2018-10-26T18:34:31.357787: step 26083, loss 0, acc 1\n",
      "2018-10-26T18:34:31.529328: step 26084, loss 0.000240407, acc 1\n",
      "2018-10-26T18:34:31.703862: step 26085, loss 0, acc 1\n",
      "2018-10-26T18:34:31.874407: step 26086, loss 1.0505e-06, acc 1\n",
      "2018-10-26T18:34:32.050935: step 26087, loss 6.86432e-06, acc 1\n",
      "2018-10-26T18:34:32.223475: step 26088, loss 0.462784, acc 0.984375\n",
      "2018-10-26T18:34:32.399006: step 26089, loss 0, acc 1\n",
      "2018-10-26T18:34:32.567555: step 26090, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:34:32.743086: step 26091, loss 0, acc 1\n",
      "2018-10-26T18:34:32.915625: step 26092, loss 2.70685e-05, acc 1\n",
      "2018-10-26T18:34:33.088165: step 26093, loss 1.10078e-05, acc 1\n",
      "2018-10-26T18:34:33.258709: step 26094, loss 8.24215e-06, acc 1\n",
      "2018-10-26T18:34:33.444213: step 26095, loss 8.3119e-05, acc 1\n",
      "2018-10-26T18:34:33.614758: step 26096, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:34:33.792283: step 26097, loss 2.59605e-05, acc 1\n",
      "2018-10-26T18:34:33.967814: step 26098, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:34:34.145340: step 26099, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:34:34.304914: step 26100, loss 1.33117e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:34:34.760696: step 26100, loss 8.90366, acc 0.727017\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26100\n",
      "\n",
      "2018-10-26T18:34:35.154561: step 26101, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:34:35.339069: step 26102, loss 0, acc 1\n",
      "2018-10-26T18:34:35.505622: step 26103, loss 7.10602e-05, acc 1\n",
      "2018-10-26T18:34:35.680155: step 26104, loss 1.88126e-07, acc 1\n",
      "2018-10-26T18:34:35.895581: step 26105, loss 1.86507e-05, acc 1\n",
      "2018-10-26T18:34:36.106018: step 26106, loss 0, acc 1\n",
      "2018-10-26T18:34:36.286536: step 26107, loss 8.25917e-06, acc 1\n",
      "2018-10-26T18:34:36.471043: step 26108, loss 0.00252514, acc 1\n",
      "2018-10-26T18:34:36.639592: step 26109, loss 0.00141222, acc 1\n",
      "2018-10-26T18:34:36.816121: step 26110, loss 3.4086e-07, acc 1\n",
      "2018-10-26T18:34:36.994643: step 26111, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:37.171172: step 26112, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:34:37.345706: step 26113, loss 1.04677e-06, acc 1\n",
      "2018-10-26T18:34:37.514255: step 26114, loss 7.44268e-05, acc 1\n",
      "2018-10-26T18:34:37.695770: step 26115, loss 1.11905e-05, acc 1\n",
      "2018-10-26T18:34:37.861327: step 26116, loss 1.1399e-06, acc 1\n",
      "2018-10-26T18:34:38.041846: step 26117, loss 6.72458e-06, acc 1\n",
      "2018-10-26T18:34:38.208406: step 26118, loss 0.0026505, acc 1\n",
      "2018-10-26T18:34:38.381936: step 26119, loss 6.66813e-07, acc 1\n",
      "2018-10-26T18:34:38.554476: step 26120, loss 0.000322482, acc 1\n",
      "2018-10-26T18:34:38.736989: step 26121, loss 2.41566e-06, acc 1\n",
      "2018-10-26T18:34:38.909527: step 26122, loss 0.00143819, acc 1\n",
      "2018-10-26T18:34:39.089048: step 26123, loss 4.82418e-07, acc 1\n",
      "2018-10-26T18:34:39.258595: step 26124, loss 5.33408e-06, acc 1\n",
      "2018-10-26T18:34:39.433128: step 26125, loss 1.62049e-07, acc 1\n",
      "2018-10-26T18:34:39.620627: step 26126, loss 0.100914, acc 0.984375\n",
      "2018-10-26T18:34:39.821093: step 26127, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:34:40.049482: step 26128, loss 3.28722e-06, acc 1\n",
      "2018-10-26T18:34:40.240969: step 26129, loss 0, acc 1\n",
      "2018-10-26T18:34:40.434453: step 26130, loss 1.26471e-06, acc 1\n",
      "2018-10-26T18:34:40.633921: step 26131, loss 0.00138378, acc 1\n",
      "2018-10-26T18:34:40.840369: step 26132, loss 5.23561e-06, acc 1\n",
      "2018-10-26T18:34:41.016897: step 26133, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:34:41.248278: step 26134, loss 1.16041e-06, acc 1\n",
      "2018-10-26T18:34:41.415831: step 26135, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:34:41.590364: step 26136, loss 8.3258e-07, acc 1\n",
      "2018-10-26T18:34:41.785842: step 26137, loss 4.17185e-06, acc 1\n",
      "2018-10-26T18:34:41.972344: step 26138, loss 5.25408e-06, acc 1\n",
      "2018-10-26T18:34:42.147875: step 26139, loss 5.19992e-06, acc 1\n",
      "2018-10-26T18:34:42.326398: step 26140, loss 1.2672e-05, acc 1\n",
      "2018-10-26T18:34:42.532847: step 26141, loss 3.32464e-06, acc 1\n",
      "2018-10-26T18:34:42.731316: step 26142, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:34:42.918815: step 26143, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:34:43.110303: step 26144, loss 1.67459e-05, acc 1\n",
      "2018-10-26T18:34:43.331712: step 26145, loss 2.2892e-05, acc 1\n",
      "2018-10-26T18:34:43.526192: step 26146, loss 5.48268e-06, acc 1\n",
      "2018-10-26T18:34:43.723665: step 26147, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:34:43.951058: step 26148, loss 7.82584e-06, acc 1\n",
      "2018-10-26T18:34:44.148530: step 26149, loss 7.19783e-06, acc 1\n",
      "2018-10-26T18:34:44.373927: step 26150, loss 2.66356e-07, acc 1\n",
      "2018-10-26T18:34:44.585362: step 26151, loss 0, acc 1\n",
      "2018-10-26T18:34:44.797794: step 26152, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:34:44.975320: step 26153, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:45.150851: step 26154, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:45.362286: step 26155, loss 1.79181e-06, acc 1\n",
      "2018-10-26T18:34:45.546793: step 26156, loss 0.000167087, acc 1\n",
      "2018-10-26T18:34:45.758229: step 26157, loss 0.000487532, acc 1\n",
      "2018-10-26T18:34:45.995595: step 26158, loss 0.000134264, acc 1\n",
      "2018-10-26T18:34:46.174118: step 26159, loss 7.79246e-06, acc 1\n",
      "2018-10-26T18:34:46.397520: step 26160, loss 1.9744e-07, acc 1\n",
      "2018-10-26T18:34:46.593995: step 26161, loss 1.08076e-05, acc 1\n",
      "2018-10-26T18:34:46.828370: step 26162, loss 0, acc 1\n",
      "2018-10-26T18:34:46.996919: step 26163, loss 0, acc 1\n",
      "2018-10-26T18:34:47.199377: step 26164, loss 7.63684e-08, acc 1\n",
      "2018-10-26T18:34:47.433753: step 26165, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:34:47.601305: step 26166, loss 9.44333e-07, acc 1\n",
      "2018-10-26T18:34:47.804760: step 26167, loss 2.38386e-05, acc 1\n",
      "2018-10-26T18:34:47.987273: step 26168, loss 3.97091e-05, acc 1\n",
      "2018-10-26T18:34:48.166794: step 26169, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:34:48.353295: step 26170, loss 2.51809e-06, acc 1\n",
      "2018-10-26T18:34:48.616137: step 26171, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:34:48.793663: step 26172, loss 9.05221e-07, acc 1\n",
      "2018-10-26T18:34:48.973184: step 26173, loss 1.03746e-06, acc 1\n",
      "2018-10-26T18:34:49.171652: step 26174, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:34:49.350176: step 26175, loss 1.88126e-07, acc 1\n",
      "2018-10-26T18:34:49.529697: step 26176, loss 0.000112708, acc 1\n",
      "2018-10-26T18:34:49.707223: step 26177, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:49.918657: step 26178, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:34:50.123111: step 26179, loss 0.0279677, acc 0.984375\n",
      "2018-10-26T18:34:50.311607: step 26180, loss 6.27701e-07, acc 1\n",
      "2018-10-26T18:34:50.503095: step 26181, loss 6.98483e-07, acc 1\n",
      "2018-10-26T18:34:50.722509: step 26182, loss 7.31291e-06, acc 1\n",
      "2018-10-26T18:34:50.922973: step 26183, loss 2.88708e-07, acc 1\n",
      "2018-10-26T18:34:51.109475: step 26184, loss 5.52396e-05, acc 1\n",
      "2018-10-26T18:34:51.340857: step 26185, loss 0, acc 1\n",
      "2018-10-26T18:34:51.510424: step 26186, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:34:51.727823: step 26187, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:34:51.898368: step 26188, loss 7.45056e-08, acc 1\n",
      "2018-10-26T18:34:52.104815: step 26189, loss 0, acc 1\n",
      "2018-10-26T18:34:52.283340: step 26190, loss 1.37086e-06, acc 1\n",
      "2018-10-26T18:34:52.464855: step 26191, loss 0, acc 1\n",
      "2018-10-26T18:34:52.678284: step 26192, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:34:52.856806: step 26193, loss 0, acc 1\n",
      "2018-10-26T18:34:53.043308: step 26194, loss 1.42489e-06, acc 1\n",
      "2018-10-26T18:34:53.269704: step 26195, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:34:53.441245: step 26196, loss 0, acc 1\n",
      "2018-10-26T18:34:53.618771: step 26197, loss 1.47149e-07, acc 1\n",
      "2018-10-26T18:34:53.786323: step 26198, loss 6.31496e-06, acc 1\n",
      "2018-10-26T18:34:53.998755: step 26199, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:34:54.167305: step 26200, loss 4.34771e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:34:54.651013: step 26200, loss 8.97559, acc 0.724203\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26200\n",
      "\n",
      "2018-10-26T18:34:55.033544: step 26201, loss 0.000103061, acc 1\n",
      "2018-10-26T18:34:55.212067: step 26202, loss 3.84598e-06, acc 1\n",
      "2018-10-26T18:34:55.424499: step 26203, loss 0.074588, acc 0.984375\n",
      "2018-10-26T18:34:55.590057: step 26204, loss 4.74133e-05, acc 1\n",
      "2018-10-26T18:34:55.837395: step 26205, loss 1.03187e-06, acc 1\n",
      "2018-10-26T18:34:56.025892: step 26206, loss 0, acc 1\n",
      "2018-10-26T18:34:56.204417: step 26207, loss 0.000390102, acc 1\n",
      "2018-10-26T18:34:56.390917: step 26208, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:56.565451: step 26209, loss 0, acc 1\n",
      "2018-10-26T18:34:56.737989: step 26210, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:34:56.911526: step 26211, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:34:57.089052: step 26212, loss 1.39715e-05, acc 1\n",
      "2018-10-26T18:34:57.255606: step 26213, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:34:57.430141: step 26214, loss 3.16647e-07, acc 1\n",
      "2018-10-26T18:34:57.597692: step 26215, loss 4.80116e-06, acc 1\n",
      "2018-10-26T18:34:57.772226: step 26216, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:34:57.938782: step 26217, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:58.120298: step 26218, loss 3.48845e-06, acc 1\n",
      "2018-10-26T18:34:58.294830: step 26219, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:34:58.472356: step 26220, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:34:58.637913: step 26221, loss 2.85395e-05, acc 1\n",
      "2018-10-26T18:34:58.816437: step 26222, loss 4.84276e-05, acc 1\n",
      "2018-10-26T18:34:58.994960: step 26223, loss 7.32042e-06, acc 1\n",
      "2018-10-26T18:34:59.169492: step 26224, loss 0.000485217, acc 1\n",
      "2018-10-26T18:34:59.345024: step 26225, loss 0.00214515, acc 1\n",
      "2018-10-26T18:34:59.516565: step 26226, loss 1.47887e-06, acc 1\n",
      "2018-10-26T18:34:59.688107: step 26227, loss 3.09196e-07, acc 1\n",
      "2018-10-26T18:34:59.853665: step 26228, loss 0, acc 1\n",
      "2018-10-26T18:35:00.031191: step 26229, loss 0.000965196, acc 1\n",
      "2018-10-26T18:35:00.200738: step 26230, loss 4.41215e-05, acc 1\n",
      "2018-10-26T18:35:00.377267: step 26231, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:35:00.549805: step 26232, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:35:00.723341: step 26233, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:35:00.894883: step 26234, loss 2.36554e-07, acc 1\n",
      "2018-10-26T18:35:01.070414: step 26235, loss 0, acc 1\n",
      "2018-10-26T18:35:01.241956: step 26236, loss 5.82936e-05, acc 1\n",
      "2018-10-26T18:35:01.420479: step 26237, loss 0.00134132, acc 1\n",
      "2018-10-26T18:35:01.590026: step 26238, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:35:01.763562: step 26239, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:35:01.929120: step 26240, loss 2.77533e-07, acc 1\n",
      "2018-10-26T18:35:02.103653: step 26241, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:35:02.270208: step 26242, loss 0, acc 1\n",
      "2018-10-26T18:35:02.452721: step 26243, loss 0, acc 1\n",
      "2018-10-26T18:35:02.619276: step 26244, loss 2.01164e-07, acc 1\n",
      "2018-10-26T18:35:02.795804: step 26245, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:35:02.964354: step 26246, loss 9.87199e-08, acc 1\n",
      "2018-10-26T18:35:03.145869: step 26247, loss 2.27241e-07, acc 1\n",
      "2018-10-26T18:35:03.323396: step 26248, loss 0.000468809, acc 1\n",
      "2018-10-26T18:35:03.504909: step 26249, loss 0.00442567, acc 1\n",
      "2018-10-26T18:35:03.675454: step 26250, loss 3.61599e-07, acc 1\n",
      "2018-10-26T18:35:03.845001: step 26251, loss 0.000399078, acc 1\n",
      "2018-10-26T18:35:04.024521: step 26252, loss 2.10651e-06, acc 1\n",
      "2018-10-26T18:35:04.199055: step 26253, loss 1.52886e-05, acc 1\n",
      "2018-10-26T18:35:04.379573: step 26254, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:35:04.559093: step 26255, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:35:04.742603: step 26256, loss 7.78566e-07, acc 1\n",
      "2018-10-26T18:35:04.921125: step 26257, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:35:05.102642: step 26258, loss 3.07333e-07, acc 1\n",
      "2018-10-26T18:35:05.285153: step 26259, loss 6.46452e-05, acc 1\n",
      "2018-10-26T18:35:05.461681: step 26260, loss 2.08416e-06, acc 1\n",
      "2018-10-26T18:35:05.631228: step 26261, loss 0, acc 1\n",
      "2018-10-26T18:35:05.805761: step 26262, loss 4.20954e-07, acc 1\n",
      "2018-10-26T18:35:05.974312: step 26263, loss 2.07485e-06, acc 1\n",
      "2018-10-26T18:35:06.153831: step 26264, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:35:06.322382: step 26265, loss 0.000788693, acc 1\n",
      "2018-10-26T18:35:06.499907: step 26266, loss 7.72187e-05, acc 1\n",
      "2018-10-26T18:35:06.670452: step 26267, loss 0.01329, acc 0.984375\n",
      "2018-10-26T18:35:06.848975: step 26268, loss 8.09296e-06, acc 1\n",
      "2018-10-26T18:35:07.013535: step 26269, loss 2.5252e-05, acc 1\n",
      "2018-10-26T18:35:07.197044: step 26270, loss 6.96616e-07, acc 1\n",
      "2018-10-26T18:35:07.370581: step 26271, loss 0.00077805, acc 1\n",
      "2018-10-26T18:35:07.554091: step 26272, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:35:07.724634: step 26273, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:35:07.905153: step 26274, loss 3.55762e-07, acc 1\n",
      "2018-10-26T18:35:08.078689: step 26275, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:35:08.259207: step 26276, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:35:08.426759: step 26277, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:35:08.610269: step 26278, loss 7.85075e-05, acc 1\n",
      "2018-10-26T18:35:08.783805: step 26279, loss 3.29157e-05, acc 1\n",
      "2018-10-26T18:35:08.967314: step 26280, loss 0, acc 1\n",
      "2018-10-26T18:35:09.138856: step 26281, loss 2.15121e-06, acc 1\n",
      "2018-10-26T18:35:09.316382: step 26282, loss 1.322e-05, acc 1\n",
      "2018-10-26T18:35:09.483934: step 26283, loss 2.35094e-05, acc 1\n",
      "2018-10-26T18:35:09.665469: step 26284, loss 5.77501e-05, acc 1\n",
      "2018-10-26T18:35:09.836991: step 26285, loss 0.000277807, acc 1\n",
      "2018-10-26T18:35:10.019526: step 26286, loss 9.4248e-07, acc 1\n",
      "2018-10-26T18:35:10.186059: step 26287, loss 2.29701e-05, acc 1\n",
      "2018-10-26T18:35:10.363584: step 26288, loss 3.89719e-05, acc 1\n",
      "2018-10-26T18:35:10.532135: step 26289, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:35:10.710657: step 26290, loss 2.32829e-07, acc 1\n",
      "2018-10-26T18:35:10.879206: step 26291, loss 0.000307128, acc 1\n",
      "2018-10-26T18:35:11.070694: step 26292, loss 6.15352e-06, acc 1\n",
      "2018-10-26T18:35:11.246226: step 26293, loss 5.32816e-06, acc 1\n",
      "2018-10-26T18:35:11.424749: step 26294, loss 2.55181e-07, acc 1\n",
      "2018-10-26T18:35:11.605266: step 26295, loss 2.65307e-05, acc 1\n",
      "2018-10-26T18:35:11.777805: step 26296, loss 3.12921e-07, acc 1\n",
      "2018-10-26T18:35:11.963310: step 26297, loss 0, acc 1\n",
      "2018-10-26T18:35:12.131860: step 26298, loss 5.05037e-05, acc 1\n",
      "2018-10-26T18:35:12.309385: step 26299, loss 6.6409e-06, acc 1\n",
      "2018-10-26T18:35:12.480927: step 26300, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:35:12.943690: step 26300, loss 9.06301, acc 0.722326\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26300\n",
      "\n",
      "2018-10-26T18:35:13.322919: step 26301, loss 0.000409896, acc 1\n",
      "2018-10-26T18:35:13.488476: step 26302, loss 1.12313e-06, acc 1\n",
      "2018-10-26T18:35:13.666003: step 26303, loss 7.09652e-07, acc 1\n",
      "2018-10-26T18:35:13.835550: step 26304, loss 0.0918893, acc 0.96875\n",
      "2018-10-26T18:35:14.036014: step 26305, loss 2.34489e-06, acc 1\n",
      "2018-10-26T18:35:14.282358: step 26306, loss 3.00682e-05, acc 1\n",
      "2018-10-26T18:35:14.452900: step 26307, loss 7.46928e-06, acc 1\n",
      "2018-10-26T18:35:14.633417: step 26308, loss 1.05609e-06, acc 1\n",
      "2018-10-26T18:35:14.807952: step 26309, loss 0, acc 1\n",
      "2018-10-26T18:35:14.994454: step 26310, loss 5.42494e-06, acc 1\n",
      "2018-10-26T18:35:15.185942: step 26311, loss 0, acc 1\n",
      "2018-10-26T18:35:15.374438: step 26312, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:35:15.560939: step 26313, loss 2.21654e-07, acc 1\n",
      "2018-10-26T18:35:15.738466: step 26314, loss 9.17828e-06, acc 1\n",
      "2018-10-26T18:35:15.926962: step 26315, loss 3.22234e-07, acc 1\n",
      "2018-10-26T18:35:16.095512: step 26316, loss 1.11759e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:35:16.275032: step 26317, loss 8.19188e-06, acc 1\n",
      "2018-10-26T18:35:16.442585: step 26318, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:35:16.620110: step 26319, loss 8.90078e-05, acc 1\n",
      "2018-10-26T18:35:16.796638: step 26320, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:35:16.969177: step 26321, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:35:17.138724: step 26322, loss 3.59488e-07, acc 1\n",
      "2018-10-26T18:35:17.314255: step 26323, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:35:17.490783: step 26324, loss 0.00204286, acc 1\n",
      "2018-10-26T18:35:17.663323: step 26325, loss 0, acc 1\n",
      "2018-10-26T18:35:17.831873: step 26326, loss 0, acc 1\n",
      "2018-10-26T18:35:18.007404: step 26327, loss 8.015e-06, acc 1\n",
      "2018-10-26T18:35:18.179942: step 26328, loss 2.90866e-05, acc 1\n",
      "2018-10-26T18:35:18.356470: step 26329, loss 3.5841e-05, acc 1\n",
      "2018-10-26T18:35:18.524023: step 26330, loss 2.59214e-05, acc 1\n",
      "2018-10-26T18:35:18.696562: step 26331, loss 3.13931e-05, acc 1\n",
      "2018-10-26T18:35:18.873090: step 26332, loss 9.53652e-07, acc 1\n",
      "2018-10-26T18:35:19.053607: step 26333, loss 0, acc 1\n",
      "2018-10-26T18:35:19.224152: step 26334, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:35:19.399683: step 26335, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:35:19.568233: step 26336, loss 2.52556e-06, acc 1\n",
      "2018-10-26T18:35:19.741769: step 26337, loss 1.15666e-06, acc 1\n",
      "2018-10-26T18:35:19.913311: step 26338, loss 1.24355e-05, acc 1\n",
      "2018-10-26T18:35:20.093828: step 26339, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:35:20.261381: step 26340, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:35:20.449877: step 26341, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:35:20.615435: step 26342, loss 2.97252e-06, acc 1\n",
      "2018-10-26T18:35:20.793958: step 26343, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:35:20.963517: step 26344, loss 2.16987e-06, acc 1\n",
      "2018-10-26T18:35:21.141031: step 26345, loss 0, acc 1\n",
      "2018-10-26T18:35:21.310578: step 26346, loss 4.97598e-05, acc 1\n",
      "2018-10-26T18:35:21.496082: step 26347, loss 4.43305e-07, acc 1\n",
      "2018-10-26T18:35:21.665629: step 26348, loss 5.6996e-07, acc 1\n",
      "2018-10-26T18:35:21.840162: step 26349, loss 0, acc 1\n",
      "2018-10-26T18:35:22.012702: step 26350, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:35:22.188233: step 26351, loss 1.85509e-06, acc 1\n",
      "2018-10-26T18:35:22.356782: step 26352, loss 2.22688e-05, acc 1\n",
      "2018-10-26T18:35:22.538298: step 26353, loss 0, acc 1\n",
      "2018-10-26T18:35:22.707844: step 26354, loss 3.75651e-06, acc 1\n",
      "2018-10-26T18:35:22.888361: step 26355, loss 7.86686e-05, acc 1\n",
      "2018-10-26T18:35:23.060901: step 26356, loss 0.00706896, acc 1\n",
      "2018-10-26T18:35:23.237429: step 26357, loss 2.83099e-06, acc 1\n",
      "2018-10-26T18:35:23.412960: step 26358, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:35:23.594491: step 26359, loss 1.1604e-06, acc 1\n",
      "2018-10-26T18:35:23.763024: step 26360, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:35:23.932572: step 26361, loss 1.40439e-06, acc 1\n",
      "2018-10-26T18:35:24.109100: step 26362, loss 0, acc 1\n",
      "2018-10-26T18:35:24.289618: step 26363, loss 0, acc 1\n",
      "2018-10-26T18:35:24.460163: step 26364, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:35:24.640680: step 26365, loss 7.39453e-07, acc 1\n",
      "2018-10-26T18:35:24.823193: step 26366, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:35:24.996729: step 26367, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:35:25.175252: step 26368, loss 2.36554e-07, acc 1\n",
      "2018-10-26T18:35:25.341807: step 26369, loss 1.38947e-06, acc 1\n",
      "2018-10-26T18:35:25.517338: step 26370, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:35:25.684889: step 26371, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:35:25.866405: step 26372, loss 0.0938649, acc 0.984375\n",
      "2018-10-26T18:35:26.043932: step 26373, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:35:26.222453: step 26374, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:35:26.395990: step 26375, loss 1.21999e-06, acc 1\n",
      "2018-10-26T18:35:26.568530: step 26376, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:35:26.746055: step 26377, loss 0, acc 1\n",
      "2018-10-26T18:35:26.912610: step 26378, loss 4.99668e-06, acc 1\n",
      "2018-10-26T18:35:27.089138: step 26379, loss 2.71945e-07, acc 1\n",
      "2018-10-26T18:35:27.261677: step 26380, loss 6.02636e-06, acc 1\n",
      "2018-10-26T18:35:27.439203: step 26381, loss 7.7339e-05, acc 1\n",
      "2018-10-26T18:35:27.609747: step 26382, loss 3.55762e-07, acc 1\n",
      "2018-10-26T18:35:27.789267: step 26383, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:35:27.958814: step 26384, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:35:28.134345: step 26385, loss 5.30847e-07, acc 1\n",
      "2018-10-26T18:35:28.302895: step 26386, loss 0.00022939, acc 1\n",
      "2018-10-26T18:35:28.482416: step 26387, loss 3.101e-06, acc 1\n",
      "2018-10-26T18:35:28.648970: step 26388, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:35:28.832480: step 26389, loss 0.000133191, acc 1\n",
      "2018-10-26T18:35:29.002027: step 26390, loss 0.116477, acc 0.984375\n",
      "2018-10-26T18:35:29.181548: step 26391, loss 0.000138542, acc 1\n",
      "2018-10-26T18:35:29.352092: step 26392, loss 9.08955e-07, acc 1\n",
      "2018-10-26T18:35:29.526625: step 26393, loss 1.91473e-06, acc 1\n",
      "2018-10-26T18:35:29.695175: step 26394, loss 2.41947e-06, acc 1\n",
      "2018-10-26T18:35:29.879682: step 26395, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:35:30.058206: step 26396, loss 0, acc 1\n",
      "2018-10-26T18:35:30.233735: step 26397, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:35:30.417247: step 26398, loss 1.65212e-06, acc 1\n",
      "2018-10-26T18:35:30.583801: step 26399, loss 8.54931e-07, acc 1\n",
      "2018-10-26T18:35:30.763321: step 26400, loss 0.0111542, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:35:31.220100: step 26400, loss 9.36045, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26400\n",
      "\n",
      "2018-10-26T18:35:31.570913: step 26401, loss 0, acc 1\n",
      "2018-10-26T18:35:31.742467: step 26402, loss 0.0014934, acc 1\n",
      "2018-10-26T18:35:31.922973: step 26403, loss 4.8547e-05, acc 1\n",
      "2018-10-26T18:35:32.092519: step 26404, loss 0.00390834, acc 1\n",
      "2018-10-26T18:35:32.281017: step 26405, loss 0.000230465, acc 1\n",
      "2018-10-26T18:35:32.537331: step 26406, loss 6.92889e-07, acc 1\n",
      "2018-10-26T18:35:32.707876: step 26407, loss 1.34896e-05, acc 1\n",
      "2018-10-26T18:35:32.894377: step 26408, loss 5.31874e-05, acc 1\n",
      "2018-10-26T18:35:33.067914: step 26409, loss 1.61115e-06, acc 1\n",
      "2018-10-26T18:35:33.247435: step 26410, loss 2.848e-05, acc 1\n",
      "2018-10-26T18:35:33.414987: step 26411, loss 0.000177306, acc 1\n",
      "2018-10-26T18:35:33.603483: step 26412, loss 1.1697e-06, acc 1\n",
      "2018-10-26T18:35:33.778016: step 26413, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:35:33.963521: step 26414, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:35:34.140050: step 26415, loss 1.37646e-06, acc 1\n",
      "2018-10-26T18:35:34.315580: step 26416, loss 1.87557e-05, acc 1\n",
      "2018-10-26T18:35:34.486124: step 26417, loss 4.80559e-07, acc 1\n",
      "2018-10-26T18:35:34.671629: step 26418, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:35:34.837186: step 26419, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:35:35.016707: step 26420, loss 2.80303e-06, acc 1\n",
      "2018-10-26T18:35:35.184260: step 26421, loss 1.23959e-05, acc 1\n",
      "2018-10-26T18:35:35.365774: step 26422, loss 3.62181e-05, acc 1\n",
      "2018-10-26T18:35:35.545294: step 26423, loss 0, acc 1\n",
      "2018-10-26T18:35:35.723818: step 26424, loss 0.0565723, acc 0.984375\n",
      "2018-10-26T18:35:35.916303: step 26425, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:35:36.088842: step 26426, loss 0.000209979, acc 1\n",
      "2018-10-26T18:35:36.270358: step 26427, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:35:36.438907: step 26428, loss 5.79548e-06, acc 1\n",
      "2018-10-26T18:35:36.629398: step 26429, loss 2.72641e-05, acc 1\n",
      "2018-10-26T18:35:36.795953: step 26430, loss 5.25259e-07, acc 1\n",
      "2018-10-26T18:35:36.975474: step 26431, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:35:37.143027: step 26432, loss 5.71826e-07, acc 1\n",
      "2018-10-26T18:35:37.328531: step 26433, loss 0.000130377, acc 1\n",
      "2018-10-26T18:35:37.498076: step 26434, loss 5.23398e-07, acc 1\n",
      "2018-10-26T18:35:37.679592: step 26435, loss 2.12517e-06, acc 1\n",
      "2018-10-26T18:35:37.852132: step 26436, loss 2.94453e-05, acc 1\n",
      "2018-10-26T18:35:38.037635: step 26437, loss 3.76582e-06, acc 1\n",
      "2018-10-26T18:35:38.205188: step 26438, loss 4.74971e-07, acc 1\n",
      "2018-10-26T18:35:38.386703: step 26439, loss 0, acc 1\n",
      "2018-10-26T18:35:38.554255: step 26440, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:35:38.740757: step 26441, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:35:38.914294: step 26442, loss 2.55181e-07, acc 1\n",
      "2018-10-26T18:35:39.099797: step 26443, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:35:39.275328: step 26444, loss 4.34338e-06, acc 1\n",
      "2018-10-26T18:35:39.453851: step 26445, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:35:39.643346: step 26446, loss 5.06631e-07, acc 1\n",
      "2018-10-26T18:35:39.815884: step 26447, loss 0, acc 1\n",
      "2018-10-26T18:35:40.000391: step 26448, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:35:40.169938: step 26449, loss 4.81828e-06, acc 1\n",
      "2018-10-26T18:35:40.343474: step 26450, loss 9.53047e-06, acc 1\n",
      "2018-10-26T18:35:40.513022: step 26451, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:35:40.692542: step 26452, loss 3.85176e-06, acc 1\n",
      "2018-10-26T18:35:40.861091: step 26453, loss 3.1665e-08, acc 1\n",
      "2018-10-26T18:35:41.037620: step 26454, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:35:41.214148: step 26455, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:35:41.392671: step 26456, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:35:41.563215: step 26457, loss 1.43977e-06, acc 1\n",
      "2018-10-26T18:35:41.735754: step 26458, loss 0.000474324, acc 1\n",
      "2018-10-26T18:35:41.919264: step 26459, loss 0, acc 1\n",
      "2018-10-26T18:35:42.084822: step 26460, loss 0, acc 1\n",
      "2018-10-26T18:35:42.261350: step 26461, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:35:42.434886: step 26462, loss 1.15688e-05, acc 1\n",
      "2018-10-26T18:35:42.619394: step 26463, loss 7.51265e-05, acc 1\n",
      "2018-10-26T18:35:42.786945: step 26464, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:35:42.965469: step 26465, loss 1.89393e-05, acc 1\n",
      "2018-10-26T18:35:43.131026: step 26466, loss 4.52616e-07, acc 1\n",
      "2018-10-26T18:35:43.305560: step 26467, loss 0, acc 1\n",
      "2018-10-26T18:35:43.477101: step 26468, loss 0, acc 1\n",
      "2018-10-26T18:35:43.656621: step 26469, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:35:43.828164: step 26470, loss 1.2144e-06, acc 1\n",
      "2018-10-26T18:35:44.003695: step 26471, loss 0.000272565, acc 1\n",
      "2018-10-26T18:35:44.171247: step 26472, loss 3.36731e-06, acc 1\n",
      "2018-10-26T18:35:44.345780: step 26473, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:35:44.520315: step 26474, loss 1.89989e-07, acc 1\n",
      "2018-10-26T18:35:44.693851: step 26475, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:35:44.864396: step 26476, loss 5.43838e-06, acc 1\n",
      "2018-10-26T18:35:45.043915: step 26477, loss 0, acc 1\n",
      "2018-10-26T18:35:45.207495: step 26478, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:35:45.388994: step 26479, loss 0, acc 1\n",
      "2018-10-26T18:35:45.603421: step 26480, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:35:45.787927: step 26481, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:35:45.981412: step 26482, loss 0, acc 1\n",
      "2018-10-26T18:35:46.169908: step 26483, loss 0.000558238, acc 1\n",
      "2018-10-26T18:35:46.358404: step 26484, loss 0.000304388, acc 1\n",
      "2018-10-26T18:35:46.545902: step 26485, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:35:46.745370: step 26486, loss 0.000187444, acc 1\n",
      "2018-10-26T18:35:46.918906: step 26487, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:35:47.134330: step 26488, loss 0.00251123, acc 1\n",
      "2018-10-26T18:35:47.303877: step 26489, loss 3.82168e-06, acc 1\n",
      "2018-10-26T18:35:47.482401: step 26490, loss 0, acc 1\n",
      "2018-10-26T18:35:47.677878: step 26491, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:35:47.865377: step 26492, loss 2.73226e-06, acc 1\n",
      "2018-10-26T18:35:48.044898: step 26493, loss 0, acc 1\n",
      "2018-10-26T18:35:48.221425: step 26494, loss 1.54594e-06, acc 1\n",
      "2018-10-26T18:35:48.437848: step 26495, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:35:48.642302: step 26496, loss 7.63667e-07, acc 1\n",
      "2018-10-26T18:35:48.823816: step 26497, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:35:49.052206: step 26498, loss 1.18646e-06, acc 1\n",
      "2018-10-26T18:35:49.247684: step 26499, loss 0.00228627, acc 1\n",
      "2018-10-26T18:35:49.432197: step 26500, loss 0.00518346, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:35:49.977733: step 26500, loss 9.13342, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26500\n",
      "\n",
      "2018-10-26T18:35:50.348742: step 26501, loss 6.92889e-07, acc 1\n",
      "2018-10-26T18:35:50.530257: step 26502, loss 0, acc 1\n",
      "2018-10-26T18:35:50.742689: step 26503, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:35:50.967090: step 26504, loss 0, acc 1\n",
      "2018-10-26T18:35:51.223405: step 26505, loss 7.63126e-06, acc 1\n",
      "2018-10-26T18:35:51.435837: step 26506, loss 5.53199e-07, acc 1\n",
      "2018-10-26T18:35:51.671210: step 26507, loss 0.00162573, acc 1\n",
      "2018-10-26T18:35:51.887630: step 26508, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:35:52.079118: step 26509, loss 2.39518e-06, acc 1\n",
      "2018-10-26T18:35:52.330447: step 26510, loss 9.872e-08, acc 1\n",
      "2018-10-26T18:35:52.507972: step 26511, loss 8.2328e-07, acc 1\n",
      "2018-10-26T18:35:52.727386: step 26512, loss 3.81704e-05, acc 1\n",
      "2018-10-26T18:35:52.939818: step 26513, loss 5.97548e-05, acc 1\n",
      "2018-10-26T18:35:53.138288: step 26514, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:35:53.329777: step 26515, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:35:53.537231: step 26516, loss 1.23559e-05, acc 1\n",
      "2018-10-26T18:35:53.750652: step 26517, loss 0, acc 1\n",
      "2018-10-26T18:35:53.945133: step 26518, loss 0.018444, acc 0.984375\n",
      "2018-10-26T18:35:54.136621: step 26519, loss 4.7497e-07, acc 1\n",
      "2018-10-26T18:35:54.344067: step 26520, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:35:54.525582: step 26521, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:35:54.740008: step 26522, loss 1.09001e-05, acc 1\n",
      "2018-10-26T18:35:54.923519: step 26523, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:35:55.124980: step 26524, loss 0, acc 1\n",
      "2018-10-26T18:35:55.322453: step 26525, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:35:55.499979: step 26526, loss 2.72112e-06, acc 1\n",
      "2018-10-26T18:35:55.726374: step 26527, loss 2.22019e-06, acc 1\n",
      "2018-10-26T18:35:55.898913: step 26528, loss 4.62072e-06, acc 1\n",
      "2018-10-26T18:35:56.079431: step 26529, loss 6.61166e-06, acc 1\n",
      "2018-10-26T18:35:56.277899: step 26530, loss 3.9651e-06, acc 1\n",
      "2018-10-26T18:35:56.462408: step 26531, loss 5.42873e-06, acc 1\n",
      "2018-10-26T18:35:56.655893: step 26532, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:35:56.839399: step 26533, loss 5.86498e-05, acc 1\n",
      "2018-10-26T18:35:57.063801: step 26534, loss 3.63214e-07, acc 1\n",
      "2018-10-26T18:35:57.254291: step 26535, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:35:57.450766: step 26536, loss 2.29593e-05, acc 1\n",
      "2018-10-26T18:35:57.650233: step 26537, loss 0.0039425, acc 1\n",
      "2018-10-26T18:35:57.853690: step 26538, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:35:58.044180: step 26539, loss 0.000266347, acc 1\n",
      "2018-10-26T18:35:58.240656: step 26540, loss 1.21328e-05, acc 1\n",
      "2018-10-26T18:35:58.422171: step 26541, loss 8.96052e-05, acc 1\n",
      "2018-10-26T18:35:58.644576: step 26542, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:35:58.827089: step 26543, loss 1.83591e-05, acc 1\n",
      "2018-10-26T18:35:59.006609: step 26544, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:35:59.228017: step 26545, loss 2.39145e-06, acc 1\n",
      "2018-10-26T18:35:59.398562: step 26546, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:35:59.578083: step 26547, loss 0.0297991, acc 0.984375\n",
      "2018-10-26T18:35:59.794504: step 26548, loss 0.0032078, acc 1\n",
      "2018-10-26T18:35:59.985993: step 26549, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:36:00.167508: step 26550, loss 0, acc 1\n",
      "2018-10-26T18:36:00.371961: step 26551, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:36:00.564446: step 26552, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:36:00.744965: step 26553, loss 0.000168475, acc 1\n",
      "2018-10-26T18:36:00.932464: step 26554, loss 0.0479822, acc 0.984375\n",
      "2018-10-26T18:36:01.135920: step 26555, loss 3.73252e-06, acc 1\n",
      "2018-10-26T18:36:01.330400: step 26556, loss 0, acc 1\n",
      "2018-10-26T18:36:01.508924: step 26557, loss 7.28277e-07, acc 1\n",
      "2018-10-26T18:36:01.689441: step 26558, loss 4.77905e-05, acc 1\n",
      "2018-10-26T18:36:01.869958: step 26559, loss 2.10478e-07, acc 1\n",
      "2018-10-26T18:36:02.043496: step 26560, loss 1.19019e-06, acc 1\n",
      "2018-10-26T18:36:02.228002: step 26561, loss 9.40911e-06, acc 1\n",
      "2018-10-26T18:36:02.401538: step 26562, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:36:02.582056: step 26563, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:36:02.754596: step 26564, loss 0, acc 1\n",
      "2018-10-26T18:36:02.937108: step 26565, loss 2.23503e-06, acc 1\n",
      "2018-10-26T18:36:03.111641: step 26566, loss 2.31915e-05, acc 1\n",
      "2018-10-26T18:36:03.298143: step 26567, loss 1.19648e-05, acc 1\n",
      "2018-10-26T18:36:03.473674: step 26568, loss 9.04628e-06, acc 1\n",
      "2018-10-26T18:36:03.659178: step 26569, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:36:03.826731: step 26570, loss 3.2778e-05, acc 1\n",
      "2018-10-26T18:36:04.010240: step 26571, loss 8.56816e-08, acc 1\n",
      "2018-10-26T18:36:04.182780: step 26572, loss 0.000140423, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:36:04.370279: step 26573, loss 0.000285412, acc 1\n",
      "2018-10-26T18:36:04.544812: step 26574, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:36:04.715357: step 26575, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:36:04.883907: step 26576, loss 3.18511e-07, acc 1\n",
      "2018-10-26T18:36:05.055447: step 26577, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:36:05.225992: step 26578, loss 0.000179843, acc 1\n",
      "2018-10-26T18:36:05.403518: step 26579, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:36:05.574062: step 26580, loss 2.49592e-07, acc 1\n",
      "2018-10-26T18:36:05.749593: step 26581, loss 0.0144763, acc 0.984375\n",
      "2018-10-26T18:36:05.926122: step 26582, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:36:06.110629: step 26583, loss 0.000504299, acc 1\n",
      "2018-10-26T18:36:06.275188: step 26584, loss 0, acc 1\n",
      "2018-10-26T18:36:06.449723: step 26585, loss 5.43883e-07, acc 1\n",
      "2018-10-26T18:36:06.620268: step 26586, loss 1.58131e-06, acc 1\n",
      "2018-10-26T18:36:06.792806: step 26587, loss 8.31495e-06, acc 1\n",
      "2018-10-26T18:36:06.965345: step 26588, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:07.138880: step 26589, loss 4.69381e-07, acc 1\n",
      "2018-10-26T18:36:07.310423: step 26590, loss 4.71946e-06, acc 1\n",
      "2018-10-26T18:36:07.485954: step 26591, loss 2.66039e-05, acc 1\n",
      "2018-10-26T18:36:07.654504: step 26592, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:07.831032: step 26593, loss 3.82697e-05, acc 1\n",
      "2018-10-26T18:36:07.998585: step 26594, loss 5.88585e-07, acc 1\n",
      "2018-10-26T18:36:08.174115: step 26595, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:36:08.343662: step 26596, loss 4.98178e-06, acc 1\n",
      "2018-10-26T18:36:08.527172: step 26597, loss 1.12632e-05, acc 1\n",
      "2018-10-26T18:36:08.721652: step 26598, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:36:08.956026: step 26599, loss 2.35611e-06, acc 1\n",
      "2018-10-26T18:36:09.152501: step 26600, loss 5.02913e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:36:09.636208: step 26600, loss 9.35274, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26600\n",
      "\n",
      "2018-10-26T18:36:10.072545: step 26601, loss 0, acc 1\n",
      "2018-10-26T18:36:10.252065: step 26602, loss 2.94597e-05, acc 1\n",
      "2018-10-26T18:36:10.424604: step 26603, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:36:10.596145: step 26604, loss 0.000352639, acc 1\n",
      "2018-10-26T18:36:10.777661: step 26605, loss 0, acc 1\n",
      "2018-10-26T18:36:10.948206: step 26606, loss 0, acc 1\n",
      "2018-10-26T18:36:11.131716: step 26607, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:36:11.309241: step 26608, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:36:11.485769: step 26609, loss 2.2611e-06, acc 1\n",
      "2018-10-26T18:36:11.663295: step 26610, loss 0, acc 1\n",
      "2018-10-26T18:36:11.837828: step 26611, loss 7.28283e-07, acc 1\n",
      "2018-10-26T18:36:12.021338: step 26612, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:36:12.202853: step 26613, loss 6.15124e-06, acc 1\n",
      "2018-10-26T18:36:12.377388: step 26614, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:36:12.561894: step 26615, loss 1.78431e-06, acc 1\n",
      "2018-10-26T18:36:12.733436: step 26616, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:36:12.920935: step 26617, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:36:13.091480: step 26618, loss 5.0956e-06, acc 1\n",
      "2018-10-26T18:36:13.264018: step 26619, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:13.436558: step 26620, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:36:13.613085: step 26621, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:36:13.785623: step 26622, loss 0, acc 1\n",
      "2018-10-26T18:36:13.970131: step 26623, loss 4.60068e-07, acc 1\n",
      "2018-10-26T18:36:14.138682: step 26624, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:36:14.310222: step 26625, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:36:14.481765: step 26626, loss 6.07128e-06, acc 1\n",
      "2018-10-26T18:36:14.657295: step 26627, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:36:14.827841: step 26628, loss 3.80501e-06, acc 1\n",
      "2018-10-26T18:36:15.005365: step 26629, loss 0.00743114, acc 1\n",
      "2018-10-26T18:36:15.177290: step 26630, loss 1.89745e-05, acc 1\n",
      "2018-10-26T18:36:15.361796: step 26631, loss 0, acc 1\n",
      "2018-10-26T18:36:15.532340: step 26632, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:36:15.712858: step 26633, loss 0.000136331, acc 1\n",
      "2018-10-26T18:36:15.899360: step 26634, loss 5.06632e-07, acc 1\n",
      "2018-10-26T18:36:16.086859: step 26635, loss 1.91851e-07, acc 1\n",
      "2018-10-26T18:36:16.266379: step 26636, loss 2.23066e-05, acc 1\n",
      "2018-10-26T18:36:16.438919: step 26637, loss 0.0380254, acc 0.984375\n",
      "2018-10-26T18:36:16.622429: step 26638, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:16.791961: step 26639, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:17.004393: step 26640, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:36:17.177930: step 26641, loss 0.00177232, acc 1\n",
      "2018-10-26T18:36:17.356453: step 26642, loss 0.00103452, acc 1\n",
      "2018-10-26T18:36:17.531984: step 26643, loss 6.37012e-07, acc 1\n",
      "2018-10-26T18:36:17.761373: step 26644, loss 1.11758e-07, acc 1\n",
      "2018-10-26T18:36:17.931916: step 26645, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:36:18.112433: step 26646, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:36:18.288962: step 26647, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:36:18.471474: step 26648, loss 2.78069e-05, acc 1\n",
      "2018-10-26T18:36:18.644013: step 26649, loss 3.66937e-07, acc 1\n",
      "2018-10-26T18:36:18.820541: step 26650, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:36:18.993080: step 26651, loss 0.000113609, acc 1\n",
      "2018-10-26T18:36:19.172601: step 26652, loss 3.68802e-07, acc 1\n",
      "2018-10-26T18:36:19.344142: step 26653, loss 4.37715e-07, acc 1\n",
      "2018-10-26T18:36:19.524660: step 26654, loss 0.0448646, acc 0.984375\n",
      "2018-10-26T18:36:19.691215: step 26655, loss 1.19391e-06, acc 1\n",
      "2018-10-26T18:36:19.875723: step 26656, loss 3.18511e-07, acc 1\n",
      "2018-10-26T18:36:20.058235: step 26657, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:36:20.232768: step 26658, loss 7.93478e-07, acc 1\n",
      "2018-10-26T18:36:20.411292: step 26659, loss 2.16066e-07, acc 1\n",
      "2018-10-26T18:36:20.580838: step 26660, loss 1.01798e-05, acc 1\n",
      "2018-10-26T18:36:20.760358: step 26661, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:20.926912: step 26662, loss 1.17528e-06, acc 1\n",
      "2018-10-26T18:36:21.112417: step 26663, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:21.280966: step 26664, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:36:21.462484: step 26665, loss 2.42142e-07, acc 1\n",
      "2018-10-26T18:36:21.634025: step 26666, loss 0.000589913, acc 1\n",
      "2018-10-26T18:36:21.812546: step 26667, loss 0.000862201, acc 1\n",
      "2018-10-26T18:36:21.978104: step 26668, loss 7.6553e-07, acc 1\n",
      "2018-10-26T18:36:22.159619: step 26669, loss 3.94876e-07, acc 1\n",
      "2018-10-26T18:36:22.327171: step 26670, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:22.506692: step 26671, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:36:22.678233: step 26672, loss 2.49593e-07, acc 1\n",
      "2018-10-26T18:36:22.862742: step 26673, loss 1.35897e-05, acc 1\n",
      "2018-10-26T18:36:23.030293: step 26674, loss 5.14082e-07, acc 1\n",
      "2018-10-26T18:36:23.206821: step 26675, loss 2.23749e-05, acc 1\n",
      "2018-10-26T18:36:23.387340: step 26676, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:36:23.566860: step 26677, loss 2.86846e-07, acc 1\n",
      "2018-10-26T18:36:23.752364: step 26678, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:36:23.918919: step 26679, loss 2.61307e-06, acc 1\n",
      "2018-10-26T18:36:24.107415: step 26680, loss 2.70081e-07, acc 1\n",
      "2018-10-26T18:36:24.274968: step 26681, loss 1.70609e-06, acc 1\n",
      "2018-10-26T18:36:24.453491: step 26682, loss 3.91615e-05, acc 1\n",
      "2018-10-26T18:36:24.643982: step 26683, loss 5.30846e-07, acc 1\n",
      "2018-10-26T18:36:24.830484: step 26684, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:36:25.011999: step 26685, loss 4.33286e-05, acc 1\n",
      "2018-10-26T18:36:25.196506: step 26686, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:25.379018: step 26687, loss 0.000861497, acc 1\n",
      "2018-10-26T18:36:25.560533: step 26688, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:25.732075: step 26689, loss 0.000159122, acc 1\n",
      "2018-10-26T18:36:25.915597: step 26690, loss 8.19563e-08, acc 1\n",
      "2018-10-26T18:36:26.086129: step 26691, loss 4.04701e-06, acc 1\n",
      "2018-10-26T18:36:26.270640: step 26692, loss 9.31322e-08, acc 1\n",
      "2018-10-26T18:36:26.441179: step 26693, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:26.622695: step 26694, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:36:26.790248: step 26695, loss 1.31615e-05, acc 1\n",
      "2018-10-26T18:36:26.967773: step 26696, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:36:27.134328: step 26697, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:36:27.314846: step 26698, loss 0.00023252, acc 1\n",
      "2018-10-26T18:36:27.492371: step 26699, loss 8.59716e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:36:27.689844: step 26700, loss 7.15254e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:36:28.156597: step 26700, loss 9.33979, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26700\n",
      "\n",
      "2018-10-26T18:36:28.547593: step 26701, loss 1.47519e-06, acc 1\n",
      "2018-10-26T18:36:28.760012: step 26702, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:36:28.941525: step 26703, loss 1.62049e-07, acc 1\n",
      "2018-10-26T18:36:29.130022: step 26704, loss 0.000552012, acc 1\n",
      "2018-10-26T18:36:29.372375: step 26705, loss 0, acc 1\n",
      "2018-10-26T18:36:29.550898: step 26706, loss 0, acc 1\n",
      "2018-10-26T18:36:29.720444: step 26707, loss 8.71738e-06, acc 1\n",
      "2018-10-26T18:36:29.896972: step 26708, loss 0.00168345, acc 1\n",
      "2018-10-26T18:36:30.071507: step 26709, loss 8.85521e-05, acc 1\n",
      "2018-10-26T18:36:30.241053: step 26710, loss 0.0517169, acc 0.984375\n",
      "2018-10-26T18:36:30.415587: step 26711, loss 1.70609e-06, acc 1\n",
      "2018-10-26T18:36:30.589123: step 26712, loss 1.4081e-06, acc 1\n",
      "2018-10-26T18:36:30.778619: step 26713, loss 8.38188e-08, acc 1\n",
      "2018-10-26T18:36:30.955146: step 26714, loss 3.73387e-05, acc 1\n",
      "2018-10-26T18:36:31.135663: step 26715, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:31.321211: step 26716, loss 3.32074e-06, acc 1\n",
      "2018-10-26T18:36:31.503680: step 26717, loss 1.11941e-06, acc 1\n",
      "2018-10-26T18:36:31.677217: step 26718, loss 7.09064e-05, acc 1\n",
      "2018-10-26T18:36:31.854742: step 26719, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:36:32.029276: step 26720, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:32.214801: step 26721, loss 0, acc 1\n",
      "2018-10-26T18:36:32.389313: step 26722, loss 9.92763e-07, acc 1\n",
      "2018-10-26T18:36:32.570829: step 26723, loss 2.02277e-06, acc 1\n",
      "2018-10-26T18:36:32.738381: step 26724, loss 7.88269e-06, acc 1\n",
      "2018-10-26T18:36:32.929870: step 26725, loss 0.000792773, acc 1\n",
      "2018-10-26T18:36:33.132329: step 26726, loss 2.2695e-05, acc 1\n",
      "2018-10-26T18:36:33.331797: step 26727, loss 9.98346e-07, acc 1\n",
      "2018-10-26T18:36:33.529269: step 26728, loss 1.08776e-06, acc 1\n",
      "2018-10-26T18:36:33.701807: step 26729, loss 1.64835e-06, acc 1\n",
      "2018-10-26T18:36:33.878335: step 26730, loss 0.000144869, acc 1\n",
      "2018-10-26T18:36:34.049877: step 26731, loss 0, acc 1\n",
      "2018-10-26T18:36:34.227403: step 26732, loss 2.57044e-07, acc 1\n",
      "2018-10-26T18:36:34.397947: step 26733, loss 0, acc 1\n",
      "2018-10-26T18:36:34.584448: step 26734, loss 3.09731e-06, acc 1\n",
      "2018-10-26T18:36:34.753996: step 26735, loss 0.000504571, acc 1\n",
      "2018-10-26T18:36:34.933516: step 26736, loss 3.36411e-05, acc 1\n",
      "2018-10-26T18:36:35.110044: step 26737, loss 1.80676e-07, acc 1\n",
      "2018-10-26T18:36:35.288567: step 26738, loss 1.21528e-05, acc 1\n",
      "2018-10-26T18:36:35.465097: step 26739, loss 1.83834e-06, acc 1\n",
      "2018-10-26T18:36:35.643639: step 26740, loss 1.19207e-06, acc 1\n",
      "2018-10-26T18:36:35.819150: step 26741, loss 7.86021e-07, acc 1\n",
      "2018-10-26T18:36:35.988697: step 26742, loss 7.76486e-05, acc 1\n",
      "2018-10-26T18:36:36.167220: step 26743, loss 0.00225324, acc 1\n",
      "2018-10-26T18:36:36.334772: step 26744, loss 0.000190877, acc 1\n",
      "2018-10-26T18:36:36.511300: step 26745, loss 8.93837e-06, acc 1\n",
      "2018-10-26T18:36:36.680848: step 26746, loss 8.14732e-06, acc 1\n",
      "2018-10-26T18:36:36.866352: step 26747, loss 2.02748e-05, acc 1\n",
      "2018-10-26T18:36:37.040332: step 26748, loss 0.000141732, acc 1\n",
      "2018-10-26T18:36:37.212870: step 26749, loss 0.000320611, acc 1\n",
      "2018-10-26T18:36:37.380424: step 26750, loss 2.64855e-06, acc 1\n",
      "2018-10-26T18:36:37.564930: step 26751, loss 0.00395039, acc 1\n",
      "2018-10-26T18:36:37.734478: step 26752, loss 0.000874346, acc 1\n",
      "2018-10-26T18:36:37.913000: step 26753, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:36:38.092521: step 26754, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:36:38.270047: step 26755, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:36:38.455551: step 26756, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:36:38.628090: step 26757, loss 2.56443e-05, acc 1\n",
      "2018-10-26T18:36:38.807620: step 26758, loss 0.000111855, acc 1\n",
      "2018-10-26T18:36:38.986135: step 26759, loss 6.34691e-05, acc 1\n",
      "2018-10-26T18:36:39.162661: step 26760, loss 0.000961476, acc 1\n",
      "2018-10-26T18:36:39.333205: step 26761, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:39.514721: step 26762, loss 0, acc 1\n",
      "2018-10-26T18:36:39.680278: step 26763, loss 1.00589e-05, acc 1\n",
      "2018-10-26T18:36:39.861793: step 26764, loss 2.19791e-07, acc 1\n",
      "2018-10-26T18:36:40.034332: step 26765, loss 3.73809e-06, acc 1\n",
      "2018-10-26T18:36:40.229810: step 26766, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:36:40.398359: step 26767, loss 4.5234e-05, acc 1\n",
      "2018-10-26T18:36:40.589849: step 26768, loss 3.29685e-07, acc 1\n",
      "2018-10-26T18:36:40.766376: step 26769, loss 1.44162e-06, acc 1\n",
      "2018-10-26T18:36:40.945897: step 26770, loss 1.72387e-05, acc 1\n",
      "2018-10-26T18:36:41.119433: step 26771, loss 2.16308e-05, acc 1\n",
      "2018-10-26T18:36:41.301946: step 26772, loss 3.41013e-06, acc 1\n",
      "2018-10-26T18:36:41.484458: step 26773, loss 5.54864e-05, acc 1\n",
      "2018-10-26T18:36:41.658992: step 26774, loss 1.25725e-06, acc 1\n",
      "2018-10-26T18:36:41.834522: step 26775, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:36:42.001078: step 26776, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:42.182593: step 26777, loss 1.92775e-06, acc 1\n",
      "2018-10-26T18:36:42.367099: step 26778, loss 3.03609e-07, acc 1\n",
      "2018-10-26T18:36:42.547617: step 26779, loss 5.64371e-07, acc 1\n",
      "2018-10-26T18:36:42.714186: step 26780, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:36:42.899676: step 26781, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:36:43.063239: step 26782, loss 1.10175e-05, acc 1\n",
      "2018-10-26T18:36:43.243767: step 26783, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:36:43.420286: step 26784, loss 0, acc 1\n",
      "2018-10-26T18:36:43.596814: step 26785, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:36:43.780323: step 26786, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:43.945881: step 26787, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:36:44.123407: step 26788, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:36:44.297940: step 26789, loss 1.59807e-06, acc 1\n",
      "2018-10-26T18:36:44.492421: step 26790, loss 0.00229139, acc 1\n",
      "2018-10-26T18:36:44.664960: step 26791, loss 0, acc 1\n",
      "2018-10-26T18:36:44.845478: step 26792, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:45.020011: step 26793, loss 0.00012018, acc 1\n",
      "2018-10-26T18:36:45.189558: step 26794, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:36:45.372071: step 26795, loss 0.000144846, acc 1\n",
      "2018-10-26T18:36:45.540620: step 26796, loss 0.00599692, acc 1\n",
      "2018-10-26T18:36:45.719143: step 26797, loss 6.78047e-06, acc 1\n",
      "2018-10-26T18:36:45.886695: step 26798, loss 0, acc 1\n",
      "2018-10-26T18:36:46.070205: step 26799, loss 2.54404e-05, acc 1\n",
      "2018-10-26T18:36:46.238755: step 26800, loss 1.49377e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:36:46.709496: step 26800, loss 9.42034, acc 0.71576\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26800\n",
      "\n",
      "2018-10-26T18:36:47.066989: step 26801, loss 1.70052e-06, acc 1\n",
      "2018-10-26T18:36:47.236537: step 26802, loss 2.04775e-05, acc 1\n",
      "2018-10-26T18:36:47.420046: step 26803, loss 0.00866607, acc 1\n",
      "2018-10-26T18:36:47.592585: step 26804, loss 0.000125373, acc 1\n",
      "2018-10-26T18:36:47.797038: step 26805, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:36:48.047369: step 26806, loss 7.67594e-06, acc 1\n",
      "2018-10-26T18:36:48.222901: step 26807, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:36:48.416384: step 26808, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:48.583936: step 26809, loss 0.0304441, acc 0.984375\n",
      "2018-10-26T18:36:48.763456: step 26810, loss 0, acc 1\n",
      "2018-10-26T18:36:48.933004: step 26811, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:49.113521: step 26812, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:36:49.285062: step 26813, loss 0.000639876, acc 1\n",
      "2018-10-26T18:36:49.466578: step 26814, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:36:49.635128: step 26815, loss 2.20523e-06, acc 1\n",
      "2018-10-26T18:36:49.812653: step 26816, loss 0.000697181, acc 1\n",
      "2018-10-26T18:36:49.986201: step 26817, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:36:50.171695: step 26818, loss 4.30268e-07, acc 1\n",
      "2018-10-26T18:36:50.345230: step 26819, loss 0, acc 1\n",
      "2018-10-26T18:36:50.527743: step 26820, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:36:50.699699: step 26821, loss 2.47153e-06, acc 1\n",
      "2018-10-26T18:36:50.883208: step 26822, loss 3.55695e-05, acc 1\n",
      "2018-10-26T18:36:51.118581: step 26823, loss 1.3411e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:36:51.322036: step 26824, loss 9.67559e-05, acc 1\n",
      "2018-10-26T18:36:51.538458: step 26825, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:36:51.724961: step 26826, loss 3.44586e-07, acc 1\n",
      "2018-10-26T18:36:51.921437: step 26827, loss 0, acc 1\n",
      "2018-10-26T18:36:52.097963: step 26828, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:52.322364: step 26829, loss 3.43807e-06, acc 1\n",
      "2018-10-26T18:36:52.490913: step 26830, loss 1.20509e-06, acc 1\n",
      "2018-10-26T18:36:52.692375: step 26831, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:52.887853: step 26832, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:36:53.061389: step 26833, loss 0.000754854, acc 1\n",
      "2018-10-26T18:36:53.253875: step 26834, loss 6.38878e-07, acc 1\n",
      "2018-10-26T18:36:53.443368: step 26835, loss 1.07977e-05, acc 1\n",
      "2018-10-26T18:36:53.626878: step 26836, loss 1.56613e-05, acc 1\n",
      "2018-10-26T18:36:53.802409: step 26837, loss 0.0001525, acc 1\n",
      "2018-10-26T18:36:54.015839: step 26838, loss 0.000278213, acc 1\n",
      "2018-10-26T18:36:54.207329: step 26839, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:54.425745: step 26840, loss 3.01747e-07, acc 1\n",
      "2018-10-26T18:36:54.671088: step 26841, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:36:54.882525: step 26842, loss 2.68757e-06, acc 1\n",
      "2018-10-26T18:36:55.111910: step 26843, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:36:55.300407: step 26844, loss 3.22234e-07, acc 1\n",
      "2018-10-26T18:36:55.538771: step 26845, loss 8.05027e-05, acc 1\n",
      "2018-10-26T18:36:55.754195: step 26846, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:36:55.980590: step 26847, loss 2.19397e-05, acc 1\n",
      "2018-10-26T18:36:56.157117: step 26848, loss 0, acc 1\n",
      "2018-10-26T18:36:56.370549: step 26849, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:36:56.533113: step 26850, loss 9.16065e-05, acc 1\n",
      "2018-10-26T18:36:56.717620: step 26851, loss 4.15367e-07, acc 1\n",
      "2018-10-26T18:36:56.923072: step 26852, loss 1.27324e-05, acc 1\n",
      "2018-10-26T18:36:57.100597: step 26853, loss 2.68022e-06, acc 1\n",
      "2018-10-26T18:36:57.270144: step 26854, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:36:57.470608: step 26855, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:36:57.694012: step 26856, loss 0.000134312, acc 1\n",
      "2018-10-26T18:36:57.867549: step 26857, loss 2.11937e-05, acc 1\n",
      "2018-10-26T18:36:58.092945: step 26858, loss 1.73428e-05, acc 1\n",
      "2018-10-26T18:36:58.296403: step 26859, loss 5.02911e-07, acc 1\n",
      "2018-10-26T18:36:58.507837: step 26860, loss 0, acc 1\n",
      "2018-10-26T18:36:58.682370: step 26861, loss 0, acc 1\n",
      "2018-10-26T18:36:58.872862: step 26862, loss 1.25794e-05, acc 1\n",
      "2018-10-26T18:36:59.104243: step 26863, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:36:59.276782: step 26864, loss 2.23516e-07, acc 1\n",
      "2018-10-26T18:36:59.451317: step 26865, loss 1.06167e-06, acc 1\n",
      "2018-10-26T18:36:59.665744: step 26866, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:36:59.836287: step 26867, loss 7.32002e-07, acc 1\n",
      "2018-10-26T18:37:00.021792: step 26868, loss 8.56816e-08, acc 1\n",
      "2018-10-26T18:37:00.221259: step 26869, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:37:00.405766: step 26870, loss 5.58391e-05, acc 1\n",
      "2018-10-26T18:37:00.583292: step 26871, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:37:00.758823: step 26872, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:37:00.971255: step 26873, loss 9.59233e-07, acc 1\n",
      "2018-10-26T18:37:01.159751: step 26874, loss 4.82926e-05, acc 1\n",
      "2018-10-26T18:37:01.332291: step 26875, loss 0.0148089, acc 0.984375\n",
      "2018-10-26T18:37:01.515800: step 26876, loss 0, acc 1\n",
      "2018-10-26T18:37:01.713273: step 26877, loss 3.58715e-06, acc 1\n",
      "2018-10-26T18:37:01.895784: step 26878, loss 0.000206464, acc 1\n",
      "2018-10-26T18:37:02.076303: step 26879, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:37:02.257817: step 26880, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:37:02.453295: step 26881, loss 1.26843e-06, acc 1\n",
      "2018-10-26T18:37:02.637802: step 26882, loss 1.01281e-05, acc 1\n",
      "2018-10-26T18:37:02.858222: step 26883, loss 5.38398e-06, acc 1\n",
      "2018-10-26T18:37:03.035739: step 26884, loss 2.27971e-06, acc 1\n",
      "2018-10-26T18:37:03.232215: step 26885, loss 5.63398e-06, acc 1\n",
      "2018-10-26T18:37:03.401761: step 26886, loss 8.32594e-07, acc 1\n",
      "2018-10-26T18:37:03.614194: step 26887, loss 0, acc 1\n",
      "2018-10-26T18:37:03.778753: step 26888, loss 7.14553e-05, acc 1\n",
      "2018-10-26T18:37:03.988194: step 26889, loss 2.08998e-05, acc 1\n",
      "2018-10-26T18:37:04.157741: step 26890, loss 1.20113e-05, acc 1\n",
      "2018-10-26T18:37:04.338259: step 26891, loss 0, acc 1\n",
      "2018-10-26T18:37:04.551688: step 26892, loss 2.60191e-06, acc 1\n",
      "2018-10-26T18:37:04.725225: step 26893, loss 4.87315e-05, acc 1\n",
      "2018-10-26T18:37:04.904745: step 26894, loss 1.82343e-06, acc 1\n",
      "2018-10-26T18:37:05.102217: step 26895, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:37:05.282735: step 26896, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:37:05.453279: step 26897, loss 7.1718e-05, acc 1\n",
      "2018-10-26T18:37:05.620833: step 26898, loss 1.01511e-06, acc 1\n",
      "2018-10-26T18:37:05.832268: step 26899, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:37:06.003809: step 26900, loss 7.52394e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:37:06.490508: step 26900, loss 9.2891, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-26900\n",
      "\n",
      "2018-10-26T18:37:06.880124: step 26901, loss 0, acc 1\n",
      "2018-10-26T18:37:07.069618: step 26902, loss 2.57043e-07, acc 1\n",
      "2018-10-26T18:37:07.266095: step 26903, loss 2.86635e-06, acc 1\n",
      "2018-10-26T18:37:07.448606: step 26904, loss 0, acc 1\n",
      "2018-10-26T18:37:07.714894: step 26905, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:37:07.892419: step 26906, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:08.079919: step 26907, loss 2.46968e-06, acc 1\n",
      "2018-10-26T18:37:08.251460: step 26908, loss 0.016768, acc 0.984375\n",
      "2018-10-26T18:37:08.428986: step 26909, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:37:08.595540: step 26910, loss 1.41929e-06, acc 1\n",
      "2018-10-26T18:37:08.771071: step 26911, loss 1.52228e-05, acc 1\n",
      "2018-10-26T18:37:08.937626: step 26912, loss 9.54887e-06, acc 1\n",
      "2018-10-26T18:37:09.119142: step 26913, loss 0, acc 1\n",
      "2018-10-26T18:37:09.291680: step 26914, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:37:09.467211: step 26915, loss 2.37456e-05, acc 1\n",
      "2018-10-26T18:37:09.633767: step 26916, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:37:09.821266: step 26917, loss 2.11027e-06, acc 1\n",
      "2018-10-26T18:37:09.986824: step 26918, loss 0, acc 1\n",
      "2018-10-26T18:37:10.172328: step 26919, loss 0, acc 1\n",
      "2018-10-26T18:37:10.340877: step 26920, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:37:10.516409: step 26921, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:37:10.686953: step 26922, loss 0, acc 1\n",
      "2018-10-26T18:37:10.868468: step 26923, loss 4.89868e-07, acc 1\n",
      "2018-10-26T18:37:11.041007: step 26924, loss 1.46096e-05, acc 1\n",
      "2018-10-26T18:37:11.219530: step 26925, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:37:11.394063: step 26926, loss 8.03163e-06, acc 1\n",
      "2018-10-26T18:37:11.570592: step 26927, loss 0.132618, acc 0.984375\n",
      "2018-10-26T18:37:11.742134: step 26928, loss 3.22234e-07, acc 1\n",
      "2018-10-26T18:37:11.923649: step 26929, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:37:12.099180: step 26930, loss 0.000142616, acc 1\n",
      "2018-10-26T18:37:12.283686: step 26931, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:37:12.459217: step 26932, loss 1.65441e-05, acc 1\n",
      "2018-10-26T18:37:12.642735: step 26933, loss 0, acc 1\n",
      "2018-10-26T18:37:12.813273: step 26934, loss 6.74345e-06, acc 1\n",
      "2018-10-26T18:37:13.002765: step 26935, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:13.171314: step 26936, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:37:13.355822: step 26937, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:37:13.524372: step 26938, loss 2.01165e-07, acc 1\n",
      "2018-10-26T18:37:13.710873: step 26939, loss 0, acc 1\n",
      "2018-10-26T18:37:13.882416: step 26940, loss 0, acc 1\n",
      "2018-10-26T18:37:14.062932: step 26941, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:37:14.233477: step 26942, loss 0.00039328, acc 1\n",
      "2018-10-26T18:37:14.422971: step 26943, loss 0.000259092, acc 1\n",
      "2018-10-26T18:37:14.595510: step 26944, loss 0, acc 1\n",
      "2018-10-26T18:37:14.781014: step 26945, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:37:14.951559: step 26946, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:37:15.140055: step 26947, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:37:15.305613: step 26948, loss 1.76448e-05, acc 1\n",
      "2018-10-26T18:37:15.494108: step 26949, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:37:15.665651: step 26950, loss 8.36306e-07, acc 1\n",
      "2018-10-26T18:37:15.849160: step 26951, loss 2.98392e-05, acc 1\n",
      "2018-10-26T18:37:16.019704: step 26952, loss 9.20132e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:37:16.199225: step 26953, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:37:16.369769: step 26954, loss 0.000948354, acc 1\n",
      "2018-10-26T18:37:16.551285: step 26955, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:37:16.726815: step 26956, loss 6.85439e-07, acc 1\n",
      "2018-10-26T18:37:16.901348: step 26957, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:17.066907: step 26958, loss 1.30566e-06, acc 1\n",
      "2018-10-26T18:37:17.247424: step 26959, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:37:17.411984: step 26960, loss 2.64494e-07, acc 1\n",
      "2018-10-26T18:37:17.582529: step 26961, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:37:17.756065: step 26962, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:37:17.941569: step 26963, loss 0.000117881, acc 1\n",
      "2018-10-26T18:37:18.112114: step 26964, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:37:18.288642: step 26965, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:37:18.457192: step 26966, loss 0.000701088, acc 1\n",
      "2018-10-26T18:37:18.634717: step 26967, loss 9.92768e-07, acc 1\n",
      "2018-10-26T18:37:18.807257: step 26968, loss 5.43886e-07, acc 1\n",
      "2018-10-26T18:37:18.990766: step 26969, loss 5.13261e-06, acc 1\n",
      "2018-10-26T18:37:19.155327: step 26970, loss 2.62431e-06, acc 1\n",
      "2018-10-26T18:37:19.330858: step 26971, loss 2.23516e-07, acc 1\n",
      "2018-10-26T18:37:19.496415: step 26972, loss 4.28403e-07, acc 1\n",
      "2018-10-26T18:37:19.675935: step 26973, loss 0, acc 1\n",
      "2018-10-26T18:37:19.845483: step 26974, loss 2.96646e-05, acc 1\n",
      "2018-10-26T18:37:20.024005: step 26975, loss 0.0215094, acc 0.984375\n",
      "2018-10-26T18:37:20.201531: step 26976, loss 1.77316e-06, acc 1\n",
      "2018-10-26T18:37:20.379057: step 26977, loss 1.23598e-05, acc 1\n",
      "2018-10-26T18:37:20.550599: step 26978, loss 4.05424e-05, acc 1\n",
      "2018-10-26T18:37:20.717153: step 26979, loss 8.75442e-08, acc 1\n",
      "2018-10-26T18:37:20.894679: step 26980, loss 0, acc 1\n",
      "2018-10-26T18:37:21.065224: step 26981, loss 0, acc 1\n",
      "2018-10-26T18:37:21.237763: step 26982, loss 0.0492439, acc 0.984375\n",
      "2018-10-26T18:37:21.408319: step 26983, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:37:21.581843: step 26984, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:37:21.749395: step 26985, loss 1.90537e-06, acc 1\n",
      "2018-10-26T18:37:21.930910: step 26986, loss 7.46742e-06, acc 1\n",
      "2018-10-26T18:37:22.104447: step 26987, loss 6.23982e-07, acc 1\n",
      "2018-10-26T18:37:22.286959: step 26988, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:37:22.466480: step 26989, loss 6.01901e-06, acc 1\n",
      "2018-10-26T18:37:22.630043: step 26990, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:37:22.805573: step 26991, loss 0.0267749, acc 0.984375\n",
      "2018-10-26T18:37:22.972128: step 26992, loss 0, acc 1\n",
      "2018-10-26T18:37:23.152646: step 26993, loss 8.27005e-07, acc 1\n",
      "2018-10-26T18:37:23.321196: step 26994, loss 1.50308e-06, acc 1\n",
      "2018-10-26T18:37:23.502719: step 26995, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:37:23.674253: step 26996, loss 3.81838e-07, acc 1\n",
      "2018-10-26T18:37:23.853773: step 26997, loss 4.17227e-07, acc 1\n",
      "2018-10-26T18:37:24.025315: step 26998, loss 4.56861e-06, acc 1\n",
      "2018-10-26T18:37:24.203838: step 26999, loss 3.14785e-07, acc 1\n",
      "2018-10-26T18:37:24.369395: step 27000, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:37:24.829166: step 27000, loss 9.31964, acc 0.726079\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27000\n",
      "\n",
      "2018-10-26T18:37:25.192893: step 27001, loss 0, acc 1\n",
      "2018-10-26T18:37:25.358452: step 27002, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:37:25.536973: step 27003, loss 3.50883e-06, acc 1\n",
      "2018-10-26T18:37:25.707518: step 27004, loss 4.8428e-07, acc 1\n",
      "2018-10-26T18:37:25.909977: step 27005, loss 0, acc 1\n",
      "2018-10-26T18:37:26.152330: step 27006, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:37:26.321877: step 27007, loss 1.93803e-05, acc 1\n",
      "2018-10-26T18:37:26.522341: step 27008, loss 4.52616e-07, acc 1\n",
      "2018-10-26T18:37:26.703856: step 27009, loss 5.67259e-06, acc 1\n",
      "2018-10-26T18:37:26.878390: step 27010, loss 0, acc 1\n",
      "2018-10-26T18:37:27.054918: step 27011, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:27.236433: step 27012, loss 6.51924e-08, acc 1\n",
      "2018-10-26T18:37:27.432909: step 27013, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:37:27.616418: step 27014, loss 2.61593e-05, acc 1\n",
      "2018-10-26T18:37:27.793943: step 27015, loss 2.10478e-07, acc 1\n",
      "2018-10-26T18:37:27.963491: step 27016, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:37:28.151988: step 27017, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:37:28.319540: step 27018, loss 0, acc 1\n",
      "2018-10-26T18:37:28.504046: step 27019, loss 0.000331119, acc 1\n",
      "2018-10-26T18:37:28.673594: step 27020, loss 7.52491e-07, acc 1\n",
      "2018-10-26T18:37:28.857103: step 27021, loss 0.000157326, acc 1\n",
      "2018-10-26T18:37:29.023658: step 27022, loss 0.000253667, acc 1\n",
      "2018-10-26T18:37:29.205173: step 27023, loss 2.08615e-07, acc 1\n",
      "2018-10-26T18:37:29.387685: step 27024, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:37:29.568204: step 27025, loss 3.93016e-07, acc 1\n",
      "2018-10-26T18:37:29.741740: step 27026, loss 2.80425e-05, acc 1\n",
      "2018-10-26T18:37:29.921260: step 27027, loss 1.071e-06, acc 1\n",
      "2018-10-26T18:37:30.087815: step 27028, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:37:30.268333: step 27029, loss 0.000564199, acc 1\n",
      "2018-10-26T18:37:30.440871: step 27030, loss 0, acc 1\n",
      "2018-10-26T18:37:30.626377: step 27031, loss 6.48296e-06, acc 1\n",
      "2018-10-26T18:37:30.797918: step 27032, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:30.976441: step 27033, loss 6.22117e-07, acc 1\n",
      "2018-10-26T18:37:31.143993: step 27034, loss 0.00156445, acc 1\n",
      "2018-10-26T18:37:31.326506: step 27035, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:37:31.499045: step 27036, loss 0.000268479, acc 1\n",
      "2018-10-26T18:37:31.673579: step 27037, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:31.851103: step 27038, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:37:32.026636: step 27039, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:37:32.195184: step 27040, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:37:32.385676: step 27041, loss 0.000331062, acc 1\n",
      "2018-10-26T18:37:32.556219: step 27042, loss 4.39462e-05, acc 1\n",
      "2018-10-26T18:37:32.735740: step 27043, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:37:32.909277: step 27044, loss 0.000383497, acc 1\n",
      "2018-10-26T18:37:33.088797: step 27045, loss 0, acc 1\n",
      "2018-10-26T18:37:33.259341: step 27046, loss 1.07794e-05, acc 1\n",
      "2018-10-26T18:37:33.441854: step 27047, loss 0.0186202, acc 0.984375\n",
      "2018-10-26T18:37:33.609406: step 27048, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:33.789924: step 27049, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:37:33.971438: step 27050, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:37:34.153951: step 27051, loss 3.58501e-05, acc 1\n",
      "2018-10-26T18:37:34.330478: step 27052, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:34.514987: step 27053, loss 0.304496, acc 0.984375\n",
      "2018-10-26T18:37:34.693509: step 27054, loss 2.47059e-05, acc 1\n",
      "2018-10-26T18:37:34.887989: step 27055, loss 2.98021e-07, acc 1\n",
      "2018-10-26T18:37:35.054545: step 27056, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:37:35.236059: step 27057, loss 1.265e-05, acc 1\n",
      "2018-10-26T18:37:35.412588: step 27058, loss 7.27198e-06, acc 1\n",
      "2018-10-26T18:37:35.596098: step 27059, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:37:35.765644: step 27060, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:37:35.947160: step 27061, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:37:36.115709: step 27062, loss 0.00018357, acc 1\n",
      "2018-10-26T18:37:36.297225: step 27063, loss 0.0616567, acc 0.984375\n",
      "2018-10-26T18:37:36.463808: step 27064, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:37:36.641304: step 27065, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:36.807859: step 27066, loss 9.63712e-05, acc 1\n",
      "2018-10-26T18:37:36.988378: step 27067, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:37:37.156927: step 27068, loss 1.14364e-06, acc 1\n",
      "2018-10-26T18:37:37.337445: step 27069, loss 0.00170106, acc 1\n",
      "2018-10-26T18:37:37.505995: step 27070, loss 1.55176e-05, acc 1\n",
      "2018-10-26T18:37:37.692497: step 27071, loss 0.00223694, acc 1\n",
      "2018-10-26T18:37:37.859051: step 27072, loss 1.89978e-06, acc 1\n",
      "2018-10-26T18:37:38.043559: step 27073, loss 0.000436252, acc 1\n",
      "2018-10-26T18:37:38.217095: step 27074, loss 0.00152784, acc 1\n",
      "2018-10-26T18:37:38.395618: step 27075, loss 1.76951e-07, acc 1\n",
      "2018-10-26T18:37:38.565165: step 27076, loss 2.44361e-06, acc 1\n",
      "2018-10-26T18:37:38.744685: step 27077, loss 1.3095e-05, acc 1\n",
      "2018-10-26T18:37:38.912238: step 27078, loss 8.43738e-06, acc 1\n",
      "2018-10-26T18:37:39.097742: step 27079, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:37:39.270281: step 27080, loss 6.33298e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:37:39.453793: step 27081, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:39.623337: step 27082, loss 7.45056e-08, acc 1\n",
      "2018-10-26T18:37:39.795876: step 27083, loss 1.78245e-06, acc 1\n",
      "2018-10-26T18:37:39.962431: step 27084, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:37:40.145941: step 27085, loss 1.49563e-06, acc 1\n",
      "2018-10-26T18:37:40.322469: step 27086, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:37:40.494012: step 27087, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:37:40.678519: step 27088, loss 0.000305301, acc 1\n",
      "2018-10-26T18:37:40.853051: step 27089, loss 3.35273e-07, acc 1\n",
      "2018-10-26T18:37:41.034567: step 27090, loss 1.78433e-06, acc 1\n",
      "2018-10-26T18:37:41.208103: step 27091, loss 7.14161e-06, acc 1\n",
      "2018-10-26T18:37:41.388621: step 27092, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:37:41.556173: step 27093, loss 6.51924e-08, acc 1\n",
      "2018-10-26T18:37:41.728720: step 27094, loss 7.65233e-06, acc 1\n",
      "2018-10-26T18:37:41.905240: step 27095, loss 3.4645e-07, acc 1\n",
      "2018-10-26T18:37:42.085766: step 27096, loss 1.64423e-05, acc 1\n",
      "2018-10-26T18:37:42.252313: step 27097, loss 0.00232661, acc 1\n",
      "2018-10-26T18:37:42.428842: step 27098, loss 0.00014525, acc 1\n",
      "2018-10-26T18:37:42.599385: step 27099, loss 0, acc 1\n",
      "2018-10-26T18:37:42.773919: step 27100, loss 4.65661e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:37:43.231697: step 27100, loss 9.29178, acc 0.723265\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27100\n",
      "\n",
      "2018-10-26T18:37:43.599210: step 27101, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:37:43.764769: step 27102, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:37:43.938304: step 27103, loss 0, acc 1\n",
      "2018-10-26T18:37:44.111840: step 27104, loss 4.63792e-07, acc 1\n",
      "2018-10-26T18:37:44.303329: step 27105, loss 0.129853, acc 0.984375\n",
      "2018-10-26T18:37:44.543687: step 27106, loss 8.16976e-06, acc 1\n",
      "2018-10-26T18:37:44.710241: step 27107, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:37:44.889762: step 27108, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:37:45.063298: step 27109, loss 0, acc 1\n",
      "2018-10-26T18:37:45.240824: step 27110, loss 3.52332e-05, acc 1\n",
      "2018-10-26T18:37:45.409374: step 27111, loss 0, acc 1\n",
      "2018-10-26T18:37:45.583907: step 27112, loss 1.8794e-05, acc 1\n",
      "2018-10-26T18:37:45.752457: step 27113, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:37:45.925993: step 27114, loss 2.52741e-06, acc 1\n",
      "2018-10-26T18:37:46.095540: step 27115, loss 0.0150932, acc 0.984375\n",
      "2018-10-26T18:37:46.272070: step 27116, loss 8.42992e-06, acc 1\n",
      "2018-10-26T18:37:46.445605: step 27117, loss 1.60924e-06, acc 1\n",
      "2018-10-26T18:37:46.619141: step 27118, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:37:46.788688: step 27119, loss 1.25213e-05, acc 1\n",
      "2018-10-26T18:37:46.975190: step 27120, loss 7.87889e-06, acc 1\n",
      "2018-10-26T18:37:47.139750: step 27121, loss 3.04515e-06, acc 1\n",
      "2018-10-26T18:37:47.319271: step 27122, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:37:47.484828: step 27123, loss 3.53305e-06, acc 1\n",
      "2018-10-26T18:37:47.665346: step 27124, loss 3.49596e-06, acc 1\n",
      "2018-10-26T18:37:47.839904: step 27125, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:37:48.024387: step 27126, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:37:48.198920: step 27127, loss 0.0022073, acc 1\n",
      "2018-10-26T18:37:48.377443: step 27128, loss 4.86823e-06, acc 1\n",
      "2018-10-26T18:37:48.544996: step 27129, loss 0.0291938, acc 0.984375\n",
      "2018-10-26T18:37:48.723519: step 27130, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:37:48.890074: step 27131, loss 0.0155016, acc 0.984375\n",
      "2018-10-26T18:37:49.064607: step 27132, loss 1.91841e-06, acc 1\n",
      "2018-10-26T18:37:49.231162: step 27133, loss 0.000578258, acc 1\n",
      "2018-10-26T18:37:49.421653: step 27134, loss 5.30848e-07, acc 1\n",
      "2018-10-26T18:37:49.588209: step 27135, loss 4.55352e-06, acc 1\n",
      "2018-10-26T18:37:49.770721: step 27136, loss 8.94044e-07, acc 1\n",
      "2018-10-26T18:37:49.936287: step 27137, loss 0, acc 1\n",
      "2018-10-26T18:37:50.108818: step 27138, loss 1.06169e-06, acc 1\n",
      "2018-10-26T18:37:50.283351: step 27139, loss 2.62632e-07, acc 1\n",
      "2018-10-26T18:37:50.465864: step 27140, loss 5.8713e-05, acc 1\n",
      "2018-10-26T18:37:50.641396: step 27141, loss 8.58637e-06, acc 1\n",
      "2018-10-26T18:37:50.816926: step 27142, loss 0, acc 1\n",
      "2018-10-26T18:37:50.990462: step 27143, loss 0.00933159, acc 1\n",
      "2018-10-26T18:37:51.167989: step 27144, loss 2.01164e-07, acc 1\n",
      "2018-10-26T18:37:51.341524: step 27145, loss 9.49947e-08, acc 1\n",
      "2018-10-26T18:37:51.528025: step 27146, loss 3.65046e-05, acc 1\n",
      "2018-10-26T18:37:51.706549: step 27147, loss 0, acc 1\n",
      "2018-10-26T18:37:51.882081: step 27148, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:37:52.051627: step 27149, loss 2.81258e-07, acc 1\n",
      "2018-10-26T18:37:52.228156: step 27150, loss 3.97364e-09, acc 1\n",
      "2018-10-26T18:37:52.394711: step 27151, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:37:52.571238: step 27152, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:37:52.738792: step 27153, loss 0, acc 1\n",
      "2018-10-26T18:37:52.917314: step 27154, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:37:53.085863: step 27155, loss 0.0215006, acc 0.984375\n",
      "2018-10-26T18:37:53.264386: step 27156, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:37:53.480808: step 27157, loss 2.63491e-05, acc 1\n",
      "2018-10-26T18:37:53.661325: step 27158, loss 0, acc 1\n",
      "2018-10-26T18:37:53.843838: step 27159, loss 0.039484, acc 0.984375\n",
      "2018-10-26T18:37:54.028345: step 27160, loss 0.00532434, acc 1\n",
      "2018-10-26T18:37:54.231802: step 27161, loss 7.24552e-07, acc 1\n",
      "2018-10-26T18:37:54.417305: step 27162, loss 0, acc 1\n",
      "2018-10-26T18:37:54.598821: step 27163, loss 4.20608e-05, acc 1\n",
      "2018-10-26T18:37:54.823221: step 27164, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:37:54.998753: step 27165, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:37:55.174283: step 27166, loss 0.000259267, acc 1\n",
      "2018-10-26T18:37:55.351809: step 27167, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:37:55.533324: step 27168, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:37:55.705863: step 27169, loss 1.05422e-06, acc 1\n",
      "2018-10-26T18:37:55.889373: step 27170, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:37:56.056926: step 27171, loss 1.1399e-06, acc 1\n",
      "2018-10-26T18:37:56.235448: step 27172, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:37:56.407988: step 27173, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:37:56.589502: step 27174, loss 1.76569e-06, acc 1\n",
      "2018-10-26T18:37:56.773012: step 27175, loss 2.21691e-05, acc 1\n",
      "2018-10-26T18:37:56.973477: step 27176, loss 4.53879e-06, acc 1\n",
      "2018-10-26T18:37:57.180923: step 27177, loss 3.03332e-05, acc 1\n",
      "2018-10-26T18:37:57.382384: step 27178, loss 3.52571e-06, acc 1\n",
      "2018-10-26T18:37:57.579856: step 27179, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:37:57.772343: step 27180, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:37:57.983780: step 27181, loss 3.25371e-06, acc 1\n",
      "2018-10-26T18:37:58.158310: step 27182, loss 5.23342e-06, acc 1\n",
      "2018-10-26T18:37:58.363783: step 27183, loss 1.23593e-05, acc 1\n",
      "2018-10-26T18:37:58.546274: step 27184, loss 2.49593e-07, acc 1\n",
      "2018-10-26T18:37:58.752723: step 27185, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:37:58.978122: step 27186, loss 7.26431e-08, acc 1\n",
      "2018-10-26T18:37:59.187560: step 27187, loss 0, acc 1\n",
      "2018-10-26T18:37:59.369075: step 27188, loss 8.12093e-07, acc 1\n",
      "2018-10-26T18:37:59.596469: step 27189, loss 3.52188e-06, acc 1\n",
      "2018-10-26T18:37:59.774991: step 27190, loss 1.05981e-06, acc 1\n",
      "2018-10-26T18:38:00.004379: step 27191, loss 4.65626e-06, acc 1\n",
      "2018-10-26T18:38:00.186890: step 27192, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:38:00.408300: step 27193, loss 5.91092e-06, acc 1\n",
      "2018-10-26T18:38:00.630705: step 27194, loss 0.000791096, acc 1\n",
      "2018-10-26T18:38:00.817206: step 27195, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:38:01.057566: step 27196, loss 3.4086e-07, acc 1\n",
      "2018-10-26T18:38:01.276979: step 27197, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:38:01.456499: step 27198, loss 0.00173819, acc 1\n",
      "2018-10-26T18:38:01.698851: step 27199, loss 0.0184509, acc 0.984375\n",
      "2018-10-26T18:38:01.868398: step 27200, loss 3.72529e-09, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:38:02.375045: step 27200, loss 9.7877, acc 0.707317\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27200\n",
      "\n",
      "2018-10-26T18:38:02.745055: step 27201, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:02.924576: step 27202, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:38:03.157953: step 27203, loss 8.75442e-08, acc 1\n",
      "2018-10-26T18:38:03.343457: step 27204, loss 0, acc 1\n",
      "2018-10-26T18:38:03.629692: step 27205, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:38:03.878029: step 27206, loss 0.0891418, acc 0.96875\n",
      "2018-10-26T18:38:04.055554: step 27207, loss 1.76951e-07, acc 1\n",
      "2018-10-26T18:38:04.266989: step 27208, loss 8.27354e-06, acc 1\n",
      "2018-10-26T18:38:04.438532: step 27209, loss 9.872e-08, acc 1\n",
      "2018-10-26T18:38:04.675896: step 27210, loss 8.69359e-06, acc 1\n",
      "2018-10-26T18:38:04.858409: step 27211, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:05.030948: step 27212, loss 1.85227e-05, acc 1\n",
      "2018-10-26T18:38:05.201493: step 27213, loss 5.01044e-07, acc 1\n",
      "2018-10-26T18:38:05.409937: step 27214, loss 0.000207414, acc 1\n",
      "2018-10-26T18:38:05.580480: step 27215, loss 4.49392e-06, acc 1\n",
      "2018-10-26T18:38:05.754016: step 27216, loss 3.6485e-06, acc 1\n",
      "2018-10-26T18:38:05.943509: step 27217, loss 2.77083e-05, acc 1\n",
      "2018-10-26T18:38:06.134000: step 27218, loss 1.45286e-07, acc 1\n",
      "2018-10-26T18:38:06.318508: step 27219, loss 0, acc 1\n",
      "2018-10-26T18:38:06.492044: step 27220, loss 0.00156329, acc 1\n",
      "2018-10-26T18:38:06.700487: step 27221, loss 0.000648236, acc 1\n",
      "2018-10-26T18:38:06.867042: step 27222, loss 8.5472e-06, acc 1\n",
      "2018-10-26T18:38:07.066510: step 27223, loss 2.79006e-06, acc 1\n",
      "2018-10-26T18:38:07.240045: step 27224, loss 7.54353e-07, acc 1\n",
      "2018-10-26T18:38:07.462452: step 27225, loss 4.6794e-05, acc 1\n",
      "2018-10-26T18:38:07.637982: step 27226, loss 4.6246e-06, acc 1\n",
      "2018-10-26T18:38:07.809524: step 27227, loss 1.11755e-06, acc 1\n",
      "2018-10-26T18:38:07.984058: step 27228, loss 4.61868e-06, acc 1\n",
      "2018-10-26T18:38:08.161583: step 27229, loss 2.8312e-07, acc 1\n",
      "2018-10-26T18:38:08.329136: step 27230, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:38:08.503669: step 27231, loss 7.08784e-05, acc 1\n",
      "2018-10-26T18:38:08.714107: step 27232, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:38:08.923548: step 27233, loss 0.047544, acc 0.984375\n",
      "2018-10-26T18:38:09.103068: step 27234, loss 0.000960133, acc 1\n",
      "2018-10-26T18:38:09.325473: step 27235, loss 4.16762e-05, acc 1\n",
      "2018-10-26T18:38:09.494024: step 27236, loss 2.34494e-06, acc 1\n",
      "2018-10-26T18:38:09.701469: step 27237, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:09.880989: step 27238, loss 0.000179878, acc 1\n",
      "2018-10-26T18:38:10.057517: step 27239, loss 8.08367e-07, acc 1\n",
      "2018-10-26T18:38:10.273941: step 27240, loss 1.18833e-06, acc 1\n",
      "2018-10-26T18:38:10.447476: step 27241, loss 1.3634e-06, acc 1\n",
      "2018-10-26T18:38:10.622009: step 27242, loss 4.54607e-06, acc 1\n",
      "2018-10-26T18:38:10.837434: step 27243, loss 0, acc 1\n",
      "2018-10-26T18:38:11.009973: step 27244, loss 0.000182221, acc 1\n",
      "2018-10-26T18:38:11.187499: step 27245, loss 0, acc 1\n",
      "2018-10-26T18:38:11.369014: step 27246, loss 0.000265066, acc 1\n",
      "2018-10-26T18:38:11.564492: step 27247, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:38:11.776925: step 27248, loss 0, acc 1\n",
      "2018-10-26T18:38:11.955447: step 27249, loss 1.47149e-07, acc 1\n",
      "2018-10-26T18:38:12.150924: step 27250, loss 2.09537e-05, acc 1\n",
      "2018-10-26T18:38:12.353384: step 27251, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:12.531906: step 27252, loss 0.0147908, acc 0.984375\n",
      "2018-10-26T18:38:12.726389: step 27253, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:38:12.908899: step 27254, loss 0, acc 1\n",
      "2018-10-26T18:38:13.093406: step 27255, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:38:13.316809: step 27256, loss 3.837e-07, acc 1\n",
      "2018-10-26T18:38:13.485359: step 27257, loss 4.60068e-07, acc 1\n",
      "2018-10-26T18:38:13.664880: step 27258, loss 4.3543e-06, acc 1\n",
      "2018-10-26T18:38:13.836422: step 27259, loss 3.16626e-06, acc 1\n",
      "2018-10-26T18:38:14.010955: step 27260, loss 0.00916396, acc 1\n",
      "2018-10-26T18:38:14.179504: step 27261, loss 0, acc 1\n",
      "2018-10-26T18:38:14.367003: step 27262, loss 2.08538e-05, acc 1\n",
      "2018-10-26T18:38:14.534556: step 27263, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:14.715073: step 27264, loss 0, acc 1\n",
      "2018-10-26T18:38:14.892599: step 27265, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:38:15.070124: step 27266, loss 8.00918e-07, acc 1\n",
      "2018-10-26T18:38:15.249644: step 27267, loss 0.00145382, acc 1\n",
      "2018-10-26T18:38:15.429166: step 27268, loss 1.87588e-05, acc 1\n",
      "2018-10-26T18:38:15.600707: step 27269, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:15.776238: step 27270, loss 5.08494e-07, acc 1\n",
      "2018-10-26T18:38:15.952766: step 27271, loss 8.00936e-08, acc 1\n",
      "2018-10-26T18:38:16.129295: step 27272, loss 4.74967e-07, acc 1\n",
      "2018-10-26T18:38:16.305823: step 27273, loss 2.12341e-07, acc 1\n",
      "2018-10-26T18:38:16.487338: step 27274, loss 4.93337e-06, acc 1\n",
      "2018-10-26T18:38:16.666859: step 27275, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:38:16.832416: step 27276, loss 3.62138e-05, acc 1\n",
      "2018-10-26T18:38:17.012933: step 27277, loss 0, acc 1\n",
      "2018-10-26T18:38:17.184476: step 27278, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:38:17.356016: step 27279, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:38:17.533543: step 27280, loss 3.0349e-05, acc 1\n",
      "2018-10-26T18:38:17.713063: step 27281, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:17.880615: step 27282, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:38:18.062130: step 27283, loss 4.88006e-07, acc 1\n",
      "2018-10-26T18:38:18.232674: step 27284, loss 0.000536675, acc 1\n",
      "2018-10-26T18:38:18.406211: step 27285, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:38:18.576756: step 27286, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:38:18.767246: step 27287, loss 0.000150012, acc 1\n",
      "2018-10-26T18:38:18.946767: step 27288, loss 1.06396e-05, acc 1\n",
      "2018-10-26T18:38:19.124292: step 27289, loss 1.34326e-05, acc 1\n",
      "2018-10-26T18:38:19.300821: step 27290, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:38:19.468373: step 27291, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:38:19.649896: step 27292, loss 2.40451e-06, acc 1\n",
      "2018-10-26T18:38:19.817440: step 27293, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:38:19.994966: step 27294, loss 0, acc 1\n",
      "2018-10-26T18:38:20.164513: step 27295, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:38:20.351014: step 27296, loss 0.000513278, acc 1\n",
      "2018-10-26T18:38:20.525548: step 27297, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:38:20.703074: step 27298, loss 2.52368e-06, acc 1\n",
      "2018-10-26T18:38:20.882594: step 27299, loss 0, acc 1\n",
      "2018-10-26T18:38:21.044164: step 27300, loss 2.38418e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:38:21.505932: step 27300, loss 9.65427, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27300\n",
      "\n",
      "2018-10-26T18:38:21.881838: step 27301, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:22.067342: step 27302, loss 8.69869e-05, acc 1\n",
      "2018-10-26T18:38:22.234895: step 27303, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:38:22.419403: step 27304, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:38:22.651780: step 27305, loss 2.26304e-06, acc 1\n",
      "2018-10-26T18:38:22.850250: step 27306, loss 1.55547e-05, acc 1\n",
      "2018-10-26T18:38:23.036752: step 27307, loss 1.47148e-07, acc 1\n",
      "2018-10-26T18:38:23.228241: step 27308, loss 8.38188e-08, acc 1\n",
      "2018-10-26T18:38:23.409756: step 27309, loss 2.63175e-06, acc 1\n",
      "2018-10-26T18:38:23.592268: step 27310, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:38:23.762812: step 27311, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:38:23.943329: step 27312, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:38:24.109884: step 27313, loss 8.26423e-06, acc 1\n",
      "2018-10-26T18:38:24.288408: step 27314, loss 0.00117941, acc 1\n",
      "2018-10-26T18:38:24.461944: step 27315, loss 8.10972e-06, acc 1\n",
      "2018-10-26T18:38:24.656425: step 27316, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:24.840932: step 27317, loss 0, acc 1\n",
      "2018-10-26T18:38:25.023444: step 27318, loss 1.75269e-06, acc 1\n",
      "2018-10-26T18:38:25.208948: step 27319, loss 7.20837e-07, acc 1\n",
      "2018-10-26T18:38:25.377498: step 27320, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:38:25.563002: step 27321, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:25.729558: step 27322, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:25.914064: step 27323, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:26.081616: step 27324, loss 8.74388e-06, acc 1\n",
      "2018-10-26T18:38:26.266124: step 27325, loss 0, acc 1\n",
      "2018-10-26T18:38:26.439660: step 27326, loss 0, acc 1\n",
      "2018-10-26T18:38:26.617185: step 27327, loss 6.6508e-05, acc 1\n",
      "2018-10-26T18:38:26.788728: step 27328, loss 9.96498e-07, acc 1\n",
      "2018-10-26T18:38:26.974232: step 27329, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:38:27.144776: step 27330, loss 0, acc 1\n",
      "2018-10-26T18:38:27.329283: step 27331, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:38:27.499828: step 27332, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:38:27.690318: step 27333, loss 6.12798e-07, acc 1\n",
      "2018-10-26T18:38:27.863871: step 27334, loss 1.56462e-07, acc 1\n",
      "2018-10-26T18:38:28.039386: step 27335, loss 1.45093e-06, acc 1\n",
      "2018-10-26T18:38:28.211925: step 27336, loss 0.000202539, acc 1\n",
      "2018-10-26T18:38:28.383467: step 27337, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:28.551019: step 27338, loss 7.12549e-05, acc 1\n",
      "2018-10-26T18:38:28.747495: step 27339, loss 0.00145933, acc 1\n",
      "2018-10-26T18:38:28.947958: step 27340, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:38:29.143438: step 27341, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:38:29.327943: step 27342, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:38:29.501480: step 27343, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:38:29.680002: step 27344, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:38:29.871491: step 27345, loss 1.20587e-05, acc 1\n",
      "2018-10-26T18:38:30.055998: step 27346, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:38:30.230531: step 27347, loss 0, acc 1\n",
      "2018-10-26T18:38:30.404068: step 27348, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:38:30.574612: step 27349, loss 4.06051e-07, acc 1\n",
      "2018-10-26T18:38:30.755130: step 27350, loss 1.24779e-05, acc 1\n",
      "2018-10-26T18:38:30.922683: step 27351, loss 1.93949e-05, acc 1\n",
      "2018-10-26T18:38:31.097216: step 27352, loss 5.70424e-06, acc 1\n",
      "2018-10-26T18:38:31.269754: step 27353, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:38:31.434315: step 27354, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:31.611859: step 27355, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:38:31.792359: step 27356, loss 1.30422e-05, acc 1\n",
      "2018-10-26T18:38:31.963900: step 27357, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:38:32.139431: step 27358, loss 0, acc 1\n",
      "2018-10-26T18:38:32.308978: step 27359, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:38:32.490493: step 27360, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:38:32.659043: step 27361, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:32.841556: step 27362, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:38:33.010104: step 27363, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:33.186633: step 27364, loss 0, acc 1\n",
      "2018-10-26T18:38:33.355184: step 27365, loss 0, acc 1\n",
      "2018-10-26T18:38:33.532709: step 27366, loss 0, acc 1\n",
      "2018-10-26T18:38:33.701259: step 27367, loss 0.0382636, acc 0.984375\n",
      "2018-10-26T18:38:33.885767: step 27368, loss 4.99181e-07, acc 1\n",
      "2018-10-26T18:38:34.065286: step 27369, loss 2.06264e-05, acc 1\n",
      "2018-10-26T18:38:34.243809: step 27370, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:38:34.418342: step 27371, loss 1.24979e-06, acc 1\n",
      "2018-10-26T18:38:34.596866: step 27372, loss 0, acc 1\n",
      "2018-10-26T18:38:34.781372: step 27373, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:38:34.955906: step 27374, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:38:35.137421: step 27375, loss 1.89852e-05, acc 1\n",
      "2018-10-26T18:38:35.323923: step 27376, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:38:35.500451: step 27377, loss 3.83862e-06, acc 1\n",
      "2018-10-26T18:38:35.678977: step 27378, loss 0.0124109, acc 0.984375\n",
      "2018-10-26T18:38:35.852511: step 27379, loss 3.20372e-07, acc 1\n",
      "2018-10-26T18:38:36.028041: step 27380, loss 5.04773e-07, acc 1\n",
      "2018-10-26T18:38:36.197589: step 27381, loss 3.20622e-05, acc 1\n",
      "2018-10-26T18:38:36.378106: step 27382, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:36.548650: step 27383, loss 2.62052e-06, acc 1\n",
      "2018-10-26T18:38:36.726176: step 27384, loss 0, acc 1\n",
      "2018-10-26T18:38:36.892731: step 27385, loss 0, acc 1\n",
      "2018-10-26T18:38:37.071254: step 27386, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:38:37.237809: step 27387, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:38:37.414337: step 27388, loss 0.0459927, acc 0.984375\n",
      "2018-10-26T18:38:37.583893: step 27389, loss 7.24552e-07, acc 1\n",
      "2018-10-26T18:38:37.765400: step 27390, loss 0, acc 1\n",
      "2018-10-26T18:38:37.937939: step 27391, loss 1.07471e-06, acc 1\n",
      "2018-10-26T18:38:38.122454: step 27392, loss 0.0364737, acc 0.984375\n",
      "2018-10-26T18:38:38.289998: step 27393, loss 3.03609e-07, acc 1\n",
      "2018-10-26T18:38:38.473509: step 27394, loss 0.00119463, acc 1\n",
      "2018-10-26T18:38:38.653027: step 27395, loss 0, acc 1\n",
      "2018-10-26T18:38:38.827561: step 27396, loss 5.03443e-05, acc 1\n",
      "2018-10-26T18:38:39.003092: step 27397, loss 1.39115e-05, acc 1\n",
      "2018-10-26T18:38:39.169647: step 27398, loss 0, acc 1\n",
      "2018-10-26T18:38:39.345179: step 27399, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:38:39.518716: step 27400, loss 0.000198094, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:38:39.982475: step 27400, loss 9.6734, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27400\n",
      "\n",
      "2018-10-26T18:38:40.366133: step 27401, loss 0.00012421, acc 1\n",
      "2018-10-26T18:38:40.546651: step 27402, loss 0, acc 1\n",
      "2018-10-26T18:38:40.714202: step 27403, loss 0.000108263, acc 1\n",
      "2018-10-26T18:38:40.899706: step 27404, loss 0, acc 1\n",
      "2018-10-26T18:38:41.093190: step 27405, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:41.325569: step 27406, loss 2.43811e-06, acc 1\n",
      "2018-10-26T18:38:41.500103: step 27407, loss 4.32652e-06, acc 1\n",
      "2018-10-26T18:38:41.686603: step 27408, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:38:41.853159: step 27409, loss 2.55162e-06, acc 1\n",
      "2018-10-26T18:38:42.035671: step 27410, loss 3.45856e-06, acc 1\n",
      "2018-10-26T18:38:42.208210: step 27411, loss 3.27822e-07, acc 1\n",
      "2018-10-26T18:38:42.384739: step 27412, loss 6.33299e-08, acc 1\n",
      "2018-10-26T18:38:42.560269: step 27413, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:38:42.736798: step 27414, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:42.911332: step 27415, loss 0.000209791, acc 1\n",
      "2018-10-26T18:38:43.096839: step 27416, loss 0.000123483, acc 1\n",
      "2018-10-26T18:38:43.264389: step 27417, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:38:43.441913: step 27418, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:38:43.608469: step 27419, loss 0.241756, acc 0.984375\n",
      "2018-10-26T18:38:43.790982: step 27420, loss 0, acc 1\n",
      "2018-10-26T18:38:43.961526: step 27421, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:38:44.151019: step 27422, loss 7.30731e-06, acc 1\n",
      "2018-10-26T18:38:44.328547: step 27423, loss 9.59233e-07, acc 1\n",
      "2018-10-26T18:38:44.508065: step 27424, loss 4.39709e-06, acc 1\n",
      "2018-10-26T18:38:44.678610: step 27425, loss 9.90896e-07, acc 1\n",
      "2018-10-26T18:38:44.855138: step 27426, loss 2.53318e-07, acc 1\n",
      "2018-10-26T18:38:45.032664: step 27427, loss 2.55181e-07, acc 1\n",
      "2018-10-26T18:38:45.213181: step 27428, loss 3.99313e-05, acc 1\n",
      "2018-10-26T18:38:45.378739: step 27429, loss 0.00404431, acc 1\n",
      "2018-10-26T18:38:45.559257: step 27430, loss 5.60767e-05, acc 1\n",
      "2018-10-26T18:38:45.729802: step 27431, loss 0.000648116, acc 1\n",
      "2018-10-26T18:38:45.912313: step 27432, loss 1.97439e-07, acc 1\n",
      "2018-10-26T18:38:46.087845: step 27433, loss 2.02651e-05, acc 1\n",
      "2018-10-26T18:38:46.268362: step 27434, loss 0.000211499, acc 1\n",
      "2018-10-26T18:38:46.450875: step 27435, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:38:46.621419: step 27436, loss 1.09732e-05, acc 1\n",
      "2018-10-26T18:38:46.803931: step 27437, loss 9.27573e-07, acc 1\n",
      "2018-10-26T18:38:46.971484: step 27438, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:38:47.157987: step 27439, loss 8.14323e-06, acc 1\n",
      "2018-10-26T18:38:47.324541: step 27440, loss 1.10193e-05, acc 1\n",
      "2018-10-26T18:38:47.503063: step 27441, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:47.673608: step 27442, loss 0, acc 1\n",
      "2018-10-26T18:38:47.856120: step 27443, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:38:48.028659: step 27444, loss 0, acc 1\n",
      "2018-10-26T18:38:48.223141: step 27445, loss 1.86824e-05, acc 1\n",
      "2018-10-26T18:38:48.390691: step 27446, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:38:48.572223: step 27447, loss 8.39192e-05, acc 1\n",
      "2018-10-26T18:38:48.744746: step 27448, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:38:48.934239: step 27449, loss 1.73403e-06, acc 1\n",
      "2018-10-26T18:38:49.102789: step 27450, loss 0, acc 1\n",
      "2018-10-26T18:38:49.284305: step 27451, loss 3.97067e-06, acc 1\n",
      "2018-10-26T18:38:49.456843: step 27452, loss 2.50009e-05, acc 1\n",
      "2018-10-26T18:38:49.643345: step 27453, loss 2.48085e-06, acc 1\n",
      "2018-10-26T18:38:49.805910: step 27454, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:38:49.991415: step 27455, loss 7.45056e-08, acc 1\n",
      "2018-10-26T18:38:50.161959: step 27456, loss 2.44005e-07, acc 1\n",
      "2018-10-26T18:38:50.346466: step 27457, loss 0.000469957, acc 1\n",
      "2018-10-26T18:38:50.513021: step 27458, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:38:50.701518: step 27459, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:38:50.876051: step 27460, loss 3.91155e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:38:51.060558: step 27461, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:38:51.224121: step 27462, loss 0.000204453, acc 1\n",
      "2018-10-26T18:38:51.402644: step 27463, loss 9.68573e-08, acc 1\n",
      "2018-10-26T18:38:51.579174: step 27464, loss 0, acc 1\n",
      "2018-10-26T18:38:51.756699: step 27465, loss 1.02815e-06, acc 1\n",
      "2018-10-26T18:38:51.930236: step 27466, loss 0.00199225, acc 1\n",
      "2018-10-26T18:38:52.106763: step 27467, loss 2.4773e-07, acc 1\n",
      "2018-10-26T18:38:52.275624: step 27468, loss 1.21071e-07, acc 1\n",
      "2018-10-26T18:38:52.453149: step 27469, loss 3.14785e-07, acc 1\n",
      "2018-10-26T18:38:52.620702: step 27470, loss 4.98746e-05, acc 1\n",
      "2018-10-26T18:38:52.800223: step 27471, loss 0.000586285, acc 1\n",
      "2018-10-26T18:38:52.967774: step 27472, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:38:53.143305: step 27473, loss 5.86725e-07, acc 1\n",
      "2018-10-26T18:38:53.311856: step 27474, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:38:53.495365: step 27475, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:38:53.662922: step 27476, loss 0.00013467, acc 1\n",
      "2018-10-26T18:38:53.848435: step 27477, loss 4.73108e-07, acc 1\n",
      "2018-10-26T18:38:54.015973: step 27478, loss 1.04305e-06, acc 1\n",
      "2018-10-26T18:38:54.192502: step 27479, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:38:54.366039: step 27480, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:38:54.551543: step 27481, loss 4.41198e-06, acc 1\n",
      "2018-10-26T18:38:54.783926: step 27482, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:38:54.977405: step 27483, loss 5.9231e-07, acc 1\n",
      "2018-10-26T18:38:55.154930: step 27484, loss 8.46005e-06, acc 1\n",
      "2018-10-26T18:38:55.330462: step 27485, loss 0.000103192, acc 1\n",
      "2018-10-26T18:38:55.505993: step 27486, loss 9.30746e-06, acc 1\n",
      "2018-10-26T18:38:55.680526: step 27487, loss 5.07314e-06, acc 1\n",
      "2018-10-26T18:38:55.856057: step 27488, loss 5.32253e-06, acc 1\n",
      "2018-10-26T18:38:56.033584: step 27489, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:38:56.221082: step 27490, loss 0, acc 1\n",
      "2018-10-26T18:38:56.393833: step 27491, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:38:56.570362: step 27492, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:56.743916: step 27493, loss 0, acc 1\n",
      "2018-10-26T18:38:56.918432: step 27494, loss 0, acc 1\n",
      "2018-10-26T18:38:57.092965: step 27495, loss 6.59957e-05, acc 1\n",
      "2018-10-26T18:38:57.261515: step 27496, loss 1.97242e-06, acc 1\n",
      "2018-10-26T18:38:57.438043: step 27497, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:38:57.608587: step 27498, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:38:57.790103: step 27499, loss 0, acc 1\n",
      "2018-10-26T18:38:57.958652: step 27500, loss 1.71363e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:38:58.419421: step 27500, loss 9.57934, acc 0.716698\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27500\n",
      "\n",
      "2018-10-26T18:38:58.816352: step 27501, loss 2.02381e-05, acc 1\n",
      "2018-10-26T18:38:58.995871: step 27502, loss 2.71944e-07, acc 1\n",
      "2018-10-26T18:38:59.166416: step 27503, loss 9.80271e-06, acc 1\n",
      "2018-10-26T18:38:59.353916: step 27504, loss 0.00111916, acc 1\n",
      "2018-10-26T18:38:59.601254: step 27505, loss 1.52234e-05, acc 1\n",
      "2018-10-26T18:38:59.784764: step 27506, loss 0.0926768, acc 0.984375\n",
      "2018-10-26T18:38:59.964285: step 27507, loss 0.000675504, acc 1\n",
      "2018-10-26T18:39:00.159763: step 27508, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:00.344269: step 27509, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:00.526782: step 27510, loss 0.000220074, acc 1\n",
      "2018-10-26T18:39:00.701315: step 27511, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:39:00.881833: step 27512, loss 5.84786e-06, acc 1\n",
      "2018-10-26T18:39:01.055369: step 27513, loss 5.28983e-07, acc 1\n",
      "2018-10-26T18:39:01.236884: step 27514, loss 1.23116e-06, acc 1\n",
      "2018-10-26T18:39:01.402442: step 27515, loss 9.61097e-07, acc 1\n",
      "2018-10-26T18:39:01.581962: step 27516, loss 1.82715e-06, acc 1\n",
      "2018-10-26T18:39:01.751509: step 27517, loss 0, acc 1\n",
      "2018-10-26T18:39:01.938011: step 27518, loss 1.32247e-07, acc 1\n",
      "2018-10-26T18:39:02.105564: step 27519, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:39:02.290070: step 27520, loss 0, acc 1\n",
      "2018-10-26T18:39:02.480561: step 27521, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:39:02.693991: step 27522, loss 2.12341e-07, acc 1\n",
      "2018-10-26T18:39:02.870520: step 27523, loss 6.86423e-06, acc 1\n",
      "2018-10-26T18:39:03.102899: step 27524, loss 7.6554e-07, acc 1\n",
      "2018-10-26T18:39:03.270450: step 27525, loss 9.872e-08, acc 1\n",
      "2018-10-26T18:39:03.485901: step 27526, loss 9.02281e-05, acc 1\n",
      "2018-10-26T18:39:03.663400: step 27527, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:39:03.839929: step 27528, loss 3.43387e-05, acc 1\n",
      "2018-10-26T18:39:04.053359: step 27529, loss 2.5754e-05, acc 1\n",
      "2018-10-26T18:39:04.247840: step 27530, loss 1.51343e-05, acc 1\n",
      "2018-10-26T18:39:04.437333: step 27531, loss 2.64846e-06, acc 1\n",
      "2018-10-26T18:39:04.644779: step 27532, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:39:04.820309: step 27533, loss 0, acc 1\n",
      "2018-10-26T18:39:05.015788: step 27534, loss 0, acc 1\n",
      "2018-10-26T18:39:05.193313: step 27535, loss 0, acc 1\n",
      "2018-10-26T18:39:05.412727: step 27536, loss 0, acc 1\n",
      "2018-10-26T18:39:05.580279: step 27537, loss 3.61349e-07, acc 1\n",
      "2018-10-26T18:39:05.775758: step 27538, loss 6.77988e-07, acc 1\n",
      "2018-10-26T18:39:05.963256: step 27539, loss 0, acc 1\n",
      "2018-10-26T18:39:06.212590: step 27540, loss 0, acc 1\n",
      "2018-10-26T18:39:06.394104: step 27541, loss 0, acc 1\n",
      "2018-10-26T18:39:06.625486: step 27542, loss 7.22355e-06, acc 1\n",
      "2018-10-26T18:39:06.799024: step 27543, loss 0.000110779, acc 1\n",
      "2018-10-26T18:39:07.001482: step 27544, loss 2.62632e-07, acc 1\n",
      "2018-10-26T18:39:07.198955: step 27545, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:39:07.399419: step 27546, loss 0.000550332, acc 1\n",
      "2018-10-26T18:39:07.619832: step 27547, loss 0, acc 1\n",
      "2018-10-26T18:39:07.795361: step 27548, loss 5.02906e-07, acc 1\n",
      "2018-10-26T18:39:07.977874: step 27549, loss 0, acc 1\n",
      "2018-10-26T18:39:08.191303: step 27550, loss 2.95403e-06, acc 1\n",
      "2018-10-26T18:39:08.366834: step 27551, loss 2.66715e-06, acc 1\n",
      "2018-10-26T18:39:08.547352: step 27552, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:39:08.763773: step 27553, loss 4.69383e-07, acc 1\n",
      "2018-10-26T18:39:08.957257: step 27554, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:39:09.150739: step 27555, loss 0, acc 1\n",
      "2018-10-26T18:39:09.369156: step 27556, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:39:09.579593: step 27557, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:39:09.798010: step 27558, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:39:09.983514: step 27559, loss 8.6053e-07, acc 1\n",
      "2018-10-26T18:39:10.165030: step 27560, loss 1.53288e-06, acc 1\n",
      "2018-10-26T18:39:10.421345: step 27561, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:39:10.587899: step 27562, loss 1.1814e-05, acc 1\n",
      "2018-10-26T18:39:10.793350: step 27563, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:39:10.966888: step 27564, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:39:11.148402: step 27565, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:39:11.343880: step 27566, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:11.545342: step 27567, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:39:11.729849: step 27568, loss 0.00685586, acc 1\n",
      "2018-10-26T18:39:11.909369: step 27569, loss 1.42113e-06, acc 1\n",
      "2018-10-26T18:39:12.132772: step 27570, loss 0, acc 1\n",
      "2018-10-26T18:39:12.308303: step 27571, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:39:12.489818: step 27572, loss 2.49191e-05, acc 1\n",
      "2018-10-26T18:39:12.673328: step 27573, loss 0.00138112, acc 1\n",
      "2018-10-26T18:39:12.884764: step 27574, loss 0.000171812, acc 1\n",
      "2018-10-26T18:39:13.094204: step 27575, loss 2.5477e-05, acc 1\n",
      "2018-10-26T18:39:13.276716: step 27576, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:39:13.498124: step 27577, loss 0, acc 1\n",
      "2018-10-26T18:39:13.676647: step 27578, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:39:13.862152: step 27579, loss 0.0241659, acc 0.984375\n",
      "2018-10-26T18:39:14.036685: step 27580, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:39:14.221193: step 27581, loss 0.00239439, acc 1\n",
      "2018-10-26T18:39:14.429635: step 27582, loss 3.17735e-06, acc 1\n",
      "2018-10-26T18:39:14.656030: step 27583, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:39:14.832559: step 27584, loss 3.01171e-06, acc 1\n",
      "2018-10-26T18:39:15.047983: step 27585, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:39:15.219525: step 27586, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:39:15.439935: step 27587, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:39:15.610481: step 27588, loss 1.11759e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:39:15.791995: step 27589, loss 4.09777e-07, acc 1\n",
      "2018-10-26T18:39:15.999442: step 27590, loss 0.00144457, acc 1\n",
      "2018-10-26T18:39:16.170983: step 27591, loss 7.04065e-07, acc 1\n",
      "2018-10-26T18:39:16.349505: step 27592, loss 1.13841e-05, acc 1\n",
      "2018-10-26T18:39:16.559943: step 27593, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:39:16.739465: step 27594, loss 6.51775e-05, acc 1\n",
      "2018-10-26T18:39:16.913000: step 27595, loss 0.000129755, acc 1\n",
      "2018-10-26T18:39:17.126430: step 27596, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:39:17.300963: step 27597, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:39:17.476495: step 27598, loss 8.78192e-06, acc 1\n",
      "2018-10-26T18:39:17.643049: step 27599, loss 5.15947e-07, acc 1\n",
      "2018-10-26T18:39:17.844511: step 27600, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:39:18.321237: step 27600, loss 10.067, acc 0.709193\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27600\n",
      "\n",
      "2018-10-26T18:39:18.700749: step 27601, loss 6.12329e-06, acc 1\n",
      "2018-10-26T18:39:18.878274: step 27602, loss 3.40268e-06, acc 1\n",
      "2018-10-26T18:39:19.055800: step 27603, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:39:19.229337: step 27604, loss 5.23316e-06, acc 1\n",
      "2018-10-26T18:39:19.429800: step 27605, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:39:19.674148: step 27606, loss 1.73171e-05, acc 1\n",
      "2018-10-26T18:39:19.855664: step 27607, loss 0, acc 1\n",
      "2018-10-26T18:39:20.038175: step 27608, loss 9.85308e-07, acc 1\n",
      "2018-10-26T18:39:20.213706: step 27609, loss 7.14534e-06, acc 1\n",
      "2018-10-26T18:39:20.385247: step 27610, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:39:20.562774: step 27611, loss 0, acc 1\n",
      "2018-10-26T18:39:20.734315: step 27612, loss 5.36433e-07, acc 1\n",
      "2018-10-26T18:39:20.909846: step 27613, loss 5.83085e-06, acc 1\n",
      "2018-10-26T18:39:21.081388: step 27614, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:39:21.258914: step 27615, loss 1.18345e-05, acc 1\n",
      "2018-10-26T18:39:21.428461: step 27616, loss 0.0381838, acc 0.984375\n",
      "2018-10-26T18:39:21.606989: step 27617, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:21.783511: step 27618, loss 0, acc 1\n",
      "2018-10-26T18:39:21.956051: step 27619, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:39:22.126595: step 27620, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:39:22.300132: step 27621, loss 1.13785e-05, acc 1\n",
      "2018-10-26T18:39:22.471673: step 27622, loss 0.000122311, acc 1\n",
      "2018-10-26T18:39:22.657177: step 27623, loss 4.23508e-06, acc 1\n",
      "2018-10-26T18:39:22.827722: step 27624, loss 0, acc 1\n",
      "2018-10-26T18:39:23.008239: step 27625, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:39:23.181775: step 27626, loss 0, acc 1\n",
      "2018-10-26T18:39:23.352321: step 27627, loss 1.56462e-07, acc 1\n",
      "2018-10-26T18:39:23.532838: step 27628, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:23.702386: step 27629, loss 5.03407e-05, acc 1\n",
      "2018-10-26T18:39:23.882903: step 27630, loss 0.000595818, acc 1\n",
      "2018-10-26T18:39:24.055442: step 27631, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:39:24.235960: step 27632, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:39:24.408498: step 27633, loss 0.127424, acc 0.984375\n",
      "2018-10-26T18:39:24.589016: step 27634, loss 1.47148e-07, acc 1\n",
      "2018-10-26T18:39:24.757565: step 27635, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:39:24.944067: step 27636, loss 2.00785e-06, acc 1\n",
      "2018-10-26T18:39:25.114613: step 27637, loss 3.14784e-07, acc 1\n",
      "2018-10-26T18:39:25.300116: step 27638, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:39:25.470660: step 27639, loss 8.47468e-06, acc 1\n",
      "2018-10-26T18:39:25.650181: step 27640, loss 0.000187865, acc 1\n",
      "2018-10-26T18:39:25.826709: step 27641, loss 1.86263e-07, acc 1\n",
      "2018-10-26T18:39:26.007260: step 27642, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:39:26.182758: step 27643, loss 7.47415e-06, acc 1\n",
      "2018-10-26T18:39:26.364273: step 27644, loss 0.000307013, acc 1\n",
      "2018-10-26T18:39:26.552771: step 27645, loss 4.75938e-05, acc 1\n",
      "2018-10-26T18:39:26.721320: step 27646, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:39:26.898844: step 27647, loss 7.45056e-08, acc 1\n",
      "2018-10-26T18:39:27.067394: step 27648, loss 2.90735e-06, acc 1\n",
      "2018-10-26T18:39:27.244920: step 27649, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:39:27.414467: step 27650, loss 0.0116088, acc 0.984375\n",
      "2018-10-26T18:39:27.597976: step 27651, loss 8.88468e-07, acc 1\n",
      "2018-10-26T18:39:27.768521: step 27652, loss 8.50066e-06, acc 1\n",
      "2018-10-26T18:39:27.948042: step 27653, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:28.114596: step 27654, loss 7.67392e-07, acc 1\n",
      "2018-10-26T18:39:28.293119: step 27655, loss 0.0541378, acc 0.984375\n",
      "2018-10-26T18:39:28.469648: step 27656, loss 4.30537e-05, acc 1\n",
      "2018-10-26T18:39:28.646176: step 27657, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:39:28.824699: step 27658, loss 3.00056e-06, acc 1\n",
      "2018-10-26T18:39:29.008210: step 27659, loss 0.00149873, acc 1\n",
      "2018-10-26T18:39:29.185734: step 27660, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:29.356278: step 27661, loss 1.99104e-06, acc 1\n",
      "2018-10-26T18:39:29.535799: step 27662, loss 4.05797e-05, acc 1\n",
      "2018-10-26T18:39:29.715319: step 27663, loss 2.64493e-07, acc 1\n",
      "2018-10-26T18:39:29.896834: step 27664, loss 0, acc 1\n",
      "2018-10-26T18:39:30.086327: step 27665, loss 0, acc 1\n",
      "2018-10-26T18:39:30.260862: step 27666, loss 6.51924e-08, acc 1\n",
      "2018-10-26T18:39:30.445369: step 27667, loss 0.000204423, acc 1\n",
      "2018-10-26T18:39:30.630874: step 27668, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:39:30.798426: step 27669, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:39:30.976948: step 27670, loss 0, acc 1\n",
      "2018-10-26T18:39:31.143504: step 27671, loss 1.76126e-05, acc 1\n",
      "2018-10-26T18:39:31.323023: step 27672, loss 2.7567e-07, acc 1\n",
      "2018-10-26T18:39:31.497559: step 27673, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:39:31.676081: step 27674, loss 1.75638e-06, acc 1\n",
      "2018-10-26T18:39:31.849617: step 27675, loss 1.3187e-06, acc 1\n",
      "2018-10-26T18:39:32.033126: step 27676, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:39:32.203672: step 27677, loss 1.5871e-05, acc 1\n",
      "2018-10-26T18:39:32.382194: step 27678, loss 0.0163227, acc 0.984375\n",
      "2018-10-26T18:39:32.546754: step 27679, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:39:32.734253: step 27680, loss 2.60768e-07, acc 1\n",
      "2018-10-26T18:39:32.904797: step 27681, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:39:33.088307: step 27682, loss 3.2596e-07, acc 1\n",
      "2018-10-26T18:39:33.258851: step 27683, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:39:33.447348: step 27684, loss 0.0103963, acc 1\n",
      "2018-10-26T18:39:33.613903: step 27685, loss 0, acc 1\n",
      "2018-10-26T18:39:33.791429: step 27686, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:39:33.963968: step 27687, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:34.151467: step 27688, loss 2.05066e-06, acc 1\n",
      "2018-10-26T18:39:34.319018: step 27689, loss 9.14105e-06, acc 1\n",
      "2018-10-26T18:39:34.498539: step 27690, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:34.666724: step 27691, loss 0.000414287, acc 1\n",
      "2018-10-26T18:39:34.847242: step 27692, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:39:35.025766: step 27693, loss 1.48118e-05, acc 1\n",
      "2018-10-26T18:39:35.205286: step 27694, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:39:35.382812: step 27695, loss 0, acc 1\n",
      "2018-10-26T18:39:35.555351: step 27696, loss 9.32535e-06, acc 1\n",
      "2018-10-26T18:39:35.733873: step 27697, loss 1.46025e-06, acc 1\n",
      "2018-10-26T18:39:35.905414: step 27698, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:39:36.085940: step 27699, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:39:36.264455: step 27700, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:39:36.741182: step 27700, loss 9.80004, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27700\n",
      "\n",
      "2018-10-26T18:39:37.119022: step 27701, loss 6.38874e-07, acc 1\n",
      "2018-10-26T18:39:37.305505: step 27702, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:39:37.475051: step 27703, loss 1.0325e-05, acc 1\n",
      "2018-10-26T18:39:37.656567: step 27704, loss 6.16525e-07, acc 1\n",
      "2018-10-26T18:39:37.842072: step 27705, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:39:38.097389: step 27706, loss 2.62631e-07, acc 1\n",
      "2018-10-26T18:39:38.267934: step 27707, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:39:38.455432: step 27708, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:39:38.628978: step 27709, loss 2.88707e-07, acc 1\n",
      "2018-10-26T18:39:38.819461: step 27710, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:39:38.986014: step 27711, loss 0, acc 1\n",
      "2018-10-26T18:39:39.174511: step 27712, loss 0.000371994, acc 1\n",
      "2018-10-26T18:39:39.343061: step 27713, loss 3.72529e-09, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:39:39.525573: step 27714, loss 0.000186198, acc 1\n",
      "2018-10-26T18:39:39.699109: step 27715, loss 0, acc 1\n",
      "2018-10-26T18:39:39.871648: step 27716, loss 0.00407193, acc 1\n",
      "2018-10-26T18:39:40.039201: step 27717, loss 0.000339911, acc 1\n",
      "2018-10-26T18:39:40.220716: step 27718, loss 3.06405e-05, acc 1\n",
      "2018-10-26T18:39:40.388268: step 27719, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:39:40.564797: step 27720, loss 2.86845e-07, acc 1\n",
      "2018-10-26T18:39:40.730354: step 27721, loss 1.93517e-06, acc 1\n",
      "2018-10-26T18:39:40.903891: step 27722, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:39:41.076429: step 27723, loss 4.52557e-06, acc 1\n",
      "2018-10-26T18:39:41.254952: step 27724, loss 3.57624e-07, acc 1\n",
      "2018-10-26T18:39:41.421507: step 27725, loss 3.57703e-05, acc 1\n",
      "2018-10-26T18:39:41.595044: step 27726, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:39:41.761599: step 27727, loss 2.38417e-07, acc 1\n",
      "2018-10-26T18:39:41.934138: step 27728, loss 3.59486e-07, acc 1\n",
      "2018-10-26T18:39:42.104681: step 27729, loss 3.89989e-06, acc 1\n",
      "2018-10-26T18:39:42.289189: step 27730, loss 0.00310937, acc 1\n",
      "2018-10-26T18:39:42.458737: step 27731, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:39:42.635265: step 27732, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:39:42.806806: step 27733, loss 6.87307e-07, acc 1\n",
      "2018-10-26T18:39:42.979345: step 27734, loss 0, acc 1\n",
      "2018-10-26T18:39:43.147895: step 27735, loss 0, acc 1\n",
      "2018-10-26T18:39:43.322428: step 27736, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:39:43.488984: step 27737, loss 4.65574e-05, acc 1\n",
      "2018-10-26T18:39:43.668504: step 27738, loss 1.05609e-06, acc 1\n",
      "2018-10-26T18:39:43.837054: step 27739, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:39:44.009592: step 27740, loss 8.43761e-07, acc 1\n",
      "2018-10-26T18:39:44.178142: step 27741, loss 9.87199e-08, acc 1\n",
      "2018-10-26T18:39:44.354671: step 27742, loss 0, acc 1\n",
      "2018-10-26T18:39:44.520229: step 27743, loss 1.11755e-06, acc 1\n",
      "2018-10-26T18:39:44.694762: step 27744, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:39:44.862314: step 27745, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:39:45.035851: step 27746, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:39:45.207392: step 27747, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:39:45.388907: step 27748, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:39:45.556459: step 27749, loss 6.14661e-07, acc 1\n",
      "2018-10-26T18:39:45.731991: step 27750, loss 2.48351e-07, acc 1\n",
      "2018-10-26T18:39:45.909516: step 27751, loss 1.77129e-06, acc 1\n",
      "2018-10-26T18:39:46.087042: step 27752, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:39:46.263572: step 27753, loss 0.000192888, acc 1\n",
      "2018-10-26T18:39:46.443091: step 27754, loss 0, acc 1\n",
      "2018-10-26T18:39:46.625604: step 27755, loss 0.000416135, acc 1\n",
      "2018-10-26T18:39:46.799139: step 27756, loss 1.32248e-07, acc 1\n",
      "2018-10-26T18:39:46.977663: step 27757, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:39:47.159178: step 27758, loss 1.1613e-05, acc 1\n",
      "2018-10-26T18:39:47.345682: step 27759, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:39:47.524201: step 27760, loss 0, acc 1\n",
      "2018-10-26T18:39:47.702725: step 27761, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:39:47.884240: step 27762, loss 6.95782e-06, acc 1\n",
      "2018-10-26T18:39:48.059771: step 27763, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:39:48.244278: step 27764, loss 8.56815e-08, acc 1\n",
      "2018-10-26T18:39:48.421804: step 27765, loss 0, acc 1\n",
      "2018-10-26T18:39:48.607309: step 27766, loss 1.42486e-06, acc 1\n",
      "2018-10-26T18:39:48.776856: step 27767, loss 9.77862e-07, acc 1\n",
      "2018-10-26T18:39:48.967345: step 27768, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:39:49.134899: step 27769, loss 6.76136e-07, acc 1\n",
      "2018-10-26T18:39:49.310429: step 27770, loss 0.000313098, acc 1\n",
      "2018-10-26T18:39:49.484962: step 27771, loss 0, acc 1\n",
      "2018-10-26T18:39:49.661491: step 27772, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:39:49.830041: step 27773, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:39:50.005573: step 27774, loss 0, acc 1\n",
      "2018-10-26T18:39:50.170132: step 27775, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:39:50.343669: step 27776, loss 2.30951e-06, acc 1\n",
      "2018-10-26T18:39:50.511221: step 27777, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:39:50.690741: step 27778, loss 0, acc 1\n",
      "2018-10-26T18:39:50.858294: step 27779, loss 3.48313e-07, acc 1\n",
      "2018-10-26T18:39:51.035819: step 27780, loss 3.93014e-07, acc 1\n",
      "2018-10-26T18:39:51.202375: step 27781, loss 1.12668e-05, acc 1\n",
      "2018-10-26T18:39:51.382893: step 27782, loss 7.17102e-07, acc 1\n",
      "2018-10-26T18:39:51.558423: step 27783, loss 0, acc 1\n",
      "2018-10-26T18:39:51.743927: step 27784, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:39:51.921453: step 27785, loss 6.70552e-08, acc 1\n",
      "2018-10-26T18:39:52.105960: step 27786, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:39:52.277502: step 27787, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:39:52.456024: step 27788, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:52.622580: step 27789, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:39:52.801103: step 27790, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:39:52.968655: step 27791, loss 1.25473e-05, acc 1\n",
      "2018-10-26T18:39:53.157151: step 27792, loss 2.17356e-06, acc 1\n",
      "2018-10-26T18:39:53.328693: step 27793, loss 0, acc 1\n",
      "2018-10-26T18:39:53.506219: step 27794, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:39:53.674769: step 27795, loss 8.12104e-07, acc 1\n",
      "2018-10-26T18:39:53.849303: step 27796, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:39:54.018850: step 27797, loss 0, acc 1\n",
      "2018-10-26T18:39:54.197372: step 27798, loss 0, acc 1\n",
      "2018-10-26T18:39:54.360934: step 27799, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:39:54.535469: step 27800, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:39:54.994243: step 27800, loss 9.79769, acc 0.719512\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27800\n",
      "\n",
      "2018-10-26T18:39:55.369643: step 27801, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:55.549162: step 27802, loss 3.87215e-06, acc 1\n",
      "2018-10-26T18:39:55.728683: step 27803, loss 0, acc 1\n",
      "2018-10-26T18:39:55.895238: step 27804, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:39:56.110662: step 27805, loss 1.70053e-06, acc 1\n",
      "2018-10-26T18:39:56.375953: step 27806, loss 7.82309e-08, acc 1\n",
      "2018-10-26T18:39:56.549489: step 27807, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:39:56.735022: step 27808, loss 1.05981e-06, acc 1\n",
      "2018-10-26T18:39:56.900552: step 27809, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:39:57.084086: step 27810, loss 0, acc 1\n",
      "2018-10-26T18:39:57.252612: step 27811, loss 1.70676e-05, acc 1\n",
      "2018-10-26T18:39:57.435123: step 27812, loss 1.32802e-06, acc 1\n",
      "2018-10-26T18:39:57.608660: step 27813, loss 6.36719e-06, acc 1\n",
      "2018-10-26T18:39:57.787183: step 27814, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:39:57.956730: step 27815, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:39:58.139242: step 27816, loss 6.30193e-06, acc 1\n",
      "2018-10-26T18:39:58.311781: step 27817, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:39:58.492298: step 27818, loss 0, acc 1\n",
      "2018-10-26T18:39:58.665835: step 27819, loss 0.00224053, acc 1\n",
      "2018-10-26T18:39:58.842364: step 27820, loss 1.13618e-06, acc 1\n",
      "2018-10-26T18:39:59.026870: step 27821, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:39:59.190433: step 27822, loss 1.56461e-07, acc 1\n",
      "2018-10-26T18:39:59.378930: step 27823, loss 1.68374e-06, acc 1\n",
      "2018-10-26T18:39:59.549474: step 27824, loss 2.75625e-05, acc 1\n",
      "2018-10-26T18:39:59.729991: step 27825, loss 0, acc 1\n",
      "2018-10-26T18:39:59.898541: step 27826, loss 0, acc 1\n",
      "2018-10-26T18:40:00.092042: step 27827, loss 8.50066e-06, acc 1\n",
      "2018-10-26T18:40:00.264564: step 27828, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:40:00.446078: step 27829, loss 3.9766e-05, acc 1\n",
      "2018-10-26T18:40:00.618311: step 27830, loss 0, acc 1\n",
      "2018-10-26T18:40:00.807804: step 27831, loss 0, acc 1\n",
      "2018-10-26T18:40:00.981340: step 27832, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:40:01.162855: step 27833, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:40:01.331404: step 27834, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:40:01.511923: step 27835, loss 0.000861762, acc 1\n",
      "2018-10-26T18:40:01.678477: step 27836, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:40:01.861987: step 27837, loss 3.44586e-07, acc 1\n",
      "2018-10-26T18:40:02.031534: step 27838, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:40:02.214070: step 27839, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:40:02.378608: step 27840, loss 3.60753e-06, acc 1\n",
      "2018-10-26T18:40:02.563114: step 27841, loss 4.09383e-06, acc 1\n",
      "2018-10-26T18:40:02.730667: step 27842, loss 0.00170161, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:40:02.924150: step 27843, loss 0.000413819, acc 1\n",
      "2018-10-26T18:40:03.096688: step 27844, loss 0.000913368, acc 1\n",
      "2018-10-26T18:40:03.279201: step 27845, loss 0, acc 1\n",
      "2018-10-26T18:40:03.454731: step 27846, loss 2.60962e-05, acc 1\n",
      "2018-10-26T18:40:03.625276: step 27847, loss 1.62049e-07, acc 1\n",
      "2018-10-26T18:40:03.794823: step 27848, loss 0, acc 1\n",
      "2018-10-26T18:40:03.988306: step 27849, loss 0, acc 1\n",
      "2018-10-26T18:40:04.153863: step 27850, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:04.338370: step 27851, loss 2.03762e-06, acc 1\n",
      "2018-10-26T18:40:04.509912: step 27852, loss 1.09802e-05, acc 1\n",
      "2018-10-26T18:40:04.680457: step 27853, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:40:04.851001: step 27854, loss 2.92036e-06, acc 1\n",
      "2018-10-26T18:40:05.036505: step 27855, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:05.204058: step 27856, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:05.380587: step 27857, loss 2.60015e-06, acc 1\n",
      "2018-10-26T18:40:05.556118: step 27858, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:05.731648: step 27859, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:40:05.906181: step 27860, loss 9.73564e-05, acc 1\n",
      "2018-10-26T18:40:06.083708: step 27861, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:06.260236: step 27862, loss 8.81714e-06, acc 1\n",
      "2018-10-26T18:40:06.427788: step 27863, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:06.601325: step 27864, loss 5.42614e-05, acc 1\n",
      "2018-10-26T18:40:06.772867: step 27865, loss 1.43423e-07, acc 1\n",
      "2018-10-26T18:40:06.946403: step 27866, loss 0.00012532, acc 1\n",
      "2018-10-26T18:40:07.112958: step 27867, loss 2.16065e-07, acc 1\n",
      "2018-10-26T18:40:07.291481: step 27868, loss 0, acc 1\n",
      "2018-10-26T18:40:07.464019: step 27869, loss 0.00126948, acc 1\n",
      "2018-10-26T18:40:07.639552: step 27870, loss 3.0433e-06, acc 1\n",
      "2018-10-26T18:40:07.808101: step 27871, loss 1.01699e-06, acc 1\n",
      "2018-10-26T18:40:07.992607: step 27872, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:40:08.166144: step 27873, loss 1.67637e-07, acc 1\n",
      "2018-10-26T18:40:08.388562: step 27874, loss 4.71242e-07, acc 1\n",
      "2018-10-26T18:40:08.596992: step 27875, loss 9.33171e-07, acc 1\n",
      "2018-10-26T18:40:08.811420: step 27876, loss 0.000853752, acc 1\n",
      "2018-10-26T18:40:08.996925: step 27877, loss 0, acc 1\n",
      "2018-10-26T18:40:09.201378: step 27878, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:40:09.394860: step 27879, loss 1.78172e-05, acc 1\n",
      "2018-10-26T18:40:09.600313: step 27880, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:40:09.781827: step 27881, loss 1.68937e-06, acc 1\n",
      "2018-10-26T18:40:09.962344: step 27882, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:40:10.171785: step 27883, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:40:10.339337: step 27884, loss 0.00107846, acc 1\n",
      "2018-10-26T18:40:10.527834: step 27885, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:40:10.718624: step 27886, loss 1.86264e-07, acc 1\n",
      "2018-10-26T18:40:10.906124: step 27887, loss 0.0470091, acc 0.984375\n",
      "2018-10-26T18:40:11.084645: step 27888, loss 0, acc 1\n",
      "2018-10-26T18:40:11.282118: step 27889, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:40:11.464630: step 27890, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:40:11.652129: step 27891, loss 0, acc 1\n",
      "2018-10-26T18:40:11.843617: step 27892, loss 2.80561e-05, acc 1\n",
      "2018-10-26T18:40:12.083975: step 27893, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:40:12.271474: step 27894, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:40:12.471938: step 27895, loss 2.03029e-05, acc 1\n",
      "2018-10-26T18:40:12.708308: step 27896, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:40:12.900792: step 27897, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:40:13.122201: step 27898, loss 6.03941e-05, acc 1\n",
      "2018-10-26T18:40:13.288756: step 27899, loss 3.35274e-07, acc 1\n",
      "2018-10-26T18:40:13.480244: step 27900, loss 3.78746e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:40:13.977916: step 27900, loss 10.175, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-27900\n",
      "\n",
      "2018-10-26T18:40:14.387395: step 27901, loss 0.000129259, acc 1\n",
      "2018-10-26T18:40:14.594841: step 27902, loss 2.34579e-05, acc 1\n",
      "2018-10-26T18:40:14.836196: step 27903, loss 0, acc 1\n",
      "2018-10-26T18:40:15.056606: step 27904, loss 1.13619e-06, acc 1\n",
      "2018-10-26T18:40:15.329877: step 27905, loss 7.95329e-07, acc 1\n",
      "2018-10-26T18:40:15.547298: step 27906, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:40:15.762720: step 27907, loss 1.78289e-05, acc 1\n",
      "2018-10-26T18:40:15.971164: step 27908, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:40:16.181600: step 27909, loss 8.23273e-07, acc 1\n",
      "2018-10-26T18:40:16.370098: step 27910, loss 0.000117979, acc 1\n",
      "2018-10-26T18:40:16.590509: step 27911, loss 2.08049e-06, acc 1\n",
      "2018-10-26T18:40:16.799949: step 27912, loss 0.00147116, acc 1\n",
      "2018-10-26T18:40:16.982462: step 27913, loss 1.93366e-05, acc 1\n",
      "2018-10-26T18:40:17.187913: step 27914, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:40:17.380398: step 27915, loss 8.21925e-05, acc 1\n",
      "2018-10-26T18:40:17.562911: step 27916, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:40:17.743428: step 27917, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:17.947882: step 27918, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:40:18.122415: step 27919, loss 9.04437e-06, acc 1\n",
      "2018-10-26T18:40:18.305925: step 27920, loss 3.00057e-06, acc 1\n",
      "2018-10-26T18:40:18.471483: step 27921, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:40:18.682918: step 27922, loss 3.61349e-07, acc 1\n",
      "2018-10-26T18:40:18.855458: step 27923, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:40:19.046946: step 27924, loss 0.000199684, acc 1\n",
      "2018-10-26T18:40:19.213501: step 27925, loss 0.000256208, acc 1\n",
      "2018-10-26T18:40:19.394018: step 27926, loss 0.0523501, acc 0.984375\n",
      "2018-10-26T18:40:19.568552: step 27927, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:40:19.749069: step 27928, loss 0, acc 1\n",
      "2018-10-26T18:40:19.959507: step 27929, loss 0, acc 1\n",
      "2018-10-26T18:40:20.156979: step 27930, loss 0.00114719, acc 1\n",
      "2018-10-26T18:40:20.354452: step 27931, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:40:20.572869: step 27932, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:40:20.748400: step 27933, loss 2.64811e-05, acc 1\n",
      "2018-10-26T18:40:20.960832: step 27934, loss 1.30815e-05, acc 1\n",
      "2018-10-26T18:40:21.142347: step 27935, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:40:21.349793: step 27936, loss 1.26535e-05, acc 1\n",
      "2018-10-26T18:40:21.536294: step 27937, loss 1.91851e-07, acc 1\n",
      "2018-10-26T18:40:21.719804: step 27938, loss 2.07299e-06, acc 1\n",
      "2018-10-26T18:40:21.934231: step 27939, loss 5.08494e-07, acc 1\n",
      "2018-10-26T18:40:22.119735: step 27940, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:40:22.302248: step 27941, loss 1.22976e-05, acc 1\n",
      "2018-10-26T18:40:22.508695: step 27942, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:40:22.678242: step 27943, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:40:22.860756: step 27944, loss 0, acc 1\n",
      "2018-10-26T18:40:23.054238: step 27945, loss 8.86608e-07, acc 1\n",
      "2018-10-26T18:40:23.255700: step 27946, loss 1.45841e-06, acc 1\n",
      "2018-10-26T18:40:23.429237: step 27947, loss 4.04166e-06, acc 1\n",
      "2018-10-26T18:40:23.604768: step 27948, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:40:23.812213: step 27949, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:40:23.999712: step 27950, loss 2.65972e-06, acc 1\n",
      "2018-10-26T18:40:24.182225: step 27951, loss 0, acc 1\n",
      "2018-10-26T18:40:24.370721: step 27952, loss 0.00548126, acc 1\n",
      "2018-10-26T18:40:24.600108: step 27953, loss 0.000600458, acc 1\n",
      "2018-10-26T18:40:24.770653: step 27954, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:40:24.954162: step 27955, loss 0, acc 1\n",
      "2018-10-26T18:40:25.117725: step 27956, loss 5.40076e-06, acc 1\n",
      "2018-10-26T18:40:25.302233: step 27957, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:40:25.468787: step 27958, loss 1.2591e-06, acc 1\n",
      "2018-10-26T18:40:25.654292: step 27959, loss 9.31321e-08, acc 1\n",
      "2018-10-26T18:40:25.824835: step 27960, loss 0.0626305, acc 0.984375\n",
      "2018-10-26T18:40:26.011338: step 27961, loss 4.17228e-07, acc 1\n",
      "2018-10-26T18:40:26.182879: step 27962, loss 0, acc 1\n",
      "2018-10-26T18:40:26.365391: step 27963, loss 0, acc 1\n",
      "2018-10-26T18:40:26.536934: step 27964, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:40:26.715457: step 27965, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:40:26.887996: step 27966, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:40:27.077489: step 27967, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:40:27.249031: step 27968, loss 8.00936e-08, acc 1\n",
      "2018-10-26T18:40:27.430546: step 27969, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:40:27.605079: step 27970, loss 0.000883191, acc 1\n",
      "2018-10-26T18:40:27.787592: step 27971, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:40:27.965117: step 27972, loss 0, acc 1\n",
      "2018-10-26T18:40:28.136659: step 27973, loss 5.10356e-07, acc 1\n",
      "2018-10-26T18:40:28.317177: step 27974, loss 5.53869e-06, acc 1\n",
      "2018-10-26T18:40:28.488719: step 27975, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:40:28.660260: step 27976, loss 9.07094e-07, acc 1\n",
      "2018-10-26T18:40:28.831802: step 27977, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:40:29.012320: step 27978, loss 0.0231886, acc 0.984375\n",
      "2018-10-26T18:40:29.183861: step 27979, loss 0.00833636, acc 1\n",
      "2018-10-26T18:40:29.359392: step 27980, loss 0.00145564, acc 1\n",
      "2018-10-26T18:40:29.524950: step 27981, loss 3.2849e-05, acc 1\n",
      "2018-10-26T18:40:29.698486: step 27982, loss 0, acc 1\n",
      "2018-10-26T18:40:29.864045: step 27983, loss 8.10601e-06, acc 1\n",
      "2018-10-26T18:40:30.038578: step 27984, loss 6.77989e-07, acc 1\n",
      "2018-10-26T18:40:30.207128: step 27985, loss 0, acc 1\n",
      "2018-10-26T18:40:30.383656: step 27986, loss 0, acc 1\n",
      "2018-10-26T18:40:30.556221: step 27987, loss 0, acc 1\n",
      "2018-10-26T18:40:30.727736: step 27988, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:40:30.898281: step 27989, loss 0.000226618, acc 1\n",
      "2018-10-26T18:40:31.067827: step 27990, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:40:31.235379: step 27991, loss 0, acc 1\n",
      "2018-10-26T18:40:31.415897: step 27992, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:31.588436: step 27993, loss 0, acc 1\n",
      "2018-10-26T18:40:31.761973: step 27994, loss 2.03604e-05, acc 1\n",
      "2018-10-26T18:40:31.937503: step 27995, loss 0.000108835, acc 1\n",
      "2018-10-26T18:40:32.117023: step 27996, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:40:32.285574: step 27997, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:40:32.462102: step 27998, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:40:32.634641: step 27999, loss 2.27242e-07, acc 1\n",
      "2018-10-26T18:40:32.810172: step 28000, loss 9.46531e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:40:33.271938: step 28000, loss 9.91106, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28000\n",
      "\n",
      "2018-10-26T18:40:33.642921: step 28001, loss 3.34811e-05, acc 1\n",
      "2018-10-26T18:40:33.813465: step 28002, loss 2.28214e-05, acc 1\n",
      "2018-10-26T18:40:33.987002: step 28003, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:40:34.159541: step 28004, loss 2.62631e-07, acc 1\n",
      "2018-10-26T18:40:34.358010: step 28005, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:34.609339: step 28006, loss 0.0612483, acc 0.984375\n",
      "2018-10-26T18:40:34.776892: step 28007, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:40:34.959404: step 28008, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:40:35.126957: step 28009, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:40:35.311462: step 28010, loss 0, acc 1\n",
      "2018-10-26T18:40:35.480012: step 28011, loss 2.17054e-05, acc 1\n",
      "2018-10-26T18:40:35.656540: step 28012, loss 8.98003e-05, acc 1\n",
      "2018-10-26T18:40:35.835064: step 28013, loss 1.82538e-07, acc 1\n",
      "2018-10-26T18:40:36.017576: step 28014, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:40:36.185129: step 28015, loss 3.48311e-07, acc 1\n",
      "2018-10-26T18:40:36.365646: step 28016, loss 2.90902e-05, acc 1\n",
      "2018-10-26T18:40:36.532202: step 28017, loss 1.07659e-06, acc 1\n",
      "2018-10-26T18:40:36.704740: step 28018, loss 0, acc 1\n",
      "2018-10-26T18:40:36.869301: step 28019, loss 1.66919e-05, acc 1\n",
      "2018-10-26T18:40:37.047824: step 28020, loss 1.18462e-06, acc 1\n",
      "2018-10-26T18:40:37.215375: step 28021, loss 1.06205e-05, acc 1\n",
      "2018-10-26T18:40:37.395894: step 28022, loss 4.44595e-05, acc 1\n",
      "2018-10-26T18:40:37.574838: step 28023, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:37.758347: step 28024, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:40:37.936871: step 28025, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:40:38.109409: step 28026, loss 5.36433e-07, acc 1\n",
      "2018-10-26T18:40:38.286934: step 28027, loss 8.26429e-06, acc 1\n",
      "2018-10-26T18:40:38.462466: step 28028, loss 9.56176e-06, acc 1\n",
      "2018-10-26T18:40:38.635005: step 28029, loss 3.85563e-07, acc 1\n",
      "2018-10-26T18:40:38.800562: step 28030, loss 1.84401e-07, acc 1\n",
      "2018-10-26T18:40:38.985070: step 28031, loss 0, acc 1\n",
      "2018-10-26T18:40:39.158606: step 28032, loss 3.98569e-06, acc 1\n",
      "2018-10-26T18:40:39.339123: step 28033, loss 6.20249e-07, acc 1\n",
      "2018-10-26T18:40:39.506675: step 28034, loss 3.89288e-07, acc 1\n",
      "2018-10-26T18:40:39.685198: step 28035, loss 3.93552e-06, acc 1\n",
      "2018-10-26T18:40:39.855744: step 28036, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:40.037257: step 28037, loss 5.62694e-05, acc 1\n",
      "2018-10-26T18:40:40.207802: step 28038, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:40.386325: step 28039, loss 0, acc 1\n",
      "2018-10-26T18:40:40.550885: step 28040, loss 0, acc 1\n",
      "2018-10-26T18:40:40.728411: step 28041, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:40:40.901947: step 28042, loss 1.88095e-05, acc 1\n",
      "2018-10-26T18:40:41.091443: step 28043, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:40:41.269965: step 28044, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:40:41.449484: step 28045, loss 1.43231e-06, acc 1\n",
      "2018-10-26T18:40:41.619031: step 28046, loss 0, acc 1\n",
      "2018-10-26T18:40:41.798551: step 28047, loss 2.11028e-06, acc 1\n",
      "2018-10-26T18:40:41.972088: step 28048, loss 0.000548252, acc 1\n",
      "2018-10-26T18:40:42.152209: step 28049, loss 9.53108e-05, acc 1\n",
      "2018-10-26T18:40:42.325746: step 28050, loss 3.97364e-09, acc 1\n",
      "2018-10-26T18:40:42.510254: step 28051, loss 0.00165949, acc 1\n",
      "2018-10-26T18:40:42.676809: step 28052, loss 5.19783e-06, acc 1\n",
      "2018-10-26T18:40:42.856328: step 28053, loss 0.000115991, acc 1\n",
      "2018-10-26T18:40:43.029866: step 28054, loss 0, acc 1\n",
      "2018-10-26T18:40:43.207390: step 28055, loss 9.2757e-07, acc 1\n",
      "2018-10-26T18:40:43.374943: step 28056, loss 3.98369e-06, acc 1\n",
      "2018-10-26T18:40:43.560448: step 28057, loss 3.25931e-06, acc 1\n",
      "2018-10-26T18:40:43.735978: step 28058, loss 8.10231e-07, acc 1\n",
      "2018-10-26T18:40:43.917493: step 28059, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:40:44.086042: step 28060, loss 0, acc 1\n",
      "2018-10-26T18:40:44.261574: step 28061, loss 7.88259e-06, acc 1\n",
      "2018-10-26T18:40:44.429127: step 28062, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:40:44.614631: step 28063, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:44.785176: step 28064, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:40:44.969682: step 28065, loss 0, acc 1\n",
      "2018-10-26T18:40:45.145212: step 28066, loss 2.4773e-07, acc 1\n",
      "2018-10-26T18:40:45.320745: step 28067, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:40:45.491288: step 28068, loss 7.75973e-06, acc 1\n",
      "2018-10-26T18:40:45.705715: step 28069, loss 5.68102e-07, acc 1\n",
      "2018-10-26T18:40:45.875263: step 28070, loss 0, acc 1\n",
      "2018-10-26T18:40:46.058772: step 28071, loss 8.05061e-05, acc 1\n",
      "2018-10-26T18:40:46.227324: step 28072, loss 2.61507e-06, acc 1\n",
      "2018-10-26T18:40:46.408837: step 28073, loss 3.27353e-05, acc 1\n",
      "2018-10-26T18:40:46.580378: step 28074, loss 7.05934e-07, acc 1\n",
      "2018-10-26T18:40:46.765882: step 28075, loss 8.32583e-07, acc 1\n",
      "2018-10-26T18:40:46.929445: step 28076, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:40:47.113953: step 28077, loss 3.08611e-06, acc 1\n",
      "2018-10-26T18:40:47.279510: step 28078, loss 0.0735221, acc 0.984375\n",
      "2018-10-26T18:40:47.464017: step 28079, loss 2.76963e-06, acc 1\n",
      "2018-10-26T18:40:47.631570: step 28080, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:40:47.811091: step 28081, loss 2.94859e-05, acc 1\n",
      "2018-10-26T18:40:47.985624: step 28082, loss 3.94877e-07, acc 1\n",
      "2018-10-26T18:40:48.168135: step 28083, loss 3.85564e-07, acc 1\n",
      "2018-10-26T18:40:48.335688: step 28084, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:40:48.525193: step 28085, loss 4.15365e-07, acc 1\n",
      "2018-10-26T18:40:48.698718: step 28086, loss 2.01898e-06, acc 1\n",
      "2018-10-26T18:40:48.913146: step 28087, loss 2.54241e-06, acc 1\n",
      "2018-10-26T18:40:49.110642: step 28088, loss 1.10638e-06, acc 1\n",
      "2018-10-26T18:40:49.318066: step 28089, loss 1.43637e-05, acc 1\n",
      "2018-10-26T18:40:49.496594: step 28090, loss 4.53707e-06, acc 1\n",
      "2018-10-26T18:40:49.683089: step 28091, loss 0.0524989, acc 0.984375\n",
      "2018-10-26T18:40:49.891532: step 28092, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:40:50.078034: step 28093, loss 2.84982e-07, acc 1\n",
      "2018-10-26T18:40:50.259548: step 28094, loss 6.43532e-05, acc 1\n",
      "2018-10-26T18:40:50.445053: step 28095, loss 0.00250126, acc 1\n",
      "2018-10-26T18:40:50.627565: step 28096, loss 0.00011211, acc 1\n",
      "2018-10-26T18:40:50.797123: step 28097, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:40:50.976632: step 28098, loss 1.37272e-06, acc 1\n",
      "2018-10-26T18:40:51.145181: step 28099, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:40:51.333678: step 28100, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:40:51.801428: step 28100, loss 10.3061, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28100\n",
      "\n",
      "2018-10-26T18:40:52.126806: step 28101, loss 1.24422e-06, acc 1\n",
      "2018-10-26T18:40:52.303335: step 28102, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:40:52.484850: step 28103, loss 2.73606e-06, acc 1\n",
      "2018-10-26T18:40:52.651405: step 28104, loss 0.00151341, acc 1\n",
      "2018-10-26T18:40:52.835911: step 28105, loss 4.00465e-07, acc 1\n",
      "2018-10-26T18:40:53.075272: step 28106, loss 0, acc 1\n",
      "2018-10-26T18:40:53.243821: step 28107, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:53.426334: step 28108, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:53.592889: step 28109, loss 0.000593821, acc 1\n",
      "2018-10-26T18:40:53.773406: step 28110, loss 1.51986e-06, acc 1\n",
      "2018-10-26T18:40:53.951929: step 28111, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:54.134442: step 28112, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:40:54.312964: step 28113, loss 4.17228e-07, acc 1\n",
      "2018-10-26T18:40:54.495477: step 28114, loss 1.01325e-06, acc 1\n",
      "2018-10-26T18:40:54.667019: step 28115, loss 0.000238713, acc 1\n",
      "2018-10-26T18:40:54.850529: step 28116, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:40:55.024064: step 28117, loss 0.0204769, acc 0.984375\n",
      "2018-10-26T18:40:55.195606: step 28118, loss 4.14011e-06, acc 1\n",
      "2018-10-26T18:40:55.370141: step 28119, loss 0.000760323, acc 1\n",
      "2018-10-26T18:40:55.559634: step 28120, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:55.739155: step 28121, loss 0.000644582, acc 1\n",
      "2018-10-26T18:40:55.916681: step 28122, loss 0, acc 1\n",
      "2018-10-26T18:40:56.086227: step 28123, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:40:56.268739: step 28124, loss 7.57768e-05, acc 1\n",
      "2018-10-26T18:40:56.442276: step 28125, loss 3.98601e-07, acc 1\n",
      "2018-10-26T18:40:56.618805: step 28126, loss 0.00393148, acc 1\n",
      "2018-10-26T18:40:56.783365: step 28127, loss 2.45868e-07, acc 1\n",
      "2018-10-26T18:40:56.960890: step 28128, loss 0, acc 1\n",
      "2018-10-26T18:40:57.130437: step 28129, loss 1.3783e-06, acc 1\n",
      "2018-10-26T18:40:57.305968: step 28130, loss 1.73226e-07, acc 1\n",
      "2018-10-26T18:40:57.474517: step 28131, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:40:57.650049: step 28132, loss 1.39698e-07, acc 1\n",
      "2018-10-26T18:40:57.820593: step 28133, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:40:58.002109: step 28134, loss 0, acc 1\n",
      "2018-10-26T18:40:58.171655: step 28135, loss 5.397e-05, acc 1\n",
      "2018-10-26T18:40:58.349180: step 28136, loss 1.48725e-05, acc 1\n",
      "2018-10-26T18:40:58.520722: step 28137, loss 2.08044e-06, acc 1\n",
      "2018-10-26T18:40:58.735150: step 28138, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:40:58.907689: step 28139, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:40:59.086212: step 28140, loss 1.17582e-05, acc 1\n",
      "2018-10-26T18:40:59.258751: step 28141, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:40:59.433285: step 28142, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:40:59.603829: step 28143, loss 0.00933735, acc 1\n",
      "2018-10-26T18:40:59.778363: step 28144, loss 5.51338e-07, acc 1\n",
      "2018-10-26T18:40:59.952896: step 28145, loss 9.68573e-08, acc 1\n",
      "2018-10-26T18:41:00.137403: step 28146, loss 0, acc 1\n",
      "2018-10-26T18:41:00.308945: step 28147, loss 5.40074e-06, acc 1\n",
      "2018-10-26T18:41:00.487468: step 28148, loss 0, acc 1\n",
      "2018-10-26T18:41:00.656017: step 28149, loss 1.27958e-06, acc 1\n",
      "2018-10-26T18:41:00.836535: step 28150, loss 4.26543e-07, acc 1\n",
      "2018-10-26T18:41:01.005084: step 28151, loss 4.26542e-07, acc 1\n",
      "2018-10-26T18:41:01.186600: step 28152, loss 3.55763e-07, acc 1\n",
      "2018-10-26T18:41:01.357143: step 28153, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:41:01.537662: step 28154, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:41:01.711198: step 28155, loss 5.15946e-07, acc 1\n",
      "2018-10-26T18:41:01.885733: step 28156, loss 5.70289e-05, acc 1\n",
      "2018-10-26T18:41:02.056276: step 28157, loss 0.000212761, acc 1\n",
      "2018-10-26T18:41:02.234799: step 28158, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:02.406341: step 28159, loss 6.40743e-07, acc 1\n",
      "2018-10-26T18:41:02.580874: step 28160, loss 2.15973e-05, acc 1\n",
      "2018-10-26T18:41:02.762391: step 28161, loss 9.35582e-05, acc 1\n",
      "2018-10-26T18:41:02.938918: step 28162, loss 9.92758e-07, acc 1\n",
      "2018-10-26T18:41:03.123432: step 28163, loss 9.18257e-07, acc 1\n",
      "2018-10-26T18:41:03.292972: step 28164, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:03.474486: step 28165, loss 0.000409036, acc 1\n",
      "2018-10-26T18:41:03.645032: step 28166, loss 1.13061e-06, acc 1\n",
      "2018-10-26T18:41:03.820563: step 28167, loss 1.45653e-06, acc 1\n",
      "2018-10-26T18:41:03.992104: step 28168, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:41:04.171625: step 28169, loss 8.92326e-06, acc 1\n",
      "2018-10-26T18:41:04.340174: step 28170, loss 0.000234414, acc 1\n",
      "2018-10-26T18:41:04.517699: step 28171, loss 0.0376543, acc 0.984375\n",
      "2018-10-26T18:41:04.687247: step 28172, loss 0, acc 1\n",
      "2018-10-26T18:41:04.864772: step 28173, loss 9.49948e-08, acc 1\n",
      "2018-10-26T18:41:05.043295: step 28174, loss 0.00452803, acc 1\n",
      "2018-10-26T18:41:05.220820: step 28175, loss 7.13377e-07, acc 1\n",
      "2018-10-26T18:41:05.408320: step 28176, loss 3.41944e-06, acc 1\n",
      "2018-10-26T18:41:05.574875: step 28177, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:05.759381: step 28178, loss 1.50458e-05, acc 1\n",
      "2018-10-26T18:41:05.932919: step 28179, loss 1.3411e-07, acc 1\n",
      "2018-10-26T18:41:06.113436: step 28180, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:41:06.292957: step 28181, loss 0.00095989, acc 1\n",
      "2018-10-26T18:41:06.466493: step 28182, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:06.651997: step 28183, loss 6.76553e-06, acc 1\n",
      "2018-10-26T18:41:06.828525: step 28184, loss 0, acc 1\n",
      "2018-10-26T18:41:07.004056: step 28185, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:41:07.176596: step 28186, loss 4.91734e-07, acc 1\n",
      "2018-10-26T18:41:07.359107: step 28187, loss 1.92189e-05, acc 1\n",
      "2018-10-26T18:41:07.528655: step 28188, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:41:07.710170: step 28189, loss 2.69329e-06, acc 1\n",
      "2018-10-26T18:41:07.877722: step 28190, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:41:08.060235: step 28191, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:41:08.231777: step 28192, loss 0, acc 1\n",
      "2018-10-26T18:41:08.414289: step 28193, loss 0, acc 1\n",
      "2018-10-26T18:41:08.584834: step 28194, loss 6.48187e-07, acc 1\n",
      "2018-10-26T18:41:08.771335: step 28195, loss 5.69963e-07, acc 1\n",
      "2018-10-26T18:41:08.938887: step 28196, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:41:09.116412: step 28197, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:41:09.291944: step 28198, loss 0.000130134, acc 1\n",
      "2018-10-26T18:41:09.478445: step 28199, loss 3.86451e-06, acc 1\n",
      "2018-10-26T18:41:09.651981: step 28200, loss 5.96046e-09, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:41:10.117738: step 28200, loss 9.97367, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28200\n",
      "\n",
      "2018-10-26T18:41:10.481463: step 28201, loss 0.000381194, acc 1\n",
      "2018-10-26T18:41:10.661981: step 28202, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:41:10.849479: step 28203, loss 4.59261e-06, acc 1\n",
      "2018-10-26T18:41:11.017032: step 28204, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:41:11.224477: step 28205, loss 1.3038e-06, acc 1\n",
      "2018-10-26T18:41:11.457855: step 28206, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:41:11.637374: step 28207, loss 2.69646e-05, acc 1\n",
      "2018-10-26T18:41:11.822879: step 28208, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:41:12.007386: step 28209, loss 6.04524e-05, acc 1\n",
      "2018-10-26T18:41:12.187904: step 28210, loss 1.23675e-06, acc 1\n",
      "2018-10-26T18:41:12.359446: step 28211, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:41:12.539963: step 28212, loss 0.000203107, acc 1\n",
      "2018-10-26T18:41:12.709510: step 28213, loss 3.09196e-07, acc 1\n",
      "2018-10-26T18:41:12.899005: step 28214, loss 1.59434e-06, acc 1\n",
      "2018-10-26T18:41:13.068551: step 28215, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:41:13.252061: step 28216, loss 8.02599e-05, acc 1\n",
      "2018-10-26T18:41:13.427592: step 28217, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:41:13.611101: step 28218, loss 1.0952e-06, acc 1\n",
      "2018-10-26T18:41:13.779650: step 28219, loss 0, acc 1\n",
      "2018-10-26T18:41:13.964158: step 28220, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:41:14.182574: step 28221, loss 3.16649e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:41:14.398996: step 28222, loss 5.49362e-05, acc 1\n",
      "2018-10-26T18:41:14.577519: step 28223, loss 0.000600138, acc 1\n",
      "2018-10-26T18:41:14.794939: step 28224, loss 7.24552e-07, acc 1\n",
      "2018-10-26T18:41:14.968474: step 28225, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:41:15.187888: step 28226, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:41:15.355441: step 28227, loss 9.28078e-06, acc 1\n",
      "2018-10-26T18:41:15.529974: step 28228, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:41:15.752380: step 28229, loss 4.84511e-05, acc 1\n",
      "2018-10-26T18:41:15.918935: step 28230, loss 5.06932e-06, acc 1\n",
      "2018-10-26T18:41:16.094466: step 28231, loss 1.82343e-06, acc 1\n",
      "2018-10-26T18:41:16.296925: step 28232, loss 0.00226288, acc 1\n",
      "2018-10-26T18:41:16.497389: step 28233, loss 4.135e-05, acc 1\n",
      "2018-10-26T18:41:16.681897: step 28234, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:41:16.892335: step 28235, loss 5.11212e-06, acc 1\n",
      "2018-10-26T18:41:17.099780: step 28236, loss 2.90359e-06, acc 1\n",
      "2018-10-26T18:41:17.299246: step 28237, loss 0.000668059, acc 1\n",
      "2018-10-26T18:41:17.496719: step 28238, loss 2.77533e-07, acc 1\n",
      "2018-10-26T18:41:17.713142: step 28239, loss 8.32381e-06, acc 1\n",
      "2018-10-26T18:41:17.885680: step 28240, loss 2.08102e-05, acc 1\n",
      "2018-10-26T18:41:18.120054: step 28241, loss 0, acc 1\n",
      "2018-10-26T18:41:18.313536: step 28242, loss 4.17177e-06, acc 1\n",
      "2018-10-26T18:41:18.520983: step 28243, loss 0, acc 1\n",
      "2018-10-26T18:41:18.736407: step 28244, loss 0, acc 1\n",
      "2018-10-26T18:41:18.961805: step 28245, loss 1.65222e-05, acc 1\n",
      "2018-10-26T18:41:19.167256: step 28246, loss 3.87768e-06, acc 1\n",
      "2018-10-26T18:41:19.382683: step 28247, loss 0, acc 1\n",
      "2018-10-26T18:41:19.563198: step 28248, loss 6.01416e-05, acc 1\n",
      "2018-10-26T18:41:19.777626: step 28249, loss 0, acc 1\n",
      "2018-10-26T18:41:19.956158: step 28250, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:41:20.190521: step 28251, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:20.385002: step 28252, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:41:20.606410: step 28253, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:41:20.810865: step 28254, loss 8.1768e-07, acc 1\n",
      "2018-10-26T18:41:20.990384: step 28255, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:41:21.217777: step 28256, loss 0.000103678, acc 1\n",
      "2018-10-26T18:41:21.441187: step 28257, loss 0, acc 1\n",
      "2018-10-26T18:41:21.623693: step 28258, loss 1.06171e-07, acc 1\n",
      "2018-10-26T18:41:21.839118: step 28259, loss 2.00224e-06, acc 1\n",
      "2018-10-26T18:41:22.014647: step 28260, loss 0, acc 1\n",
      "2018-10-26T18:41:22.230072: step 28261, loss 0, acc 1\n",
      "2018-10-26T18:41:22.407598: step 28262, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:41:22.616042: step 28263, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:41:22.820496: step 28264, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:41:23.019962: step 28265, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:41:23.200480: step 28266, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:41:23.384987: step 28267, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:41:23.600413: step 28268, loss 9.05232e-07, acc 1\n",
      "2018-10-26T18:41:23.768961: step 28269, loss 6.13478e-06, acc 1\n",
      "2018-10-26T18:41:23.943495: step 28270, loss 0, acc 1\n",
      "2018-10-26T18:41:24.108054: step 28271, loss 9.49934e-07, acc 1\n",
      "2018-10-26T18:41:24.328465: step 28272, loss 4.09543e-05, acc 1\n",
      "2018-10-26T18:41:24.501005: step 28273, loss 0.000748871, acc 1\n",
      "2018-10-26T18:41:24.679527: step 28274, loss 6.77858e-06, acc 1\n",
      "2018-10-26T18:41:24.848078: step 28275, loss 0, acc 1\n",
      "2018-10-26T18:41:25.030591: step 28276, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:41:25.202132: step 28277, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:41:25.378660: step 28278, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:41:25.552196: step 28279, loss 7.50205e-05, acc 1\n",
      "2018-10-26T18:41:25.731716: step 28280, loss 2.42142e-07, acc 1\n",
      "2018-10-26T18:41:25.907249: step 28281, loss 3.46449e-07, acc 1\n",
      "2018-10-26T18:41:26.102726: step 28282, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:41:26.298203: step 28283, loss 0, acc 1\n",
      "2018-10-26T18:41:26.478721: step 28284, loss 4.15688e-06, acc 1\n",
      "2018-10-26T18:41:26.704119: step 28285, loss 1.07657e-06, acc 1\n",
      "2018-10-26T18:41:26.904582: step 28286, loss 2.81257e-07, acc 1\n",
      "2018-10-26T18:41:27.094077: step 28287, loss 1.42489e-06, acc 1\n",
      "2018-10-26T18:41:27.277587: step 28288, loss 2.18659e-06, acc 1\n",
      "2018-10-26T18:41:27.483038: step 28289, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:27.654579: step 28290, loss 8.80599e-06, acc 1\n",
      "2018-10-26T18:41:27.865016: step 28291, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:41:28.037555: step 28292, loss 0.000247593, acc 1\n",
      "2018-10-26T18:41:28.213087: step 28293, loss 0.000281776, acc 1\n",
      "2018-10-26T18:41:28.431504: step 28294, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:41:28.622994: step 28295, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:28.829441: step 28296, loss 0, acc 1\n",
      "2018-10-26T18:41:29.068801: step 28297, loss 0, acc 1\n",
      "2018-10-26T18:41:29.264279: step 28298, loss 0.000153548, acc 1\n",
      "2018-10-26T18:41:29.476710: step 28299, loss 2.56555e-05, acc 1\n",
      "2018-10-26T18:41:29.727041: step 28300, loss 1.86265e-09, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:41:30.274578: step 28300, loss 9.98037, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28300\n",
      "\n",
      "2018-10-26T18:41:30.720388: step 28301, loss 5.33056e-06, acc 1\n",
      "2018-10-26T18:41:30.939801: step 28302, loss 0, acc 1\n",
      "2018-10-26T18:41:31.136278: step 28303, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:41:31.339733: step 28304, loss 2.30726e-05, acc 1\n",
      "2018-10-26T18:41:31.599051: step 28305, loss 3.99901e-05, acc 1\n",
      "2018-10-26T18:41:31.775568: step 28306, loss 4.73108e-07, acc 1\n",
      "2018-10-26T18:41:31.961073: step 28307, loss 1.66515e-06, acc 1\n",
      "2018-10-26T18:41:32.158546: step 28308, loss 8.34443e-07, acc 1\n",
      "2018-10-26T18:41:32.335074: step 28309, loss 2.00967e-06, acc 1\n",
      "2018-10-26T18:41:32.516588: step 28310, loss 4.93593e-07, acc 1\n",
      "2018-10-26T18:41:32.694115: step 28311, loss 1.50681e-06, acc 1\n",
      "2018-10-26T18:41:32.885603: step 28312, loss 3.59487e-07, acc 1\n",
      "2018-10-26T18:41:33.071108: step 28313, loss 1.00207e-06, acc 1\n",
      "2018-10-26T18:41:33.260601: step 28314, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:41:33.457078: step 28315, loss 9.16396e-07, acc 1\n",
      "2018-10-26T18:41:33.651556: step 28316, loss 6.87301e-07, acc 1\n",
      "2018-10-26T18:41:33.860998: step 28317, loss 3.3094e-05, acc 1\n",
      "2018-10-26T18:41:34.055477: step 28318, loss 0, acc 1\n",
      "2018-10-26T18:41:34.266913: step 28319, loss 9.49934e-07, acc 1\n",
      "2018-10-26T18:41:34.457405: step 28320, loss 1.34292e-06, acc 1\n",
      "2018-10-26T18:41:34.657868: step 28321, loss 0, acc 1\n",
      "2018-10-26T18:41:34.845380: step 28322, loss 4.48891e-07, acc 1\n",
      "2018-10-26T18:41:35.050817: step 28323, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:41:35.241309: step 28324, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:35.441773: step 28325, loss 2.29275e-06, acc 1\n",
      "2018-10-26T18:41:35.631267: step 28326, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:41:35.824750: step 28327, loss 3.58518e-06, acc 1\n",
      "2018-10-26T18:41:36.015241: step 28328, loss 0, acc 1\n",
      "2018-10-26T18:41:36.204737: step 28329, loss 1.02919e-05, acc 1\n",
      "2018-10-26T18:41:36.406196: step 28330, loss 8.19543e-07, acc 1\n",
      "2018-10-26T18:41:36.608655: step 28331, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:41:36.800144: step 28332, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:41:36.982656: step 28333, loss 3.63212e-07, acc 1\n",
      "2018-10-26T18:41:37.165168: step 28334, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:37.356657: step 28335, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:37.527201: step 28336, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:41:37.707718: step 28337, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:41:37.882253: step 28338, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:41:38.062770: step 28339, loss 8.4004e-07, acc 1\n",
      "2018-10-26T18:41:38.234311: step 28340, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:38.424802: step 28341, loss 0.00476232, acc 1\n",
      "2018-10-26T18:41:38.598339: step 28342, loss 0.000199528, acc 1\n",
      "2018-10-26T18:41:38.783843: step 28343, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:41:38.961369: step 28344, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:39.154853: step 28345, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:41:39.334373: step 28346, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:41:39.523867: step 28347, loss 1.91474e-05, acc 1\n",
      "2018-10-26T18:41:39.692416: step 28348, loss 9.3132e-08, acc 1\n",
      "2018-10-26T18:41:39.880912: step 28349, loss 4.8939e-05, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:41:40.057441: step 28350, loss 1.78103e-05, acc 1\n",
      "2018-10-26T18:41:40.237958: step 28351, loss 0, acc 1\n",
      "2018-10-26T18:41:40.425457: step 28352, loss 9.92482e-06, acc 1\n",
      "2018-10-26T18:41:40.594007: step 28353, loss 4.28403e-07, acc 1\n",
      "2018-10-26T18:41:40.770536: step 28354, loss 5.94527e-05, acc 1\n",
      "2018-10-26T18:41:40.943074: step 28355, loss 4.07916e-07, acc 1\n",
      "2018-10-26T18:41:41.126584: step 28356, loss 0.00477217, acc 1\n",
      "2018-10-26T18:41:41.305107: step 28357, loss 0.000132769, acc 1\n",
      "2018-10-26T18:41:41.487620: step 28358, loss 0.00122891, acc 1\n",
      "2018-10-26T18:41:41.670132: step 28359, loss 2.8312e-07, acc 1\n",
      "2018-10-26T18:41:41.848655: step 28360, loss 1.12743e-05, acc 1\n",
      "2018-10-26T18:41:42.036155: step 28361, loss 0.022209, acc 0.984375\n",
      "2018-10-26T18:41:42.208692: step 28362, loss 1.2293e-06, acc 1\n",
      "2018-10-26T18:41:42.393226: step 28363, loss 0, acc 1\n",
      "2018-10-26T18:41:42.565739: step 28364, loss 8.45621e-07, acc 1\n",
      "2018-10-26T18:41:42.749248: step 28365, loss 2.00222e-06, acc 1\n",
      "2018-10-26T18:41:42.920790: step 28366, loss 6.73578e-05, acc 1\n",
      "2018-10-26T18:41:43.095324: step 28367, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:41:43.273846: step 28368, loss 4.14384e-06, acc 1\n",
      "2018-10-26T18:41:43.451372: step 28369, loss 2.05668e-05, acc 1\n",
      "2018-10-26T18:41:43.631890: step 28370, loss 0.000400366, acc 1\n",
      "2018-10-26T18:41:43.823378: step 28371, loss 0, acc 1\n",
      "2018-10-26T18:41:43.995918: step 28372, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:41:44.177433: step 28373, loss 0.000880789, acc 1\n",
      "2018-10-26T18:41:44.354958: step 28374, loss 0, acc 1\n",
      "2018-10-26T18:41:44.541459: step 28375, loss 1.62049e-07, acc 1\n",
      "2018-10-26T18:41:44.722974: step 28376, loss 0, acc 1\n",
      "2018-10-26T18:41:44.907482: step 28377, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:41:45.076032: step 28378, loss 3.69133e-06, acc 1\n",
      "2018-10-26T18:41:45.259542: step 28379, loss 0.0378637, acc 0.984375\n",
      "2018-10-26T18:41:45.438065: step 28380, loss 3.19784e-06, acc 1\n",
      "2018-10-26T18:41:45.620577: step 28381, loss 4.41199e-06, acc 1\n",
      "2018-10-26T18:41:45.795110: step 28382, loss 0, acc 1\n",
      "2018-10-26T18:41:45.987596: step 28383, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:41:46.164124: step 28384, loss 8.22147e-05, acc 1\n",
      "2018-10-26T18:41:46.343645: step 28385, loss 5.85925e-06, acc 1\n",
      "2018-10-26T18:41:46.526157: step 28386, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:41:46.702685: step 28387, loss 0.01451, acc 0.984375\n",
      "2018-10-26T18:41:46.879213: step 28388, loss 1.18647e-06, acc 1\n",
      "2018-10-26T18:41:47.057737: step 28389, loss 1.18274e-06, acc 1\n",
      "2018-10-26T18:41:47.236260: step 28390, loss 1.35973e-07, acc 1\n",
      "2018-10-26T18:41:47.415781: step 28391, loss 3.2596e-07, acc 1\n",
      "2018-10-26T18:41:47.592309: step 28392, loss 0.000314121, acc 1\n",
      "2018-10-26T18:41:47.777813: step 28393, loss 3.37135e-07, acc 1\n",
      "2018-10-26T18:41:47.950352: step 28394, loss 5.40387e-05, acc 1\n",
      "2018-10-26T18:41:48.139845: step 28395, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:41:48.325349: step 28396, loss 0.000212382, acc 1\n",
      "2018-10-26T18:41:48.507863: step 28397, loss 1.83958e-05, acc 1\n",
      "2018-10-26T18:41:48.691371: step 28398, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:41:48.858923: step 28399, loss 7.17107e-07, acc 1\n",
      "2018-10-26T18:41:49.037448: step 28400, loss 1.39698e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:41:49.516167: step 28400, loss 10.3093, acc 0.710131\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28400\n",
      "\n",
      "2018-10-26T18:41:49.880963: step 28401, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:41:50.060485: step 28402, loss 3.47114e-05, acc 1\n",
      "2018-10-26T18:41:50.240004: step 28403, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:41:50.432489: step 28404, loss 7.0779e-05, acc 1\n",
      "2018-10-26T18:41:50.687808: step 28405, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:41:50.881290: step 28406, loss 2.21479e-05, acc 1\n",
      "2018-10-26T18:41:51.063803: step 28407, loss 0.207448, acc 0.984375\n",
      "2018-10-26T18:41:51.255291: step 28408, loss 0, acc 1\n",
      "2018-10-26T18:41:51.438802: step 28409, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:41:51.620316: step 28410, loss 0, acc 1\n",
      "2018-10-26T18:41:51.808812: step 28411, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:41:51.990327: step 28412, loss 0, acc 1\n",
      "2018-10-26T18:41:52.172839: step 28413, loss 7.02201e-07, acc 1\n",
      "2018-10-26T18:41:52.353358: step 28414, loss 1.54593e-06, acc 1\n",
      "2018-10-26T18:41:52.534872: step 28415, loss 0, acc 1\n",
      "2018-10-26T18:41:52.715390: step 28416, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:41:52.888926: step 28417, loss 5.15889e-05, acc 1\n",
      "2018-10-26T18:41:53.064458: step 28418, loss 1.07512e-05, acc 1\n",
      "2018-10-26T18:41:53.231013: step 28419, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:41:53.408539: step 28420, loss 7.63684e-08, acc 1\n",
      "2018-10-26T18:41:53.575093: step 28421, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:41:53.768577: step 28422, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:41:53.948096: step 28423, loss 4.63007e-06, acc 1\n",
      "2018-10-26T18:41:54.132604: step 28424, loss 5.9976e-07, acc 1\n",
      "2018-10-26T18:41:54.297164: step 28425, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:41:54.480674: step 28426, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:41:54.656205: step 28427, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:41:54.835725: step 28428, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:41:55.009261: step 28429, loss 2.82122e-05, acc 1\n",
      "2018-10-26T18:41:55.184792: step 28430, loss 3.04499e-05, acc 1\n",
      "2018-10-26T18:41:55.364312: step 28431, loss 5.36435e-07, acc 1\n",
      "2018-10-26T18:41:55.534856: step 28432, loss 7.62567e-06, acc 1\n",
      "2018-10-26T18:41:55.714378: step 28433, loss 9.18258e-07, acc 1\n",
      "2018-10-26T18:41:55.893897: step 28434, loss 6.51924e-08, acc 1\n",
      "2018-10-26T18:41:56.081396: step 28435, loss 0.108782, acc 0.984375\n",
      "2018-10-26T18:41:56.284854: step 28436, loss 2.33234e-05, acc 1\n",
      "2018-10-26T18:41:56.462378: step 28437, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:41:56.641899: step 28438, loss 5.61123e-06, acc 1\n",
      "2018-10-26T18:41:56.812444: step 28439, loss 0, acc 1\n",
      "2018-10-26T18:41:57.005926: step 28440, loss 2.7567e-07, acc 1\n",
      "2018-10-26T18:41:57.183452: step 28441, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:41:57.366962: step 28442, loss 1.45281e-06, acc 1\n",
      "2018-10-26T18:41:57.551469: step 28443, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:41:57.719022: step 28444, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:41:57.904525: step 28445, loss 0, acc 1\n",
      "2018-10-26T18:41:58.072079: step 28446, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:41:58.251598: step 28447, loss 5.61326e-05, acc 1\n",
      "2018-10-26T18:41:58.432116: step 28448, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:41:58.611637: step 28449, loss 0, acc 1\n",
      "2018-10-26T18:41:58.797140: step 28450, loss 0, acc 1\n",
      "2018-10-26T18:41:58.965690: step 28451, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:41:59.143216: step 28452, loss 8.52114e-06, acc 1\n",
      "2018-10-26T18:41:59.323734: step 28453, loss 2.10478e-07, acc 1\n",
      "2018-10-26T18:41:59.508257: step 28454, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:41:59.694743: step 28455, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:41:59.860300: step 28456, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:42:00.050792: step 28457, loss 1.42865e-05, acc 1\n",
      "2018-10-26T18:42:00.219342: step 28458, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:42:00.408834: step 28459, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:00.575389: step 28460, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:42:00.765880: step 28461, loss 0, acc 1\n",
      "2018-10-26T18:42:00.939416: step 28462, loss 4.50754e-07, acc 1\n",
      "2018-10-26T18:42:01.127914: step 28463, loss 2.94296e-07, acc 1\n",
      "2018-10-26T18:42:01.296463: step 28464, loss 5.42316e-06, acc 1\n",
      "2018-10-26T18:42:01.482965: step 28465, loss 1.64435e-05, acc 1\n",
      "2018-10-26T18:42:01.649519: step 28466, loss 0, acc 1\n",
      "2018-10-26T18:42:01.832032: step 28467, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:42:02.001579: step 28468, loss 1.32032e-05, acc 1\n",
      "2018-10-26T18:42:02.183093: step 28469, loss 0.00195627, acc 1\n",
      "2018-10-26T18:42:02.352642: step 28470, loss 2.78664e-05, acc 1\n",
      "2018-10-26T18:42:02.533159: step 28471, loss 3.30884e-05, acc 1\n",
      "2018-10-26T18:42:02.697719: step 28472, loss 4.08238e-06, acc 1\n",
      "2018-10-26T18:42:02.880231: step 28473, loss 0.00107013, acc 1\n",
      "2018-10-26T18:42:03.051773: step 28474, loss 3.01867e-05, acc 1\n",
      "2018-10-26T18:42:03.235282: step 28475, loss 7.45056e-08, acc 1\n",
      "2018-10-26T18:42:03.412808: step 28476, loss 1.32988e-06, acc 1\n",
      "2018-10-26T18:42:03.602303: step 28477, loss 0, acc 1\n",
      "2018-10-26T18:42:03.770851: step 28478, loss 9.31313e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:42:03.954361: step 28479, loss 4.3321e-06, acc 1\n",
      "2018-10-26T18:42:04.125903: step 28480, loss 1.89047e-06, acc 1\n",
      "2018-10-26T18:42:04.301434: step 28481, loss 2.2135e-05, acc 1\n",
      "2018-10-26T18:42:04.468986: step 28482, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:42:04.655488: step 28483, loss 0, acc 1\n",
      "2018-10-26T18:42:04.827029: step 28484, loss 0, acc 1\n",
      "2018-10-26T18:42:04.999568: step 28485, loss 1.8419e-05, acc 1\n",
      "2018-10-26T18:42:05.169116: step 28486, loss 7.26417e-07, acc 1\n",
      "2018-10-26T18:42:05.344647: step 28487, loss 2.53318e-07, acc 1\n",
      "2018-10-26T18:42:05.516190: step 28488, loss 7.99058e-07, acc 1\n",
      "2018-10-26T18:42:05.697704: step 28489, loss 4.81084e-06, acc 1\n",
      "2018-10-26T18:42:05.864259: step 28490, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:42:06.047768: step 28491, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:42:06.229285: step 28492, loss 7.18965e-07, acc 1\n",
      "2018-10-26T18:42:06.413790: step 28493, loss 0.00016136, acc 1\n",
      "2018-10-26T18:42:06.603283: step 28494, loss 0, acc 1\n",
      "2018-10-26T18:42:06.778814: step 28495, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:06.952352: step 28496, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:42:07.123895: step 28497, loss 0, acc 1\n",
      "2018-10-26T18:42:07.298426: step 28498, loss 8.67752e-06, acc 1\n",
      "2018-10-26T18:42:07.469968: step 28499, loss 4.96902e-06, acc 1\n",
      "2018-10-26T18:42:07.642507: step 28500, loss 2.39485e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:42:08.112252: step 28500, loss 10.1463, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28500\n",
      "\n",
      "2018-10-26T18:42:08.493776: step 28501, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:42:08.681275: step 28502, loss 0, acc 1\n",
      "2018-10-26T18:42:08.851799: step 28503, loss 9.18966e-05, acc 1\n",
      "2018-10-26T18:42:09.031319: step 28504, loss 0.000158572, acc 1\n",
      "2018-10-26T18:42:09.270680: step 28505, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:42:09.469150: step 28506, loss 0, acc 1\n",
      "2018-10-26T18:42:09.646675: step 28507, loss 3.03768e-06, acc 1\n",
      "2018-10-26T18:42:09.830185: step 28508, loss 4.48891e-07, acc 1\n",
      "2018-10-26T18:42:10.001727: step 28509, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:10.184239: step 28510, loss 2.10478e-07, acc 1\n",
      "2018-10-26T18:42:10.359770: step 28511, loss 0, acc 1\n",
      "2018-10-26T18:42:10.542981: step 28512, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:42:10.711531: step 28513, loss 0, acc 1\n",
      "2018-10-26T18:42:10.890055: step 28514, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:42:11.062593: step 28515, loss 0, acc 1\n",
      "2018-10-26T18:42:11.245107: step 28516, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:42:11.417645: step 28517, loss 1.37643e-06, acc 1\n",
      "2018-10-26T18:42:11.597165: step 28518, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:42:11.763720: step 28519, loss 7.93488e-06, acc 1\n",
      "2018-10-26T18:42:11.952218: step 28520, loss 0, acc 1\n",
      "2018-10-26T18:42:12.126750: step 28521, loss 1.93709e-06, acc 1\n",
      "2018-10-26T18:42:12.302282: step 28522, loss 7.87143e-06, acc 1\n",
      "2018-10-26T18:42:12.467838: step 28523, loss 0.00041421, acc 1\n",
      "2018-10-26T18:42:12.654341: step 28524, loss 3.7528e-06, acc 1\n",
      "2018-10-26T18:42:12.821893: step 28525, loss 0.000175848, acc 1\n",
      "2018-10-26T18:42:13.004406: step 28526, loss 6.09073e-07, acc 1\n",
      "2018-10-26T18:42:13.173952: step 28527, loss 0.000213316, acc 1\n",
      "2018-10-26T18:42:13.351478: step 28528, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:42:13.524018: step 28529, loss 5.47524e-06, acc 1\n",
      "2018-10-26T18:42:13.701543: step 28530, loss 4.72115e-06, acc 1\n",
      "2018-10-26T18:42:13.870092: step 28531, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:42:14.057592: step 28532, loss 0, acc 1\n",
      "2018-10-26T18:42:14.229133: step 28533, loss 2.86846e-07, acc 1\n",
      "2018-10-26T18:42:14.409651: step 28534, loss 0, acc 1\n",
      "2018-10-26T18:42:14.582190: step 28535, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:42:14.755726: step 28536, loss 4.33991e-07, acc 1\n",
      "2018-10-26T18:42:14.930259: step 28537, loss 6.70551e-08, acc 1\n",
      "2018-10-26T18:42:15.111775: step 28538, loss 0, acc 1\n",
      "2018-10-26T18:42:15.282340: step 28539, loss 0, acc 1\n",
      "2018-10-26T18:42:15.462837: step 28540, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:42:15.633380: step 28541, loss 6.10083e-06, acc 1\n",
      "2018-10-26T18:42:15.820880: step 28542, loss 0, acc 1\n",
      "2018-10-26T18:42:15.985440: step 28543, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:42:16.167953: step 28544, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:42:16.337500: step 28545, loss 5.09374e-05, acc 1\n",
      "2018-10-26T18:42:16.517020: step 28546, loss 1.24979e-06, acc 1\n",
      "2018-10-26T18:42:16.687564: step 28547, loss 7.01315e-06, acc 1\n",
      "2018-10-26T18:42:16.868082: step 28548, loss 5.57018e-06, acc 1\n",
      "2018-10-26T18:42:17.043613: step 28549, loss 5.58985e-05, acc 1\n",
      "2018-10-26T18:42:17.221139: step 28550, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:42:17.402653: step 28551, loss 0, acc 1\n",
      "2018-10-26T18:42:17.577188: step 28552, loss 0.000163719, acc 1\n",
      "2018-10-26T18:42:17.756707: step 28553, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:42:17.927252: step 28554, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:42:18.103781: step 28555, loss 5.0543e-05, acc 1\n",
      "2018-10-26T18:42:18.272330: step 28556, loss 9.71068e-06, acc 1\n",
      "2018-10-26T18:42:18.460826: step 28557, loss 0, acc 1\n",
      "2018-10-26T18:42:18.637355: step 28558, loss 2.18636e-05, acc 1\n",
      "2018-10-26T18:42:18.816876: step 28559, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:19.003376: step 28560, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:42:19.182897: step 28561, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:42:19.371395: step 28562, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:42:19.550914: step 28563, loss 0, acc 1\n",
      "2018-10-26T18:42:19.761351: step 28564, loss 0, acc 1\n",
      "2018-10-26T18:42:19.959821: step 28565, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:42:20.142334: step 28566, loss 0.00022931, acc 1\n",
      "2018-10-26T18:42:20.361747: step 28567, loss 2.70081e-07, acc 1\n",
      "2018-10-26T18:42:20.539274: step 28568, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:42:20.751705: step 28569, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:42:20.929232: step 28570, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:21.124709: step 28571, loss 3.66937e-07, acc 1\n",
      "2018-10-26T18:42:21.335147: step 28572, loss 0, acc 1\n",
      "2018-10-26T18:42:21.523642: step 28573, loss 5.97902e-07, acc 1\n",
      "2018-10-26T18:42:21.724107: step 28574, loss 0.000595014, acc 1\n",
      "2018-10-26T18:42:21.916593: step 28575, loss 0.000336215, acc 1\n",
      "2018-10-26T18:42:22.110076: step 28576, loss 4.20158e-06, acc 1\n",
      "2018-10-26T18:42:22.289597: step 28577, loss 2.16065e-07, acc 1\n",
      "2018-10-26T18:42:22.507016: step 28578, loss 2.57775e-06, acc 1\n",
      "2018-10-26T18:42:22.684541: step 28579, loss 4.2654e-07, acc 1\n",
      "2018-10-26T18:42:22.900963: step 28580, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:42:23.087465: step 28581, loss 2.30966e-07, acc 1\n",
      "2018-10-26T18:42:23.270975: step 28582, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:42:23.474430: step 28583, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:42:23.666923: step 28584, loss 2.51944e-05, acc 1\n",
      "2018-10-26T18:42:23.865387: step 28585, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:42:24.090784: step 28586, loss 2.82164e-05, acc 1\n",
      "2018-10-26T18:42:24.309200: step 28587, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:42:24.503680: step 28588, loss 4.48891e-07, acc 1\n",
      "2018-10-26T18:42:24.699159: step 28589, loss 5.74902e-06, acc 1\n",
      "2018-10-26T18:42:24.909596: step 28590, loss 0, acc 1\n",
      "2018-10-26T18:42:25.089117: step 28591, loss 1.36714e-06, acc 1\n",
      "2018-10-26T18:42:25.298557: step 28592, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:25.485059: step 28593, loss 0.000170141, acc 1\n",
      "2018-10-26T18:42:25.691508: step 28594, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:42:25.909923: step 28595, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:42:26.107396: step 28596, loss 0, acc 1\n",
      "2018-10-26T18:42:26.362714: step 28597, loss 1.43417e-06, acc 1\n",
      "2018-10-26T18:42:26.538244: step 28598, loss 2.23395e-05, acc 1\n",
      "2018-10-26T18:42:26.756662: step 28599, loss 9.94636e-07, acc 1\n",
      "2018-10-26T18:42:26.945158: step 28600, loss 1.04308e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:42:27.480726: step 28600, loss 10.1444, acc 0.717636\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28600\n",
      "\n",
      "2018-10-26T18:42:27.879296: step 28601, loss 7.71523e-06, acc 1\n",
      "2018-10-26T18:42:28.080758: step 28602, loss 7.20491e-06, acc 1\n",
      "2018-10-26T18:42:28.266283: step 28603, loss 0, acc 1\n",
      "2018-10-26T18:42:28.487670: step 28604, loss 7.91605e-07, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:42:28.752961: step 28605, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:42:28.934476: step 28606, loss 1.40157e-05, acc 1\n",
      "2018-10-26T18:42:29.156881: step 28607, loss 8.37639e-05, acc 1\n",
      "2018-10-26T18:42:29.336402: step 28608, loss 6.33298e-08, acc 1\n",
      "2018-10-26T18:42:29.511935: step 28609, loss 6.43691e-05, acc 1\n",
      "2018-10-26T18:42:29.679486: step 28610, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:42:29.883939: step 28611, loss 6.57507e-07, acc 1\n",
      "2018-10-26T18:42:30.054484: step 28612, loss 0, acc 1\n",
      "2018-10-26T18:42:30.234005: step 28613, loss 5.50907e-06, acc 1\n",
      "2018-10-26T18:42:30.401556: step 28614, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:42:30.590055: step 28615, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:42:30.764586: step 28616, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:42:30.940118: step 28617, loss 1.30071e-05, acc 1\n",
      "2018-10-26T18:42:31.106673: step 28618, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:42:31.283201: step 28619, loss 8.94068e-08, acc 1\n",
      "2018-10-26T18:42:31.454743: step 28620, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:42:31.637254: step 28621, loss 0.00155062, acc 1\n",
      "2018-10-26T18:42:31.825752: step 28622, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:42:32.045165: step 28623, loss 0.000489649, acc 1\n",
      "2018-10-26T18:42:32.221693: step 28624, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:42:32.407198: step 28625, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:32.597688: step 28626, loss 7.42274e-06, acc 1\n",
      "2018-10-26T18:42:32.785188: step 28627, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:42:32.982660: step 28628, loss 0.000103871, acc 1\n",
      "2018-10-26T18:42:33.177142: step 28629, loss 8.00936e-08, acc 1\n",
      "2018-10-26T18:42:33.358655: step 28630, loss 1.06171e-07, acc 1\n",
      "2018-10-26T18:42:33.576451: step 28631, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:42:33.773922: step 28632, loss 5.02914e-08, acc 1\n",
      "2018-10-26T18:42:33.965411: step 28633, loss 3.88499e-06, acc 1\n",
      "2018-10-26T18:42:34.181833: step 28634, loss 0, acc 1\n",
      "2018-10-26T18:42:34.350382: step 28635, loss 1.77873e-06, acc 1\n",
      "2018-10-26T18:42:34.559823: step 28636, loss 0, acc 1\n",
      "2018-10-26T18:42:34.732362: step 28637, loss 1.86812e-06, acc 1\n",
      "2018-10-26T18:42:34.923850: step 28638, loss 5.90347e-06, acc 1\n",
      "2018-10-26T18:42:35.113343: step 28639, loss 3.84797e-05, acc 1\n",
      "2018-10-26T18:42:35.287878: step 28640, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:42:35.505297: step 28641, loss 0, acc 1\n",
      "2018-10-26T18:42:35.686811: step 28642, loss 0, acc 1\n",
      "2018-10-26T18:42:35.873313: step 28643, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:42:36.085746: step 28644, loss 2.21653e-07, acc 1\n",
      "2018-10-26T18:42:36.258285: step 28645, loss 0, acc 1\n",
      "2018-10-26T18:42:36.433816: step 28646, loss 0, acc 1\n",
      "2018-10-26T18:42:36.611365: step 28647, loss 0.000122046, acc 1\n",
      "2018-10-26T18:42:36.783880: step 28648, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:42:36.961405: step 28649, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:42:37.130952: step 28650, loss 1.56958e-07, acc 1\n",
      "2018-10-26T18:42:37.302494: step 28651, loss 7.58079e-07, acc 1\n",
      "2018-10-26T18:42:37.483012: step 28652, loss 0, acc 1\n",
      "2018-10-26T18:42:37.650564: step 28653, loss 4.73106e-07, acc 1\n",
      "2018-10-26T18:42:37.822106: step 28654, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:37.992650: step 28655, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:42:38.168182: step 28656, loss 6.13309e-06, acc 1\n",
      "2018-10-26T18:42:38.339723: step 28657, loss 1.67455e-05, acc 1\n",
      "2018-10-26T18:42:38.529216: step 28658, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:42:38.696770: step 28659, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:38.870306: step 28660, loss 6.24196e-05, acc 1\n",
      "2018-10-26T18:42:39.037859: step 28661, loss 4.65594e-06, acc 1\n",
      "2018-10-26T18:42:39.217378: step 28662, loss 0, acc 1\n",
      "2018-10-26T18:42:39.384932: step 28663, loss 0, acc 1\n",
      "2018-10-26T18:42:39.561459: step 28664, loss 0.000768444, acc 1\n",
      "2018-10-26T18:42:39.733000: step 28665, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:39.915513: step 28666, loss 6.5142e-06, acc 1\n",
      "2018-10-26T18:42:40.088051: step 28667, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:42:40.268571: step 28668, loss 0.000124986, acc 1\n",
      "2018-10-26T18:42:40.436122: step 28669, loss 0, acc 1\n",
      "2018-10-26T18:42:40.610655: step 28670, loss 1.14343e-05, acc 1\n",
      "2018-10-26T18:42:40.778209: step 28671, loss 4.36544e-06, acc 1\n",
      "2018-10-26T18:42:40.952741: step 28672, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:42:41.127277: step 28673, loss 0, acc 1\n",
      "2018-10-26T18:42:41.302806: step 28674, loss 0.00023616, acc 1\n",
      "2018-10-26T18:42:41.476343: step 28675, loss 0, acc 1\n",
      "2018-10-26T18:42:41.656860: step 28676, loss 0, acc 1\n",
      "2018-10-26T18:42:41.823415: step 28677, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:42:42.002935: step 28678, loss 1.60552e-06, acc 1\n",
      "2018-10-26T18:42:42.170487: step 28679, loss 9.68427e-06, acc 1\n",
      "2018-10-26T18:42:42.354995: step 28680, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:42:42.527534: step 28681, loss 0.00409124, acc 1\n",
      "2018-10-26T18:42:42.707055: step 28682, loss 0.000605133, acc 1\n",
      "2018-10-26T18:42:42.871615: step 28683, loss 2.06023e-05, acc 1\n",
      "2018-10-26T18:42:43.054127: step 28684, loss 7.82309e-08, acc 1\n",
      "2018-10-26T18:42:43.227663: step 28685, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:42:43.409178: step 28686, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:42:43.575733: step 28687, loss 0, acc 1\n",
      "2018-10-26T18:42:43.758245: step 28688, loss 0, acc 1\n",
      "2018-10-26T18:42:43.929788: step 28689, loss 0, acc 1\n",
      "2018-10-26T18:42:44.104321: step 28690, loss 0, acc 1\n",
      "2018-10-26T18:42:44.270876: step 28691, loss 1.32057e-06, acc 1\n",
      "2018-10-26T18:42:44.451393: step 28692, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:42:44.616951: step 28693, loss 0.000533737, acc 1\n",
      "2018-10-26T18:42:44.796471: step 28694, loss 8.38188e-08, acc 1\n",
      "2018-10-26T18:42:44.965021: step 28695, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:42:45.141549: step 28696, loss 0, acc 1\n",
      "2018-10-26T18:42:45.306111: step 28697, loss 3.17365e-06, acc 1\n",
      "2018-10-26T18:42:45.488622: step 28698, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:42:45.662159: step 28699, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:42:45.842676: step 28700, loss 5.26197e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:42:46.302448: step 28700, loss 10.2413, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28700\n",
      "\n",
      "2018-10-26T18:42:46.670016: step 28701, loss 2.73382e-05, acc 1\n",
      "2018-10-26T18:42:46.838565: step 28702, loss 3.90548e-06, acc 1\n",
      "2018-10-26T18:42:47.023072: step 28703, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:42:47.189626: step 28704, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:42:47.393083: step 28705, loss 1.67818e-06, acc 1\n",
      "2018-10-26T18:42:47.634438: step 28706, loss 2.75845e-06, acc 1\n",
      "2018-10-26T18:42:47.806977: step 28707, loss 0, acc 1\n",
      "2018-10-26T18:42:47.996471: step 28708, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:42:48.164023: step 28709, loss 0, acc 1\n",
      "2018-10-26T18:42:48.344541: step 28710, loss 9.68573e-08, acc 1\n",
      "2018-10-26T18:42:48.516082: step 28711, loss 9.92662e-06, acc 1\n",
      "2018-10-26T18:42:48.703582: step 28712, loss 7.07791e-07, acc 1\n",
      "2018-10-26T18:42:48.875123: step 28713, loss 5.8054e-05, acc 1\n",
      "2018-10-26T18:42:49.063618: step 28714, loss 0.000290003, acc 1\n",
      "2018-10-26T18:42:49.234164: step 28715, loss 3.40861e-07, acc 1\n",
      "2018-10-26T18:42:49.415679: step 28716, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:42:49.599189: step 28717, loss 9.07082e-07, acc 1\n",
      "2018-10-26T18:42:49.783695: step 28718, loss 1.73032e-06, acc 1\n",
      "2018-10-26T18:42:49.967205: step 28719, loss 3.59822e-06, acc 1\n",
      "2018-10-26T18:42:50.150715: step 28720, loss 0, acc 1\n",
      "2018-10-26T18:42:50.327244: step 28721, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:42:50.506763: step 28722, loss 3.337e-05, acc 1\n",
      "2018-10-26T18:42:50.675313: step 28723, loss 0, acc 1\n",
      "2018-10-26T18:42:50.869795: step 28724, loss 1.125e-06, acc 1\n",
      "2018-10-26T18:42:51.037345: step 28725, loss 2.16066e-07, acc 1\n",
      "2018-10-26T18:42:51.222850: step 28726, loss 0, acc 1\n",
      "2018-10-26T18:42:51.395390: step 28727, loss 5.76678e-05, acc 1\n",
      "2018-10-26T18:42:51.574909: step 28728, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:42:51.744457: step 28729, loss 1.78245e-06, acc 1\n",
      "2018-10-26T18:42:51.929961: step 28730, loss 0.000185945, acc 1\n",
      "2018-10-26T18:42:52.105493: step 28731, loss 1.04308e-07, acc 1\n",
      "2018-10-26T18:42:52.296981: step 28732, loss 4.6573e-05, acc 1\n",
      "2018-10-26T18:42:52.478494: step 28733, loss 0, acc 1\n",
      "2018-10-26T18:42:52.665994: step 28734, loss 8.58274e-05, acc 1\n",
      "2018-10-26T18:42:52.853494: step 28735, loss 1.24981e-06, acc 1\n",
      "2018-10-26T18:42:53.033014: step 28736, loss 2.587e-06, acc 1\n",
      "2018-10-26T18:42:53.207547: step 28737, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:42:53.385074: step 28738, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:42:53.558609: step 28739, loss 0.000113153, acc 1\n",
      "2018-10-26T18:42:53.734140: step 28740, loss 9.39316e-05, acc 1\n",
      "2018-10-26T18:42:53.902690: step 28741, loss 3.65076e-07, acc 1\n",
      "2018-10-26T18:42:54.086199: step 28742, loss 0, acc 1\n",
      "2018-10-26T18:42:54.257742: step 28743, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:42:54.432275: step 28744, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:42:54.604814: step 28745, loss 0, acc 1\n",
      "2018-10-26T18:42:54.786329: step 28746, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:42:54.965850: step 28747, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:42:55.148368: step 28748, loss 0.00822141, acc 1\n",
      "2018-10-26T18:42:55.329877: step 28749, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:42:55.499424: step 28750, loss 8.00936e-08, acc 1\n",
      "2018-10-26T18:42:55.677947: step 28751, loss 3.86657e-05, acc 1\n",
      "2018-10-26T18:42:55.854476: step 28752, loss 0, acc 1\n",
      "2018-10-26T18:42:56.068903: step 28753, loss 1.01138e-06, acc 1\n",
      "2018-10-26T18:42:56.235457: step 28754, loss 7.69254e-07, acc 1\n",
      "2018-10-26T18:42:56.416972: step 28755, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:42:56.585522: step 28756, loss 2.42143e-07, acc 1\n",
      "2018-10-26T18:42:56.763048: step 28757, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:42:56.929603: step 28758, loss 7.63683e-08, acc 1\n",
      "2018-10-26T18:42:57.105134: step 28759, loss 0.000472463, acc 1\n",
      "2018-10-26T18:42:57.274680: step 28760, loss 8.38188e-08, acc 1\n",
      "2018-10-26T18:42:57.448217: step 28761, loss 1.56462e-07, acc 1\n",
      "2018-10-26T18:42:57.613774: step 28762, loss 1.52736e-07, acc 1\n",
      "2018-10-26T18:42:57.797284: step 28763, loss 3.16647e-07, acc 1\n",
      "2018-10-26T18:42:57.976804: step 28764, loss 0, acc 1\n",
      "2018-10-26T18:42:58.163306: step 28765, loss 3.04974e-05, acc 1\n",
      "2018-10-26T18:42:58.334849: step 28766, loss 0, acc 1\n",
      "2018-10-26T18:42:58.509382: step 28767, loss 0.00380614, acc 1\n",
      "2018-10-26T18:42:58.676934: step 28768, loss 0, acc 1\n",
      "2018-10-26T18:42:58.859446: step 28769, loss 1.40997e-05, acc 1\n",
      "2018-10-26T18:42:59.026998: step 28770, loss 2.92433e-07, acc 1\n",
      "2018-10-26T18:42:59.203527: step 28771, loss 9.68573e-08, acc 1\n",
      "2018-10-26T18:42:59.373073: step 28772, loss 3.93013e-07, acc 1\n",
      "2018-10-26T18:42:59.549602: step 28773, loss 3.5033e-06, acc 1\n",
      "2018-10-26T18:42:59.724135: step 28774, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:42:59.901662: step 28775, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:43:00.086169: step 28776, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:43:00.262697: step 28777, loss 0, acc 1\n",
      "2018-10-26T18:43:00.434239: step 28778, loss 7.45057e-08, acc 1\n",
      "2018-10-26T18:43:00.608772: step 28779, loss 3.46449e-07, acc 1\n",
      "2018-10-26T18:43:00.783306: step 28780, loss 0.0377379, acc 0.984375\n",
      "2018-10-26T18:43:00.959835: step 28781, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:01.129391: step 28782, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:01.303923: step 28783, loss 0, acc 1\n",
      "2018-10-26T18:43:01.476454: step 28784, loss 8.56814e-08, acc 1\n",
      "2018-10-26T18:43:01.651986: step 28785, loss 1.26659e-07, acc 1\n",
      "2018-10-26T18:43:01.822529: step 28786, loss 9.25707e-07, acc 1\n",
      "2018-10-26T18:43:02.003047: step 28787, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:43:02.169602: step 28788, loss 4.74967e-07, acc 1\n",
      "2018-10-26T18:43:02.344136: step 28789, loss 0, acc 1\n",
      "2018-10-26T18:43:02.509693: step 28790, loss 5.36437e-07, acc 1\n",
      "2018-10-26T18:43:02.684227: step 28791, loss 0, acc 1\n",
      "2018-10-26T18:43:02.859758: step 28792, loss 0.000316717, acc 1\n",
      "2018-10-26T18:43:03.037284: step 28793, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:43:03.203839: step 28794, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:43:03.389343: step 28795, loss 6.61226e-07, acc 1\n",
      "2018-10-26T18:43:03.556895: step 28796, loss 0.00143968, acc 1\n",
      "2018-10-26T18:43:03.736415: step 28797, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:03.905972: step 28798, loss 3.3198e-05, acc 1\n",
      "2018-10-26T18:43:04.084485: step 28799, loss 2.03027e-05, acc 1\n",
      "2018-10-26T18:43:04.249046: step 28800, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:43:04.710813: step 28800, loss 10.0828, acc 0.730769\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28800\n",
      "\n",
      "2018-10-26T18:43:05.100123: step 28801, loss 0.000301042, acc 1\n",
      "2018-10-26T18:43:05.286624: step 28802, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:43:05.460329: step 28803, loss 3.7677e-06, acc 1\n",
      "2018-10-26T18:43:05.637855: step 28804, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:43:05.847296: step 28805, loss 0, acc 1\n",
      "2018-10-26T18:43:06.066709: step 28806, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:43:06.244236: step 28807, loss 6.19208e-06, acc 1\n",
      "2018-10-26T18:43:06.435724: step 28808, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:43:06.624220: step 28809, loss 0, acc 1\n",
      "2018-10-26T18:43:06.799750: step 28810, loss 2.49761e-06, acc 1\n",
      "2018-10-26T18:43:06.992236: step 28811, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:43:07.178738: step 28812, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:43:07.355266: step 28813, loss 0, acc 1\n",
      "2018-10-26T18:43:07.534787: step 28814, loss 1.37835e-07, acc 1\n",
      "2018-10-26T18:43:07.721289: step 28815, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:43:07.887843: step 28816, loss 0, acc 1\n",
      "2018-10-26T18:43:08.071353: step 28817, loss 6.27787e-06, acc 1\n",
      "2018-10-26T18:43:08.237908: step 28818, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:43:08.418450: step 28819, loss 1.57055e-05, acc 1\n",
      "2018-10-26T18:43:08.582986: step 28820, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:43:08.826336: step 28821, loss 4.11639e-07, acc 1\n",
      "2018-10-26T18:43:09.029793: step 28822, loss 2.01165e-07, acc 1\n",
      "2018-10-26T18:43:09.217291: step 28823, loss 5.532e-07, acc 1\n",
      "2018-10-26T18:43:09.407782: step 28824, loss 1.43265e-05, acc 1\n",
      "2018-10-26T18:43:09.594284: step 28825, loss 2.70998e-06, acc 1\n",
      "2018-10-26T18:43:09.785772: step 28826, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:43:09.990227: step 28827, loss 1.36527e-06, acc 1\n",
      "2018-10-26T18:43:10.169747: step 28828, loss 4.6938e-07, acc 1\n",
      "2018-10-26T18:43:10.355251: step 28829, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:43:10.534771: step 28830, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:43:10.713293: step 28831, loss 0.000163769, acc 1\n",
      "2018-10-26T18:43:10.895807: step 28832, loss 0.00295436, acc 1\n",
      "2018-10-26T18:43:11.075327: step 28833, loss 4.02093e-06, acc 1\n",
      "2018-10-26T18:43:11.247865: step 28834, loss 3.20156e-06, acc 1\n",
      "2018-10-26T18:43:11.430378: step 28835, loss 0, acc 1\n",
      "2018-10-26T18:43:11.595935: step 28836, loss 0, acc 1\n",
      "2018-10-26T18:43:11.768475: step 28837, loss 0.000241933, acc 1\n",
      "2018-10-26T18:43:11.965946: step 28838, loss 2.6822e-07, acc 1\n",
      "2018-10-26T18:43:12.155759: step 28839, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:12.335280: step 28840, loss 0, acc 1\n",
      "2018-10-26T18:43:12.513803: step 28841, loss 3.45296e-06, acc 1\n",
      "2018-10-26T18:43:12.686342: step 28842, loss 0, acc 1\n",
      "2018-10-26T18:43:12.866859: step 28843, loss 1.84205e-06, acc 1\n",
      "2018-10-26T18:43:13.036407: step 28844, loss 2.19791e-07, acc 1\n",
      "2018-10-26T18:43:13.216925: step 28845, loss 0, acc 1\n",
      "2018-10-26T18:43:13.384477: step 28846, loss 8.19542e-07, acc 1\n",
      "2018-10-26T18:43:13.564995: step 28847, loss 0, acc 1\n",
      "2018-10-26T18:43:13.737533: step 28848, loss 4.65655e-07, acc 1\n",
      "2018-10-26T18:43:13.927028: step 28849, loss 1.60926e-06, acc 1\n",
      "2018-10-26T18:43:14.100563: step 28850, loss 0, acc 1\n",
      "2018-10-26T18:43:14.285071: step 28851, loss 0.00039245, acc 1\n",
      "2018-10-26T18:43:14.460602: step 28852, loss 6.4639e-06, acc 1\n",
      "2018-10-26T18:43:14.643114: step 28853, loss 4.63545e-06, acc 1\n",
      "2018-10-26T18:43:14.812661: step 28854, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:43:15.004149: step 28855, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:43:15.179681: step 28856, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:43:15.368177: step 28857, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:43:15.540716: step 28858, loss 2.96158e-07, acc 1\n",
      "2018-10-26T18:43:15.722231: step 28859, loss 1.52543e-06, acc 1\n",
      "2018-10-26T18:43:15.904743: step 28860, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:43:16.082269: step 28861, loss 5.17177e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:43:16.267773: step 28862, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:16.447293: step 28863, loss 0.000534338, acc 1\n",
      "2018-10-26T18:43:16.618835: step 28864, loss 0, acc 1\n",
      "2018-10-26T18:43:16.798356: step 28865, loss 2.17356e-06, acc 1\n",
      "2018-10-26T18:43:17.000815: step 28866, loss 7.15786e-05, acc 1\n",
      "2018-10-26T18:43:17.177343: step 28867, loss 0, acc 1\n",
      "2018-10-26T18:43:17.354869: step 28868, loss 3.43352e-05, acc 1\n",
      "2018-10-26T18:43:17.522421: step 28869, loss 0, acc 1\n",
      "2018-10-26T18:43:17.737889: step 28870, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:17.908405: step 28871, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:43:18.091899: step 28872, loss 0, acc 1\n",
      "2018-10-26T18:43:18.267432: step 28873, loss 3.39151e-06, acc 1\n",
      "2018-10-26T18:43:18.442961: step 28874, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:18.620487: step 28875, loss 0, acc 1\n",
      "2018-10-26T18:43:18.797015: step 28876, loss 0, acc 1\n",
      "2018-10-26T18:43:18.981522: step 28877, loss 0.000109644, acc 1\n",
      "2018-10-26T18:43:19.153064: step 28878, loss 0, acc 1\n",
      "2018-10-26T18:43:19.327597: step 28879, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:43:19.500136: step 28880, loss 1.78813e-07, acc 1\n",
      "2018-10-26T18:43:19.677662: step 28881, loss 1.62524e-05, acc 1\n",
      "2018-10-26T18:43:19.847209: step 28882, loss 0, acc 1\n",
      "2018-10-26T18:43:20.032714: step 28883, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:43:20.200266: step 28884, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:43:20.377792: step 28885, loss 2.94709e-05, acc 1\n",
      "2018-10-26T18:43:20.549336: step 28886, loss 7.87879e-07, acc 1\n",
      "2018-10-26T18:43:20.726859: step 28887, loss 0.00075262, acc 1\n",
      "2018-10-26T18:43:20.893414: step 28888, loss 0, acc 1\n",
      "2018-10-26T18:43:21.068944: step 28889, loss 0.0878197, acc 0.984375\n",
      "2018-10-26T18:43:21.241484: step 28890, loss 2.92967e-06, acc 1\n",
      "2018-10-26T18:43:21.421004: step 28891, loss 2.54474e-05, acc 1\n",
      "2018-10-26T18:43:21.588558: step 28892, loss 0, acc 1\n",
      "2018-10-26T18:43:21.768076: step 28893, loss 0, acc 1\n",
      "2018-10-26T18:43:21.936627: step 28894, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:43:22.114152: step 28895, loss 0.000130096, acc 1\n",
      "2018-10-26T18:43:22.286691: step 28896, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:22.465215: step 28897, loss 3.66511e-05, acc 1\n",
      "2018-10-26T18:43:22.628777: step 28898, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:22.811290: step 28899, loss 1.09896e-07, acc 1\n",
      "2018-10-26T18:43:22.975849: step 28900, loss 2.55181e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:43:23.437616: step 28900, loss 10.2776, acc 0.712008\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-28900\n",
      "\n",
      "2018-10-26T18:43:23.795629: step 28901, loss 1.51426e-06, acc 1\n",
      "2018-10-26T18:43:23.969166: step 28902, loss 6.81713e-07, acc 1\n",
      "2018-10-26T18:43:24.153672: step 28903, loss 6.20248e-07, acc 1\n",
      "2018-10-26T18:43:24.339177: step 28904, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:43:24.571557: step 28905, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:43:24.804932: step 28906, loss 2.70255e-06, acc 1\n",
      "2018-10-26T18:43:24.976474: step 28907, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:43:25.159984: step 28908, loss 2.03027e-07, acc 1\n",
      "2018-10-26T18:43:25.328533: step 28909, loss 2.65965e-06, acc 1\n",
      "2018-10-26T18:43:25.543958: step 28910, loss 0.000940427, acc 1\n",
      "2018-10-26T18:43:25.739463: step 28911, loss 3.46789e-06, acc 1\n",
      "2018-10-26T18:43:25.927932: step 28912, loss 0, acc 1\n",
      "2018-10-26T18:43:26.150338: step 28913, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:43:26.333847: step 28914, loss 8.00935e-08, acc 1\n",
      "2018-10-26T18:43:26.550269: step 28915, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:26.736770: step 28916, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:43:26.932248: step 28917, loss 1.65775e-07, acc 1\n",
      "2018-10-26T18:43:27.154676: step 28918, loss 8.00936e-08, acc 1\n",
      "2018-10-26T18:43:27.326196: step 28919, loss 3.05471e-07, acc 1\n",
      "2018-10-26T18:43:27.521674: step 28920, loss 5.65151e-05, acc 1\n",
      "2018-10-26T18:43:27.687232: step 28921, loss 1.73225e-07, acc 1\n",
      "2018-10-26T18:43:27.910636: step 28922, loss 8.83038e-05, acc 1\n",
      "2018-10-26T18:43:28.117083: step 28923, loss 5.97427e-06, acc 1\n",
      "2018-10-26T18:43:28.296603: step 28924, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:43:28.522005: step 28925, loss 1.99302e-07, acc 1\n",
      "2018-10-26T18:43:28.732439: step 28926, loss 5.0842e-06, acc 1\n",
      "2018-10-26T18:43:28.906972: step 28927, loss 1.20323e-06, acc 1\n",
      "2018-10-26T18:43:29.123394: step 28928, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:43:29.292942: step 28929, loss 3.37161e-05, acc 1\n",
      "2018-10-26T18:43:29.513352: step 28930, loss 6.89178e-08, acc 1\n",
      "2018-10-26T18:43:29.720798: step 28931, loss 0.00056996, acc 1\n",
      "2018-10-26T18:43:29.911289: step 28932, loss 0.000725187, acc 1\n",
      "2018-10-26T18:43:30.123721: step 28933, loss 7.30731e-06, acc 1\n",
      "2018-10-26T18:43:30.327179: step 28934, loss 2.19791e-07, acc 1\n",
      "2018-10-26T18:43:30.525648: step 28935, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:30.735088: step 28936, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:43:30.926576: step 28937, loss 1.695e-07, acc 1\n",
      "2018-10-26T18:43:31.120060: step 28938, loss 0.000133642, acc 1\n",
      "2018-10-26T18:43:31.341468: step 28939, loss 3.05072e-06, acc 1\n",
      "2018-10-26T18:43:31.531959: step 28940, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:31.775309: step 28941, loss 5.45746e-07, acc 1\n",
      "2018-10-26T18:43:31.975773: step 28942, loss 0, acc 1\n",
      "2018-10-26T18:43:32.188206: step 28943, loss 0.0687332, acc 0.984375\n",
      "2018-10-26T18:43:32.415598: step 28944, loss 0, acc 1\n",
      "2018-10-26T18:43:32.598110: step 28945, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:43:32.791594: step 28946, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:43:33.030955: step 28947, loss 3.85563e-07, acc 1\n",
      "2018-10-26T18:43:33.223440: step 28948, loss 0, acc 1\n",
      "2018-10-26T18:43:33.442853: step 28949, loss 0, acc 1\n",
      "2018-10-26T18:43:33.636336: step 28950, loss 9.33804e-08, acc 1\n",
      "2018-10-26T18:43:33.855752: step 28951, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:34.040258: step 28952, loss 0.00011818, acc 1\n",
      "2018-10-26T18:43:34.265654: step 28953, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:43:34.438194: step 28954, loss 8.00673e-05, acc 1\n",
      "2018-10-26T18:43:34.620707: step 28955, loss 0, acc 1\n",
      "2018-10-26T18:43:34.799229: step 28956, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:43:35.011662: step 28957, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:35.189188: step 28958, loss 0.0204292, acc 0.984375\n",
      "2018-10-26T18:43:35.373694: step 28959, loss 0, acc 1\n",
      "2018-10-26T18:43:35.542244: step 28960, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:35.752682: step 28961, loss 6.31424e-07, acc 1\n",
      "2018-10-26T18:43:35.931205: step 28962, loss 0.00444916, acc 1\n",
      "2018-10-26T18:43:36.109727: step 28963, loss 0, acc 1\n",
      "2018-10-26T18:43:36.285259: step 28964, loss 0, acc 1\n",
      "2018-10-26T18:43:36.477744: step 28965, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:43:36.642305: step 28966, loss 6.37012e-07, acc 1\n",
      "2018-10-26T18:43:36.820828: step 28967, loss 0, acc 1\n",
      "2018-10-26T18:43:36.994364: step 28968, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:37.175879: step 28969, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:43:37.392301: step 28970, loss 0.00015494, acc 1\n",
      "2018-10-26T18:43:37.605731: step 28971, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:37.793229: step 28972, loss 0, acc 1\n",
      "2018-10-26T18:43:38.024612: step 28973, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:43:38.194159: step 28974, loss 2.25379e-07, acc 1\n",
      "2018-10-26T18:43:38.407589: step 28975, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:38.580127: step 28976, loss 7.26431e-08, acc 1\n",
      "2018-10-26T18:43:38.760644: step 28977, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:43:38.961109: step 28978, loss 1.37999e-05, acc 1\n",
      "2018-10-26T18:43:39.152598: step 28979, loss 8.25138e-05, acc 1\n",
      "2018-10-26T18:43:39.370017: step 28980, loss 0, acc 1\n",
      "2018-10-26T18:43:39.549537: step 28981, loss 5.64126e-05, acc 1\n",
      "2018-10-26T18:43:39.730055: step 28982, loss 1.17156e-06, acc 1\n",
      "2018-10-26T18:43:39.932514: step 28983, loss 0.000286924, acc 1\n",
      "2018-10-26T18:43:40.126995: step 28984, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:40.303522: step 28985, loss 6.28562e-05, acc 1\n",
      "2018-10-26T18:43:40.483042: step 28986, loss 1.88166e-05, acc 1\n",
      "2018-10-26T18:43:40.701459: step 28987, loss 0, acc 1\n",
      "2018-10-26T18:43:40.880980: step 28988, loss 2.9057e-07, acc 1\n",
      "2018-10-26T18:43:41.070473: step 28989, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:43:41.235045: step 28990, loss 1.40997e-06, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:43:41.448463: step 28991, loss 2.36554e-07, acc 1\n",
      "2018-10-26T18:43:41.621999: step 28992, loss 0, acc 1\n",
      "2018-10-26T18:43:41.800523: step 28993, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:43:41.967078: step 28994, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:43:42.185494: step 28995, loss 0.0332627, acc 0.984375\n",
      "2018-10-26T18:43:42.355042: step 28996, loss 0, acc 1\n",
      "2018-10-26T18:43:42.531569: step 28997, loss 3.01142e-05, acc 1\n",
      "2018-10-26T18:43:42.697128: step 28998, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:43:42.873656: step 28999, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:43:43.053175: step 29000, loss 2.34499e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:43:43.509958: step 29000, loss 10.2891, acc 0.713884\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29000\n",
      "\n",
      "2018-10-26T18:43:43.887821: step 29001, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:43:44.062355: step 29002, loss 0, acc 1\n",
      "2018-10-26T18:43:44.239880: step 29003, loss 1.43231e-06, acc 1\n",
      "2018-10-26T18:43:44.418404: step 29004, loss 0, acc 1\n",
      "2018-10-26T18:43:44.629838: step 29005, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:43:44.858228: step 29006, loss 0, acc 1\n",
      "2018-10-26T18:43:45.030767: step 29007, loss 7.14906e-06, acc 1\n",
      "2018-10-26T18:43:45.211285: step 29008, loss 1.80676e-07, acc 1\n",
      "2018-10-26T18:43:45.377839: step 29009, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:45.553371: step 29010, loss 4.24677e-07, acc 1\n",
      "2018-10-26T18:43:45.723915: step 29011, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:45.903435: step 29012, loss 0, acc 1\n",
      "2018-10-26T18:43:46.074978: step 29013, loss 2.74505e-05, acc 1\n",
      "2018-10-26T18:43:46.255496: step 29014, loss 6.66813e-07, acc 1\n",
      "2018-10-26T18:43:46.424045: step 29015, loss 5.17807e-07, acc 1\n",
      "2018-10-26T18:43:46.602568: step 29016, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:43:46.771117: step 29017, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:43:46.950637: step 29018, loss 0, acc 1\n",
      "2018-10-26T18:43:47.122179: step 29019, loss 1.08961e-06, acc 1\n",
      "2018-10-26T18:43:47.300702: step 29020, loss 1.52171e-06, acc 1\n",
      "2018-10-26T18:43:47.471246: step 29021, loss 2.46759e-05, acc 1\n",
      "2018-10-26T18:43:47.661738: step 29022, loss 1.56641e-06, acc 1\n",
      "2018-10-26T18:43:47.832283: step 29023, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:43:48.010805: step 29024, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:43:48.178357: step 29025, loss 1.58324e-07, acc 1\n",
      "2018-10-26T18:43:48.359872: step 29026, loss 0.0003881, acc 1\n",
      "2018-10-26T18:43:48.528422: step 29027, loss 5.52206e-05, acc 1\n",
      "2018-10-26T18:43:48.706945: step 29028, loss 0, acc 1\n",
      "2018-10-26T18:43:48.871505: step 29029, loss 0, acc 1\n",
      "2018-10-26T18:43:49.048033: step 29030, loss 8.95917e-07, acc 1\n",
      "2018-10-26T18:43:49.222567: step 29031, loss 0.00198314, acc 1\n",
      "2018-10-26T18:43:49.404083: step 29032, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:49.574627: step 29033, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:43:49.753150: step 29034, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:43:49.922696: step 29035, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:43:50.102217: step 29036, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:43:50.267775: step 29037, loss 0, acc 1\n",
      "2018-10-26T18:43:50.446298: step 29038, loss 1.08033e-07, acc 1\n",
      "2018-10-26T18:43:50.625818: step 29039, loss 0.000341377, acc 1\n",
      "2018-10-26T18:43:50.810326: step 29040, loss 5.3457e-07, acc 1\n",
      "2018-10-26T18:43:50.992837: step 29041, loss 1.21071e-07, acc 1\n",
      "2018-10-26T18:43:51.170364: step 29042, loss 0, acc 1\n",
      "2018-10-26T18:43:51.337915: step 29043, loss 1.35224e-06, acc 1\n",
      "2018-10-26T18:43:51.515441: step 29044, loss 0, acc 1\n",
      "2018-10-26T18:43:51.685985: step 29045, loss 3.22469e-05, acc 1\n",
      "2018-10-26T18:43:51.859522: step 29046, loss 0, acc 1\n",
      "2018-10-26T18:43:52.030066: step 29047, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:43:52.211581: step 29048, loss 4.65661e-08, acc 1\n",
      "2018-10-26T18:43:52.387112: step 29049, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:43:52.572617: step 29050, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:43:52.752136: step 29051, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:43:52.932655: step 29052, loss 6.28614e-05, acc 1\n",
      "2018-10-26T18:43:53.114169: step 29053, loss 0, acc 1\n",
      "2018-10-26T18:43:53.289700: step 29054, loss 9.78744e-06, acc 1\n",
      "2018-10-26T18:43:53.455258: step 29055, loss 3.60939e-06, acc 1\n",
      "2018-10-26T18:43:53.631787: step 29056, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:53.799339: step 29057, loss 0, acc 1\n",
      "2018-10-26T18:43:53.977861: step 29058, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:43:54.151398: step 29059, loss 3.93013e-07, acc 1\n",
      "2018-10-26T18:43:54.337901: step 29060, loss 7.52491e-07, acc 1\n",
      "2018-10-26T18:43:54.513431: step 29061, loss 0, acc 1\n",
      "2018-10-26T18:43:54.701927: step 29062, loss 7.51649e-05, acc 1\n",
      "2018-10-26T18:43:54.870477: step 29063, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:43:55.049000: step 29064, loss 0, acc 1\n",
      "2018-10-26T18:43:55.220543: step 29065, loss 5.40167e-08, acc 1\n",
      "2018-10-26T18:43:55.402057: step 29066, loss 0, acc 1\n",
      "2018-10-26T18:43:55.574596: step 29067, loss 3.38596e-06, acc 1\n",
      "2018-10-26T18:43:55.766085: step 29068, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:43:55.936631: step 29069, loss 9.0126e-06, acc 1\n",
      "2018-10-26T18:43:56.114155: step 29070, loss 0.00207934, acc 1\n",
      "2018-10-26T18:43:56.281707: step 29071, loss 3.3341e-07, acc 1\n",
      "2018-10-26T18:43:56.458235: step 29072, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:43:56.622795: step 29073, loss 7.00496e-05, acc 1\n",
      "2018-10-26T18:43:56.807301: step 29074, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:43:56.995798: step 29075, loss 6.59237e-06, acc 1\n",
      "2018-10-26T18:43:57.216211: step 29076, loss 0, acc 1\n",
      "2018-10-26T18:43:57.390743: step 29077, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:57.573255: step 29078, loss 8.59653e-06, acc 1\n",
      "2018-10-26T18:43:57.740808: step 29079, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:43:57.929304: step 29080, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:43:58.095859: step 29081, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:58.276377: step 29082, loss 2.42485e-05, acc 1\n",
      "2018-10-26T18:43:58.492799: step 29083, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:43:58.672320: step 29084, loss 3.25419e-05, acc 1\n",
      "2018-10-26T18:43:58.848847: step 29085, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:43:59.031360: step 29086, loss 3.4066e-05, acc 1\n",
      "2018-10-26T18:43:59.197915: step 29087, loss 8.76709e-06, acc 1\n",
      "2018-10-26T18:43:59.379430: step 29088, loss 2.67842e-05, acc 1\n",
      "2018-10-26T18:43:59.546982: step 29089, loss 4.35801e-06, acc 1\n",
      "2018-10-26T18:43:59.729494: step 29090, loss 0, acc 1\n",
      "2018-10-26T18:43:59.900039: step 29091, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:44:00.092525: step 29092, loss 0, acc 1\n",
      "2018-10-26T18:44:00.261075: step 29093, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:44:00.447575: step 29094, loss 2.21082e-06, acc 1\n",
      "2018-10-26T18:44:00.623107: step 29095, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:44:00.799635: step 29096, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:44:00.969182: step 29097, loss 0, acc 1\n",
      "2018-10-26T18:44:01.149699: step 29098, loss 2.4028e-07, acc 1\n",
      "2018-10-26T18:44:01.317252: step 29099, loss 4.58204e-07, acc 1\n",
      "2018-10-26T18:44:01.498767: step 29100, loss 0.00208152, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:44:01.960533: step 29100, loss 10.2653, acc 0.711069\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29100\n",
      "\n",
      "2018-10-26T18:44:02.368130: step 29101, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:44:02.552638: step 29102, loss 5.77419e-08, acc 1\n",
      "2018-10-26T18:44:02.719193: step 29103, loss 6.48187e-05, acc 1\n",
      "2018-10-26T18:44:02.907690: step 29104, loss 4.88499e-06, acc 1\n",
      "2018-10-26T18:44:03.135081: step 29105, loss 3.84775e-06, acc 1\n",
      "2018-10-26T18:44:03.328565: step 29106, loss 9.05113e-06, acc 1\n",
      "2018-10-26T18:44:03.510079: step 29107, loss 1.38142e-05, acc 1\n",
      "2018-10-26T18:44:03.697578: step 29108, loss 1.88126e-07, acc 1\n",
      "2018-10-26T18:44:03.887072: step 29109, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:04.076566: step 29110, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:44:04.261073: step 29111, loss 0, acc 1\n",
      "2018-10-26T18:44:04.439596: step 29112, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:04.626098: step 29113, loss 0, acc 1\n",
      "2018-10-26T18:44:04.794647: step 29114, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:04.975165: step 29115, loss 5.43883e-07, acc 1\n",
      "2018-10-26T18:44:05.145709: step 29116, loss 3.91155e-08, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:44:05.333208: step 29117, loss 1.03375e-06, acc 1\n",
      "2018-10-26T18:44:05.499764: step 29118, loss 2.9057e-07, acc 1\n",
      "2018-10-26T18:44:05.677289: step 29119, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:44:05.859801: step 29120, loss 2.60016e-06, acc 1\n",
      "2018-10-26T18:44:06.035334: step 29121, loss 8.07807e-06, acc 1\n",
      "2018-10-26T18:44:06.215850: step 29122, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:44:06.383403: step 29123, loss 6.8419e-06, acc 1\n",
      "2018-10-26T18:44:06.554944: step 29124, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:44:06.717510: step 29125, loss 2.58907e-07, acc 1\n",
      "2018-10-26T18:44:06.890049: step 29126, loss 0.0436369, acc 0.984375\n",
      "2018-10-26T18:44:07.060593: step 29127, loss 1.35037e-06, acc 1\n",
      "2018-10-26T18:44:07.241111: step 29128, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:44:07.410657: step 29129, loss 0.000682957, acc 1\n",
      "2018-10-26T18:44:07.593169: step 29130, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:44:07.769698: step 29131, loss 5.13024e-05, acc 1\n",
      "2018-10-26T18:44:07.955203: step 29132, loss 5.53196e-07, acc 1\n",
      "2018-10-26T18:44:08.121758: step 29133, loss 0.000422477, acc 1\n",
      "2018-10-26T18:44:08.301279: step 29134, loss 1.47148e-07, acc 1\n",
      "2018-10-26T18:44:08.478803: step 29135, loss 5.0337e-05, acc 1\n",
      "2018-10-26T18:44:08.654334: step 29136, loss 0, acc 1\n",
      "2018-10-26T18:44:08.830863: step 29137, loss 1.56461e-07, acc 1\n",
      "2018-10-26T18:44:09.003402: step 29138, loss 1.62606e-06, acc 1\n",
      "2018-10-26T18:44:09.182922: step 29139, loss 5.27099e-05, acc 1\n",
      "2018-10-26T18:44:09.360449: step 29140, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:44:09.536976: step 29141, loss 0, acc 1\n",
      "2018-10-26T18:44:09.704529: step 29142, loss 0, acc 1\n",
      "2018-10-26T18:44:09.880059: step 29143, loss 8.73156e-06, acc 1\n",
      "2018-10-26T18:44:10.049607: step 29144, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:10.227132: step 29145, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:44:10.392690: step 29146, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:44:10.574204: step 29147, loss 4.80555e-07, acc 1\n",
      "2018-10-26T18:44:10.744749: step 29148, loss 0, acc 1\n",
      "2018-10-26T18:44:10.927262: step 29149, loss 3.44587e-07, acc 1\n",
      "2018-10-26T18:44:11.096808: step 29150, loss 4.50884e-06, acc 1\n",
      "2018-10-26T18:44:11.275332: step 29151, loss 0, acc 1\n",
      "2018-10-26T18:44:11.447870: step 29152, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:44:11.626394: step 29153, loss 0.00121564, acc 1\n",
      "2018-10-26T18:44:11.799930: step 29154, loss 0.00213404, acc 1\n",
      "2018-10-26T18:44:11.981445: step 29155, loss 7.2643e-08, acc 1\n",
      "2018-10-26T18:44:12.148998: step 29156, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:44:12.327522: step 29157, loss 2.41194e-06, acc 1\n",
      "2018-10-26T18:44:12.499062: step 29158, loss 1.30467e-05, acc 1\n",
      "2018-10-26T18:44:12.677585: step 29159, loss 0, acc 1\n",
      "2018-10-26T18:44:12.853117: step 29160, loss 3.49775e-06, acc 1\n",
      "2018-10-26T18:44:13.025655: step 29161, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:13.196200: step 29162, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:44:13.372728: step 29163, loss 3.48311e-07, acc 1\n",
      "2018-10-26T18:44:13.543272: step 29164, loss 0, acc 1\n",
      "2018-10-26T18:44:13.726782: step 29165, loss 6.3131e-06, acc 1\n",
      "2018-10-26T18:44:13.894334: step 29166, loss 6.18667e-06, acc 1\n",
      "2018-10-26T18:44:14.077843: step 29167, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:44:14.243402: step 29168, loss 0, acc 1\n",
      "2018-10-26T18:44:14.422922: step 29169, loss 2.79394e-07, acc 1\n",
      "2018-10-26T18:44:14.600448: step 29170, loss 0.000158305, acc 1\n",
      "2018-10-26T18:44:14.775978: step 29171, loss 8.49343e-07, acc 1\n",
      "2018-10-26T18:44:14.943531: step 29172, loss 0.000997823, acc 1\n",
      "2018-10-26T18:44:15.118065: step 29173, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:44:15.287611: step 29174, loss 3.13452e-06, acc 1\n",
      "2018-10-26T18:44:15.476108: step 29175, loss 7.38089e-05, acc 1\n",
      "2018-10-26T18:44:15.646653: step 29176, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:44:15.823181: step 29177, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:44:15.991730: step 29178, loss 0, acc 1\n",
      "2018-10-26T18:44:16.175240: step 29179, loss 1.93715e-07, acc 1\n",
      "2018-10-26T18:44:16.352766: step 29180, loss 8.61981e-06, acc 1\n",
      "2018-10-26T18:44:16.531288: step 29181, loss 1.52717e-05, acc 1\n",
      "2018-10-26T18:44:16.698841: step 29182, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:44:16.872378: step 29183, loss 4.28403e-07, acc 1\n",
      "2018-10-26T18:44:17.044916: step 29184, loss 0, acc 1\n",
      "2018-10-26T18:44:17.223439: step 29185, loss 1.85868e-05, acc 1\n",
      "2018-10-26T18:44:17.388997: step 29186, loss 1.21071e-07, acc 1\n",
      "2018-10-26T18:44:17.565525: step 29187, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:17.733078: step 29188, loss 1.89747e-05, acc 1\n",
      "2018-10-26T18:44:17.915591: step 29189, loss 3.4511e-06, acc 1\n",
      "2018-10-26T18:44:18.081147: step 29190, loss 0, acc 1\n",
      "2018-10-26T18:44:18.254684: step 29191, loss 0, acc 1\n",
      "2018-10-26T18:44:18.424231: step 29192, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:44:18.603751: step 29193, loss 1.4001e-05, acc 1\n",
      "2018-10-26T18:44:18.773298: step 29194, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:44:18.962792: step 29195, loss 5.30764e-06, acc 1\n",
      "2018-10-26T18:44:19.135332: step 29196, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:44:19.312857: step 29197, loss 4.09249e-05, acc 1\n",
      "2018-10-26T18:44:19.482403: step 29198, loss 0, acc 1\n",
      "2018-10-26T18:44:19.658932: step 29199, loss 3.29035e-05, acc 1\n",
      "2018-10-26T18:44:19.831472: step 29200, loss 3.72529e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:44:20.293237: step 29200, loss 10.6687, acc 0.701689\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29200\n",
      "\n",
      "2018-10-26T18:44:20.669832: step 29201, loss 0.0113858, acc 0.984375\n",
      "2018-10-26T18:44:20.850350: step 29202, loss 1.44536e-06, acc 1\n",
      "2018-10-26T18:44:21.024883: step 29203, loss 0, acc 1\n",
      "2018-10-26T18:44:21.205400: step 29204, loss 0, acc 1\n",
      "2018-10-26T18:44:21.387913: step 29205, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:44:21.641237: step 29206, loss 6.27699e-07, acc 1\n",
      "2018-10-26T18:44:21.806794: step 29207, loss 0, acc 1\n",
      "2018-10-26T18:44:21.992298: step 29208, loss 2.45545e-05, acc 1\n",
      "2018-10-26T18:44:22.160848: step 29209, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:22.346352: step 29210, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:44:22.518891: step 29211, loss 3.11059e-07, acc 1\n",
      "2018-10-26T18:44:22.702401: step 29212, loss 0, acc 1\n",
      "2018-10-26T18:44:22.870951: step 29213, loss 1.80676e-07, acc 1\n",
      "2018-10-26T18:44:23.056455: step 29214, loss 2.12625e-05, acc 1\n",
      "2018-10-26T18:44:23.231986: step 29215, loss 2.28532e-06, acc 1\n",
      "2018-10-26T18:44:23.410509: step 29216, loss 2.57043e-07, acc 1\n",
      "2018-10-26T18:44:23.578061: step 29217, loss 3.20721e-06, acc 1\n",
      "2018-10-26T18:44:23.765561: step 29218, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:44:23.931118: step 29219, loss 5.7742e-08, acc 1\n",
      "2018-10-26T18:44:24.121609: step 29220, loss 0, acc 1\n",
      "2018-10-26T18:44:24.298137: step 29221, loss 0.000226257, acc 1\n",
      "2018-10-26T18:44:24.488629: step 29222, loss 2.7528e-06, acc 1\n",
      "2018-10-26T18:44:24.654186: step 29223, loss 8.20011e-06, acc 1\n",
      "2018-10-26T18:44:24.854650: step 29224, loss 0, acc 1\n",
      "2018-10-26T18:44:25.021206: step 29225, loss 9.46335e-06, acc 1\n",
      "2018-10-26T18:44:25.198731: step 29226, loss 6.17345e-06, acc 1\n",
      "2018-10-26T18:44:25.369275: step 29227, loss 0, acc 1\n",
      "2018-10-26T18:44:25.553783: step 29228, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:44:25.725324: step 29229, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:44:25.910829: step 29230, loss 6.23974e-07, acc 1\n",
      "2018-10-26T18:44:26.081373: step 29231, loss 0.000579708, acc 1\n",
      "2018-10-26T18:44:26.263886: step 29232, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:44:26.441411: step 29233, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:44:26.620931: step 29234, loss 0, acc 1\n",
      "2018-10-26T18:44:26.792472: step 29235, loss 0, acc 1\n",
      "2018-10-26T18:44:26.968003: step 29236, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:27.145530: step 29237, loss 5.12219e-07, acc 1\n",
      "2018-10-26T18:44:27.330037: step 29238, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:44:27.495595: step 29239, loss 0, acc 1\n",
      "2018-10-26T18:44:27.678108: step 29240, loss 0, acc 1\n",
      "2018-10-26T18:44:27.845663: step 29241, loss 3.43987e-05, acc 1\n",
      "2018-10-26T18:44:28.028172: step 29242, loss 5.31322e-06, acc 1\n",
      "2018-10-26T18:44:28.197719: step 29243, loss 0, acc 1\n",
      "2018-10-26T18:44:28.388209: step 29244, loss 1.23984e-05, acc 1\n",
      "2018-10-26T18:44:28.552770: step 29245, loss 5.58793e-09, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:44:28.731293: step 29246, loss 0, acc 1\n",
      "2018-10-26T18:44:28.895852: step 29247, loss 5.77409e-07, acc 1\n",
      "2018-10-26T18:44:29.071384: step 29248, loss 4.76831e-07, acc 1\n",
      "2018-10-26T18:44:29.239934: step 29249, loss 0, acc 1\n",
      "2018-10-26T18:44:29.413470: step 29250, loss 0, acc 1\n",
      "2018-10-26T18:44:29.581023: step 29251, loss 1.45838e-06, acc 1\n",
      "2018-10-26T18:44:29.765530: step 29252, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:44:29.939066: step 29253, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:44:30.117589: step 29254, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:44:30.283147: step 29255, loss 2.14225e-05, acc 1\n",
      "2018-10-26T18:44:30.463664: step 29256, loss 0, acc 1\n",
      "2018-10-26T18:44:30.628225: step 29257, loss 5.01044e-07, acc 1\n",
      "2018-10-26T18:44:30.811734: step 29258, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:44:30.980284: step 29259, loss 3.53364e-05, acc 1\n",
      "2018-10-26T18:44:31.156812: step 29260, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:44:31.370241: step 29261, loss 9.49947e-08, acc 1\n",
      "2018-10-26T18:44:31.583672: step 29262, loss 0, acc 1\n",
      "2018-10-26T18:44:31.768179: step 29263, loss 5.66234e-07, acc 1\n",
      "2018-10-26T18:44:31.979614: step 29264, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:44:32.156143: step 29265, loss 2.02829e-06, acc 1\n",
      "2018-10-26T18:44:32.362608: step 29266, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:44:32.529145: step 29267, loss 3.66537e-06, acc 1\n",
      "2018-10-26T18:44:32.709662: step 29268, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:32.912123: step 29269, loss 0, acc 1\n",
      "2018-10-26T18:44:33.088650: step 29270, loss 0, acc 1\n",
      "2018-10-26T18:44:33.287120: step 29271, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:33.485590: step 29272, loss 5.19669e-07, acc 1\n",
      "2018-10-26T18:44:33.663115: step 29273, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:44:33.846625: step 29274, loss 0, acc 1\n",
      "2018-10-26T18:44:34.026146: step 29275, loss 0, acc 1\n",
      "2018-10-26T18:44:34.198685: step 29276, loss 0, acc 1\n",
      "2018-10-26T18:44:34.374216: step 29277, loss 0, acc 1\n",
      "2018-10-26T18:44:34.599613: step 29278, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:44:34.795094: step 29279, loss 0.0313221, acc 0.984375\n",
      "2018-10-26T18:44:34.992563: step 29280, loss 0.000104711, acc 1\n",
      "2018-10-26T18:44:35.243891: step 29281, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:44:35.461313: step 29282, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:44:35.677733: step 29283, loss 0.000327278, acc 1\n",
      "2018-10-26T18:44:35.865232: step 29284, loss 0, acc 1\n",
      "2018-10-26T18:44:36.081654: step 29285, loss 2.72776e-05, acc 1\n",
      "2018-10-26T18:44:36.272146: step 29286, loss 6.53782e-07, acc 1\n",
      "2018-10-26T18:44:36.453660: step 29287, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:44:36.666092: step 29288, loss 0, acc 1\n",
      "2018-10-26T18:44:36.837633: step 29289, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:44:37.020147: step 29290, loss 6.16527e-07, acc 1\n",
      "2018-10-26T18:44:37.240558: step 29291, loss 2.17728e-06, acc 1\n",
      "2018-10-26T18:44:37.442019: step 29292, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:44:37.630516: step 29293, loss 0.00018245, acc 1\n",
      "2018-10-26T18:44:37.867881: step 29294, loss 0, acc 1\n",
      "2018-10-26T18:44:38.045407: step 29295, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:44:38.262827: step 29296, loss 2.53318e-07, acc 1\n",
      "2018-10-26T18:44:38.471268: step 29297, loss 0, acc 1\n",
      "2018-10-26T18:44:38.651786: step 29298, loss 0, acc 1\n",
      "2018-10-26T18:44:38.869205: step 29299, loss 9.66548e-05, acc 1\n",
      "2018-10-26T18:44:39.038753: step 29300, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:44:39.567340: step 29300, loss 10.7721, acc 0.708255\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29300\n",
      "\n",
      "2018-10-26T18:44:39.924634: step 29301, loss 1.56763e-05, acc 1\n",
      "2018-10-26T18:44:40.135074: step 29302, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:44:40.308609: step 29303, loss 9.49946e-08, acc 1\n",
      "2018-10-26T18:44:40.488129: step 29304, loss 8.12911e-06, acc 1\n",
      "2018-10-26T18:44:40.752423: step 29305, loss 7.15241e-07, acc 1\n",
      "2018-10-26T18:44:40.940919: step 29306, loss 9.81598e-07, acc 1\n",
      "2018-10-26T18:44:41.120439: step 29307, loss 8.38168e-07, acc 1\n",
      "2018-10-26T18:44:41.316915: step 29308, loss 0.0789601, acc 0.984375\n",
      "2018-10-26T18:44:41.536328: step 29309, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:41.715850: step 29310, loss 0.000744998, acc 1\n",
      "2018-10-26T18:44:41.905343: step 29311, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:44:42.085861: step 29312, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:42.265381: step 29313, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:44:42.435924: step 29314, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:44:42.618437: step 29315, loss 0, acc 1\n",
      "2018-10-26T18:44:42.791974: step 29316, loss 2.50065e-05, acc 1\n",
      "2018-10-26T18:44:42.967505: step 29317, loss 9.18257e-07, acc 1\n",
      "2018-10-26T18:44:43.136054: step 29318, loss 1.80275e-05, acc 1\n",
      "2018-10-26T18:44:43.316572: step 29319, loss 0, acc 1\n",
      "2018-10-26T18:44:43.516039: step 29320, loss 0, acc 1\n",
      "2018-10-26T18:44:43.720492: step 29321, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:43.902008: step 29322, loss 1.75088e-07, acc 1\n",
      "2018-10-26T18:44:44.119426: step 29323, loss 1.572e-06, acc 1\n",
      "2018-10-26T18:44:44.331859: step 29324, loss 0, acc 1\n",
      "2018-10-26T18:44:44.520356: step 29325, loss 1.13621e-07, acc 1\n",
      "2018-10-26T18:44:44.733785: step 29326, loss 0.000134912, acc 1\n",
      "2018-10-26T18:44:44.911312: step 29327, loss 3.32355e-05, acc 1\n",
      "2018-10-26T18:44:45.124741: step 29328, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:45.310246: step 29329, loss 2.24805e-06, acc 1\n",
      "2018-10-26T18:44:45.518688: step 29330, loss 8.35752e-05, acc 1\n",
      "2018-10-26T18:44:45.699206: step 29331, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:44:45.881718: step 29332, loss 8.48455e-05, acc 1\n",
      "2018-10-26T18:44:46.097143: step 29333, loss 2.92782e-06, acc 1\n",
      "2018-10-26T18:44:46.264695: step 29334, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:44:46.447207: step 29335, loss 7.09652e-07, acc 1\n",
      "2018-10-26T18:44:46.653656: step 29336, loss 0, acc 1\n",
      "2018-10-26T18:44:46.832179: step 29337, loss 3.37688e-05, acc 1\n",
      "2018-10-26T18:44:47.010701: step 29338, loss 4.88389e-05, acc 1\n",
      "2018-10-26T18:44:47.189225: step 29339, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:44:47.410634: step 29340, loss 0, acc 1\n",
      "2018-10-26T18:44:47.584169: step 29341, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:47.767679: step 29342, loss 0.000864789, acc 1\n",
      "2018-10-26T18:44:47.972133: step 29343, loss 2.7567e-07, acc 1\n",
      "2018-10-26T18:44:48.154645: step 29344, loss 0, acc 1\n",
      "2018-10-26T18:44:48.336160: step 29345, loss 7.54353e-07, acc 1\n",
      "2018-10-26T18:44:48.515681: step 29346, loss 2.53635e-05, acc 1\n",
      "2018-10-26T18:44:48.683232: step 29347, loss 0, acc 1\n",
      "2018-10-26T18:44:48.869735: step 29348, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:44:49.033298: step 29349, loss 0, acc 1\n",
      "2018-10-26T18:44:49.211821: step 29350, loss 4.08238e-06, acc 1\n",
      "2018-10-26T18:44:49.381368: step 29351, loss 2.38417e-07, acc 1\n",
      "2018-10-26T18:44:49.565876: step 29352, loss 0, acc 1\n",
      "2018-10-26T18:44:49.732430: step 29353, loss 1.21071e-07, acc 1\n",
      "2018-10-26T18:44:49.913945: step 29354, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:44:50.090475: step 29355, loss 0.000183149, acc 1\n",
      "2018-10-26T18:44:50.270990: step 29356, loss 7.8231e-08, acc 1\n",
      "2018-10-26T18:44:50.456495: step 29357, loss 1.978e-06, acc 1\n",
      "2018-10-26T18:44:50.630032: step 29358, loss 5.73686e-07, acc 1\n",
      "2018-10-26T18:44:50.815545: step 29359, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:44:50.985084: step 29360, loss 6.02205e-05, acc 1\n",
      "2018-10-26T18:44:51.172582: step 29361, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:44:51.341564: step 29362, loss 3.92209e-05, acc 1\n",
      "2018-10-26T18:44:51.528045: step 29363, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:44:51.698589: step 29364, loss 4.47028e-07, acc 1\n",
      "2018-10-26T18:44:51.885090: step 29365, loss 2.67267e-06, acc 1\n",
      "2018-10-26T18:44:52.055635: step 29366, loss 1.89989e-07, acc 1\n",
      "2018-10-26T18:44:52.236152: step 29367, loss 2.15356e-05, acc 1\n",
      "2018-10-26T18:44:52.406696: step 29368, loss 0, acc 1\n",
      "2018-10-26T18:44:52.592202: step 29369, loss 0.00068959, acc 1\n",
      "2018-10-26T18:44:52.760751: step 29370, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:44:52.947252: step 29371, loss 0, acc 1\n",
      "2018-10-26T18:44:53.116800: step 29372, loss 7.86023e-07, acc 1\n",
      "2018-10-26T18:44:53.302304: step 29373, loss 5.58187e-05, acc 1\n",
      "2018-10-26T18:44:53.473845: step 29374, loss 0, acc 1\n",
      "2018-10-26T18:44:53.657355: step 29375, loss 2.54506e-05, acc 1\n",
      "2018-10-26T18:44:53.833883: step 29376, loss 1.86265e-09, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:44:54.016396: step 29377, loss 1.11383e-06, acc 1\n",
      "2018-10-26T18:44:54.191927: step 29378, loss 0, acc 1\n",
      "2018-10-26T18:44:54.367458: step 29379, loss 1.27586e-06, acc 1\n",
      "2018-10-26T18:44:54.542989: step 29380, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:44:54.722509: step 29381, loss 0, acc 1\n",
      "2018-10-26T18:44:54.897043: step 29382, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:55.069582: step 29383, loss 9.24356e-06, acc 1\n",
      "2018-10-26T18:44:55.244116: step 29384, loss 1.99105e-06, acc 1\n",
      "2018-10-26T18:44:55.416654: step 29385, loss 1.78059e-06, acc 1\n",
      "2018-10-26T18:44:55.591189: step 29386, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:44:55.755749: step 29387, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:44:55.928289: step 29388, loss 0.000218665, acc 1\n",
      "2018-10-26T18:44:56.094843: step 29389, loss 7.45056e-08, acc 1\n",
      "2018-10-26T18:44:56.266384: step 29390, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:44:56.440918: step 29391, loss 5.32708e-07, acc 1\n",
      "2018-10-26T18:44:56.616448: step 29392, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:44:56.784001: step 29393, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:44:56.963522: step 29394, loss 1.19209e-07, acc 1\n",
      "2018-10-26T18:44:57.136061: step 29395, loss 0, acc 1\n",
      "2018-10-26T18:44:57.315581: step 29396, loss 0, acc 1\n",
      "2018-10-26T18:44:57.484131: step 29397, loss 6.09075e-07, acc 1\n",
      "2018-10-26T18:44:57.673625: step 29398, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:44:57.840179: step 29399, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:44:58.008729: step 29400, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:44:58.473487: step 29400, loss 10.2512, acc 0.714822\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29400\n",
      "\n",
      "2018-10-26T18:44:58.854285: step 29401, loss 0, acc 1\n",
      "2018-10-26T18:44:59.034801: step 29402, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:44:59.204348: step 29403, loss 0, acc 1\n",
      "2018-10-26T18:44:59.378882: step 29404, loss 0, acc 1\n",
      "2018-10-26T18:44:59.573362: step 29405, loss 7.78578e-06, acc 1\n",
      "2018-10-26T18:44:59.808733: step 29406, loss 3.18771e-05, acc 1\n",
      "2018-10-26T18:44:59.977284: step 29407, loss 2.38783e-06, acc 1\n",
      "2018-10-26T18:45:00.175753: step 29408, loss 7.00345e-07, acc 1\n",
      "2018-10-26T18:45:00.341311: step 29409, loss 2.93525e-06, acc 1\n",
      "2018-10-26T18:45:00.521828: step 29410, loss 0, acc 1\n",
      "2018-10-26T18:45:00.691376: step 29411, loss 6.89177e-08, acc 1\n",
      "2018-10-26T18:45:00.872890: step 29412, loss 3.1606e-06, acc 1\n",
      "2018-10-26T18:45:01.075350: step 29413, loss 0, acc 1\n",
      "2018-10-26T18:45:01.248885: step 29414, loss 5.58784e-07, acc 1\n",
      "2018-10-26T18:45:01.424417: step 29415, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:45:01.590972: step 29416, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:01.770493: step 29417, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:45:01.942034: step 29418, loss 0.000118351, acc 1\n",
      "2018-10-26T18:45:02.127539: step 29419, loss 5.54289e-05, acc 1\n",
      "2018-10-26T18:45:02.295090: step 29420, loss 0, acc 1\n",
      "2018-10-26T18:45:02.473614: step 29421, loss 9.41704e-05, acc 1\n",
      "2018-10-26T18:45:02.641166: step 29422, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:45:02.826671: step 29423, loss 5.00229e-06, acc 1\n",
      "2018-10-26T18:45:02.998797: step 29424, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:45:03.177319: step 29425, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:45:03.341880: step 29426, loss 2.29104e-07, acc 1\n",
      "2018-10-26T18:45:03.517410: step 29427, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:45:03.684962: step 29428, loss 0.000486384, acc 1\n",
      "2018-10-26T18:45:03.865480: step 29429, loss 1.79549e-06, acc 1\n",
      "2018-10-26T18:45:04.029044: step 29430, loss 0, acc 1\n",
      "2018-10-26T18:45:04.210573: step 29431, loss 1.50874e-07, acc 1\n",
      "2018-10-26T18:45:04.378111: step 29432, loss 4.25965e-05, acc 1\n",
      "2018-10-26T18:45:04.557655: step 29433, loss 0, acc 1\n",
      "2018-10-26T18:45:04.729173: step 29434, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:45:04.917669: step 29435, loss 4.26875e-05, acc 1\n",
      "2018-10-26T18:45:05.090209: step 29436, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:45:05.266737: step 29437, loss 1.7695e-07, acc 1\n",
      "2018-10-26T18:45:05.434289: step 29438, loss 3.01746e-07, acc 1\n",
      "2018-10-26T18:45:05.617798: step 29439, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:05.784354: step 29440, loss 0, acc 1\n",
      "2018-10-26T18:45:05.967864: step 29441, loss 0.0897599, acc 0.984375\n",
      "2018-10-26T18:45:06.136412: step 29442, loss 0, acc 1\n",
      "2018-10-26T18:45:06.315933: step 29443, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:45:06.487475: step 29444, loss 1.34967e-05, acc 1\n",
      "2018-10-26T18:45:06.665001: step 29445, loss 0, acc 1\n",
      "2018-10-26T18:45:06.834548: step 29446, loss 1.30647e-05, acc 1\n",
      "2018-10-26T18:45:07.023044: step 29447, loss 2.94941e-05, acc 1\n",
      "2018-10-26T18:45:07.196580: step 29448, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:45:07.374105: step 29449, loss 0, acc 1\n",
      "2018-10-26T18:45:07.564598: step 29450, loss 1.35972e-07, acc 1\n",
      "2018-10-26T18:45:07.734144: step 29451, loss 1.28522e-07, acc 1\n",
      "2018-10-26T18:45:07.913664: step 29452, loss 4.86143e-07, acc 1\n",
      "2018-10-26T18:45:08.080219: step 29453, loss 2.70081e-07, acc 1\n",
      "2018-10-26T18:45:08.264727: step 29454, loss 8.90545e-06, acc 1\n",
      "2018-10-26T18:45:08.436268: step 29455, loss 0, acc 1\n",
      "2018-10-26T18:45:08.615788: step 29456, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:45:08.783341: step 29457, loss 8.04628e-05, acc 1\n",
      "2018-10-26T18:45:08.973832: step 29458, loss 7.13742e-05, acc 1\n",
      "2018-10-26T18:45:09.141384: step 29459, loss 0, acc 1\n",
      "2018-10-26T18:45:09.319907: step 29460, loss 0, acc 1\n",
      "2018-10-26T18:45:09.488457: step 29461, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:45:09.669972: step 29462, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:45:09.839519: step 29463, loss 0, acc 1\n",
      "2018-10-26T18:45:10.021034: step 29464, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:45:10.194599: step 29465, loss 1.57463e-05, acc 1\n",
      "2018-10-26T18:45:10.372096: step 29466, loss 0, acc 1\n",
      "2018-10-26T18:45:10.545633: step 29467, loss 0, acc 1\n",
      "2018-10-26T18:45:10.732134: step 29468, loss 0.000259376, acc 1\n",
      "2018-10-26T18:45:10.899686: step 29469, loss 4.57957e-06, acc 1\n",
      "2018-10-26T18:45:11.081201: step 29470, loss 0, acc 1\n",
      "2018-10-26T18:45:11.257730: step 29471, loss 2.33003e-06, acc 1\n",
      "2018-10-26T18:45:11.436253: step 29472, loss 0, acc 1\n",
      "2018-10-26T18:45:11.604802: step 29473, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:45:11.784323: step 29474, loss 7.43845e-05, acc 1\n",
      "2018-10-26T18:45:11.960850: step 29475, loss 5.97903e-07, acc 1\n",
      "2018-10-26T18:45:12.144360: step 29476, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:45:12.318895: step 29477, loss 0.000153146, acc 1\n",
      "2018-10-26T18:45:12.503402: step 29478, loss 0, acc 1\n",
      "2018-10-26T18:45:12.670954: step 29479, loss 4.80922e-05, acc 1\n",
      "2018-10-26T18:45:12.854463: step 29480, loss 0, acc 1\n",
      "2018-10-26T18:45:13.028000: step 29481, loss 0.000183851, acc 1\n",
      "2018-10-26T18:45:13.208517: step 29482, loss 5.20152e-06, acc 1\n",
      "2018-10-26T18:45:13.383051: step 29483, loss 0, acc 1\n",
      "2018-10-26T18:45:13.567559: step 29484, loss 0, acc 1\n",
      "2018-10-26T18:45:13.734112: step 29485, loss 0.000196779, acc 1\n",
      "2018-10-26T18:45:13.913655: step 29486, loss 2.68219e-07, acc 1\n",
      "2018-10-26T18:45:14.080188: step 29487, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:45:14.260706: step 29488, loss 0, acc 1\n",
      "2018-10-26T18:45:14.432248: step 29489, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:45:14.615757: step 29490, loss 9.90898e-07, acc 1\n",
      "2018-10-26T18:45:14.791288: step 29491, loss 0, acc 1\n",
      "2018-10-26T18:45:14.972804: step 29492, loss 1.00581e-06, acc 1\n",
      "2018-10-26T18:45:15.159305: step 29493, loss 5.4968e-05, acc 1\n",
      "2018-10-26T18:45:15.328852: step 29494, loss 0.000292853, acc 1\n",
      "2018-10-26T18:45:15.512362: step 29495, loss 0, acc 1\n",
      "2018-10-26T18:45:15.681908: step 29496, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:45:15.861429: step 29497, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:45:16.036960: step 29498, loss 0, acc 1\n",
      "2018-10-26T18:45:16.219472: step 29499, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:16.386028: step 29500, loss 4.04191e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:45:16.860759: step 29500, loss 10.0544, acc 0.726079\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29500\n",
      "\n",
      "2018-10-26T18:45:17.237637: step 29501, loss 9.02939e-06, acc 1\n",
      "2018-10-26T18:45:17.424120: step 29502, loss 6.51925e-08, acc 1\n",
      "2018-10-26T18:45:17.590675: step 29503, loss 0, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:45:17.774185: step 29504, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:45:17.995594: step 29505, loss 0.0813344, acc 0.984375\n",
      "2018-10-26T18:45:18.207028: step 29506, loss 0.000654618, acc 1\n",
      "2018-10-26T18:45:18.373584: step 29507, loss 2.76017e-05, acc 1\n",
      "2018-10-26T18:45:18.555098: step 29508, loss 9.77858e-07, acc 1\n",
      "2018-10-26T18:45:18.727636: step 29509, loss 1.34105e-06, acc 1\n",
      "2018-10-26T18:45:18.906161: step 29510, loss 0, acc 1\n",
      "2018-10-26T18:45:19.089671: step 29511, loss 5.20336e-06, acc 1\n",
      "2018-10-26T18:45:19.266198: step 29512, loss 7.68823e-06, acc 1\n",
      "2018-10-26T18:45:19.445718: step 29513, loss 9.87199e-08, acc 1\n",
      "2018-10-26T18:45:19.614268: step 29514, loss 0, acc 1\n",
      "2018-10-26T18:45:19.791794: step 29515, loss 1.9538e-06, acc 1\n",
      "2018-10-26T18:45:19.958349: step 29516, loss 7.12115e-06, acc 1\n",
      "2018-10-26T18:45:20.135874: step 29517, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:45:20.306419: step 29518, loss 7.63671e-07, acc 1\n",
      "2018-10-26T18:45:20.480952: step 29519, loss 3.68574e-06, acc 1\n",
      "2018-10-26T18:45:20.651496: step 29520, loss 5.2154e-08, acc 1\n",
      "2018-10-26T18:45:20.828025: step 29521, loss 0, acc 1\n",
      "2018-10-26T18:45:21.003557: step 29522, loss 3.18509e-07, acc 1\n",
      "2018-10-26T18:45:21.179088: step 29523, loss 3.75141e-05, acc 1\n",
      "2018-10-26T18:45:21.353630: step 29524, loss 8.38189e-08, acc 1\n",
      "2018-10-26T18:45:21.530149: step 29525, loss 0.000175288, acc 1\n",
      "2018-10-26T18:45:21.705680: step 29526, loss 2.52368e-06, acc 1\n",
      "2018-10-26T18:45:21.874230: step 29527, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:22.059734: step 29528, loss 9.68573e-08, acc 1\n",
      "2018-10-26T18:45:22.267179: step 29529, loss 0, acc 1\n",
      "2018-10-26T18:45:22.452683: step 29530, loss 1.30385e-07, acc 1\n",
      "2018-10-26T18:45:22.641181: step 29531, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:45:22.834663: step 29532, loss 0, acc 1\n",
      "2018-10-26T18:45:23.028147: step 29533, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:45:23.223624: step 29534, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:45:23.407134: step 29535, loss 1.34291e-06, acc 1\n",
      "2018-10-26T18:45:23.583663: step 29536, loss 4.02327e-07, acc 1\n",
      "2018-10-26T18:45:23.758203: step 29537, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:45:23.930736: step 29538, loss 0, acc 1\n",
      "2018-10-26T18:45:24.103273: step 29539, loss 1.50093e-05, acc 1\n",
      "2018-10-26T18:45:24.268832: step 29540, loss 4.89868e-07, acc 1\n",
      "2018-10-26T18:45:24.449349: step 29541, loss 1.95359e-05, acc 1\n",
      "2018-10-26T18:45:24.624881: step 29542, loss 0, acc 1\n",
      "2018-10-26T18:45:24.801409: step 29543, loss 0, acc 1\n",
      "2018-10-26T18:45:24.971953: step 29544, loss 3.31549e-07, acc 1\n",
      "2018-10-26T18:45:25.149479: step 29545, loss 0, acc 1\n",
      "2018-10-26T18:45:25.322018: step 29546, loss 5.96046e-08, acc 1\n",
      "2018-10-26T18:45:25.509516: step 29547, loss 6.09515e-05, acc 1\n",
      "2018-10-26T18:45:25.698013: step 29548, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:45:25.920419: step 29549, loss 8.50252e-06, acc 1\n",
      "2018-10-26T18:45:26.094953: step 29550, loss 0.000171446, acc 1\n",
      "2018-10-26T18:45:26.286441: step 29551, loss 6.35954e-05, acc 1\n",
      "2018-10-26T18:45:26.453993: step 29552, loss 1.66326e-06, acc 1\n",
      "2018-10-26T18:45:26.665428: step 29553, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:45:26.831984: step 29554, loss 0, acc 1\n",
      "2018-10-26T18:45:27.026463: step 29555, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:45:27.200998: step 29556, loss 7.91312e-05, acc 1\n",
      "2018-10-26T18:45:27.381515: step 29557, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:27.548070: step 29558, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:45:27.767484: step 29559, loss 9.12694e-08, acc 1\n",
      "2018-10-26T18:45:27.933041: step 29560, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:45:28.112562: step 29561, loss 1.54914e-05, acc 1\n",
      "2018-10-26T18:45:28.324994: step 29562, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:28.512493: step 29563, loss 4.13502e-07, acc 1\n",
      "2018-10-26T18:45:28.685034: step 29564, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:45:28.946334: step 29565, loss 0, acc 1\n",
      "2018-10-26T18:45:29.163754: step 29566, loss 4.01244e-05, acc 1\n",
      "2018-10-26T18:45:29.361227: step 29567, loss 2.12795e-05, acc 1\n",
      "2018-10-26T18:45:29.545733: step 29568, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:45:29.738218: step 29569, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:45:29.937685: step 29570, loss 5.1967e-07, acc 1\n",
      "2018-10-26T18:45:30.120198: step 29571, loss 9.36258e-06, acc 1\n",
      "2018-10-26T18:45:30.294732: step 29572, loss 0, acc 1\n",
      "2018-10-26T18:45:30.471260: step 29573, loss 4.97318e-07, acc 1\n",
      "2018-10-26T18:45:30.639810: step 29574, loss 2.25178e-06, acc 1\n",
      "2018-10-26T18:45:30.818333: step 29575, loss 0, acc 1\n",
      "2018-10-26T18:45:30.987880: step 29576, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:45:31.164407: step 29577, loss 6.40224e-05, acc 1\n",
      "2018-10-26T18:45:31.328968: step 29578, loss 0, acc 1\n",
      "2018-10-26T18:45:31.522452: step 29579, loss 1.22934e-07, acc 1\n",
      "2018-10-26T18:45:31.696984: step 29580, loss 1.44536e-06, acc 1\n",
      "2018-10-26T18:45:31.868527: step 29581, loss 0, acc 1\n",
      "2018-10-26T18:45:32.052036: step 29582, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:45:32.224576: step 29583, loss 0.0180477, acc 0.984375\n",
      "2018-10-26T18:45:32.402100: step 29584, loss 0.000218338, acc 1\n",
      "2018-10-26T18:45:32.569653: step 29585, loss 2.75834e-06, acc 1\n",
      "2018-10-26T18:45:32.748176: step 29586, loss 3.05471e-07, acc 1\n",
      "2018-10-26T18:45:32.920715: step 29587, loss 0.000122579, acc 1\n",
      "2018-10-26T18:45:33.101233: step 29588, loss 0, acc 1\n",
      "2018-10-26T18:45:33.276764: step 29589, loss 0.000106918, acc 1\n",
      "2018-10-26T18:45:33.455287: step 29590, loss 3.33849e-05, acc 1\n",
      "2018-10-26T18:45:33.633809: step 29591, loss 0.0166573, acc 0.984375\n",
      "2018-10-26T18:45:33.798371: step 29592, loss 3.46602e-06, acc 1\n",
      "2018-10-26T18:45:33.983875: step 29593, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:45:34.152425: step 29594, loss 0, acc 1\n",
      "2018-10-26T18:45:34.330947: step 29595, loss 0, acc 1\n",
      "2018-10-26T18:45:34.497503: step 29596, loss 1.62791e-06, acc 1\n",
      "2018-10-26T18:45:34.685003: step 29597, loss 6.51924e-08, acc 1\n",
      "2018-10-26T18:45:34.853551: step 29598, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:45:35.033072: step 29599, loss 0.000279342, acc 1\n",
      "2018-10-26T18:45:35.203615: step 29600, loss 4.47034e-08, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:45:35.662389: step 29600, loss 10.1571, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29600\n",
      "\n",
      "2018-10-26T18:45:36.043674: step 29601, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:36.229178: step 29602, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:45:36.403712: step 29603, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:45:36.588218: step 29604, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:45:36.806636: step 29605, loss 0, acc 1\n",
      "2018-10-26T18:45:37.031036: step 29606, loss 1.21072e-07, acc 1\n",
      "2018-10-26T18:45:37.233496: step 29607, loss 7.82309e-08, acc 1\n",
      "2018-10-26T18:45:37.444929: step 29608, loss 2.20371e-05, acc 1\n",
      "2018-10-26T18:45:37.648388: step 29609, loss 1.27959e-06, acc 1\n",
      "2018-10-26T18:45:37.852841: step 29610, loss 3.72299e-06, acc 1\n",
      "2018-10-26T18:45:38.041337: step 29611, loss 2.41695e-05, acc 1\n",
      "2018-10-26T18:45:38.244793: step 29612, loss 1.88492e-06, acc 1\n",
      "2018-10-26T18:45:38.431294: step 29613, loss 1.49563e-06, acc 1\n",
      "2018-10-26T18:45:38.634760: step 29614, loss 2.70805e-06, acc 1\n",
      "2018-10-26T18:45:38.827237: step 29615, loss 8.07231e-05, acc 1\n",
      "2018-10-26T18:45:39.028699: step 29616, loss 5.51333e-07, acc 1\n",
      "2018-10-26T18:45:39.255093: step 29617, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:45:39.463536: step 29618, loss 0.000202101, acc 1\n",
      "2018-10-26T18:45:39.650038: step 29619, loss 4.26544e-07, acc 1\n",
      "2018-10-26T18:45:39.878428: step 29620, loss 0.000130653, acc 1\n",
      "2018-10-26T18:45:40.050967: step 29621, loss 0, acc 1\n",
      "2018-10-26T18:45:40.267388: step 29622, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:45:40.437932: step 29623, loss 0, acc 1\n",
      "2018-10-26T18:45:40.622440: step 29624, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:45:40.844845: step 29625, loss 0, acc 1\n",
      "2018-10-26T18:45:41.028356: step 29626, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:45:41.207875: step 29627, loss 5.31136e-06, acc 1\n",
      "2018-10-26T18:45:41.435268: step 29628, loss 4.63792e-07, acc 1\n",
      "2018-10-26T18:45:41.619775: step 29629, loss 7.9325e-05, acc 1\n",
      "2018-10-26T18:45:41.821237: step 29630, loss 2.79397e-08, acc 1\n",
      "2018-10-26T18:45:42.073563: step 29631, loss 0.00106601, acc 1\n",
      "2018-10-26T18:45:42.262059: step 29632, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:42.453548: step 29633, loss 1.92181e-05, acc 1\n",
      "2018-10-26T18:45:42.671963: step 29634, loss 0.000936585, acc 1\n",
      "2018-10-26T18:45:42.876418: step 29635, loss 9.81584e-07, acc 1\n",
      "2018-10-26T18:45:43.060924: step 29636, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:45:43.291310: step 29637, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:45:43.487783: step 29638, loss 1.80676e-07, acc 1\n",
      "2018-10-26T18:45:43.691240: step 29639, loss 3.41572e-06, acc 1\n",
      "2018-10-26T18:45:43.917635: step 29640, loss 0, acc 1\n",
      "2018-10-26T18:45:44.100148: step 29641, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:44.305599: step 29642, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:45:44.494095: step 29643, loss 0, acc 1\n",
      "2018-10-26T18:45:44.669626: step 29644, loss 3.59449e-06, acc 1\n",
      "2018-10-26T18:45:44.870090: step 29645, loss 1.62049e-07, acc 1\n",
      "2018-10-26T18:45:45.096486: step 29646, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:45:45.313906: step 29647, loss 4.66709e-06, acc 1\n",
      "2018-10-26T18:45:45.523345: step 29648, loss 1.25287e-05, acc 1\n",
      "2018-10-26T18:45:45.713837: step 29649, loss 4.53587e-05, acc 1\n",
      "2018-10-26T18:45:45.926269: step 29650, loss 0.0405923, acc 0.984375\n",
      "2018-10-26T18:45:46.099804: step 29651, loss 0, acc 1\n",
      "2018-10-26T18:45:46.280323: step 29652, loss 0, acc 1\n",
      "2018-10-26T18:45:46.491757: step 29653, loss 4.09782e-08, acc 1\n",
      "2018-10-26T18:45:46.671279: step 29654, loss 4.02326e-07, acc 1\n",
      "2018-10-26T18:45:46.851795: step 29655, loss 4.43992e-06, acc 1\n",
      "2018-10-26T18:45:47.064228: step 29656, loss 1.85882e-06, acc 1\n",
      "2018-10-26T18:45:47.230784: step 29657, loss 8.56815e-08, acc 1\n",
      "2018-10-26T18:45:47.414293: step 29658, loss 8.73556e-07, acc 1\n",
      "2018-10-26T18:45:47.603786: step 29659, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:45:47.799264: step 29660, loss 1.3222e-05, acc 1\n",
      "2018-10-26T18:45:47.977788: step 29661, loss 0, acc 1\n",
      "2018-10-26T18:45:48.159303: step 29662, loss 0, acc 1\n",
      "2018-10-26T18:45:48.335831: step 29663, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:45:48.509367: step 29664, loss 1.15484e-07, acc 1\n",
      "2018-10-26T18:45:48.686893: step 29665, loss 0, acc 1\n",
      "2018-10-26T18:45:48.858435: step 29666, loss 1.97439e-07, acc 1\n",
      "2018-10-26T18:45:49.035960: step 29667, loss 6.59363e-07, acc 1\n",
      "2018-10-26T18:45:49.201518: step 29668, loss 0, acc 1\n",
      "2018-10-26T18:45:49.417940: step 29669, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:49.625385: step 29670, loss 0, acc 1\n",
      "2018-10-26T18:45:49.810891: step 29671, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:45:50.028309: step 29672, loss 0, acc 1\n",
      "2018-10-26T18:45:50.202843: step 29673, loss 1.15411e-05, acc 1\n",
      "2018-10-26T18:45:50.420261: step 29674, loss 2.01164e-07, acc 1\n",
      "2018-10-26T18:45:50.588811: step 29675, loss 0, acc 1\n",
      "2018-10-26T18:45:50.767334: step 29676, loss 0, acc 1\n",
      "2018-10-26T18:45:50.973782: step 29677, loss 3.83474e-06, acc 1\n",
      "2018-10-26T18:45:51.150311: step 29678, loss 1.2293e-06, acc 1\n",
      "2018-10-26T18:45:51.331826: step 29679, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:51.550243: step 29680, loss 4.36577e-05, acc 1\n",
      "2018-10-26T18:45:51.717795: step 29681, loss 3.37475e-06, acc 1\n",
      "2018-10-26T18:45:51.897315: step 29682, loss 0, acc 1\n",
      "2018-10-26T18:45:52.063870: step 29683, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:45:52.277302: step 29684, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:45:52.448841: step 29685, loss 0, acc 1\n",
      "2018-10-26T18:45:52.629360: step 29686, loss 2.23517e-08, acc 1\n",
      "2018-10-26T18:45:52.831818: step 29687, loss 0, acc 1\n",
      "2018-10-26T18:45:53.011339: step 29688, loss 0.000294475, acc 1\n",
      "2018-10-26T18:45:53.185872: step 29689, loss 1.01883e-06, acc 1\n",
      "2018-10-26T18:45:53.368385: step 29690, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:45:53.566855: step 29691, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:45:53.751362: step 29692, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:45:53.927890: step 29693, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:45:54.108407: step 29694, loss 0, acc 1\n",
      "2018-10-26T18:45:54.275960: step 29695, loss 1.49011e-07, acc 1\n",
      "2018-10-26T18:45:54.453486: step 29696, loss 9.85308e-07, acc 1\n",
      "2018-10-26T18:45:54.621038: step 29697, loss 0.000144307, acc 1\n",
      "2018-10-26T18:45:54.795572: step 29698, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:45:54.970105: step 29699, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:45:55.139652: step 29700, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:45:55.604410: step 29700, loss 10.3463, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29700\n",
      "\n",
      "2018-10-26T18:45:55.973897: step 29701, loss 0.000150024, acc 1\n",
      "2018-10-26T18:45:56.149430: step 29702, loss 1.26578e-05, acc 1\n",
      "2018-10-26T18:45:56.335930: step 29703, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:45:56.508470: step 29704, loss 4.28408e-08, acc 1\n",
      "2018-10-26T18:45:56.700956: step 29705, loss 0, acc 1\n",
      "2018-10-26T18:45:56.943308: step 29706, loss 2.0489e-07, acc 1\n",
      "2018-10-26T18:45:57.113852: step 29707, loss 1.38949e-06, acc 1\n",
      "2018-10-26T18:45:57.298360: step 29708, loss 0, acc 1\n",
      "2018-10-26T18:45:57.471896: step 29709, loss 2.40453e-06, acc 1\n",
      "2018-10-26T18:45:57.654408: step 29710, loss 3.91155e-08, acc 1\n",
      "2018-10-26T18:45:57.829939: step 29711, loss 0, acc 1\n",
      "2018-10-26T18:45:58.010456: step 29712, loss 2.44005e-07, acc 1\n",
      "2018-10-26T18:45:58.179006: step 29713, loss 0, acc 1\n",
      "2018-10-26T18:45:58.364511: step 29714, loss 4.09781e-08, acc 1\n",
      "2018-10-26T18:45:58.565972: step 29715, loss 7.20307e-06, acc 1\n",
      "2018-10-26T18:45:58.751477: step 29716, loss 6.42853e-06, acc 1\n",
      "2018-10-26T18:45:58.929002: step 29717, loss 0, acc 1\n",
      "2018-10-26T18:45:59.112513: step 29718, loss 0, acc 1\n",
      "2018-10-26T18:45:59.281061: step 29719, loss 1.60187e-07, acc 1\n",
      "2018-10-26T18:45:59.463573: step 29720, loss 1.97439e-07, acc 1\n",
      "2018-10-26T18:45:59.633122: step 29721, loss 0.000309188, acc 1\n",
      "2018-10-26T18:45:59.824997: step 29722, loss 1.24797e-07, acc 1\n",
      "2018-10-26T18:46:00.002523: step 29723, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:46:00.200992: step 29724, loss 3.3432e-06, acc 1\n",
      "2018-10-26T18:46:00.375526: step 29725, loss 1.87557e-06, acc 1\n",
      "2018-10-26T18:46:00.558038: step 29726, loss 0.000100001, acc 1\n",
      "2018-10-26T18:46:00.726588: step 29727, loss 0, acc 1\n",
      "2018-10-26T18:46:00.912092: step 29728, loss 7.14888e-05, acc 1\n",
      "2018-10-26T18:46:01.084631: step 29729, loss 2.49593e-07, acc 1\n",
      "2018-10-26T18:46:01.263155: step 29730, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:46:01.430706: step 29731, loss 0, acc 1\n",
      "2018-10-26T18:46:01.612222: step 29732, loss 0, acc 1\n",
      "2018-10-26T18:46:01.774787: step 29733, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:46:01.958298: step 29734, loss 4.8428e-07, acc 1\n",
      "2018-10-26T18:46:02.128841: step 29735, loss 1.20199e-05, acc 1\n",
      "2018-10-26T18:46:02.313348: step 29736, loss 5.08426e-06, acc 1\n",
      "2018-10-26T18:46:02.482896: step 29737, loss 8.85626e-06, acc 1\n",
      "2018-10-26T18:46:02.676378: step 29738, loss 0, acc 1\n",
      "2018-10-26T18:46:02.842933: step 29739, loss 0, acc 1\n",
      "2018-10-26T18:46:03.025447: step 29740, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:03.192998: step 29741, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:46:03.374514: step 29742, loss 2.98023e-08, acc 1\n",
      "2018-10-26T18:46:03.546055: step 29743, loss 4.95456e-07, acc 1\n",
      "2018-10-26T18:46:03.731559: step 29744, loss 0, acc 1\n",
      "2018-10-26T18:46:03.903100: step 29745, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:46:04.086610: step 29746, loss 0, acc 1\n",
      "2018-10-26T18:46:04.256157: step 29747, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:46:04.439667: step 29748, loss 0.00114647, acc 1\n",
      "2018-10-26T18:46:04.610211: step 29749, loss 0, acc 1\n",
      "2018-10-26T18:46:04.794718: step 29750, loss 1.11758e-07, acc 1\n",
      "2018-10-26T18:46:04.961273: step 29751, loss 3.837e-07, acc 1\n",
      "2018-10-26T18:46:05.140793: step 29752, loss 8.94067e-08, acc 1\n",
      "2018-10-26T18:46:05.310341: step 29753, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:46:05.497840: step 29754, loss 1.28517e-06, acc 1\n",
      "2018-10-26T18:46:05.661403: step 29755, loss 4.23891e-06, acc 1\n",
      "2018-10-26T18:46:05.844913: step 29756, loss 4.29095e-06, acc 1\n",
      "2018-10-26T18:46:06.015458: step 29757, loss 0, acc 1\n",
      "2018-10-26T18:46:06.200961: step 29758, loss 2.425e-06, acc 1\n",
      "2018-10-26T18:46:06.374497: step 29759, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:46:06.553021: step 29760, loss 2.76951e-06, acc 1\n",
      "2018-10-26T18:46:06.720572: step 29761, loss 9.31322e-09, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-10-26T18:46:06.907074: step 29762, loss 5.02913e-08, acc 1\n",
      "2018-10-26T18:46:07.077620: step 29763, loss 4.6988e-05, acc 1\n",
      "2018-10-26T18:46:07.259134: step 29764, loss 4.01899e-05, acc 1\n",
      "2018-10-26T18:46:07.431697: step 29765, loss 1.09147e-06, acc 1\n",
      "2018-10-26T18:46:07.614185: step 29766, loss 0, acc 1\n",
      "2018-10-26T18:46:07.797722: step 29767, loss 1.85447e-05, acc 1\n",
      "2018-10-26T18:46:07.970234: step 29768, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:46:08.147768: step 29769, loss 1.31541e-05, acc 1\n",
      "2018-10-26T18:46:08.317306: step 29770, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:46:08.492838: step 29771, loss 1.16225e-06, acc 1\n",
      "2018-10-26T18:46:08.661387: step 29772, loss 7.00339e-07, acc 1\n",
      "2018-10-26T18:46:08.846891: step 29773, loss 0, acc 1\n",
      "2018-10-26T18:46:09.021425: step 29774, loss 3.72529e-08, acc 1\n",
      "2018-10-26T18:46:09.192966: step 29775, loss 2.43035e-05, acc 1\n",
      "2018-10-26T18:46:09.363511: step 29776, loss 1.89989e-07, acc 1\n",
      "2018-10-26T18:46:09.541037: step 29777, loss 3.83852e-06, acc 1\n",
      "2018-10-26T18:46:09.706594: step 29778, loss 3.65074e-07, acc 1\n",
      "2018-10-26T18:46:09.888111: step 29779, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:10.060648: step 29780, loss 0, acc 1\n",
      "2018-10-26T18:46:10.236180: step 29781, loss 8.47481e-07, acc 1\n",
      "2018-10-26T18:46:10.406724: step 29782, loss 0, acc 1\n",
      "2018-10-26T18:46:10.581555: step 29783, loss 0, acc 1\n",
      "2018-10-26T18:46:10.756089: step 29784, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:46:10.934612: step 29785, loss 1.17346e-07, acc 1\n",
      "2018-10-26T18:46:11.105156: step 29786, loss 4.52619e-07, acc 1\n",
      "2018-10-26T18:46:11.284677: step 29787, loss 7.61194e-05, acc 1\n",
      "2018-10-26T18:46:11.453226: step 29788, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:11.626762: step 29789, loss 1.68747e-06, acc 1\n",
      "2018-10-26T18:46:11.796309: step 29790, loss 2.06181e-06, acc 1\n",
      "2018-10-26T18:46:11.975830: step 29791, loss 0.000525841, acc 1\n",
      "2018-10-26T18:46:12.146373: step 29792, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:12.321905: step 29793, loss 1.4156e-07, acc 1\n",
      "2018-10-26T18:46:12.488459: step 29794, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:46:12.665986: step 29795, loss 5.32708e-07, acc 1\n",
      "2018-10-26T18:46:12.830546: step 29796, loss 0, acc 1\n",
      "2018-10-26T18:46:13.010066: step 29797, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:46:13.184599: step 29798, loss 0.000117006, acc 1\n",
      "2018-10-26T18:46:13.363123: step 29799, loss 6.46329e-07, acc 1\n",
      "2018-10-26T18:46:13.535662: step 29800, loss 6.82282e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:46:13.993438: step 29800, loss 10.3476, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29800\n",
      "\n",
      "2018-10-26T18:46:14.379005: step 29801, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:46:14.553537: step 29802, loss 1.00583e-07, acc 1\n",
      "2018-10-26T18:46:14.722088: step 29803, loss 1.0617e-07, acc 1\n",
      "2018-10-26T18:46:14.909587: step 29804, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:15.098084: step 29805, loss 5.02831e-05, acc 1\n",
      "2018-10-26T18:46:15.328467: step 29806, loss 1.54599e-07, acc 1\n",
      "2018-10-26T18:46:15.501008: step 29807, loss 1.1479e-05, acc 1\n",
      "2018-10-26T18:46:15.696486: step 29808, loss 5.73684e-07, acc 1\n",
      "2018-10-26T18:46:15.864037: step 29809, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:16.038571: step 29810, loss 0, acc 1\n",
      "2018-10-26T18:46:16.222080: step 29811, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:46:16.396614: step 29812, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:16.577131: step 29813, loss 1.1922e-05, acc 1\n",
      "2018-10-26T18:46:16.748673: step 29814, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:16.922210: step 29815, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:46:17.092754: step 29816, loss 2.46413e-06, acc 1\n",
      "2018-10-26T18:46:17.271277: step 29817, loss 4.47034e-08, acc 1\n",
      "2018-10-26T18:46:17.447805: step 29818, loss 9.59239e-07, acc 1\n",
      "2018-10-26T18:46:17.632313: step 29819, loss 4.84287e-08, acc 1\n",
      "2018-10-26T18:46:17.812830: step 29820, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:46:17.989359: step 29821, loss 2.13295e-05, acc 1\n",
      "2018-10-26T18:46:18.166884: step 29822, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:46:18.339423: step 29823, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:46:18.518943: step 29824, loss 2.42849e-05, acc 1\n",
      "2018-10-26T18:46:18.695472: step 29825, loss 2.47731e-07, acc 1\n",
      "2018-10-26T18:46:18.871002: step 29826, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:46:19.047530: step 29827, loss 1.57758e-06, acc 1\n",
      "2018-10-26T18:46:19.231041: step 29828, loss 2.94295e-07, acc 1\n",
      "2018-10-26T18:46:19.404577: step 29829, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:19.580107: step 29830, loss 1.5422e-06, acc 1\n",
      "2018-10-26T18:46:19.755639: step 29831, loss 4.05445e-06, acc 1\n",
      "2018-10-26T18:46:19.937154: step 29832, loss 1.59621e-06, acc 1\n",
      "2018-10-26T18:46:20.109692: step 29833, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:46:20.289213: step 29834, loss 1.57386e-06, acc 1\n",
      "2018-10-26T18:46:20.462759: step 29835, loss 0, acc 1\n",
      "2018-10-26T18:46:20.644265: step 29836, loss 0, acc 1\n",
      "2018-10-26T18:46:20.824807: step 29837, loss 3.63212e-07, acc 1\n",
      "2018-10-26T18:46:20.996324: step 29838, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:46:21.175845: step 29839, loss 8.19562e-08, acc 1\n",
      "2018-10-26T18:46:21.340404: step 29840, loss 8.19869e-05, acc 1\n",
      "2018-10-26T18:46:21.527903: step 29841, loss 8.9264e-05, acc 1\n",
      "2018-10-26T18:46:21.694458: step 29842, loss 0, acc 1\n",
      "2018-10-26T18:46:21.869990: step 29843, loss 5.4202e-07, acc 1\n",
      "2018-10-26T18:46:22.052504: step 29844, loss 0.000886779, acc 1\n",
      "2018-10-26T18:46:22.238007: step 29845, loss 8.75441e-08, acc 1\n",
      "2018-10-26T18:46:22.406555: step 29846, loss 9.953e-05, acc 1\n",
      "2018-10-26T18:46:22.588071: step 29847, loss 2.62631e-07, acc 1\n",
      "2018-10-26T18:46:22.754627: step 29848, loss 2.28603e-05, acc 1\n",
      "2018-10-26T18:46:22.933148: step 29849, loss 0, acc 1\n",
      "2018-10-26T18:46:23.099724: step 29850, loss 2.32457e-07, acc 1\n",
      "2018-10-26T18:46:23.281219: step 29851, loss 0, acc 1\n",
      "2018-10-26T18:46:23.452761: step 29852, loss 9.75996e-07, acc 1\n",
      "2018-10-26T18:46:23.633278: step 29853, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:23.803823: step 29854, loss 7.82309e-08, acc 1\n",
      "2018-10-26T18:46:23.979354: step 29855, loss 3.11059e-07, acc 1\n",
      "2018-10-26T18:46:24.154885: step 29856, loss 5.40166e-08, acc 1\n",
      "2018-10-26T18:46:24.337397: step 29857, loss 7.29614e-06, acc 1\n",
      "2018-10-26T18:46:24.505947: step 29858, loss 0, acc 1\n",
      "2018-10-26T18:46:24.686464: step 29859, loss 1.86264e-08, acc 1\n",
      "2018-10-26T18:46:24.855014: step 29860, loss 1.47148e-07, acc 1\n",
      "2018-10-26T18:46:25.033537: step 29861, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:25.203085: step 29862, loss 0.000746452, acc 1\n",
      "2018-10-26T18:46:25.383602: step 29863, loss 0.000268724, acc 1\n",
      "2018-10-26T18:46:25.550157: step 29864, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:46:25.735663: step 29865, loss 5.58794e-09, acc 1\n",
      "2018-10-26T18:46:25.908200: step 29866, loss 1.95577e-07, acc 1\n",
      "2018-10-26T18:46:26.088719: step 29867, loss 0, acc 1\n",
      "2018-10-26T18:46:26.261257: step 29868, loss 7.07804e-08, acc 1\n",
      "2018-10-26T18:46:26.441775: step 29869, loss 1.08592e-05, acc 1\n",
      "2018-10-26T18:46:26.610325: step 29870, loss 0, acc 1\n",
      "2018-10-26T18:46:26.789845: step 29871, loss 0.000573763, acc 1\n",
      "2018-10-26T18:46:26.962383: step 29872, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:46:27.151879: step 29873, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:27.325413: step 29874, loss 6.05434e-06, acc 1\n",
      "2018-10-26T18:46:27.502940: step 29875, loss 0, acc 1\n",
      "2018-10-26T18:46:27.675478: step 29876, loss 1.84955e-06, acc 1\n",
      "2018-10-26T18:46:27.854001: step 29877, loss 0.000340808, acc 1\n",
      "2018-10-26T18:46:28.032525: step 29878, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:46:28.214039: step 29879, loss 1.11759e-08, acc 1\n",
      "2018-10-26T18:46:28.385581: step 29880, loss 5.58793e-08, acc 1\n",
      "2018-10-26T18:46:28.577069: step 29881, loss 8.75442e-08, acc 1\n",
      "2018-10-26T18:46:28.744622: step 29882, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:28.929129: step 29883, loss 7.09214e-06, acc 1\n",
      "2018-10-26T18:46:29.101668: step 29884, loss 3.54797e-06, acc 1\n",
      "2018-10-26T18:46:29.281188: step 29885, loss 1.30385e-08, acc 1\n",
      "2018-10-26T18:46:29.449738: step 29886, loss 0, acc 1\n",
      "2018-10-26T18:46:29.633247: step 29887, loss 0, acc 1\n",
      "2018-10-26T18:46:29.809776: step 29888, loss 0.000327172, acc 1\n",
      "2018-10-26T18:46:30.003259: step 29889, loss 2.22943e-06, acc 1\n",
      "2018-10-26T18:46:30.183804: step 29890, loss 0.000537293, acc 1\n",
      "2018-10-26T18:46:30.361303: step 29891, loss 0, acc 1\n",
      "2018-10-26T18:46:30.542817: step 29892, loss 1.0952e-06, acc 1\n",
      "2018-10-26T18:46:30.717351: step 29893, loss 4.52616e-07, acc 1\n",
      "2018-10-26T18:46:30.888892: step 29894, loss 1.06169e-06, acc 1\n",
      "2018-10-26T18:46:31.078386: step 29895, loss 0, acc 1\n",
      "2018-10-26T18:46:31.270872: step 29896, loss 0.000116087, acc 1\n",
      "2018-10-26T18:46:31.490628: step 29897, loss 0, acc 1\n",
      "2018-10-26T18:46:31.701066: step 29898, loss 9.36872e-05, acc 1\n",
      "2018-10-26T18:46:31.877594: step 29899, loss 3.35276e-08, acc 1\n",
      "2018-10-26T18:46:32.097007: step 29900, loss 0, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:46:32.587702: step 29900, loss 10.2916, acc 0.721388\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-29900\n",
      "\n",
      "2018-10-26T18:46:32.987353: step 29901, loss 0, acc 1\n",
      "2018-10-26T18:46:33.155902: step 29902, loss 7.45058e-09, acc 1\n",
      "2018-10-26T18:46:33.338415: step 29903, loss 0, acc 1\n",
      "2018-10-26T18:46:33.537883: step 29904, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:46:33.795195: step 29905, loss 0, acc 1\n",
      "2018-10-26T18:46:33.969728: step 29906, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:46:34.184156: step 29907, loss 1.63287e-05, acc 1\n",
      "2018-10-26T18:46:34.371655: step 29908, loss 0.00661329, acc 1\n",
      "2018-10-26T18:46:34.546188: step 29909, loss 4.24282e-06, acc 1\n",
      "2018-10-26T18:46:34.719724: step 29910, loss 2.20335e-06, acc 1\n",
      "2018-10-26T18:46:34.919192: step 29911, loss 3.44991e-05, acc 1\n",
      "2018-10-26T18:46:35.121652: step 29912, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:46:35.303166: step 29913, loss 9.16402e-07, acc 1\n",
      "2018-10-26T18:46:35.479694: step 29914, loss 0, acc 1\n",
      "2018-10-26T18:46:35.654228: step 29915, loss 0, acc 1\n",
      "2018-10-26T18:46:35.873641: step 29916, loss 1.04863e-06, acc 1\n",
      "2018-10-26T18:46:36.048175: step 29917, loss 3.42723e-07, acc 1\n",
      "2018-10-26T18:46:36.229690: step 29918, loss 2.54468e-05, acc 1\n",
      "2018-10-26T18:46:36.401232: step 29919, loss 9.31322e-09, acc 1\n",
      "2018-10-26T18:46:36.576762: step 29920, loss 1.67638e-08, acc 1\n",
      "2018-10-26T18:46:36.750300: step 29921, loss 2.04891e-08, acc 1\n",
      "2018-10-26T18:46:36.926827: step 29922, loss 4.86143e-07, acc 1\n",
      "2018-10-26T18:46:37.097382: step 29923, loss 5.84865e-07, acc 1\n",
      "2018-10-26T18:46:37.272903: step 29924, loss 0, acc 1\n",
      "2018-10-26T18:46:37.441452: step 29925, loss 0, acc 1\n",
      "2018-10-26T18:46:37.622969: step 29926, loss 3.66937e-07, acc 1\n",
      "2018-10-26T18:46:37.806478: step 29927, loss 1.02445e-07, acc 1\n",
      "2018-10-26T18:46:37.984003: step 29928, loss 2.6077e-08, acc 1\n",
      "2018-10-26T18:46:38.169508: step 29929, loss 5.94962e-05, acc 1\n",
      "2018-10-26T18:46:38.338057: step 29930, loss 1.2666e-07, acc 1\n",
      "2018-10-26T18:46:38.520569: step 29931, loss 1.2591e-06, acc 1\n",
      "2018-10-26T18:46:38.694105: step 29932, loss 2.5518e-07, acc 1\n",
      "2018-10-26T18:46:38.870634: step 29933, loss 6.09075e-07, acc 1\n",
      "2018-10-26T18:46:39.038186: step 29934, loss 0.000242766, acc 1\n",
      "2018-10-26T18:46:39.214715: step 29935, loss 1.53704e-05, acc 1\n",
      "2018-10-26T18:46:39.386256: step 29936, loss 1.12872e-06, acc 1\n",
      "2018-10-26T18:46:39.564790: step 29937, loss 0.000106204, acc 1\n",
      "2018-10-26T18:46:39.739314: step 29938, loss 5.28982e-07, acc 1\n",
      "2018-10-26T18:46:39.916840: step 29939, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:46:40.089377: step 29940, loss 6.60574e-06, acc 1\n",
      "2018-10-26T18:46:40.265906: step 29941, loss 0, acc 1\n",
      "2018-10-26T18:46:40.432462: step 29942, loss 3.29653e-06, acc 1\n",
      "2018-10-26T18:46:40.607994: step 29943, loss 0.000206589, acc 1\n",
      "2018-10-26T18:46:40.781529: step 29944, loss 1.46397e-06, acc 1\n",
      "2018-10-26T18:46:40.963043: step 29945, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:41.132590: step 29946, loss 0, acc 1\n",
      "2018-10-26T18:46:41.310117: step 29947, loss 0, acc 1\n",
      "2018-10-26T18:46:41.477670: step 29948, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:46:41.661178: step 29949, loss 2.1151e-05, acc 1\n",
      "2018-10-26T18:46:41.836709: step 29950, loss 0.000173489, acc 1\n",
      "2018-10-26T18:46:42.025205: step 29951, loss 3.03224e-05, acc 1\n",
      "2018-10-26T18:46:42.194753: step 29952, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:42.374274: step 29953, loss 9.40607e-07, acc 1\n",
      "2018-10-26T18:46:42.542824: step 29954, loss 3.72529e-09, acc 1\n",
      "2018-10-26T18:46:42.724337: step 29955, loss 6.12804e-07, acc 1\n",
      "2018-10-26T18:46:42.904855: step 29956, loss 8.30009e-06, acc 1\n",
      "2018-10-26T18:46:43.112301: step 29957, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:46:43.341690: step 29958, loss 3.16649e-08, acc 1\n",
      "2018-10-26T18:46:43.556117: step 29959, loss 0.000685054, acc 1\n",
      "2018-10-26T18:46:43.769545: step 29960, loss 1.49648e-05, acc 1\n",
      "2018-10-26T18:46:44.012896: step 29961, loss 0.000934557, acc 1\n",
      "2018-10-26T18:46:44.242296: step 29962, loss 6.14672e-08, acc 1\n",
      "2018-10-26T18:46:44.474664: step 29963, loss 0.00140039, acc 1\n",
      "2018-10-26T18:46:44.695072: step 29964, loss 5.96045e-08, acc 1\n",
      "2018-10-26T18:46:44.883568: step 29965, loss 0, acc 1\n",
      "2018-10-26T18:46:45.090018: step 29966, loss 3.22206e-06, acc 1\n",
      "2018-10-26T18:46:45.271532: step 29967, loss 1.49012e-08, acc 1\n",
      "2018-10-26T18:46:45.473991: step 29968, loss 2.77708e-06, acc 1\n",
      "2018-10-26T18:46:45.643538: step 29969, loss 2.42144e-08, acc 1\n",
      "2018-10-26T18:46:45.869934: step 29970, loss 1.89612e-06, acc 1\n",
      "2018-10-26T18:46:46.041474: step 29971, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:46.229970: step 29972, loss 0, acc 1\n",
      "2018-10-26T18:46:46.441406: step 29973, loss 3.53902e-08, acc 1\n",
      "2018-10-26T18:46:46.615940: step 29974, loss 1.31497e-06, acc 1\n",
      "2018-10-26T18:46:46.791471: step 29975, loss 1.86265e-09, acc 1\n",
      "2018-10-26T18:46:47.001909: step 29976, loss 6.89167e-07, acc 1\n",
      "2018-10-26T18:46:47.177441: step 29977, loss 2.97163e-05, acc 1\n",
      "2018-10-26T18:46:47.353967: step 29978, loss 0, acc 1\n",
      "2018-10-26T18:46:47.524512: step 29979, loss 9.7051e-06, acc 1\n",
      "2018-10-26T18:46:47.751905: step 29980, loss 5.58793e-09, acc 1\n",
      "2018-10-26T18:46:47.973314: step 29981, loss 3.33198e-06, acc 1\n",
      "2018-10-26T18:46:48.192727: step 29982, loss 1.69305e-06, acc 1\n",
      "2018-10-26T18:46:48.393191: step 29983, loss 0, acc 1\n",
      "2018-10-26T18:46:48.597647: step 29984, loss 4.00272e-05, acc 1\n",
      "2018-10-26T18:46:48.807086: step 29985, loss 0, acc 1\n",
      "2018-10-26T18:46:49.040462: step 29986, loss 2.01905e-06, acc 1\n",
      "2018-10-26T18:46:49.251897: step 29987, loss 1.97249e-06, acc 1\n",
      "2018-10-26T18:46:49.450368: step 29988, loss 0.00405421, acc 1\n",
      "2018-10-26T18:46:49.665791: step 29989, loss 0, acc 1\n",
      "2018-10-26T18:46:49.843317: step 29990, loss 1.63912e-07, acc 1\n",
      "2018-10-26T18:46:50.089658: step 29991, loss 4.63357e-06, acc 1\n",
      "2018-10-26T18:46:50.293115: step 29992, loss 7.29526e-05, acc 1\n",
      "2018-10-26T18:46:50.504551: step 29993, loss 2.36554e-07, acc 1\n",
      "2018-10-26T18:46:50.705015: step 29994, loss 0, acc 1\n",
      "2018-10-26T18:46:50.887527: step 29995, loss 1.17342e-06, acc 1\n",
      "2018-10-26T18:46:51.115916: step 29996, loss 0.0408074, acc 0.984375\n",
      "2018-10-26T18:46:51.301421: step 29997, loss 0, acc 1\n",
      "2018-10-26T18:46:51.510862: step 29998, loss 0, acc 1\n",
      "2018-10-26T18:46:51.692376: step 29999, loss 0, acc 1\n",
      "2018-10-26T18:46:51.870899: step 30000, loss 3.97364e-09, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-10-26T18:46:52.373556: step 30000, loss 10.3838, acc 0.72045\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\卢昱昊\\Desktop\\cnn-text-classification-tf-master\\runs\\1540540165\\checkpoints\\model-30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
